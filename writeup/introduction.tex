

%Each human language is a system for encoding an infinite variety of thoughts into sentences, i.e. linear strings of words.
%A primary goal of the field of linguistics is to characterize and explain the commonalities and differences among these systems, and so to gain insight into the cognitive machinery that underpins this fundamental aspect of human intelligence.
Why is human language the way it is? 
In this paper, we argue that the grammars of human languages can be modeled mathematically as solutions to the problem of maximally efficient communication among agents subject to particular resource constraints on information processing during speaking and listening.
We formalize these constraints using information theory.
Using these concepts, we show novel large-scale quantitative evidence from 54 languages that information is structured in time so as to minimize memory load during language comprehension, and
we show that our formalization allows us to explain some of the universal syntactic properties of languages.

The suggestion that the structure of human language reflects a need for efficient processing under resource limitations has been present in the theoretical and functional linguistics literature for decades~\citep{yngve1961,berwick1984grammatical,hawkins1994performance,jaeger2011language,gibson2019efficiency}, and has been summed up in \citeauthor{hawkins1994efficiency}'s (\citeyear{hawkins1994efficiency}) \key{Performance--Grammar Correspondence Hypothesis} (PGCH), which holds that grammars are structured so that the typical utterance is easy to produce and comprehend under performance constraints.
The idea has also been controversial, with prominent claims that efficiency in this sense has had no or only a minimal effect on the structure of human language \citep{chomsky}.

Among theories of language based on processing efficiency, some of the most successful have been those dealing with resource constraints on working memory as deployed during language production and comprehension.
An extensive body of work from psycholinguistics has documented that limitations in incremental memory cause measurable difficulty in production and comprehension \citep{gibson1998syntactic,gibson1999memory,gibson2000dependency,lewis2005activationbased,bartek2011search,nicenboim2015working}. % TODO are there cites for production?
Complementing this work, a number of researchers in linguistic typology and corpus linguistics have shown how various universal properties of grammars can be explained in terms of memory-based processing difficulty \citep{hawkins2004efficiency,hawkins2014crosslinguistic,ferrericancho2006syntactic,gildea2010grammars,futrell2015largescale,liu2017dependency,temperley2018dependency}.
We will see that one of the most successful theories in this domain, the principle of \key{dependency length minimization}, falls out of our formalization of processing efficiency as a special case.
%Our work formalizes and generalizes this previous work.

We formalize the notion of memory constraints in language processing in terms of what we call the \key{memory--surprisal tradeoff}: the idea that it is possible to achieve greater ease of word-by-word comprehension, and greater accuracy in word-by-word production, at the cost of investing more computational resources into remembering previous words.
The shape of this tradeoff depends on the grammar of a language, and in particular its word order properties.
We characterize memory resources in a theory-neutral, information-theoretic manner based on rate--distortion theory \citep{cover2006elements} and the theory of statistical complexity \citep{crutchfield1989inferring}. % cite the crutchfield nature review, and predictive info bottleneck


%For each amount of memory invested, a language permits a certain level of ease of processing 
%The resulting tradeoff of memory and surprisal of depends on the word order properties of a language. Analogous results also hold for language production by resource-constrained speakers. We show evidence that the preferred word orders of natural languages are those that enable efficient memory--surprisal tradeoffs.

The remainder of this paper is structured as follows. In Section~\ref{sec:background}, we give a brief review of previous work on the effects of short-term memory on languages and language processing. In Section~\ref{sec:ms-tradeoff}, we describe the memory--surprisal tradeoff and how it results from rate--distortion theory, the theory of optimal information processing under resource constraints \citep{cover2006elements}. In Section~\ref{sec:info-locality}, we prove that word orders enable efficient processing in terms of the memory--surprisal tradeoff when they exhibit \emph{information locality}: whenever utterance elements that predict each other are close to each other. In Section~\ref{sec:experiment1}, languages which have previously been shown to be preferred in artificial language experiments are exactly those that enable efficient memory--surprisal tradeoffs \citep{fedzechkina2017human}. In Section~\ref{sec:experiment2}, we show that word orders of natural languages as found in dependency corpora \citep{ud} enable more efficient memory--surprisal tradeoffs than baseline word orders. Section~\ref{sec:conclusion} concludes.


%These strategies can be seen as sets of rules that differ in the way ...
%Languages differ considerably in the rules they apply to order information: English orders the object after the verb, Japanese places it before the verb \note{make more explicit here}.
%Explaining the variation in the strategies languages use to  order underlying hierarchical structures into linear strings of words has been one of the foci of linguistic research (\jd{CITE}).
%We show that these strategies reflect an underlying optimization principle: human languages are adapted to limitations in human working memory.

%We test the hypothesis that human languages represent different solutions to the problem of efficient computation with constrained working memory.






%\mhahn{original intro}
%Since the 1950s, it has been a persistent suggestion that human language processing is shaped by a resource bottleneck in short-term memory.
%Language is produced and comprehended incrementally in a way that crucially requires both speaker and listener to use an active memory store to keep track of what was previously said.
%Since short-term memory of this kind is known to be highly limited in capacity \citep{miller1956magical}, it makes sense for these capacity limits to comprise a major constraint on production and comprehension.
%Indeed, a great deal of work in sentence processing has focused on characterizing the effects of memory constraints on language processing \citep{gibson-linguistic-1998,lewis-activation-based-2005,levy2013memory}.

%At the same time, the field of functional linguistics has argued that these resource constraints not only affect online language processing, they also shape the form of human language itself.
%For example, the Performace--Grammar Correspondence Hypothesis (PGCH) of \citet{hawkins1994performance} holds that forms which are practically easier to produce and comprehend end up becoming part of the grammars of languages, and that this process can explain several of the universal properties of human languages originally documented by \citet{greenberg-universals-1963}.




