@article{fedzechkina2017human,
author = {Maryia Fedzechkina and Becky Chu and T. Florian Jaeger},
title ={Human Information Processing Shapes Language Change},
journal = {Psychological Science},
year = {2017}}

@book{ cover2006elements,
	title = {Elements of Information Theory},
	author = "Thomas M. Cover and J.A. Thomas",
	year = "2006",
	publisher = "John Wiley \& Sons",
	address={Hoboken, NJ}
}


@article{greenberg-universals-1963,
	title = {Some universals of grammar with particular reference to the order of meaningful elements},
	volume = {2},
	journal = {Universals of language},
	author = {Greenberg, Joseph H},
	year = {1963},
	pages = {73--113}
}



@incollection{levy2013memory,
  publisher = {Hove: Psychology Press},
  author = {Levy, Roger},
  booktitle = {Sentence Processing},
  title = {Memory and surprisal in human sentence comprehension},
  editor = {van Gompel, Roger P. G.},
  year = {2013},
  pages = {78–114}
}


@article{lewis-activation-based-2005,
	title = {An activation-based model of sentence processing as skilled memory retrieval},
	volume = {29},
	issn = {0364-0213},
	doi = {10.1207/s15516709cog0000\_25},
	abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated control structure that subserve sentence comprehension. The theory is derived from the application of independently motivated principles of memory and cognitive skill to the specialized task of sentence parsing. The resulting theory construes sentence processing as a series of skilled associative memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive principles are formalized in computational form in the Adaptive Control of Thought-Rational (ACT-R) architecture, and our process model is realized in ACT-R. We present the results of 6 sets of simulations: 5 simulation sets provide quantitative accounts of the effects of length and structural interference on both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-embedding difficulty is a novel one that derives from the model' complete reliance on discriminating retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT-R defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference are the key factors shaping working memory in sentence processing. We contrast the theory and empirical predictions with several related accounts of sentence-processing complexity.},
	language = {eng},
	number = {3},
	journal = {Cognitive Science},
	author = {Lewis, Richard L. and Vasishth, Shravan},
	month = may,
	year = {2005},
	pmid = {21702779},
	pages = {375--419}
}

@article{miller1956magical,
author={George A. Miller},
title={The magical number seven, plus or minus two, or, some limits on our capacity for processing information},
journal={Psychological Review},
year={1956},
volume={63},
pages={81--96}}

@article{kennedy-vagueness-2007,
	title = {Vagueness and grammar: the semantics of relative and absolute gradable adjectives},
	volume = {30},
	issn = {0165-0157, 1573-0549},
	shorttitle = {Vagueness and grammar},
	url = {http://link.springer.com/10.1007/s10988-006-9008-0},
	doi = {10.1007/s10988-006-9008-0},
	language = {en},
	number = {1},
	urldate = {2017-11-08},
	journal = {Linguistics and Philosophy},
	author = {Kennedy, Christopher},
	month = mar,
	year = {2007},
	pages = {1--45},
	file = {10988_2006_9008_Article 1..45 - vg-epub.pdf:/home/user/Zotero/storage/9C4DIGBR/vg-epub.pdf:application/pdf}
}

@article{kennedy-two-2013,
	title = {Two {Sources} of {Subjectivity}: {Qualitative} {Assessment} and {Dimensional} {Uncertainty}},
	volume = {56},
	issn = {0020-174X, 1502-3923},
	shorttitle = {Two {Sources} of {Subjectivity}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0020174X.2013.784483},
	doi = {10.1080/0020174X.2013.784483},
	language = {en},
	number = {2-3},
	urldate = {2017-11-08},
	journal = {Inquiry},
	author = {Kennedy, Christopher},
	month = apr,
	year = {2013},
	pages = {258--277},
	file = {C\:\\itools\\WMS\\TandF-Journals\\4113777\\WorkingFolder\\SINQ_A_784483.dvi - kennedy-two-sources.pdf:/home/user/Zotero/storage/N6TZM7FE/kennedy-two-sources.pdf:application/pdf}
}

@article{kaiser-experience-nodate,
	title = {Experience matters: {A} psycholinguistic investigation of subjectivity in adjective interpretation},
	shorttitle = {Experience matters},
	author = {Kaiser, Elsi and Lee, Jamie Herron},
	file = {Kaiser-Lee.pdf:/home/user/Zotero/storage/ZUVHDDSC/Kaiser-Lee.pdf:application/pdf}
}

@article{munoz-deriving-nodate,
	title = {Deriving direct experience effects from adjectival lexical semantics},
	author = {Munoz, Patrick},
	file = {Munoz.pdf:/home/user/Zotero/storage/7XM6ABAA/Munoz.pdf:application/pdf}
}

@article{barker-negotiating-2013,
	title = {Negotiating {Taste}},
	volume = {56},
	issn = {0020-174X, 1502-3923},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0020174X.2013.784482},
	doi = {10.1080/0020174X.2013.784482},
	language = {en},
	number = {2-3},
	urldate = {2017-11-08},
	journal = {Inquiry},
	author = {Barker, Chris},
	month = apr,
	year = {2013},
	pages = {240--257},
	file = {Negotiating Taste - barker-negotiating-taste.pdf:/home/user/Zotero/storage/44KUBMAK/barker-negotiating-taste.pdf:application/pdf}
}

@article{barker-dynamics-2002,
	title = {The dynamics of vagueness},
	volume = {25},
	number = {1},
	journal = {Linguistics and Philosophy},
	author = {Barker, Chris},
	year = {2002},
	pages = {1--36},
	file = {The Dynamics of Vagueness - barker02.pdf:/home/user/Zotero/storage/BK9DTTFD/barker02.pdf:application/pdf}
}

@article{yoon-i-nodate,
	title = {“{I} won’t lie, it wasn’t amazing”: {Modeling} polite indirect speech},
	shorttitle = {“{I} won’t lie, it wasn’t amazing”},
	author = {Yoon, Erica J. and Tessler, Michael Henry and Goodman, Noah D. and Frank, Michael C.},
	file = {yoon-2017-cogsci.pdf:/home/user/Zotero/storage/9EPG8RPW/yoon-2017-cogsci.pdf:application/pdf}
}

@inproceedings{yoon-talking-2016,
	title = {Talking with tact: {Polite} language as a balance between kindness and informativity},
	shorttitle = {Talking with tact},
	booktitle = {Proceedings of the 38th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	publisher = {Cognitive Science Society},
	author = {Yoon, Erica J. and Tessler, Michael Henry and Goodman, Noah D. and Frank, Michael C.},
	year = {2016},
	file = {YoonTessler2016-cogsci.pdf:/home/user/Zotero/storage/M7DIN37Q/YoonTessler2016-cogsci.pdf:application/pdf}
}

@inproceedings{burnett-signalling-2016,
	title = {Signalling games, sociolinguistic variation and the construction of style},
	booktitle = {the 40th {Penn} {Linguistics} {Colloquium}, {University} of {Pennsylvania}},
	author = {Burnett, Heather},
	year = {2016},
	file = {SMGs_Burnett.pdf:/home/user/Zotero/storage/ZN7CW6TZ/SMGs_Burnett.pdf:application/pdf}
}

@article{burnett-sociolinguistic-2017,
	title = {Sociolinguistic interaction and identity construction: {The} view from game-theoretic pragmatics},
	volume = {21},
	shorttitle = {Sociolinguistic interaction and identity construction},
	number = {2},
	journal = {Journal of Sociolinguistics},
	author = {Burnett, Heather},
	year = {2017},
	pages = {238--271},
	file = {Reasoning_Interaction_v2.pdf:/home/user/Zotero/storage/TEQZ6U6A/Reasoning_Interaction_v2.pdf:application/pdf}
}

@book{burnett-gradability-2017,
	title = {Gradability in natural language: {Logical} and grammatical foundations},
	volume = {7},
	shorttitle = {Gradability in natural language},
	publisher = {Oxford University Press},
	author = {Burnett, Heather},
	year = {2017},
	file = {SemCom_vagueness.pdf:/home/user/Zotero/storage/DZA79CI7/SemCom_vagueness.pdf:application/pdf}
}

@article{chung-empirical-2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2017-11-06},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BUXVBH5Q/1412.html:text/html;Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:/home/user/Zotero/storage/S7K6H7RZ/Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:application/pdf}
}

@book{parker-chinese-2011,
	address = {Philadelphia},
	title = {Chinese {Gigaword} {Fifth} {Edition} {LDC}2011T13},
	publisher = {Linguistic Data Consortium},
	author = {Parker, Robert},
	year = {2011}
}

@article{zhu-aligning-2015,
	title = {Aligning {Books} and {Movies}: {Towards} {Story}-like {Visual} {Explanations} by {Watching} {Movies} and {Reading} {Books}},
	shorttitle = {Aligning {Books} and {Movies}},
	url = {http://arxiv.org/abs/1506.06724},
	abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
	urldate = {2017-11-06},
	journal = {arXiv:1506.06724 [cs]},
	author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.06724},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/PCUPPBSM/1506.html:text/html;Zhu et al_2015_Aligning Books and Movies.pdf:/home/user/Zotero/storage/95TV3TZS/Zhu et al_2015_Aligning Books and Movies.pdf:application/pdf}
}

@book{graff-english-2003,
	address = {Philadelphia},
	title = {English {Gigaword} {LDC}2003T05},
	publisher = {Linguistic Data Consortium},
	author = {Graff, David and Cieri, Christopher},
	year = {2003}
}

@book{parker-arabic-2011,
	address = {Philadelphia},
	title = {Arabic {Gigaword} {Fifth} {Edition} {LDC}2011T11},
	publisher = {Linguistic Data Consortium},
	author = {Parker, Robert},
	year = {2011}
}

@inproceedings{futrell-noisy-context-2017,
	title = {Noisy-context surprisal as a human sentence processing cost model},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Futrell, Richard and Levy, Roger},
	year = {2017},
	pages = {688--698},
	file = {Noisy-context surprisal as a human sentence processing cost model - E17-1065:/home/user/Zotero/storage/MI7EK2ZN/E17-1065.pdf:application/pdf}
}

@misc{noauthor-large-scale-nodate,
	title = {Large-scale evidence of dependency length minimization in 37 languages},
	url = {http://www.pnas.org/content/112/33/10336.abstract},
	urldate = {2017-11-06},
	file = {Large-scale evidence of dependency length minimization in 37 languages:/home/user/Zotero/storage/ZC8SVZBA/10336.html:text/html}
}

@article{gibson-linguistic-1998,
	title = {Linguistic complexity: locality of syntactic dependencies},
	volume = {68},
	issn = {0010-0277},
	shorttitle = {Linguistic complexity},
	abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory--the Syntactic Prediction Locality Theory (SPLT)--has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
	language = {eng},
	number = {1},
	journal = {Cognition},
	author = {Gibson, E.},
	month = aug,
	year = {1998},
	pmid = {9775516},
	keywords = {Decision Trees, Humans, Judgment, Language Development, Linguistics, memory, Mental Processes, Models, Theoretical},
	pages = {1--76}
}

@book{godfrey-switchboard-1-1993,
	address = {Philadelphia},
	title = {Switchboard-1 {Release} 2 {LDC}97S62},
	publisher = {Linguistic Data Consortium},
	author = {Godfrey, John and Holliman, Edward},
	year = {1993}
}

@book{graff-switchboard-2-1998,
	address = {Philadelphia},
	title = {Switchboard-2 {Phase} {I} {LDC}98S75. {DVD}},
	url = {https://catalog.ldc.upenn.edu/LDC98S75},
	urldate = {2017-11-06},
	publisher = {Linguistic Data Consortium},
	author = {Graff, David and Canavan, Alexandra and Zipperlen, George},
	year = {1998},
	file = {Switchboard-2 Phase I - Linguistic Data Consortium:/home/user/Zotero/storage/QB347XNU/LDC98S75.html:text/html}
}

@book{huddleston-cambridge-2002,
	address = {Cambridge},
	title = {The {Cambridge} {Grammar} of the {English} {Language}},
	author = {Huddleston, Rodney and Pullum, Geoffrey},
	year = {2002}
}

@book{bloomfield-language-1933,
	title = {Language},
	isbn = {978-0-226-06067-5},
	abstract = {Perhaps the single most influential work of general linguistics published in this century, Leonard Bloomfield's Language is both a masterpiece of textbook writing and a classic of scholarship. Intended as an introduction to the field of linguistics, it revolutionized the field when it appeared in 1933 and became the major text of the American descriptivist school.},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Bloomfield, Leonard},
	year = {1933},
	note = {Google-Books-ID: 87BCDVsmFE4C},
	keywords = {Language Arts \& Disciplines / General, Language Arts \& Disciplines / Linguistics / General}
}

@article{whorf-grammatical-1945,
	title = {Grammatical categories},
	volume = {21},
	number = {1},
	journal = {Language},
	author = {Whorf, Benjamin Lee},
	year = {1945},
	pages = {1--11}
}

@article{baldi-bits-2010,
	title = {Of {Bits} and {Wows}: {A} {Bayesian} {Theory} of {Surprise} with {Applications} to {Attention}},
	volume = {23},
	issn = {0893-6080},
	shorttitle = {Of {Bits} and {Wows}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2860069/},
	doi = {10.1016/j.neunet.2009.12.007},
	abstract = {The amount of information contained in a piece of data can be measured by the effect this data has on its observer. Fundamentally, this effect is to transform the observer's prior beliefs into posterior beliefs, according to Bayes theorem. Thus the amount of information can be measured in a natural way by the distance (relative entropy) between the prior and posterior distributions of the observer over the available space of hypotheses. This facet of information, termed “surprise”, is important in dynamic situations where beliefs change, in particular during learning and adaptation. Surprise can often be computed analytically, for instance in the case of distributions from the exponential family, or it can be numerically approximated. During sequential Bayesian learning, surprise decreases like the inverse of the number of training examples. Theoretical properties of surprise are discussed, in particular how it differs and complements Shannon's definition of information. A computer vision neural network architecture is then presented capable of computing surprise over images and video stimuli. Hypothesizing that surprising data ought to attract natural or artificial attention systems, the output of this architecture is used in a psychophysical experiment to analyze human eye movements in the presence of natural video stimuli. Surprise is found to yield robust performance at predicting human gaze (ROC-like ordinal dominance score ∼ 0.7 compared to ∼ 0.8 for human inter-observer repeatability, ∼ 0.6 for simpler intensity contrast-based predictor, and 0.5 for chance). The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
	number = {5},
	urldate = {2017-11-05},
	journal = {Neural networks : the official journal of the International Neural Network Society},
	author = {Baldi, Pierre and Itti, Laurent},
	month = jun,
	year = {2010},
	pmid = {20080025},
	pmcid = {PMC2860069},
	pages = {649--666},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/PJAPGR88/Baldi and Itti - 2010 - Of Bits and Wows A Bayesian Theory of Surprise wi.pdf:application/pdf}
}

@article{kuperman-emotion-2014,
	title = {Emotion and language: valence and arousal affect word recognition},
	volume = {143},
	issn = {1939-2222},
	shorttitle = {Emotion and language},
	doi = {10.1037/a0035669},
	abstract = {Emotion influences most aspects of cognition and behavior, but emotional factors are conspicuously absent from current models of word recognition. The influence of emotion on word recognition has mostly been reported in prior studies on the automatic vigilance for negative stimuli, but the precise nature of this relationship is unclear. Various models of automatic vigilance have claimed that the effect of valence on response times is categorical, an inverted U, or interactive with arousal. In the present study, we used a sample of 12,658 words and included many lexical and semantic control factors to determine the precise nature of the effects of arousal and valence on word recognition. Converging empirical patterns observed in word-level and trial-level data from lexical decision and naming indicate that valence and arousal exert independent monotonic effects: Negative words are recognized more slowly than positive words, and arousing words are recognized more slowly than calming words. Valence explained about 2\% of the variance in word recognition latencies, whereas the effect of arousal was smaller. Valence and arousal do not interact, but both interact with word frequency, such that valence and arousal exert larger effects among low-frequency words than among high-frequency words. These results necessitate a new model of affective word processing whereby the degree of negativity monotonically and independently predicts the speed of responding. This research also demonstrates that incorporating emotional factors, especially valence, improves the performance of models of word recognition.},
	language = {eng},
	number = {3},
	journal = {Journal of Experimental Psychology. General},
	author = {Kuperman, Victor and Estes, Zachary and Brysbaert, Marc and Warriner, Amy Beth},
	month = jun,
	year = {2014},
	pmid = {24490848},
	pmcid = {PMC4038659},
	keywords = {Humans, Arousal, Emotions, language, Pattern Recognition, Visual},
	pages = {1065--1081}
}

@misc{noauthor-crr-nodate,
	title = {crr » {Age}-of-acquisition ({AoA}) norms for over 50 thousand {English} words},
	url = {http://crr.ugent.be/archives/806},
	abstract = {Center for Reading Research -},
	urldate = {2017-11-05},
	file = {Snapshot:/home/user/Zotero/storage/8NPUCGS9/806.html:text/html}
}

@article{kuperman-emotion-2014-1,
	title = {Emotion and language: valence and arousal affect word recognition},
	volume = {143},
	issn = {1939-2222},
	shorttitle = {Emotion and language},
	doi = {10.1037/a0035669},
	abstract = {Emotion influences most aspects of cognition and behavior, but emotional factors are conspicuously absent from current models of word recognition. The influence of emotion on word recognition has mostly been reported in prior studies on the automatic vigilance for negative stimuli, but the precise nature of this relationship is unclear. Various models of automatic vigilance have claimed that the effect of valence on response times is categorical, an inverted U, or interactive with arousal. In the present study, we used a sample of 12,658 words and included many lexical and semantic control factors to determine the precise nature of the effects of arousal and valence on word recognition. Converging empirical patterns observed in word-level and trial-level data from lexical decision and naming indicate that valence and arousal exert independent monotonic effects: Negative words are recognized more slowly than positive words, and arousing words are recognized more slowly than calming words. Valence explained about 2\% of the variance in word recognition latencies, whereas the effect of arousal was smaller. Valence and arousal do not interact, but both interact with word frequency, such that valence and arousal exert larger effects among low-frequency words than among high-frequency words. These results necessitate a new model of affective word processing whereby the degree of negativity monotonically and independently predicts the speed of responding. This research also demonstrates that incorporating emotional factors, especially valence, improves the performance of models of word recognition.},
	language = {eng},
	number = {3},
	journal = {Journal of Experimental Psychology. General},
	author = {Kuperman, Victor and Estes, Zachary and Brysbaert, Marc and Warriner, Amy Beth},
	month = jun,
	year = {2014},
	pmid = {24490848},
	pmcid = {PMC4038659},
	keywords = {Humans, Arousal, Emotions, language, Pattern Recognition, Visual},
	pages = {1065--1081}
}

@article{nguyen-computational-2015,
	title = {Computational {Sociolinguistics}: {A} {Survey}},
	shorttitle = {Computational {Sociolinguistics}},
	url = {http://arxiv.org/abs/1508.07544},
	abstract = {Language is a social phenomenon and variation is inherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of "Computational Sociolinguistics" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions employed in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges.},
	urldate = {2017-11-03},
	journal = {arXiv:1508.07544 [cs]},
	author = {Nguyen, Dong and Doğruöz, A. Seza and Rosé, Carolyn P. and de Jong, Franciska},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.07544},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1508.07544 PDF:/home/user/Zotero/storage/SAHQQBBT/Nguyen et al. - 2015 - Computational Sociolinguistics A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9X3UFITG/1508.html:text/html}
}

@misc{noauthor-notitle-nodate,
	url = {http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm},
	urldate = {2017-11-02},
	file = {:/home/user/Zotero/storage/7APPW7VU/BlogCorpus.html:text/html}
}

@book{burnett-gradability-2017-1,
	title = {Gradability in natural language: {Logical} and grammatical foundations},
	volume = {7},
	shorttitle = {Gradability in natural language},
	publisher = {Oxford University Press},
	author = {Burnett, Heather},
	year = {2017},
	file = {Context\$0020sensitivity\$0020and\$0020vagueness\$0020patterns.pdf:/home/user/Zotero/storage/KRT7ENQI/Context\$0020sensitivity\$0020and\$0020vagueness\$0020patterns.pdf:application/pdf}
}

@article{waldman-secret-2014,
	title = {The {Secret} {Rules} of {Adjective} {Order}},
	issn = {1091-2339},
	url = {http://www.slate.com/articles/arts/the\_good\_word/2014/08/the\_study\_of\_adjective\_order\_and\_gsssacpm.html},
	abstract = {A long fascinating article—or is it a fascinating long article?},
	language = {en-US},
	urldate = {2017-11-01},
	journal = {Slate},
	author = {Waldman, Katy and Bazelon, Emily and O'Rourke, Meghan and Waldman, Katy},
	month = aug,
	year = {2014},
	file = {Slate Snapshot:/home/user/Zotero/storage/HSUHNNI3/the_study_of_adjective_order_and_gsssacpm.single.html:text/html}
}

@misc{noauthor-order-nodate,
	title = {order of adjectives},
	url = {https://learnenglish.britishcouncil.org/en/english-grammar/adjectives/order-adjectives},
	urldate = {2017-11-01},
	journal = {Learn English {\textbar} British Council},
	file = {Snapshot:/home/user/Zotero/storage/TE76ARQX/order-adjectives.html:text/html}
}

@article{bakshy-uncertainty-2013,
	title = {Uncertainty in {Online} {Experiments} with {Dependent} {Data}: {An} {Evaluation} of {Bootstrap} {Methods}},
	shorttitle = {Uncertainty in {Online} {Experiments} with {Dependent} {Data}},
	url = {http://arxiv.org/abs/1304.7406},
	doi = {10.1145/2487575.2488218},
	abstract = {Many online experiments exhibit dependence between users and items. For example, in online advertising, observations that have a user or an ad in common are likely to be associated. Because of this, even in experiments involving millions of subjects, the difference in mean outcomes between control and treatment conditions can have substantial variance. Previous theoretical and simulation results demonstrate that not accounting for this kind of dependence structure can result in confidence intervals that are too narrow, leading to inaccurate hypothesis tests. We develop a framework for understanding how dependence affects uncertainty in user-item experiments and evaluate how bootstrap methods that account for differing levels of dependence perform in practice. We use three real datasets describing user behaviors on Facebook - user responses to ads, search results, and News Feed stories - to generate data for synthetic experiments in which there is no effect of the treatment on average by design. We then estimate empirical Type I error rates for each bootstrap method. Accounting for dependence within a single type of unit (i.e., within-user dependence) is often sufficient to get reasonable error rates. But when experiments have effects, as one might expect in the field, accounting for multiple units with a multiway bootstrap can be necessary to get close to the advertised Type I error rates. This work provides guidance to practitioners evaluating large-scale experiments, and highlights the importance of analysis of inferential methods for dependence structures common to online systems.},
	urldate = {2017-10-31},
	journal = {arXiv:1304.7406 [stat]},
	author = {Bakshy, Eytan and Eckles, Dean},
	year = {2013},
	note = {arXiv: 1304.7406},
	keywords = {G.3, Statistics - Applications, Statistics - Methodology},
	pages = {1303},
	file = {arXiv\:1304.7406 PDF:/home/user/Zotero/storage/C25JSNU3/Bakshy and Eckles - 2013 - Uncertainty in Online Experiments with Dependent D.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/ITJRQFF8/1304.html:text/html}
}

@misc{noauthor-6.895-nodate,
	title = {6.895 {Materials}},
	url = {http://stellar.mit.edu/S/course/6/fa07/6.895/materials.html},
	urldate = {2017-10-29},
	file = {6.895 Materials:/home/user/Zotero/storage/3D45D8RC/materials.html:text/html}
}

@book{noauthor-journal-nodate,
	title = {Journal of {Mathematical} {Psychology}},
	url = {https://www.journals.elsevier.com/journal-of-mathematical-psychology},
	abstract = {The Journal of Mathematical Psychology includes articles, monographs and reviews, notes and commentaries, and book reviews in all areas of...},
	urldate = {2017-10-28},
	file = {Snapshot:/home/user/Zotero/storage/DSB9MFCE/journal-of-mathematical-psychology.html:text/html}
}

@book{noauthor-proceedings-2017,
	address = {SOMERVILLE},
	title = {{PROCEEDINGS} {OF} {THE} 34TH {WEST} {COAST} {CONFERENCE} {ON} {FORMAL} {LINGUISTICS}.},
	isbn = {978-1-57473-471-3},
	language = {English},
	publisher = {CASCADILLA PRESS},
	year = {2017},
	note = {OCLC: 985238704},
	file = {Raising Awareness with Imperatives - paper3304.pdf:/home/user/Zotero/storage/28X5GWMR/paper3304.pdf:application/pdf}
}

@article{hunter-semdial-nodate,
	title = {{SEMDIAL} 2016 {JerSem}},
	author = {Hunter, Julie and Simons, Mandy and Stone, Matthew},
	file = {Semdial_2016_JerSem_proceedings.pdf:/home/user/Zotero/storage/7PPNTM53/Semdial_2016_JerSem_proceedings.pdf:application/pdf}
}

@article{crone-needless-2016,
	title = {Needless to {Say}},
	author = {Crone, Phil},
	year = {2016},
	file = {Needless to Say - The Informativity of Being Uninformative - NASSLLI-Presentation.pdf:/home/user/Zotero/storage/6BW7AJKX/NASSLLI-Presentation.pdf:application/pdf}
}

@article{cronestanford-assessor-relativizable-nodate,
	title = {Assessor-{Relativizable} {Predicates}},
	author = {Crone—Stanford, Phil and Santa Cruz, Deniz Rudin-UC},
	file = {CUSP_2016.pdf:/home/user/Zotero/storage/83HJWDCJ/CUSP_2016.pdf:application/pdf}
}

@article{munoz-deriving-nodate-1,
	title = {Deriving direct experience effects from adjectival lexical semantics},
	author = {Munoz, Patrick},
	file = {Munoz.pdf:/home/user/Zotero/storage/FA8GXZ9T/Munoz.pdf:application/pdf}
}

@article{umbach-commonalities-nodate,
	title = {Some commonalities and differences between dimensional and aesthetic predicates},
	author = {Umbach, Carla},
	file = {Umbach.pdf:/home/user/Zotero/storage/3NSCSF3S/Umbach.pdf:application/pdf}
}

@book{noauthor-proceedings-2017-1,
	address = {SOMERVILLE},
	title = {{PROCEEDINGS} {OF} {THE} 34TH {WEST} {COAST} {CONFERENCE} {ON} {FORMAL} {LINGUISTICS}.},
	isbn = {978-1-57473-471-3},
	language = {English},
	publisher = {CASCADILLA PRESS},
	year = {2017},
	note = {OCLC: 985238704},
	file = {Raising Awareness with Imperatives - paper3304.pdf:/home/user/Zotero/storage/W27HHUJT/paper3304.pdf:application/pdf}
}

@book{coppock-logical-2009,
	title = {The logical and empirical foundations of {Baker}'s paradox},
	publisher = {Stanford University},
	author = {Coppock, Elizabeth},
	year = {2009},
	file = {CoppockThesis.pdf:/home/user/Zotero/storage/7CK4M4ZM/CoppockThesis.pdf:application/pdf}
}

@article{coppock-predictability-nodate,
	title = {{THE} {PREDICTABILITY} {OF} {PREDICATIVITY}},
	author = {Coppock, Elizabeth},
	file = {PredictabilityOfPredicativity_36x72.pdf:/home/user/Zotero/storage/J5Q9M6CH/PredictabilityOfPredicativity_36x72.pdf:application/pdf}
}

@misc{noauthor-subjectivity-nodate,
	title = {Subjectivity in {Language} and {Thought}},
	url = {https://lucian.uchicago.edu/blogs/subjectivity2017/lassiter-abstract/},
	urldate = {2017-10-24},
	file = {Snapshot:/home/user/Zotero/storage/RAPVTRU4/lassiter-abstract.html:text/html}
}

@misc{noauthor-program-nodate,
	title = {Program {\textbar} {Subjectivity} in {Language} and {Thought}},
	url = {https://lucian.uchicago.edu/blogs/subjectivity2017/program/},
	urldate = {2017-10-24},
	file = {Snapshot:/home/user/Zotero/storage/AM3JDVK6/program.html:text/html}
}

@article{owen-bi-cross-validation-2009,
	title = {Bi-cross-validation of the {SVD} and the nonnegative matrix factorization},
	volume = {3},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0908.2062},
	doi = {10.1214/08-AOAS227},
	abstract = {This article presents a form of bi-cross-validation (BCV) for choosing the rank in outer product models, especially the singular value decomposition (SVD) and the nonnegative matrix factorization (NMF). Instead of leaving out a set of rows of the data matrix, we leave out a set of rows and a set of columns, and then predict the left out entries by low rank operations on the retained data. We prove a self-consistency result expressing the prediction error as a residual from a low rank approximation. Random matrix theory and some empirical results suggest that smaller hold-out sets lead to more over-fitting, while larger ones are more prone to under-fitting. In simulated examples we find that a method leaving out half the rows and half the columns performs well.},
	number = {2},
	urldate = {2017-10-24},
	journal = {The Annals of Applied Statistics},
	author = {Owen, Art B. and Perry, Patrick O.},
	month = jun,
	year = {2009},
	note = {arXiv: 0908.2062},
	keywords = {Statistics - Applications},
	pages = {564--594},
	file = {arXiv\:0908.2062 PDF:/home/user/Zotero/storage/6RMZKIX7/Owen and Perry - 2009 - Bi-cross-validation of the SVD and the nonnegative.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/2G5M2KES/0908.html:text/html}
}

@article{percy-cognition-2009,
	title = {Cognition and native-language grammar: {The} organizational role of adjective-noun word order in information representation},
	volume = {16},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Cognition and native-language grammar},
	url = {https://link.springer.com/article/10.3758/PBR.16.6.1037},
	doi = {10.3758/PBR.16.6.1037},
	abstract = {In the present research, we investigated the influence of native-language adjective-noun word order on category accessibility for nouns and adjectives by comparing Portuguese speakers (in whose language nouns precede adjectives) with English speakers (in whose language adjectives precede nouns). In two studies, we presented participants with different numbers of verbal or pictorial stimuli, and subsequently they answered questions about noun- and adjective-conditioned frequencies. The results demonstrated a primacy effect of nativelanguage word order. Specifically, although both populations showed a speed advantage for noun-conditioned questions, this tendency was significantly stronger for Portuguese than for American participants. We discuss the important role of native-language syntax rules for the categorization and representation of information.},
	language = {en},
	number = {6},
	urldate = {2017-10-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Percy, Elise J. and Sherman, Steven J. and Garcia-Marques, Leonel and Mata, André and Garcia-Marques, Teresa},
	month = dec,
	year = {2009},
	pages = {1037--1042},
	file = {Full Text PDF:/home/user/Zotero/storage/9RVT8WWZ/Percy et al. - 2009 - Cognition and native-language grammar The organiz.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/WD53WB48/PBR.16.6.html:text/html}
}

@article{kemmerer-knowledge-2009,
	title = {Knowledge of the semantic constraints on adjective order can be selectively impaired},
	volume = {22},
	issn = {0911-6044},
	url = {http://www.sciencedirect.com/science/article/pii/S0911604408000535},
	doi = {10.1016/j.jneuroling.2008.07.001},
	abstract = {When multiple adjectives are used to modify a noun, they tend to be sequenced in the following way according to semantic class: value{\textgreater}size{\textgreater}dimension{\textgreater}various physical properties{\textgreater}color. To investigate the neural substrates of these semantic constraints on adjective order, we administered a battery of three tests to 34 brain-damaged patients and 19 healthy participants. Six patients manifested the following performance profile: First, they failed a test that required them to discriminate between semantically determined correct and incorrect sequences of adjectives—e.g., thick blue towel vs. *blue thick towel. Second, they passed a test that assessed their knowledge of two purely syntactic aspects of adjective order—specifically, that adjectives can precede nouns, and that adjectives can precede other adjectives. Finally, they also passed a test that assessed their knowledge of the categorical (i.e., class-level) features of adjective meanings that interact with the semantic constraints underlying adjective order—e.g., that thick is a dimensional adjective and that blue is a color adjective. Taken together, these behavioral findings suggest that the six patients have selectively impaired knowledge of the abstract principles that determine how different semantic classes of adjectives are typically mapped onto different syntactic positions in NPs. To identify the neuroanatomical lesion patterns that tend to correlate with defective processing of adjective order, we combined lesion data from the six patients just described with lesion data from six other patients whom we reported in a previous study as having similar impairments [Kemmerer, D. (2000). Selective impairment of knowledge underlying adjective order: evidence for the autonomy of grammatical semantics. Journal of Neurolinguistics, 13, 57–82]. We found that the most common areas of damage included the left posterior inferior frontal gyrus and the left inferior parietal lobule. Overall, these results shed new light on the neural substrates of the syntax–semantics interface.},
	number = {1},
	urldate = {2017-10-24},
	journal = {Journal of Neurolinguistics},
	author = {Kemmerer, David and Tranel, Daniel and Zdanczyk, Cynthia},
	month = jan,
	year = {2009},
	keywords = {Adjective, Broca's area, iconicity, Inferior parietal lobule, Syntax–semantics interface},
	pages = {91--108},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/H7VUGMGF/Kemmerer et al. - 2009 - Knowledge of the semantic constraints on adjective.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/A4D4N6TK/S0911604408000535.html:text/html}
}

@misc{noauthor-dispreferred-nodate,
	title = {Dispreferred adjective orders elicit brain responses associated with lexico-semantic rather than syntactic processing - {ScienceDirect}},
	url = {http://www.sciencedirect.com/science/article/pii/S0006899312012395},
	urldate = {2017-10-24},
	file = {Dispreferred adjective orders elicit brain responses associated with lexico-semantic rather than syntactic processing - ScienceDirect:/home/user/Zotero/storage/M5F2HIFP/S0006899312012395.html:text/html}
}

@article{danks-psychological-1971,
	title = {Psychological scaling of adjective orders},
	volume = {10},
	issn = {0022-5371},
	url = {http://www.sciencedirect.com/science/article/pii/S0022537171800956},
	doi = {10.1016/S0022-5371(71)80095-6},
	abstract = {Do violations of implicit adjective-ordering rules affect scalings of grammaticalness? Subjects ranked sentences containing one of six different orders of three prenominal adjective classes, using a multiple rank ordering procedure. The scale values were consistent with expectations based upon the assumption that Ss integrate a pragmatic communication constraint and a semantic-grammatical rule.},
	number = {1},
	urldate = {2017-10-24},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Danks, Joseph H. and Glucksberg, Sam},
	month = feb,
	year = {1971},
	pages = {63--67},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/3BH6M6TA/Danks and Glucksberg - 1971 - Psychological scaling of adjective orders.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/WZ3CJ4FX/S0022537171800956.html:text/html}
}

@article{ahern-conflict-2017,
	title = {Conflict, cheap talk, and {Jespersen}'s cycle},
	volume = {10},
	issn = {1937-8912},
	url = {http://semprag.org/article/view/sp.10.11},
	doi = {10.3765/sp.10.11},
	language = {en},
	number = {11},
	urldate = {2017-10-23},
	journal = {Semantics and Pragmatics},
	author = {Ahern, Christopher and Clark, Robin},
	month = jun,
	year = {2017},
	file = {pdf.pdf:/home/user/Zotero/storage/6U8P7VUN/pdf.pdf:application/pdf}
}

@article{stark-hello-2012,
	title = {Hello, {Who} is {Calling}?: {Can} {Words} {Reveal} the {Social} {Nature} of {Conversations}?},
	shorttitle = {Hello, {Who} is {Calling}?},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3886719/},
	abstract = {This study aims to infer the social nature of conversations from their content automatically. To place this work in context, our motivation stems from the need to understand how social disengagement affects cognitive decline or depression among older adults. For this purpose, we collected a comprehensive and naturalistic corpus comprising of all the incoming and outgoing telephone calls from 10 subjects over the duration of a year. As a first step, we learned a binary classifier to filter out business related conversation, achieving an accuracy of about 85\%. This classification task provides a convenient tool to probe the nature of telephone conversations. We evaluated the utility of openings and closing in differentiating personal calls, and find that empirical results on a large corpus do not support the hypotheses by Schegloff and Sacks that personal conversations are marked by unique closing structures. For classifying different types of social relationships such as family vs other, we investigated features related to language use (entropy), hand-crafted dictionary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81\%) compared to LIWC or language use features in distinguishing different types of conversations.},
	urldate = {2017-10-23},
	journal = {Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
	author = {Stark, Anthony and Shafran, Izhak and Kaye, Jeffrey},
	year = {2012},
	pmid = {24419500},
	pmcid = {PMC3886719},
	pages = {112--119},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/R5Q2MBJX/Stark et al. - 2012 - Hello, Who is Calling Can Words Reveal the Socia.pdf:application/pdf}
}

@incollection{haegeman-fine-1997,
	address = {Dordrecht},
	title = {The {Fine} {Structure} of the {Left} {Periphery}},
	isbn = {978-0-7923-4298-4 978-94-011-5420-8},
	url = {http://www.springerlink.com/index/10.1007/978-94-011-5420-8\_7},
	urldate = {2017-10-22},
	booktitle = {Elements of {Grammar}},
	publisher = {Springer Netherlands},
	author = {Rizzi, Luigi},
	editor = {Haegeman, Liliane},
	year = {1997},
	doi = {10.1007/978-94-011-5420-8\_7},
	pages = {281--337},
	file = {The-Fine-Structure-of-the-Left-Periphery.pdf:/home/user/Zotero/storage/EITSDQTC/The-Fine-Structure-of-the-Left-Periphery.pdf:application/pdf}
}

@article{im-conservativeness-2015,
	title = {Conservativeness of untied auto-encoders},
	url = {http://arxiv.org/abs/1506.07643},
	abstract = {We discuss necessary and sufficient conditions for an auto-encoder to define a conservative vector field, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same ("tied weights"), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector field.},
	urldate = {2017-10-16},
	journal = {arXiv:1506.07643 [cs]},
	author = {Im, Daniel Jiwoong and Belghazi, Mohamed Ishmael Diwan and Memisevic, Roland},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.07643},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1506.07643 PDF:/home/user/Zotero/storage/EPEZDJ52/Im et al. - 2015 - Conservativeness of untied auto-encoders.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GI2V8HT9/1506.html:text/html}
}

@article{heusel-gans-2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent that has an individual learning rate for both the discriminator and the generator. We prove that the TTUR converges under mild assumptions to a stationary Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs, improved Wasserstein GANs, and BEGANs, outperforming conventional GAN training on CelebA, One Billion Word Benchmark, and LSUN bedrooms.},
	urldate = {2017-10-16},
	journal = {arXiv:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Klambauer, Günter and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08500},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1706.08500 PDF:/home/user/Zotero/storage/45BUP2W2/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/UQG5K557/1706.html:text/html}
}

@article{conneau-word-2017,
	title = {Word {Translation} {Without} {Parallel} {Data}},
	url = {http://arxiv.org/abs/1710.04087},
	abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.},
	urldate = {2017-10-16},
	journal = {arXiv:1710.04087 [cs]},
	author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.04087},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1710.04087 PDF:/home/user/Zotero/storage/JVD6CM9P/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/ZF47CNBH/1710.html:text/html}
}

@article{nagarajan-gradient-2017,
	title = {Gradient descent {GAN} optimization is locally stable},
	url = {http://arxiv.org/abs/1706.04156},
	abstract = {Despite their growing prominence, optimization in generative adversarial networks (GANs) is still a poorly-understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization (i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters). We show that even though GAN optimization does not correspond to a convex-concave game, even for simple parameterizations, under proper conditions, equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation. On the other hand, we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which is able to guarantee local stability for both the WGAN and for the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.},
	urldate = {2017-10-16},
	journal = {arXiv:1706.04156 [cs, math, stat]},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04156},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {arXiv\:1706.04156 PDF:/home/user/Zotero/storage/A9GZ3J7J/Nagarajan and Kolter - 2017 - Gradient descent GAN optimization is locally stabl.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/AM4VX9P2/1706.html:text/html}
}

@article{mertikopoulos-cycles-2017,
	title = {Cycles in adversarial regularized learning},
	url = {http://arxiv.org/abs/1709.02738},
	abstract = {Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system's behavior is Poincar\'e recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents' choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents' utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.},
	urldate = {2017-10-16},
	journal = {arXiv:1709.02738 [cs]},
	author = {Mertikopoulos, Panayotis and Papadimitriou, Christos and Piliouras, Georgios},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.02738},
	keywords = {Computer Science - Learning, Computer Science - Computer Science and Game Theory},
	file = {arXiv\:1709.02738 PDF:/home/user/Zotero/storage/AFGA7K7M/Mertikopoulos et al. - 2017 - Cycles in adversarial regularized learning.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/DPJTREHK/1709.html:text/html}
}

@article{salimans-improved-2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2017-10-16},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1606.03498 PDF:/home/user/Zotero/storage/WUPRTABI/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/F564SZG3/1606.html:text/html}
}

@article{unterthiner-coulomb-2017,
	title = {Coulomb {GANs}: {Provably} {Optimal} {Nash} {Equilibria} via {Potential} {Fields}},
	shorttitle = {Coulomb {GANs}},
	url = {http://arxiv.org/abs/1708.08819},
	abstract = {Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.},
	urldate = {2017-10-16},
	journal = {arXiv:1708.08819 [cs, stat]},
	author = {Unterthiner, Thomas and Nessler, Bernhard and Klambauer, Günter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.08819},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory},
	file = {arXiv\:1708.08819 PDF:/home/user/Zotero/storage/CK89999Q/Unterthiner et al. - 2017 - Coulomb GANs Provably Optimal Nash Equilibria via.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/KVX4XDJP/1708.html:text/html}
}

@article{mescheder-numerics-2017,
	title = {The {Numerics} of {GANs}},
	url = {http://arxiv.org/abs/1705.10461},
	abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
	urldate = {2017-10-16},
	journal = {arXiv:1705.10461 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10461},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1705.10461 PDF:/home/user/Zotero/storage/87Q8PBGP/Mescheder et al. - 2017 - The Numerics of GANs.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GIIBKHCX/1705.html:text/html}
}

@article{tiersma-local-1982,
	title = {Local and {General} {Markedness}},
	volume = {58},
	issn = {0097-8507},
	url = {http://www.jstor.org/stable/413959},
	doi = {10.2307/413959},
	abstract = {The work of Jakobson-and later, of Greenberg-has developed the notion of MORPHOLOGICAL MARKEDNESS. In this view, certain morphological categories (e.g. noun singulars) are considered unmarked or more basic in relation to others (e.g. noun plurals). It is shown here that there are some principled exceptions to the general markedness conventions. One example of such LOCAL MARKEDNESS, as this type of markedness reversal will be called, is that nouns whose referents naturally occur in groups or pairs show many of the effects of being unmarked in the plural. The consequences of such local markedness in terms of language acquisition, paradigm regularization, lexical borrowing, and the formation of double morphology are explored in some detail.},
	number = {4},
	urldate = {2017-02-16},
	journal = {Language},
	author = {Tiersma, Peter Meijes},
	year = {1982},
	pages = {832--849}
}

@book{deitmar-principles-2014,
	address = {Cham},
	series = {Universitext},
	title = {Principles of {Harmonic} {Analysis}},
	isbn = {978-3-319-05791-0 978-3-319-05792-7},
	url = {http://link.springer.com/10.1007/978-3-319-05792-7},
	urldate = {2017-02-16},
	publisher = {Springer International Publishing},
	author = {Deitmar, Anton and Echterhoff, Siegfried},
	year = {2014},
	doi = {10.1007/978-3-319-05792-7},
	file = {bok%3A978-3-319-05792-7.pdf:/home/user/Zotero/storage/ZBVDXRWI/bok%3A978-3-319-05792-7.pdf:application/pdf}
}

@article{serban-hierarchical-2016,
	title = {A {Hierarchical} {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Generating} {Dialogues}},
	url = {https://arxiv.org/abs/1605.06069},
	urldate = {2017-02-16},
	journal = {arXiv preprint arXiv:1605.06069},
	author = {Serban, Iulian Vlad and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
	year = {2016},
	file = {1605.06069.pdf:/home/user/Zotero/storage/NAIXNW4E/1605.06069.pdf:application/pdf}
}

@article{schlichtkrull-cross-lingual-2017,
	title = {Cross-{Lingual} {Dependency} {Parsing} with {Late} {Decoding} for {Truly} {Low}-{Resource} {Languages}},
	url = {https://arxiv.org/abs/1701.01623},
	urldate = {2017-02-16},
	journal = {arXiv preprint arXiv:1701.01623},
	author = {Schlichtkrull, Michael Sejr and Søgaard, Anders},
	year = {2017},
	file = {1701.01623v1.pdf:/home/user/Zotero/storage/5ITET7UT/1701.01623v1.pdf:application/pdf}
}

@book{ladefoged-course-2011,
	address = {Boston, MA},
	edition = {6th ed},
	title = {A course in phonetics},
	isbn = {978-1-4282-3126-9 978-1-4390-8324-6 978-1-4282-3127-6},
	publisher = {Wadsworth/Cengage Learning},
	author = {Ladefoged, Peter and Johnstone, Keith},
	year = {2011},
	keywords = {English language, Phonetics},
	file = {A-Course-in-Phonetics.pdf:/home/user/Zotero/storage/7R9JGCUT/A-Course-in-Phonetics.pdf:application/pdf}
}

@article{sorensen-bayesian-2016,
	title = {Bayesian linear mixed models using {Stan}: {A} tutorial for psychologists, linguists, and cognitive scientists},
	volume = {12},
	issn = {2292-1354},
	shorttitle = {Bayesian linear mixed models using {Stan}},
	url = {http://www.tqmp.org/RegularArticles/vol12-3/p175},
	doi = {10.20982/tqmp.12.3.p175},
	number = {3},
	urldate = {2017-02-16},
	journal = {The Quantitative Methods for Psychology},
	author = {Sorensen, Tanner and Hohenstein, Sven and Vasishth, Shravan},
	month = oct,
	year = {2016},
	pages = {175--200},
	file = {p175.pdf:/home/user/Zotero/storage/9TZBVVI4/p175.pdf:application/pdf}
}

@article{engelmann-framework-2013,
	title = {A {Framework} for {Modeling} the {Interaction} of {Syntactic} {Processing} and {Eye} {Movement} {Control}},
	volume = {5},
	issn = {17568757},
	url = {http://doi.wiley.com/10.1111/tops.12026},
	doi = {10.1111/tops.12026},
	language = {en},
	number = {3},
	urldate = {2017-02-16},
	journal = {Topics in Cognitive Science},
	author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
	month = jul,
	year = {2013},
	pages = {452--474},
	file = {REV_ISS_WEB_TOPS_12026_5-3 452..474 - EngelmannVasishthEngbertKliegl2013.pdf:/home/user/Zotero/storage/2FXAR294/EngelmannVasishthEngbertKliegl2013.pdf:application/pdf}
}

@book{nipkow-whats-2013,
	title = {What’s in {Main}},
	url = {http://isabelle.in.tum.de/doc/main.pdf},
	urldate = {2017-02-16},
	author = {Nipkow, Tobias},
	year = {2013},
	file = {main.pdf:/home/user/Zotero/storage/TP4DSNKZ/main.pdf:application/pdf}
}

@book{labov-principles-2010,
	address = {Malden, MA Oxford Chichester, West Sussex},
	series = {Language in society},
	title = {Principles of linguistic change. volume 3: {Cognitive} and cultural factors},
	isbn = {978-1-4051-1215-4 978-1-4051-1214-7},
	shorttitle = {Principles of linguistic change. volume 3},
	language = {eng},
	number = {39},
	publisher = {Wiley-Blackwell},
	author = {Labov, William},
	year = {2010},
	note = {OCLC: 254613732},
	keywords = {Kognitiver Prozess, Soziokultureller Wandel, Sprachwandel},
	file = {[William_Labov]_Principles_of_Linguistic_Change,_C(BookZZ.org).pdf:/home/user/Zotero/storage/I4T2S4U7/[William_Labov]_Principles_of_Linguistic_Change,_C(BookZZ.org).pdf:application/pdf}
}

@book{labov-principles-2006,
	address = {Malden, Mass.},
	edition = {Digital print},
	series = {Language in society},
	title = {Principles of linguistic change. {Vol}. 2: {Social} factors},
	isbn = {978-0-631-17915-3 978-0-631-17916-0},
	shorttitle = {Principles of linguistic change. {Vol}. 2},
	language = {eng},
	number = {29},
	publisher = {Blackwell},
	author = {Labov, William},
	year = {2006},
	note = {OCLC: 254748475},
	keywords = {Sprachwandel},
	file = {[William_Labov]_Principles_of_Linguistic_Change,_V(BookZZ.org).pdf:/home/user/Zotero/storage/R36CKA6Q/[William_Labov]_Principles_of_Linguistic_Change,_V(BookZZ.org).pdf:application/pdf}
}

@article{popescu-state-2011,
	title = {State of the art versus classical clustering for unsupervised word sense disambiguation},
	volume = {35},
	url = {http://link.springer.com/article/10.1007/s10462-010-9193-7},
	number = {3},
	urldate = {2017-02-16},
	journal = {Artificial Intelligence Review},
	author = {Popescu, Marius and Hristea, Florentina},
	year = {2011},
	pages = {241--264},
	file = {Luxburg07_tutorial_4488[0].pdf:/home/user/Zotero/storage/SS55ISDR/Luxburg07_tutorial_4488[0].pdf:application/pdf}
}

@article{cohen-bayesian-2016,
	title = {Bayesian {Analysis} in {Natural} {Language} {Processing}},
	volume = {9},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00719ED1V01Y201605HLT035},
	number = {2},
	urldate = {2017-02-16},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Cohen, Shay},
	year = {2016},
	pages = {1--274},
	file = {ebscohost (1).pdf:/home/user/Zotero/storage/QB59FWKU/ebscohost (1).pdf:application/pdf}
}

@book{cohen-bayesian-2016-1,
	title = {Bayesian analysis in natural language processing},
	isbn = {978-1-62705-421-8},
	url = {http://dx.doi.org/10.2200/S00719ED1V01Y201605HLT035},
	abstract = {Natural language processing (NLP) went through a profound transformation in the mid-1980s when it shifted to make heavy use of corpora and data-driven techniques to analyze language. Since then, the use of statistical techniques in NLP has evolved in several ways. One such example of evolution took place in the late 1990s or early 2000s, when full-fledged Bayesian machinery was introduced to NLP. This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it, especially in the unsupervised setting, where statistical learning is done without target prediction examples. We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area. These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed "in-house" in NLP. We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference, Bayesian estimation, and nonparametric modeling. We also cover fundamental concepts in Bayesian statistics such as prior distributions, conjugacy, and generative modeling. Finally, we cover some of the fundamental modeling techniques in NLP, such as grammar modeling and their use with Bayesian analysis.},
	language = {English},
	urldate = {2017-02-16},
	author = {Cohen, Shay},
	year = {2016},
	note = {OCLC: 959801508},
	file = {ebscohost.pdf:/home/user/Zotero/storage/6DZ2K4G4/ebscohost.pdf:application/pdf}
}

@article{from-jump-nodate,
	title = {Jump to: navigation, search ({Lafferty} et al., 2001)⇒ {John} {D}. {Lafferty}, {Andrew} {McCallum}, and {Fernando} {Pereira}.(2001).“{Conditional} {Random} {Fields}: {Probabilistic} {Models} for {Segmenting} and {Labeling} {Sequence} {Data}." {In}: {Proceedings} of the {Eighteenth} {International} {Conference} on {Machine} {Learning} ({ICML} 2001). {Subject} {Headings}: {Conditional} {Random} {Fields}, {Linear}-{Chain} {Conditional} {Random} {Field}, {Named} {Entity} {Recognition}, {Global} {Collective} {Classification} {Algorithm}.},
	shorttitle = {Jump to},
	url = {http://www.gabormelli.com/RKB/2001\_ConditionalRandomFields},
	urldate = {2017-02-16},
	author = {From, GMRKB},
	file = {fulltext.pdf:/home/user/Zotero/storage/QARIQTV4/fulltext.pdf:application/pdf}
}

@book{wunderlich-optimality-2004,
	title = {Optimality theory in morphology and syntax},
	url = {https://pdfs.semanticscholar.org/3c64/32f2d88d4f14ea4aa0884f421eb3140003be.pdf},
	urldate = {2017-02-16},
	publisher = {Encyclopedia of Language \& Linguistics-Second Edition. Oxford: Elsevier.{\textless} http://user. phil-fak. uni-duesseldorf. de/∼ wdl/{\textgreater}(Stand: 01.11. 08)},
	author = {Wunderlich, Dieter and {others}},
	year = {2004},
	file = {Optimality theory.pdf:/home/user/Zotero/storage/TZGNTAAF/Optimality theory.pdf:application/pdf}
}

@article{gomez-language-2014,
	title = {Language universals at birth},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1318261111},
	doi = {10.1073/pnas.1318261111},
	language = {en},
	number = {16},
	urldate = {2017-02-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gomez, D. M. and Berent, I. and Benavides-Varela, S. and Bion, R. A. H. and Cattarossi, L. and Nespor, M. and Mehler, J.},
	month = apr,
	year = {2014},
	pages = {5837--5841},
	file = {PNAS-2014-Gómez-5837-41.pdf:/home/user/Zotero/storage/3R8SCBRN/PNAS-2014-Gómez-5837-41.pdf:application/pdf}
}

@article{haspelmath-optimality-1999,
	title = {Optimality and diachronic adaptation},
	volume = {18},
	url = {https://www.degruyter.com/dg/viewarticle.fullcontentlink:pdfeventlink/$002fj$002fzfsw.1999.18.issue-2$002fzfsw.1999.18.2.180$002fzfsw.1999.18.2.180.pdf?t:ac=j$002fzfsw.1999.18.issue-2$002fzfsw.1999.18.2.180$002fzfsw.1999.18.2.180.xml},
	number = {2},
	urldate = {2017-02-16},
	journal = {Zeitschrift für Sprachwissenschaft},
	author = {Haspelmath, Martin},
	year = {1999},
	pages = {180--205},
	file = {roa-302-haspelmath-3.pdf:/home/user/Zotero/storage/U7AWJ2M4/roa-302-haspelmath-3.pdf:application/pdf}
}

@inproceedings{riester-partial-2009,
	title = {Partial {Accommodation} and {Activation} in {Definites}},
	url = {http://www.ims.uni-stuttgart.de/institut/mitarbeiter/arndt/doc/Seoul08Riester.pdf},
	urldate = {2017-02-16},
	booktitle = {Proceedings of the 18th {International} {Congress} of {Linguists}},
	author = {Riester, Arndt},
	year = {2009},
	pages = {134--152},
	file = {Seoul08Riester.pdf:/home/user/Zotero/storage/8A8V9E5G/Seoul08Riester.pdf:application/pdf}
}

@article{regier-language-2009,
	title = {Language, thought, and color: {Whorf} was half right},
	volume = {13},
	shorttitle = {Language, thought, and color},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661309001454},
	number = {10},
	urldate = {2017-02-16},
	journal = {Trends in cognitive sciences},
	author = {Regier, Terry and Kay, Paul},
	year = {2009},
	pages = {439--446},
	file = {tics2-published.pdf:/home/user/Zotero/storage/9AWENU93/tics2-published.pdf:application/pdf}
}

@inproceedings{van-gael-beam-2008,
	title = {Beam sampling for the infinite hidden {Markov} model},
	url = {http://dl.acm.org/citation.cfm?id=1390293},
	urldate = {2017-02-16},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	publisher = {ACM},
	author = {Van Gael, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
	year = {2008},
	pages = {1088--1095},
	file = {VanSaaTehGha08.pdf:/home/user/Zotero/storage/JCAQEPJ5/VanSaaTehGha08.pdf:application/pdf}
}

@article{woolford-introduction-2007,
	title = {Introduction to ot syntax},
	volume = {10},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.526.9225&rep=rep1&type=pdf},
	urldate = {2017-02-16},
	journal = {Phonological Studies},
	author = {Woolford, Ellen},
	year = {2007},
	pages = {119--134},
	file = {Woolford Introduction to OT Syntax.pdf:/home/user/Zotero/storage/J8VNKMVG/Woolford Introduction to OT Syntax.pdf:application/pdf}
}

@article{xu-historical-2016,
	title = {Historical {Semantic} {Chaining} and {Efficient} {Communication}: {The} {Case} of {Container} {Names}},
	volume = {40},
	issn = {03640213},
	shorttitle = {Historical {Semantic} {Chaining} and {Efficient} {Communication}},
	url = {http://doi.wiley.com/10.1111/cogs.12312},
	doi = {10.1111/cogs.12312},
	language = {en},
	number = {8},
	urldate = {2017-02-16},
	journal = {Cognitive Science},
	author = {Xu, Yang and Regier, Terry and Malt, Barbara C.},
	month = nov,
	year = {2016},
	pages = {2081--2094},
	file = {xu_regier_malt_2015_historical_chaining.pdf:/home/user/Zotero/storage/N7M4X8C2/xu_regier_malt_2015_historical_chaining.pdf:application/pdf}
}

@article{zhan-bayesian-nodate,
	title = {Bayesian {Pronoun} {Interpretation} in {Mandarin} {Chinese}},
	url = {https://pdfs.semanticscholar.org/4bdf/e649e96b02aafc664159ccbea0de70a8a313.pdf},
	urldate = {2017-02-16},
	author = {Zhan, Meilin and Levy, Roger P. and Kehler, Andrew},
	file = {zhan-levy-kehler-2016-cogsci-prefinal.pdf:/home/user/Zotero/storage/9DZ4DA66/zhan-levy-kehler-2016-cogsci-prefinal.pdf:application/pdf}
}

@article{bade-obligatory-nodate,
	title = {Obligatory {Presupposition} {Triggers} in {Discourse}},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/bitstream/handle/10900/69134/DissBade2016\_pub.pdf?sequence=1&isAllowed=y},
	urldate = {2017-02-16},
	author = {Bade, Nadine},
	file = {Obligatory Presupposition Triggers in Discourse - DissBade2016_pub.pdf:/home/user/Zotero/storage/QXQ8ZG8S/DissBade2016_pub.pdf:application/pdf}
}

@inproceedings{maurits-why-2010,
	title = {Why are some word orders more common than others? {A} uniform information density account},
	shorttitle = {Why are some word orders more common than others?},
	url = {http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account},
	urldate = {2017-02-16},
	booktitle = {Advances in neural information processing systems},
	author = {Maurits, Luke and Navarro, Dan and Perfors, Amy},
	year = {2010},
	pages = {1585--1593},
	file = {Why are some word orders more common than others? A uniform information density account - 4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account.pdf:/home/user/Zotero/storage/ADEQ6HVV/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account.pdf:application/pdf}
}

@inproceedings{fedzechkina-communicative-2013,
	title = {Communicative biases shape structures of newly acquired languages.},
	url = {https://pdfs.semanticscholar.org/967a/7356ceb448e9993fb44bae9b5284d8106f7b.pdf},
	urldate = {2017-02-16},
	booktitle = {{CogSci}},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Newport, Elissa L.},
	year = {2013},
	file = {CogSci13_camera_ready - FedzechkinaJaegerNewport13.pdf:/home/user/Zotero/storage/XX75EIJJ/FedzechkinaJaegerNewport13.pdf:application/pdf}
}

@inproceedings{fedzechkina-functional-2011,
	title = {Functional {Biases} in {Language} {Learning}: {Evidence} from {Word} {Order} and {Case}-{Marking} {Interaction}.},
	shorttitle = {Functional {Biases} in {Language} {Learning}},
	url = {http://mfedzech.github.io/docs/FedzechkinaJaegerNewport\_cogsci11.pdf},
	urldate = {2017-02-16},
	booktitle = {{CogSci}},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Newport, Elissa L.},
	year = {2011},
	file = {FedzechkinaJaegerNewport11.pdf:/home/user/Zotero/storage/K3VI27WR/FedzechkinaJaegerNewport11.pdf:application/pdf}
}

@phdthesis{fedzechkina-communicative-2014,
	title = {Communicative efficiency, language learning, and language universals},
	url = {https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=29118&itemFileId=151158},
	urldate = {2017-02-16},
	school = {University of Rochester},
	author = {Fedzechkina, Maryia},
	year = {2014},
	file = {Fedzechkina_thesis.pdf:/home/user/Zotero/storage/U3H2Z626/Fedzechkina_thesis.pdf:application/pdf}
}

@article{fedzechkina-human-2017,
	title = {Human information processing shapes language change {Short} title: {Information} processing shapes language},
	shorttitle = {Human information processing shapes language change {Short} title},
	url = {http://mfedzech.github.io/docs/FedzechkinaChuJaeger\_submitted.pdf},
	urldate = {2017-02-16},
	author = {Fedzechkina, Maryia and Chu, Becky and Jaeger, T. Florian},
	year = {2017},
	file = {FedzechkinaChuJaeger_submitted.pdf:/home/user/Zotero/storage/TSTKWTGX/FedzechkinaChuJaeger_submitted.pdf:application/pdf}
}

@article{fedzechkina-balancing-2016,
	title = {Balancing {Effort} and {Information} {Transmission} {During} {Language} {Acquisition}: {Evidence} {From} {Word} {Order} and {Case} {Marking}},
	issn = {03640213},
	shorttitle = {Balancing {Effort} and {Information} {Transmission} {During} {Language} {Acquisition}},
	url = {http://doi.wiley.com/10.1111/cogs.12346},
	doi = {10.1111/cogs.12346},
	language = {en},
	urldate = {2017-02-16},
	journal = {Cognitive Science},
	author = {Fedzechkina, Maryia and Newport, Elissa L. and Jaeger, T. Florian},
	month = feb,
	year = {2016},
	pages = {n/a--n/a},
	file = {FedzechkinaNewportJaeger16.pdf:/home/user/Zotero/storage/7RPFK8GJ/FedzechkinaNewportJaeger16.pdf:application/pdf}
}

@article{fedzechkina-miniature-2016,
	title = {Miniature artificial language learning as a complement to typological data},
	url = {https://books.google.com/books?hl=en&lr=&id=88gvDAAAQBAJ&oi=fnd&pg=PA211&dq=%22function+assignment+(e.g.,+German,+Japanese,+and+Russian),%22+%22The+origins+of+such+recurring+patterns+have+been+the+subject+of%22+%22and+MacWhinney+1982,+Giv%C3%B3n%22+%22summarize+the+typological+approach+traditionally+used+to+study%22+&ots=I6phWGrsr5&sig=N0H6jIW1lkGrTHXx\_tn4gzyU0uA},
	urldate = {2017-02-16},
	journal = {The Usage-based Study of Language Learning and Multilingualism},
	author = {Fedzechkina, Maryia and Newportb, Elissa L. and Jaegerc, T. Florian},
	year = {2016},
	pages = {211},
	file = {FedzechkinaNewportJaeger_GURT.pdf:/home/user/Zotero/storage/3JIR7FQF/FedzechkinaNewportJaeger_GURT.pdf:application/pdf}
}

@inproceedings{fedzechkina-production-2015,
	title = {Production is biased to provide informative cues early: {Evidence} from miniature artificial languages.},
	shorttitle = {Production is biased to provide informative cues early},
	url = {http://mfedzech.github.io/docs/FedzechkinaJaegerTrueswell\_cogsci15.pdf},
	urldate = {2017-02-16},
	booktitle = {{CogSci}},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Trueswell, John C.},
	year = {2015},
	file = {FedzechkinaJaegerTrueswell_cogsci15.pdf:/home/user/Zotero/storage/PZC8H4VM/FedzechkinaJaegerTrueswell_cogsci15.pdf:application/pdf}
}

@inproceedings{fedzechkina-functional-2011-1,
	title = {Functional {Biases} in {Language} {Learning}: {Evidence} from {Word} {Order} and {Case}-{Marking} {Interaction}.},
	shorttitle = {Functional {Biases} in {Language} {Learning}},
	url = {http://mfedzech.github.io/docs/FedzechkinaJaegerNewport\_cogsci11.pdf},
	urldate = {2017-02-16},
	booktitle = {{CogSci}},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Newport, Elissa L.},
	year = {2011},
	file = {FedzechkinaJaegerNewport_cogsci11.pdf:/home/user/Zotero/storage/UZM27I2V/FedzechkinaJaegerNewport_cogsci11.pdf:application/pdf}
}

@article{beckner-language-2009,
	title = {Language is a complex adaptive system: {Position} paper},
	volume = {59},
	shorttitle = {Language is a complex adaptive system},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9922.2009.00533.x/full},
	number = {s1},
	urldate = {2017-02-16},
	journal = {Language learning},
	author = {Beckner, Clay and Blythe, Richard and Bybee, Joan and Christiansen, Morten H. and Croft, William and Ellis, Nick C. and Holland, John and Ke, Jinyun and Larsen-Freeman, Diane and Schoenemann, Tom},
	year = {2009},
	pages = {1--26},
	file = {BecknerEtAl2009ComplexAdaptiveSystem.pdf:/home/user/Zotero/storage/BAFR5JCV/BecknerEtAl2009ComplexAdaptiveSystem.pdf:application/pdf}
}

@article{beckner-usage-based-2009,
	title = {A {Usage}-{Based} {Account} of {Constituency} and {Reanalysis}},
	volume = {59},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9922.2009.00534.x/full},
	number = {s1},
	urldate = {2017-02-16},
	journal = {Language Learning},
	author = {Beckner, Clay and Bybee, Joan},
	year = {2009},
	pages = {27--46},
	file = {BecknerBybee2009Constituency.pdf:/home/user/Zotero/storage/2RQXXNMB/BecknerBybee2009Constituency.pdf:application/pdf}
}

@article{bybee-phonological-2008,
	title = {Phonological and grammatical variation in exemplar models},
	volume = {1},
	url = {https://www.degruyter.com/view/j/shll.2008.1.issue-2/shll-2008-1026/shll-2008-1026.xml},
	number = {2},
	urldate = {2017-02-16},
	journal = {Studies in Hispanic and Lusophone Linguistics},
	author = {Bybee, Joan and Torres, Rena},
	year = {2008},
	pages = {399--414},
	file = {BybeeTorresCacoullos2008ExemplarModels.pdf:/home/user/Zotero/storage/TAREKC8E/BybeeTorresCacoullos2008ExemplarModels.pdf:application/pdf}
}

@article{bybee-usage-2006,
	title = {From usage to grammar: {The} mind's response to repetition},
	volume = {82},
	shorttitle = {From usage to grammar},
	url = {https://muse.jhu.edu/article/208049/summary},
	number = {4},
	urldate = {2017-02-16},
	journal = {Language},
	author = {Bybee, Joan L.},
	year = {2006},
	pages = {711--733},
	file = {Bybee2006FromUsage.pdf:/home/user/Zotero/storage/UAMVX833/Bybee2006FromUsage.pdf:application/pdf}
}

@article{bybee-restrictions-2005,
	title = {Restrictions on phonemes in affixes: a crosslinguistic test of a popular hypothesis},
	volume = {9},
	shorttitle = {Restrictions on phonemes in affixes},
	url = {https://www.degruyter.com/view/j/lity.2005.9.issue-2/lity.2005.9.2.165/lity.2005.9.2.165.xml},
	number = {2},
	urldate = {2017-02-16},
	journal = {Linguistic Typology},
	author = {Bybee, Joan},
	year = {2005},
	pages = {165--222},
	file = {Bybee2005ResPhon.pdf:/home/user/Zotero/storage/DRGJ6I7E/Bybee2005ResPhon.pdf:application/pdf}
}

@inproceedings{maurits-why-2010-1,
	title = {Why are some word orders more common than others? {A} uniform information density account},
	shorttitle = {Why are some word orders more common than others?},
	url = {http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account},
	urldate = {2017-02-16},
	booktitle = {Advances in neural information processing systems},
	author = {Maurits, Luke and Navarro, Dan and Perfors, Amy},
	year = {2010},
	pages = {1585--1593},
	file = {Why are some word orders more common than others? A uniform information density account - 4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account.pdf:/home/user/Zotero/storage/GHVA9V67/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account.pdf:application/pdf}
}

@article{spaepen-generating-2013,
	title = {Generating a lexicon without a language model: {Do} words for number count?},
	volume = {69},
	issn = {0749596X},
	shorttitle = {Generating a lexicon without a language model},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X1300048X},
	doi = {10.1016/j.jml.2013.05.004},
	language = {en},
	number = {4},
	urldate = {2017-02-16},
	journal = {Journal of Memory and Language},
	author = {Spaepen, Elizabet and Coppola, Marie and Flaherty, Molly and Spelke, Elizabeth and Goldin-Meadow, Susan},
	month = nov,
	year = {2013},
	pages = {496--505},
	file = {nihms-490776.pdf:/home/user/Zotero/storage/6N7UVP98/nihms-490776.pdf:application/pdf}
}

@article{reddy-universal-2017,
	title = {Universal {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/1702.03196},
	abstract = {Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus. In this work, we introduce UDepLambda, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase. To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions.},
	urldate = {2017-02-16},
	journal = {arXiv:1702.03196 [cs]},
	author = {Reddy, Siva and Täckström, Oscar and Petrov, Slav and Steedman, Mark and Lapata, Mirella},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.03196},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WZQD63TB/1702.html:text/html;Reddy et al_2017_Universal Semantic Parsing.pdf:/home/user/Zotero/storage/HV4KGAK9/Reddy et al_2017_Universal Semantic Parsing.pdf:application/pdf}
}

@misc{noauthor-molly-flaherty-nodate,
	title = {molly-flaherty {\textbar} {RESEARCH}},
	url = {http://www.molly-flaherty.com/emotional},
	urldate = {2017-02-16},
	journal = {molly-flaherty},
	file = {Snapshot:/home/user/Zotero/storage/AIGAS3UG/emotional.html:text/html}
}

@article{fedzechkina-language-2012,
	title = {Language learners restructure their input to facilitate efficient communication},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/109/44/17897},
	doi = {10.1073/pnas.1215776109},
	abstract = {Languages of the world display many structural similarities. We test the hypothesis that some of these structural properties may arise from biases operating during language acquisition that shape languages over time. Specifically, we investigate whether language learners are biased toward linguistic systems that strike an efficient balance between robust information transfer, on the one hand, and effort or resource demands, on the other hand, thereby increasing the communicative utility of the acquired language. In two experiments, we expose learners to miniature artificial languages designed in such a way that they do not use their formal devices (case marking) efficiently to facilitate robust information transfer. We find that learners restructure such languages in ways that facilitate efficient information transfer compared with the input language. These systematic changes introduced by the learners follow typologically frequent patterns, supporting the hypothesis that some of the structural similarities found in natural languages are shaped by biases toward communicatively efficient linguistic systems.},
	language = {en},
	number = {44},
	urldate = {2017-02-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Newport, Elissa L.},
	month = oct,
	year = {2012},
	pmid = {23071337},
	keywords = {communicative pressures, efficient information transmission, language universals, Learning biases},
	pages = {17897--17902},
	file = {Fedzechkina et al_2012_Language learners restructure their input to facilitate efficient communication.pdf:/home/user/Zotero/storage/7EPQ9JJA/Fedzechkina et al_2012_Language learners restructure their input to facilitate efficient communication.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/SKZD24EA/17897.html:text/html}
}

@misc{noauthor-kenny-nodate,
	title = {Kenny {Smith}: {Publications}},
	url = {http://www.lel.ed.ac.uk/~kenny/publications.html},
	urldate = {2017-02-16},
	file = {Kenny Smith\: Publications:/home/user/Zotero/storage/XTZ4THKC/publications.html:text/html}
}

@article{bengio-estimating-2013,
	title = {Estimating or propagating gradients through stochastic neurons for conditional computation},
	url = {https://arxiv.org/abs/1308.3432},
	urldate = {2017-02-09},
	journal = {arXiv preprint arXiv:1308.3432},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	year = {2013},
	file = {1308.3432.pdf:/home/user/Zotero/storage/QNPQPVHA/1308.3432.pdf:application/pdf}
}

@book{sten-optimality-theoretic-2001,
	title = {Optimality-theoretic {Syntax}},
	url = {https://books.google.com/books?hl=en&lr=&id=wdKBPH\_L39oC&oi=fnd&pg=PR7&dq=%22illustrates+this+point.+Sentences+(1a)+and+(1b),+but+not+(1c),+are%22+%22Projection+Principle+(EPP),+a+principle+requiring+that+every+clause+has%22+%22a+violable+principle+%60%60outranked%27%27+by+a+V2%22+&ots=tfFTHj9Zcx&sig=wQD\_mN-avJ48C2BVLSbIB9bHPCs},
	urldate = {2017-02-09},
	publisher = {MIT Press},
	author = {Sten, Géraldine Legendre Jane Barbara Grimshaw and Legendre, Géraldine and Grimshaw, Jane Barbara and Vikner, Sten and {others}},
	year = {2001},
	file = {9780262621380_sch_0001.pdf:/home/user/Zotero/storage/SRCM2VF2/9780262621380_sch_0001.pdf:application/pdf}
}

@book{horn-natural-2001,
	address = {Stanford, Calif},
	series = {The {David} {Hume} series},
	title = {A natural history of negation},
	isbn = {978-1-57586-336-8},
	publisher = {CSLI},
	author = {Horn, Laurence R.},
	year = {2001},
	keywords = {Grammar, Comparative and general, Language and languages, Negation (Logic), Negatives, Philosophy},
	file = {A-natural-history-of-negation-Laurence-R.-Horn.pdf:/home/user/Zotero/storage/RRGHEBXB/A-natural-history-of-negation-Laurence-R.-Horn.pdf:application/pdf}
}

@article{anttila-phonological-2016,
	title = {Phonological {Effects} on {Syntactic} {Variation}},
	volume = {2},
	issn = {2333-9683, 2333-9691},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-linguistics-011415-040845},
	doi = {10.1146/annurev-linguistics-011415-040845},
	language = {en},
	number = {1},
	urldate = {2017-02-09},
	journal = {Annual Review of Linguistics},
	author = {Anttila, Arto},
	month = jan,
	year = {2016},
	pages = {115--137},
	file = {annurev-linguistics-011415-040845.pdf:/home/user/Zotero/storage/9PGIW5Q8/annurev-linguistics-011415-040845.pdf:application/pdf}
}

@article{benz-best-2015,
	title = {The {Best} {Question}: {Explaining} the {Projection} {Behavior} of {Factives}},
	shorttitle = {The {Best} {Question}},
	url = {https://pdfs.semanticscholar.org/6eb5/2eedee9e9f56d8c5438b1a0eea0a21c8d5ef.pdf},
	urldate = {2017-02-09},
	author = {Benz, Anton and Jasinskaja, Katja},
	year = {2015},
	file = {BestQuestion-Resubmission-FormattedForPosting-6-8-15.pdf:/home/user/Zotero/storage/TQSXHZ3I/BestQuestion-Resubmission-FormattedForPosting-6-8-15.pdf:application/pdf}
}

@article{broekhuis-minimalist-2000,
	title = {The minimalist program and optimality theory: derivations and evaluations},
	shorttitle = {The minimalist program and optimality theory},
	url = {https://books.google.com/books?hl=en&lr=&id=\_2b-iqNri4AC&oi=fnd&pg=PA386&dq=%22evaluated+with+respect+to+a+ranking+of+violable+constraints.+This%22+%22generating+device+is+not+primarily+responsible+for+the+descriptive+adequacy+of+the%22+%22to+the+Doubly+Filled+COMP+Filter,+something+that+is+far+beyond+the+scope+of%22+&ots=2je0lDAIBd&sig=B6B5guZbz45SkE7Jr22wZYJxTJc},
	urldate = {2017-02-09},
	journal = {Optimality Theory: phonology, syntax and acquisition},
	author = {Broekhuis, Hans and Dekkers, Joost},
	year = {2000},
	pages = {386--422},
	file = {broekhuisEtAl_98_The-Mini.pdf:/home/user/Zotero/storage/XJN86322/broekhuisEtAl_98_The-Mini.pdf:application/pdf}
}

@article{cibelli-sapir-whorf-2016,
	title = {The {Sapir}-{Whorf} {Hypothesis} and {Probabilistic} {Inference}: {Evidence} from the {Domain} of {Color}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {The {Sapir}-{Whorf} {Hypothesis} and {Probabilistic} {Inference}},
	url = {http://dx.plos.org/10.1371/journal.pone.0158725},
	doi = {10.1371/journal.pone.0158725},
	language = {en},
	number = {7},
	urldate = {2017-02-09},
	journal = {PLOS ONE},
	author = {Cibelli, Emily and Xu, Yang and Austerweil, Joseph L. and Griffiths, Thomas L. and Regier, Terry},
	editor = {Osorio, Daniel},
	month = jul,
	year = {2016},
	pages = {e0158725},
	file = {cibelli-et-al-2016-preprint.pdf:/home/user/Zotero/storage/DMDM2D2W/cibelli-et-al-2016-preprint.pdf:application/pdf}
}

@article{cremers-probability-2015,
	title = {Probability judgments of gappy sentences},
	url = {http://semanticsarchive.net/Archive/DZjNWY0N/CKC-GappyProbability.pdf},
	urldate = {2017-02-09},
	journal = {Ms., Ecole Normale Supérieure},
	author = {Cremers, Alexandre and Kriz, Manuel and Chemla, Emmanuel},
	year = {2015},
	file = {CKC-GappyProbability.pdf:/home/user/Zotero/storage/AXVDKA8P/CKC-GappyProbability.pdf:application/pdf}
}

@article{degen-investigating-2015,
	title = {Investigating the distribution of some (but not all ) implicatures using corpora and web-based methods},
	volume = {8},
	copyright = {Copyright (c) 2015 Judith Degen},
	issn = {1937-8912},
	url = {http://semprag.org/article/view/sp.8.11},
	language = {en},
	number = {0},
	urldate = {2017-01-23},
	journal = {Semantics and Pragmatics},
	author = {Degen, Judith},
	month = may,
	year = {2015},
	keywords = {corpora, experimental pragmatics, GCI, scalar implicature},
	pages = {11--1--55},
	file = {Degen_2015_Investigating the distribution of some (but not all ) implicatures using.pdf:/home/user/Zotero/storage/QV5CAVKI/Degen_2015_Investigating the distribution of some (but not all ) implicatures using.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/X9Q63JM3/pdf_8_111.html:text/html}
}

@article{vasishth-processing-2013,
	title = {Processing {Chinese} {Relative} {Clauses}: {Evidence} for the {Subject}-{Relative} {Advantage}},
	volume = {8},
	issn = {1932-6203},
	shorttitle = {Processing {Chinese} {Relative} {Clauses}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0077006},
	doi = {10.1371/journal.pone.0077006},
	abstract = {A general fact about language is that subject relative clauses are easier to process than object relative clauses. Recently, several self-paced reading studies have presented surprising evidence that object relatives in Chinese are easier to process than subject relatives. We carried out three self-paced reading experiments that attempted to replicate these results. Two of our three studies found a subject-relative preference, and the third study found an object-relative advantage. Using a random effects bayesian meta-analysis of fifteen studies (including our own), we show that the overall current evidence for the subject-relative advantage is quite strong (approximate posterior probability of a subject-relative advantage given the data: 78–80\%). We argue that retrieval/integration based accounts would have difficulty explaining all three experimental results. These findings are important because they narrow the theoretical space by limiting the role of an important class of explanation—retrieval/integration cost—at least for relative clause processing in Chinese.},
	number = {10},
	urldate = {2017-01-15},
	journal = {PLOS ONE},
	author = {Vasishth, Shravan and Chen, Zhong and Li, Qiang and Guo, Gueilan},
	month = oct,
	year = {2013},
	keywords = {language, Confidence intervals, Meta-analysis, Parsers, psycholinguistics, Statistical data, Syntax, working memory},
	pages = {e77006},
	file = {Snapshot:/home/user/Zotero/storage/MQW2MD89/article.html:text/html;Vasishth et al_2013_Processing Chinese Relative Clauses.pdf:/home/user/Zotero/storage/6DKPIR6I/Vasishth et al_2013_Processing Chinese Relative Clauses.pdf:application/pdf}
}

@misc{noauthor-shravan-nodate,
	title = {Shravan {Vasishth}'s home page},
	url = {http://www.ling.uni-potsdam.de/~vasishth/courses/ESSLLI2016VasishthEngelmann.html},
	urldate = {2017-01-15},
	file = {Shravan Vasishth's home page:/home/user/Zotero/storage/3ZRKCZ65/ESSLLI2016VasishthEngelmann.html:text/html}
}

@book{walter-bagehot-english-1873,
	title = {The {English} {Constitution}},
	url = {http://archive.org/details/englishconstitu00bagegoog},
	abstract = {Book digitized by Google from the library of the University of Michigan and uploaded to the Internet Archive by user tpb.},
	language = {English},
	urldate = {2017-01-15},
	publisher = {Little, Brown, and co},
	author = {{Walter Bagehot}},
	collaborator = {{University of Michigan}},
	year = {1873}
}

@misc{noauthor-english-nodate,
	title = {The {English} {Constitution}},
	url = {https://archive.org/stream/englishconstitu00bagegoog#page/n52/mode/2up},
	urldate = {2017-01-15},
	file = {The English Constitution:/home/user/Zotero/storage/X5FUXW8N/englishconstitu00bagegoog.html:text/html}
}

@article{betancourt-conceptual-2017,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo} (posted by {Shravan})},
	url = {https://arxiv.org/abs/1701.02434},
	urldate = {2017-01-12},
	author = {Betancourt, Michael},
	month = jan,
	year = {2017},
	file = {Betancourt_2017_A Conceptual Introduction to Hamiltonian Monte Carlo.pdf:/home/user/Zotero/storage/4PXEJAZV/Betancourt_2017_A Conceptual Introduction to Hamiltonian Monte Carlo.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/J9SWQ3RA/1701.html:text/html}
}

@book{newmeyer-measuring-2014,
	title = {Measuring {Grammatical} {Complexity}},
	isbn = {978-0-19-968530-1},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301},
	urldate = {2017-01-10},
	publisher = {Oxford University Press},
	editor = {Newmeyer, Frederick J. and Preston, Laurel B.},
	month = oct,
	year = {2014},
	file = {References - Oxford Scholarship:/home/user/Zotero/storage/GN9KE2JQ/acprof-9780199685301-bibliography-1.html:text/html;Snapshot:/home/user/Zotero/storage/IAZDVPD7/acprof-9780199685301.html:text/html}
}

@misc{noauthor-looking-nodate,
	title = {Looking for a ‘{Gold} {Standard}’ to measure language complexity: what psycholinguistics and neurolinguistics can (and cannot) offer to formal linguistics - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-14},
	urldate = {2017-01-10},
	file = {Looking for a ‘Gold Standard’ to measure language complexity\: what psycholinguistics and neurolinguistics can (and cannot) offer to formal linguistics - Oxford Scholarship:/home/user/Zotero/storage/BNE6DTZ2/acprof-9780199685301-chapter-14.html:text/html}
}

@misc{noauthor-computational-nodate,
	title = {Computational complexity in the brain - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-13},
	urldate = {2017-01-10},
	file = {Computational complexity in the brain - Oxford Scholarship:/home/user/Zotero/storage/PP2DITC7/acprof-9780199685301-chapter-13.html:text/html}
}

@misc{noauthor-measurement-nodate,
	title = {Measurement of semantic complexity: how to get by if your language lacks generalized quantifiers - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-12},
	urldate = {2017-01-10},
	file = {Measurement of semantic complexity\: how to get by if your language lacks generalized quantifiers - Oxford Scholarship:/home/user/Zotero/storage/Z8ZTA3A8/acprof-9780199685301-chapter-12.html:text/html}
}

@misc{noauthor-importance-nodate,
	title = {Importance of exhaustive description in measuring linguistic complexity: the case of {English} try and pseudocoordination - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-10},
	urldate = {2017-01-10},
	file = {Importance of exhaustive description in measuring linguistic complexity\: the case of English try and pseudocoordination - Oxford Scholarship:/home/user/Zotero/storage/H6F7S3C7/acprof-9780199685301-chapter-10.html:text/html}
}

@misc{noauthor-cross-linguistic-nodate,
	title = {Cross-linguistic comparison of complexity measures in phonological systems - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-11},
	urldate = {2017-01-10},
	file = {Cross-linguistic comparison of complexity measures in phonological systems - Oxford Scholarship:/home/user/Zotero/storage/2P95N3VZ/acprof-9780199685301-chapter-11.html:text/html}
}

@misc{noauthor-constructions-nodate,
	title = {Constructions, complexity, and word order variation - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-8},
	urldate = {2017-01-10},
	file = {Constructions, complexity, and word order variation - Oxford Scholarship:/home/user/Zotero/storage/7XEC93GP/acprof-9780199685301-chapter-8.html:text/html}
}

@misc{noauthor-complexity-nodate,
	title = {Complexity of narrow syntax: {Minimalism}, representational economy, and simplest {Merge} - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-7},
	urldate = {2017-01-10},
	file = {Complexity of narrow syntax\: Minimalism, representational economy, and simplest Merge - Oxford Scholarship:/home/user/Zotero/storage/MCBXU9M6/acprof-9780199685301-chapter-7.html:text/html}
}

@misc{noauthor-complexity-nodate-1,
	title = {Complexity in comparative syntax: the view from modern parametric theory - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-6},
	urldate = {2017-01-10},
	file = {Complexity in comparative syntax\: the view from modern parametric theory - Oxford Scholarship:/home/user/Zotero/storage/GN4IK93W/acprof-9780199685301-chapter-6.html:text/html}
}

@misc{noauthor-degrees-nodate,
	title = {Degrees of complexity in syntax: a view from evolution - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-5},
	urldate = {2017-01-10},
	file = {Degrees of complexity in syntax\: a view from evolution - Oxford Scholarship:/home/user/Zotero/storage/H37C6ZBE/acprof-9780199685301-chapter-5.html:text/html}
}

@misc{noauthor-what-nodate,
	title = {What you can say without syntax: a hierarchy of grammatical complexity - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-4},
	urldate = {2017-01-10},
	file = {What you can say without syntax\: a hierarchy of grammatical complexity - Oxford Scholarship:/home/user/Zotero/storage/AVA36JSR/acprof-9780199685301-chapter-4.html:text/html}
}

@misc{noauthor-sign-nodate,
	title = {Sign languages, creoles, and the development of predication - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-3},
	urldate = {2017-01-10},
	file = {Sign languages, creoles, and the development of predication - Oxford Scholarship:/home/user/Zotero/storage/WREN2NRD/acprof-9780199685301-chapter-3.html:text/html}
}

@misc{noauthor-major-nodate,
	title = {Major contributions from formal linguistics to the complexity debate - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199685301.001.0001/acprof-9780199685301-chapter-2},
	urldate = {2017-01-10},
	file = {Major contributions from formal linguistics to the complexity debate - Oxford Scholarship:/home/user/Zotero/storage/2I8H3ID6/acprof-9780199685301-chapter-2.html:text/html}
}

@misc{noauthor-[1606.05029]-nodate,
	title = {[1606.05029] {Simple} and {Effective} {Question} {Answering} with {Recurrent} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1606.05029},
	urldate = {2016-06-17},
	file = {[1606.05029] Simple and Effective Question Answering with Recurrent Neural Networks:/home/user/Zotero/storage/6ASPFHKD/1606.html:text/html}
}

@misc{noauthor-[1606.05331]-nodate,
	title = {[1606.05331] {The} {Pattern} {Basis} {Approach} to {Circuit} {Complexity}},
	url = {https://arxiv.org/abs/1606.05331},
	urldate = {2016-06-17},
	file = {[1606.05331] The Pattern Basis Approach to Circuit Complexity:/home/user/Zotero/storage/BU6BU66H/1606.html:text/html}
}

@misc{noauthor-[1606.05050]-nodate,
	title = {[1606.05050] {Proof} {Complexity} {Lower} {Bounds} from {Algebraic} {Circuit} {Complexity}},
	url = {https://arxiv.org/abs/1606.05050},
	urldate = {2016-06-17},
	file = {[1606.05050] Proof Complexity Lower Bounds from Algebraic Circuit Complexity:/home/user/Zotero/storage/QTB2U7RF/1606.html:text/html}
}

@misc{noauthor-[1606.04393]-nodate,
	title = {[1606.04393] {EvoNet}: {Evolutionary} {Synthesis} of {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1606.04393},
	urldate = {2016-06-17},
	file = {[1606.04393] EvoNet\: Evolutionary Synthesis of Deep Neural Networks:/home/user/Zotero/storage/UFGG5RAK/1606.html:text/html}
}

@misc{noauthor-[1606.04414]-nodate,
	title = {[1606.04414] {The} {Parallel} {Knowledge} {Gradient} {Method} for {Batch} {Bayesian} {Optimization}},
	url = {https://arxiv.org/abs/1606.04414},
	urldate = {2016-06-17},
	file = {[1606.04414] The Parallel Knowledge Gradient Method for Batch Bayesian Optimization:/home/user/Zotero/storage/R3BIGWV8/1606.html:text/html}
}

@misc{noauthor-[1606.04934]-nodate,
	title = {[1606.04934] {Improving} {Variational} {Inference} with {Inverse} {Autoregressive} {Flow}},
	url = {https://arxiv.org/abs/1606.04934},
	urldate = {2016-06-17},
	file = {[1606.04934] Improving Variational Inference with Inverse Autoregressive Flow:/home/user/Zotero/storage/SPJ63M46/1606.html:text/html}
}

@misc{noauthor-[1606.04838]-nodate,
	title = {[1606.04838] {Optimization} {Methods} for {Large}-{Scale} {Machine} {Learning}},
	url = {https://arxiv.org/abs/1606.04838},
	urldate = {2016-06-17},
	file = {[1606.04838] Optimization Methods for Large-Scale Machine Learning:/home/user/Zotero/storage/26FJ6C4P/1606.html:text/html}
}

@misc{noauthor-[1606.04631]-nodate,
	title = {[1606.04631] {Bidirectional} {Long}-{Short} {Term} {Memory} for {Video} {Description}},
	url = {https://arxiv.org/abs/1606.04631},
	urldate = {2016-06-17},
	file = {[1606.04631] Bidirectional Long-Short Term Memory for Video Description:/home/user/Zotero/storage/56XXKQKW/1606.html:text/html}
}

@misc{noauthor-[1606.04686]-nodate,
	title = {[1606.04686] {Natural} {Language} {Generation} as {Planning} under {Uncertainty} {Using} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1606.04686},
	urldate = {2016-06-17},
	file = {[1606.04686] Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning:/home/user/Zotero/storage/VR57NNPE/1606.html:text/html}
}

@incollection{chattopadhyay-locally-2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Locally {Commutative} {Categories}},
	copyright = {©2003 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-40493-4 978-3-540-45061-0},
	url = {http://link.springer.com/chapter/10.1007/3-540-45061-0\_76},
	abstract = {It is known that a finite category can have all its base monoids in a variety V (i.e. be locally V, denoted ℓ V) without itself dividing a monoid in V (i.e. be globally V, denoted gV). This is in particular the case when V=Com, the variety of commutative monoids. Our main result provides a combinatorial characterization of locally commutative categories. This is the first such theorem dealing with a variety for which local differs from global. As a consequence, we show that ℓ Com ⊂ gV for every variety V that strictly contains the commutative monoids.},
	language = {en},
	number = {2719},
	urldate = {2016-06-16},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chattopadhyay, Arkadev and Thérien, Denis},
	editor = {Baeten, Jos C. M. and Lenstra, Jan Karel and Parrow, Joachim and Woeginger, Gerhard J.},
	month = jun,
	year = {2003},
	doi = {10.1007/3-540-45061-0\_76},
	keywords = {Computer Communication Networks, Data Structures, Mathematics of Computing, Software Engineering/Programming and Operating Systems, Theory of Computation},
	pages = {984--995},
	file = {Chattopadhyay_Thérien_2003_Locally Commutative Categories.pdf:/home/user/Zotero/storage/RC6JPD79/Chattopadhyay_Thérien_2003_Locally Commutative Categories.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/4JJZJ8K4/10.html:text/html}
}

@misc{noauthor-fundamentals-nodate,
	title = {Fundamentals of {Semigroup} {Theory} {\textbar} {John} {M}. {Howie} {\textbar} digital library bookzz},
	url = {http://bookzz.org/book/615048/db3853/?\_ir=1},
	urldate = {2016-06-16},
	file = {Fundamentals of Semigroup Theory | John M. Howie | digital library bookzz:/home/user/Zotero/storage/HI6QJ45W/db3853.html:text/html}
}

@misc{noauthor-fundamentals-nodate-1,
	title = {Fundamentals of {Semigroup} {Theory} {\textbar} {John} {M}. {Howie} {\textbar} digital library bookzz},
	url = {http://bookzz.org/book/615048/db3853/?\_ir=1},
	urldate = {2016-06-16},
	file = {Fundamentals of Semigroup Theory | John M. Howie | digital library bookzz:/home/user/Zotero/storage/VSDPAXM3/db3853.html:text/html}
}

@misc{noauthor-[1606.02342]-nodate,
	title = {[1606.02342] {Optimizing} {Spectral} {Learning} for {Parsing}},
	url = {https://arxiv.org/abs/1606.02342},
	urldate = {2016-06-16},
	file = {[1606.02342] Optimizing Spectral Learning for Parsing:/home/user/Zotero/storage/P6HWEM94/1606.html:text/html}
}

@misc{noauthor-[1605.06069]-nodate,
	title = {[1605.06069] {A} {Hierarchical} {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Generating} {Dialogues}},
	url = {https://arxiv.org/abs/1605.06069},
	urldate = {2016-06-16},
	file = {[1605.06069] A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues:/home/user/Zotero/storage/AD6Q5UNH/1605.html:text/html}
}

@misc{noauthor-[1603.07954]-nodate,
	title = {[1603.07954] {Improving} {Information} {Extraction} by {Acquiring} {External} {Evidence} with {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1603.07954},
	urldate = {2016-06-16},
	file = {[1603.07954] Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning:/home/user/Zotero/storage/EPC72WI3/1603.html:text/html}
}

@misc{noauthor-[1508.03040]-nodate,
	title = {[1508.03040] {Syntax} {Evolution}: {Problems} and {Recursion}},
	url = {https://arxiv.org/abs/1508.03040},
	urldate = {2016-06-16},
	file = {[1508.03040] Syntax Evolution\: Problems and Recursion:/home/user/Zotero/storage/DQ4RRAAN/1508.html:text/html}
}

@misc{noauthor-[1606.04212]-nodate,
	title = {[1606.04212] {Active} {Discriminative} {Word} {Embedding} {Learning}},
	url = {https://arxiv.org/abs/1606.04212},
	urldate = {2016-06-16},
	file = {[1606.04212] Active Discriminative Word Embedding Learning:/home/user/Zotero/storage/N52FVFPK/1606.html:text/html}
}

@misc{noauthor-[1606.04199]-nodate,
	title = {[1606.04199] {Deep} {Recurrent} {Models} with {Fast}-{Forward} {Connections} for {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1606.04199},
	urldate = {2016-06-16},
	file = {[1606.04199] Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation:/home/user/Zotero/storage/NX97D66Z/1606.html:text/html}
}

@misc{noauthor-[1606.04164]-nodate,
	title = {[1606.04164] {Zero}-{Resource} {Translation} with {Multi}-{Lingual} {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1606.04164},
	urldate = {2016-06-16},
	file = {[1606.04164] Zero-Resource Translation with Multi-Lingual Neural Machine Translation:/home/user/Zotero/storage/4FNSBXG6/1606.html:text/html}
}

@misc{noauthor-[1606.04155]-nodate,
	title = {[1606.04155] {Rationalizing} {Neural} {Predictions}},
	url = {https://arxiv.org/abs/1606.04155},
	urldate = {2016-06-16},
	file = {[1606.04155] Rationalizing Neural Predictions:/home/user/Zotero/storage/U39ENBM4/1606.html:text/html}
}

@article{pin-topologies-1991,
	title = {Topologies for the free monoid},
	volume = {137},
	issn = {0021-8693},
	url = {http://www.sciencedirect.com/science/article/pii/002186939190094O},
	doi = {10.1016/0021-8693(91)90094-O},
	abstract = {The finite group (or profinite) topology was first introduced for the free group by M. Hall, Jr. and by Reutenauer for free monoids. This is the initial topology defined by all the monoid morphisms from the free monoid into a discrete finite group. The p-adic topology is defined in the same way by replacing “group” by “p-group” in the definition. In this paper we study the properties of these topologies and their connexions with the theory of formal languages.},
	number = {2},
	urldate = {2016-06-16},
	journal = {Journal of Algebra},
	author = {Pin, J. E},
	month = mar,
	year = {1991},
	pages = {297--337},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/ENEGZSD2/002186939190094O.html:text/html}
}

@article{margolis-new-1992,
	title = {New results on the conjecture of {Rhodes} and on the topological conjecture},
	volume = {80},
	issn = {0022-4049},
	url = {http://www.sciencedirect.com/science/article/pii/0022404992901489},
	doi = {10.1016/0022-4049(92)90148-9},
	abstract = {The Conjecture of Rhodes, originally called the “type II conjecture” by Rhodes, gives an algorithm to compute the kernel of a finite semigroup. This conjecture has numerous important consequences and is one of the most attractive problems on finite semigroups. It was known that the conjecture of Rhodes is a consequence of another conjecture on the finite group topology for the free monoid. In this paper, we show that the topological conjecture and the conjecture of Rhodes are both equivalent to a third conjecture and we prove this third conjecture in a number of significant particular cases.},
	number = {3},
	urldate = {2016-06-16},
	journal = {Journal of Pure and Applied Algebra},
	author = {Margolis, S. W. and Pin, J. E.},
	month = jul,
	year = {1992},
	pages = {305--313},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/TQW23I9M/0022404992901489.html:text/html}
}

@misc{noauthor-[1602.01387]-nodate,
	title = {[1602.01387] {Unrestricted} {State} {Complexity} of {Binary} {Operations} on {Regular} {Languages}},
	url = {https://arxiv.org/abs/1602.01387},
	urldate = {2016-06-15},
	file = {[1602.01387] Unrestricted State Complexity of Binary Operations on Regular Languages:/home/user/Zotero/storage/ZM45N9RZ/1602.html:text/html}
}

@misc{noauthor-[1606.02245]-nodate,
	title = {[1606.02245] {Iterative} {Alternating} {Neural} {Attention} for {Machine} {Reading}},
	url = {https://arxiv.org/abs/1606.02245},
	urldate = {2016-06-15},
	file = {[1606.02245] Iterative Alternating Neural Attention for Machine Reading:/home/user/Zotero/storage/3DUKHSUG/1606.html:text/html}
}

@misc{noauthor-[1605.09186]-nodate,
	title = {[1605.09186] {Does} {Multimodality} {Help} {Human} and {Machine} for {Translation} and {Image} {Captioning}?},
	url = {https://arxiv.org/abs/1605.09186},
	urldate = {2016-06-15},
	file = {[1605.09186] Does Multimodality Help Human and Machine for Translation and Image Captioning?:/home/user/Zotero/storage/ZT7BB6X6/1605.html:text/html}
}

@misc{noauthor-[1606.03955]-nodate,
	title = {[1606.03955] {Avoidability} of formulas with two variables},
	url = {https://arxiv.org/abs/1606.03955},
	urldate = {2016-06-15},
	file = {[1606.03955] Avoidability of formulas with two variables:/home/user/Zotero/storage/ZHG6RTIQ/1606.html:text/html}
}

@misc{noauthor-[1606.03864]-nodate,
	title = {[1606.03864] {Neural} {Associative} {Memory} for {Dual}-{Sequence} {Modeling}},
	url = {https://arxiv.org/abs/1606.03864},
	urldate = {2016-06-15},
	file = {[1606.03864] Neural Associative Memory for Dual-Sequence Modeling:/home/user/Zotero/storage/F4GV68MF/1606.html:text/html}
}

@misc{noauthor-[1606.03821]-nodate,
	title = {[1606.03821] {Learning} to {Generate} {Compositional} {Color} {Descriptions}},
	url = {https://arxiv.org/abs/1606.03821},
	urldate = {2016-06-15},
	file = {[1606.03821] Learning to Generate Compositional Color Descriptions:/home/user/Zotero/storage/IQVMFCJR/1606.html:text/html}
}

@misc{noauthor-[1606.03777]-nodate,
	title = {[1606.03777] {Neural} {Belief} {Tracker}: {Data}-{Driven} {Dialogue} {State} {Tracking}},
	url = {https://arxiv.org/abs/1606.03777},
	urldate = {2016-06-15},
	file = {[1606.03777] Neural Belief Tracker\: Data-Driven Dialogue State Tracking:/home/user/Zotero/storage/A7RXPRI6/1606.html:text/html}
}

@misc{noauthor-[1606.03667]-nodate,
	title = {[1606.03667] {Deep} {Reinforcement} {Learning} with a {Combinatorial} {Action} {Space} for {Predicting} and {Tracking} {Popular} {Discussion} {Threads}},
	url = {https://arxiv.org/abs/1606.03667},
	urldate = {2016-06-15},
	file = {[1606.03667] Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads:/home/user/Zotero/storage/XZH8C7P2/1606.html:text/html}
}

@misc{noauthor-[1606.03622]-nodate,
	title = {[1606.03622] {Data} {Recombination} for {Neural} {Semantic} {Parsing}},
	url = {https://arxiv.org/abs/1606.03622},
	urldate = {2016-06-15},
	file = {[1606.03622] Data Recombination for Neural Semantic Parsing:/home/user/Zotero/storage/3WE5VIHM/1606.html:text/html}
}

@misc{noauthor-[1606.03568]-nodate,
	title = {[1606.03568] {Word} {Sense} {Disambiguation} using a {Bidirectional} {LSTM}},
	url = {https://arxiv.org/abs/1606.03568},
	urldate = {2016-06-15},
	file = {[1606.03568] Word Sense Disambiguation using a Bidirectional LSTM:/home/user/Zotero/storage/MP3BNET8/1606.html:text/html}
}

@article{braun-constructive-2009,
	title = {On a constructive proof of {Kolmogorov}’s superposition theorem},
	volume = {30},
	url = {http://link.springer.com/article/10.1007/s00365-009-9054-2},
	number = {3},
	urldate = {2016-06-06},
	journal = {Constructive approximation},
	author = {Braun, Jürgen and Griebel, Michael},
	year = {2009},
	pages = {653--675},
	file = {remonkoe.pdf:/home/user/Zotero/storage/3CN7DW7U/remonkoe.pdf:application/pdf}
}

@misc{noauthor-[1603.00954]-nodate,
	title = {[1603.00954] {Training} {Input}-{Output} {Recurrent} {Neural} {Networks} through {Spectral} {Methods}},
	url = {https://arxiv.org/abs/1603.00954},
	urldate = {2016-06-06},
	file = {[1603.00954] Training Input-Output Recurrent Neural Networks through Spectral Methods:/home/user/Zotero/storage/FEDXEUK4/1603.html:text/html}
}

@misc{noauthor-[1604.06057]-nodate,
	title = {[1604.06057] {Hierarchical} {Deep} {Reinforcement} {Learning}: {Integrating} {Temporal} {Abstraction} and {Intrinsic} {Motivation}},
	url = {https://arxiv.org/abs/1604.06057},
	urldate = {2016-06-03},
	file = {[1604.06057] Hierarchical Deep Reinforcement Learning\: Integrating Temporal Abstraction and Intrinsic Motivation:/home/user/Zotero/storage/PJ7SBWF2/1604.html:text/html}
}

@misc{noauthor-[1511.02386]-nodate,
	title = {[1511.02386] {Hierarchical} {Variational} {Models}},
	url = {https://arxiv.org/abs/1511.02386},
	urldate = {2016-06-03},
	file = {[1511.02386] Hierarchical Variational Models:/home/user/Zotero/storage/IKJB9PKP/1511.html:text/html}
}

@misc{noauthor-[1605.09782]-nodate,
	title = {[1605.09782] {Adversarial} {Feature} {Learning}},
	url = {https://arxiv.org/abs/1605.09782},
	urldate = {2016-06-03},
	file = {[1605.09782] Adversarial Feature Learning:/home/user/Zotero/storage/UVEBRC5G/1605.html:text/html}
}

@misc{noauthor-[1605.09674]-nodate,
	title = {[1605.09674] {Curiosity}-driven {Exploration} in {Deep} {Reinforcement} {Learning} via {Bayesian} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1605.09674},
	urldate = {2016-06-03},
	file = {[1605.09674] Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks:/home/user/Zotero/storage/KI2KTQDV/1605.html:text/html}
}

@misc{noauthor-[1605.09593]-nodate,
	title = {[1605.09593] {Controlling} {Exploration} {Improves} {Training} for {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1605.09593},
	urldate = {2016-06-03},
	file = {[1605.09593] Controlling Exploration Improves Training for Deep Neural Networks:/home/user/Zotero/storage/Z5ZZMBZE/1605.html:text/html}
}

@misc{noauthor-[1602.06291]-nodate,
	title = {[1602.06291] {Contextual} {LSTM} ({CLSTM}) models for {Large} scale {NLP} tasks},
	url = {https://arxiv.org/abs/1602.06291},
	urldate = {2016-06-03},
	file = {[1602.06291] Contextual LSTM (CLSTM) models for Large scale NLP tasks:/home/user/Zotero/storage/N3V3BIGZ/1602.html:text/html}
}

@misc{noauthor-[1605.09553]-nodate,
	title = {[1605.09553] {Attention} {Correctness} in {Neural} {Image} {Captioning}},
	url = {https://arxiv.org/abs/1605.09553},
	urldate = {2016-06-03},
	file = {[1605.09553] Attention Correctness in Neural Image Captioning:/home/user/Zotero/storage/6JTQ8FD8/1605.html:text/html}
}

@misc{noauthor-[1511.02799]-nodate,
	title = {[1511.02799] {Neural} {Module} {Networks}},
	url = {https://arxiv.org/abs/1511.02799},
	urldate = {2016-06-03},
	file = {[1511.02799] Neural Module Networks:/home/user/Zotero/storage/Q7UK8R9F/1511.html:text/html}
}

@misc{noauthor-[1411.6432]-nodate,
	title = {[1411.6432] {The} joy of implications, aka pure {Horn} functions: mainly a survey},
	url = {https://arxiv.org/abs/1411.6432},
	urldate = {2016-06-03},
	file = {[1411.6432] The joy of implications, aka pure Horn functions\: mainly a survey:/home/user/Zotero/storage/H4WDC9FG/1411.html:text/html}
}

@misc{noauthor-[1606.00061]-nodate,
	title = {[1606.00061] {Hierarchical} {Question}-{Image} {Co}-{Attention} for {Visual} {Question} {Answering}},
	url = {https://arxiv.org/abs/1606.00061},
	urldate = {2016-06-03},
	file = {[1606.00061] Hierarchical Question-Image Co-Attention for Visual Question Answering:/home/user/Zotero/storage/VXXI7AEV/1606.html:text/html}
}

@misc{noauthor-[1606.00234]-nodate,
	title = {[1606.00234] {Two}-{Way} {Visibly} {Pushdown} {Automata} and {Transducers}},
	url = {https://arxiv.org/abs/1606.00234},
	urldate = {2016-06-03},
	file = {[1606.00234] Two-Way Visibly Pushdown Automata and Transducers:/home/user/Zotero/storage/F9PEWWRQ/1606.html:text/html}
}

@misc{noauthor-[1606.00294]-nodate,
	title = {[1606.00294] {Improved} {Parsing} for {Argument}-{Clusters} {Coordination}},
	url = {https://arxiv.org/abs/1606.00294},
	urldate = {2016-06-03},
	file = {[1606.00294] Improved Parsing for Argument-Clusters Coordination:/home/user/Zotero/storage/W2I89BMU/1606.html:text/html}
}

@misc{noauthor-[1606.00819]-nodate,
	title = {[1606.00819] {Matrix} {Factorization} using {Window} {Sampling} and {Negative} {Sampling} for {Improved} {Word} {Representations}},
	url = {https://arxiv.org/abs/1606.00819},
	urldate = {2016-06-03},
	file = {[1606.00819] Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations:/home/user/Zotero/storage/2DTQRDGQ/1606.html:text/html}
}

@misc{noauthor-[1606.00589]-nodate,
	title = {[1606.00589] {Single}-{Model} {Encoder}-{Decoder} with {Explicit} {Morphological} {Representation} for {Reinflection}},
	url = {https://arxiv.org/abs/1606.00589},
	urldate = {2016-06-03},
	file = {[1606.00589] Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection:/home/user/Zotero/storage/PJNDIIVA/1606.html:text/html}
}

@misc{noauthor-[1606.00499]-nodate,
	title = {[1606.00499] {Generalizing} and {Hybridizing} {Count}-based and {Neural} {Language} {Models}},
	url = {https://arxiv.org/abs/1606.00499},
	urldate = {2016-06-03},
	file = {[1606.00499] Generalizing and Hybridizing Count-based and Neural Language Models:/home/user/Zotero/storage/TMM449HP/1606.html:text/html}
}

@inproceedings{hara-predicting-2012,
	title = {Predicting word fixations in text with a {CRF} model for capturing general reading strategies among readers},
	url = {http://anthology.aclweb.org/W/W12/W12-49.pdf#page=65},
	urldate = {2016-04-05},
	booktitle = {Proceedings of the {First} {Workshop} on {Eye}-tracking and {Natural} {Language} {Processing}},
	author = {Hara, Tadayoshi and Kano, Daichi Mochihashi2 Yoshinobu and Aizawa, Akiko},
	year = {2012},
	pages = {55--70},
	file = {Predicting Word Fixations in Text with a CRF Model for Capturing General Reading Strategies among Readers - W12-4905:/home/user/Zotero/storage/UC5W8VWX/W12-4905.pdf:application/pdf}
}

@inproceedings{hale-probabilistic-2001,
	title = {A {Probabilistic} {Earley} {Parser} as a {Psycholinguistic} {Model}},
	volume = {2},
	booktitle = {Proceedings of {NAACL}},
	author = {Hale, John},
	year = {2001},
	pages = {159--166}
}

@inproceedings{ba-learning-2015,
	title = {Learning wake-sleep recurrent attention models},
	url = {http://papers.nips.cc/paper/5861-learning-wake-sleep-recurrent-attention-models},
	urldate = {2016-06-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ba, Jimmy and Salakhutdinov, Ruslan R. and Grosse, Roger B. and Frey, Brendan J.},
	year = {2015},
	pages = {2575--2583},
	file = {Learning Wake-Sleep Recurrent Attention Models - 5861-learning-wake-sleep-recurrent-attention-models.pdf:/home/user/Zotero/storage/WN35JIJW/5861-learning-wake-sleep-recurrent-attention-models.pdf:application/pdf}
}

@inproceedings{barrett-dundee-2015,
	title = {The {Dundee} {Treebank}},
	url = {http://bib.irb.hr/datoteka/789398.barrett2015-dundee.pdf},
	urldate = {2016-06-02},
	booktitle = {The 14th {International} {Workshop} on {Treebanks} and {Linguistic} {Theories} ({TLT} 14)},
	author = {Barrett, Maria and Agić, Željko and Søgaard, Anders},
	year = {2015},
	file = {789398.barrett2015-dundee.pdf:/home/user/Zotero/storage/44MAVQCQ/789398.barrett2015-dundee.pdf:application/pdf}
}

@article{carpenter-what-nodate,
	title = {What {Eyes} {Do} {While} {Your} {Mind} is {Reading}},
	url = {https://www.researchgate.net/publication/248420563\_What\_Eyes\_Do\_While\_Your\_Mind\_is\_Reading},
	abstract = {What Eyes Do While Your Mind is Reading on ResearchGate, the professional network for scientists.},
	urldate = {2016-06-01},
	journal = {ResearchGate},
	author = {Carpenter, P. and Just, M.},
	file = {0.pdf:/home/user/Zotero/storage/2SFTWCN4/0.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/6XIHNVT9/248420563_What_Eyes_Do_While_Your_Mind_is_Reading.html:text/html}
}

@misc{noauthor-[1602.07764]-nodate,
	title = {[1602.07764] {Reinforcement} {Learning} of {POMDPs} using {Spectral} {Methods}},
	url = {https://arxiv.org/abs/1602.07764},
	urldate = {2016-06-01},
	file = {[1602.07764] Reinforcement Learning of POMDPs using Spectral Methods:/home/user/Zotero/storage/FHCVWN6T/1602.html:text/html}
}

@misc{noauthor-[1509.08101]-nodate,
	title = {[1509.08101] {Representation} {Benefits} of {Deep} {Feedforward} {Networks}},
	url = {http://arxiv.org/abs/1509.08101},
	urldate = {2016-06-01},
	file = {[1509.08101] Representation Benefits of Deep Feedforward Networks:/home/user/Zotero/storage/W2DAUJVW/1509.html:text/html}
}

@misc{noauthor-[1605.08361]-nodate,
	title = {[1605.08361] {No} bad local minima: {Data} independent training error guarantees for multilayer neural networks},
	url = {https://arxiv.org/abs/1605.08361},
	urldate = {2016-06-01},
	file = {[1605.08361] No bad local minima\: Data independent training error guarantees for multilayer neural networks:/home/user/Zotero/storage/KKWV8SDZ/1605.html:text/html}
}

@misc{noauthor-[1603.04733]-nodate,
	title = {[1603.04733] {Structured} and {Efficient} {Variational} {Deep} {Learning} with {Matrix} {Gaussian} {Posteriors}},
	url = {https://arxiv.org/abs/1603.04733},
	urldate = {2016-06-01},
	file = {[1603.04733] Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors:/home/user/Zotero/storage/A72P5XNA/1603.html:text/html}
}

@misc{noauthor-[1603.01431]-nodate,
	title = {[1603.01431] {Normalization} {Propagation}: {A} {Parametric} {Technique} for {Removing} {Internal} {Covariate} {Shift} in {Deep} {Networks}},
	url = {https://arxiv.org/abs/1603.01431},
	urldate = {2016-06-01},
	file = {[1603.01431] Normalization Propagation\: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks:/home/user/Zotero/storage/4DQU6P75/1603.html:text/html}
}

@misc{noauthor-[1605.09081]-nodate,
	title = {[1605.09081] {Understanding} {Convolutional} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1605.09081},
	urldate = {2016-06-01},
	file = {[1605.09081] Understanding Convolutional Neural Networks:/home/user/Zotero/storage/PXB2HTBD/1605.html:text/html}
}

@misc{noauthor-[1605.09096]-nodate,
	title = {[1605.09096] {Diachronic} {Word} {Embeddings} {Reveal} {Statistical} {Laws} of {Semantic} {Change}},
	url = {https://arxiv.org/abs/1605.09096},
	urldate = {2016-06-01},
	file = {[1605.09096] Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change:/home/user/Zotero/storage/PHIIW2X4/1605.html:text/html}
}

@misc{noauthor-[1605.09090]-nodate,
	title = {[1605.09090] {Learning} {Natural} {Language} {Inference} using {Bidirectional} {LSTM} model and {Inner}-{Attention}},
	url = {https://arxiv.org/abs/1605.09090},
	urldate = {2016-06-01},
	file = {[1605.09090] Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention:/home/user/Zotero/storage/P7W37IEI/1605.html:text/html}
}

@misc{noauthor-[1605.08900]-nodate,
	title = {[1605.08900] {Aspect} {Level} {Sentiment} {Classification} with {Deep} {Memory} {Network}},
	url = {https://arxiv.org/abs/1605.08900},
	urldate = {2016-06-01},
	file = {[1605.08900] Aspect Level Sentiment Classification with Deep Memory Network:/home/user/Zotero/storage/VBDIWX6D/1605.html:text/html}
}

@misc{noauthor-[1509.05009]-nodate,
	title = {[1509.05009] {On} the {Expressive} {Power} of {Deep} {Learning}: {A} {Tensor} {Analysis}},
	url = {https://arxiv.org/abs/1509.05009},
	urldate = {2016-05-30},
	file = {[1509.05009] On the Expressive Power of Deep Learning\: A Tensor Analysis:/home/user/Zotero/storage/SS4TH6PT/1509.html:text/html}
}

@misc{noauthor-[1603.05691]-nodate,
	title = {[1603.05691] {Do} {Deep} {Convolutional} {Nets} {Really} {Need} to be {Deep} ({Or} {Even} {Convolutional})?},
	url = {https://arxiv.org/abs/1603.05691},
	urldate = {2016-05-30},
	file = {[1603.05691] Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?:/home/user/Zotero/storage/EG2C3NJN/1603.html:text/html}
}

@misc{noauthor-[1605.08491]-nodate,
	title = {[1605.08491] {Provable} {Algorithms} for {Inference} in {Topic} {Models}},
	url = {https://arxiv.org/abs/1605.08491},
	urldate = {2016-05-30},
	file = {[1605.08491] Provable Algorithms for Inference in Topic Models:/home/user/Zotero/storage/II3FIF6Q/1605.html:text/html}
}

@misc{noauthor-[1603.04351]-nodate,
	title = {[1603.04351] {Simple} and {Accurate} {Dependency} {Parsing} {Using} {Bidirectional} {LSTM} {Feature} {Representations}},
	url = {https://arxiv.org/abs/1603.04351},
	urldate = {2016-05-30},
	file = {[1603.04351] Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations:/home/user/Zotero/storage/A99JS4RN/1603.html:text/html}
}

@misc{noauthor-[1603.00375]-nodate,
	title = {[1603.00375] {Easy}-{First} {Dependency} {Parsing} with {Hierarchical} {Tree} {LSTMs}},
	url = {https://arxiv.org/abs/1603.00375},
	urldate = {2016-05-30},
	file = {[1603.00375] Easy-First Dependency Parsing with Hierarchical Tree LSTMs:/home/user/Zotero/storage/3I8EGJ7B/1603.html:text/html}
}

@article{koenig-word-2005,
	title = {Word learning},
	url = {http://www.georgiaaugusta.de/de/document/download/770ca77b7f74345ded3609ab6e1c2049.pdf/Koenig\_Woodward\_OUP.pdf},
	urldate = {2016-05-28},
	author = {Koenig, Melissa and Woodward, Amanda},
	year = {2005},
	file = {oxfordhb-9780198568971-e-037:/home/user/Zotero/storage/3F8UWCIW/oxfordhb-9780198568971-e-037.pdf:application/pdf}
}

@misc{noauthor-[1605.07079]-nodate,
	title = {[1605.07079] {Fast} {Bayesian} {Optimization} of {Machine} {Learning} {Hyperparameters} on {Large} {Datasets}},
	url = {https://arxiv.org/abs/1605.07079},
	urldate = {2016-05-30},
	file = {[1605.07079] Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets:/home/user/Zotero/storage/56ZDF3AE/1605.html:text/html}
}

@article{hochreiter-long-1997,
	title = {Long short-term memory},
	volume = {9},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	year = {1997},
	pages = {1735--1780},
	file = {Hochreiter97_lstm:/home/user/Zotero/storage/P897ADMP/Hochreiter97_lstm.pdf:application/pdf}
}

@inproceedings{mikolov-recurrent-2010,
	title = {Recurrent neural network based language model},
	booktitle = {Proceedings of {INTERSPEECH}},
	author = {Mikolov, Tom{\'a} and Karafi{\'a}t, Martin and Burget, Luk{\'a} and Èernocký, Jan and {Khudanpur, Sanjeev}},
	year = {2010}
}

@article{kutas-event-related-2007,
	title = {Event-related brain potential ({ERP}) studies of sentence processing},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA385&dq=%22and+without+much+conscious+reflection+on+the+computations+and%22+%22potentials%3B+IPSPs).+The+scalp-recorded+activity+is+the+sum+of+the+EPSPs+and%22+%22different+information+types+to+create+new+structures%E2%80%94many+processes%22+&ots=aURCaPKNwm&sig=l\_B4Oeq60QMKwJjcXU0QZk7mQIE},
	urldate = {2016-05-28},
	journal = {The Oxford Handbook of Psycholinguistics},
	author = {Kutas, Marta and Federmeier, Kara D.},
	year = {2007},
	pages = {385},
	file = {41605200512.pdf:/home/user/Zotero/storage/GJHJXEH3/41605200512.pdf:application/pdf}
}

@book{rice-genetics-2007,
	title = {Genetics of language disorders},
	url = {http://www.academia.edu/download/30850611/OxRiceSmolik.pdf},
	urldate = {2016-05-28},
	publisher = {Hillsdale, New Jersey: Erlbaum},
	author = {Rice, Mabel L. and Smolik, Filip},
	year = {2007},
	file = {oxfordhb-9780198568971-e-042:/home/user/Zotero/storage/4B4CKFC6/oxfordhb-9780198568971-e-042.pdf:application/pdf}
}

@article{glenberg-language-2007,
	title = {Language and action: creating sensible combinations of ideas},
	shorttitle = {Language and action},
	url = {http://psychology-dev.clas.asu.edu/sites/default/files/Language%26ActionChap21.pdf},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Glenberg, Arthur M.},
	year = {2007},
	pages = {361--370},
	file = {oxfordhb-9780198568971-e-021:/home/user/Zotero/storage/9XDXBDEP/oxfordhb-9780198568971-e-021.pdf:application/pdf}
}

@article{kutas-processing-2007,
	title = {{PROCESSING} language is one ofthe major integrativeacts excels, asit},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA385&dq=%22of+the+(human)+brain+that+have+been+implicated+in+language+processing+(Binder%22+%22patterns+of+biological+activity+(such+as+ion+flow+across+neural+membranes)%22+%22to+the+currents+at+the+receiving+end+of+neurons,+i.e.%22+&ots=aURCaPKPph&sig=eEPuAnsoXeoVUK1CINZtoA50Pgc},
	urldate = {2016-05-28},
	journal = {The Oxford Handbook of Psycholinguistics},
	author = {Kutas, Marta and Federmeier, Kara D.},
	year = {2007},
	pages = {385},
	file = {oxfordhb-9780198568971-e-023:/home/user/Zotero/storage/3KJU7MUF/oxfordhb-9780198568971-e-023.pdf:application/pdf}
}

@article{bornkessel-schlesewsky-neuroimaging-2007,
	title = {Neuroimaging studies of sentence and discourse comprehension},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA407&dq=%22have+taken+on+a+new+dimension.+First,+neuroimaging+techniques+allow+for%22+%22processing+and,+thereby,+to+dissociate+the+neuroanatomical+correlates%22+%22thought+to+be+responsible+for+language+production+(i.e.+%E2%80%9CBroca%27s+area%E2%80%9D+in%22+&ots=aURCaPKPpe&sig=sFZzRVk3LvqSzx76y8s9c0lEFUE},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Bornkessel-Schlesewsky, Ina and Friederici, Angela D.},
	year = {2007},
	pages = {407--424},
	file = {oxfordhb-9780198568971-e-024:/home/user/Zotero/storage/4ZFA6KCA/oxfordhb-9780198568971-e-024.pdf:application/pdf}
}

@article{martin-sentence-level-2007,
	title = {Sentence-level deficits in aphasia},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA425&dq=%22deficits+in%22+%22on+syntax-first+vs.+constraint-based+sentence+processing+theories.%22+%22order+to+account+for+sentence+production+and+comprehension+patterns%22+%22syntactic+knowledge.+Although+evidence+for+independence+of+these%22+&ots=aURCaPKPok&sig=x0Ux2XyCm\_5\_0FBVhfby8ikQfB0},
	urldate = {2016-05-28},
	journal = {The Oxford Handbook of Psycholinguistics},
	author = {Martin, Randi C. and Vuong, Loan C. and Crowther, Jason E.},
	year = {2007},
	pages = {425},
	file = {oxfordhb-9780198568971-e-025:/home/user/Zotero/storage/NFWFCA6A/oxfordhb-9780198568971-e-025.pdf:application/pdf}
}

@article{garrod-alignment-2007,
	title = {Alignment in dialogue},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA443&dq=%22this+chapter,+we+argue+that+psycholinguists+need+to+think+in+a+different+way%22+%22simply+assume+that+interlocutors+seek+to+align+their+mental+states,+just+as+we%22+&ots=aURCaPKOxn&sig=iINZOB4xwsG3N18FAaXgL2tEhVE},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Garrod, Simon and Pickering, Martin J.},
	year = {2007},
	pages = {443--451},
	file = {oxfordhb-9780198568971-e-026:/home/user/Zotero/storage/9BIPBDJI/oxfordhb-9780198568971-e-026.pdf:application/pdf}
}

@article{ferreira-at-2007,
	title = {{AT} the heart of the faculty of language are the processes of grammatical encoding.},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA453&dq=%22encoding+carries+out+its+duties+is+to+understand+a+significant+part+of+the%22+%22or+stages,+the+first+involving+selection+and+the+second+involving+retrieval.%22+%22in+the+coming+years,+largely+due+to+the+confluence+of+their+central%22+&ots=aURCaPKOxg&sig=1G7cYM6JmwLClD0NSbqccPpSuNM},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Ferreira, Victor S. and Slevc, L. Robert},
	year = {2007},
	pages = {453},
	file = {oxfordhb-9780198568971-e-027:/home/user/Zotero/storage/6D9JNWKJ/oxfordhb-9780198568971-e-027.pdf:application/pdf}
}

@article{van-gompel-syntactic-2007,
	title = {Syntactic parsing},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA289&dq=%22syntactic+structures+during+language+comprehension+are+commonly%22+%22For+example,+many+experiments+have+shown+that+readers+slow+down+in%22+%22difficulty+because+by+the+lawyer+rules+out+this+analysis.+The+difficulty+that%22+&ots=aURCaPKOvh&sig=ejV7rVbFm875AhWFalOTCxWgmhk},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Van Gompel, Roger PG and Pickering, Martin J.},
	year = {2007},
	pages = {289--307},
	file = {oxfordhb-9780198568971-e-017:/home/user/Zotero/storage/TUIBWPEP/oxfordhb-9780198568971-e-017.pdf:application/pdf}
}

@article{port-problem-2007,
	title = {The problem of speech patterns in time},
	url = {http://www.cs.indiana.edu/~port/pap/Port.sp.patts.in.time.Gaskell-Chap30.pdf},
	urldate = {2016-05-28},
	journal = {Gaskell, Gareth M.(Hg.): The Oxford Handbook of Psycholinguistics},
	author = {Port, Robert},
	year = {2007},
	pages = {503--514},
	file = {oxfordhb-9780198568971-e-030:/home/user/Zotero/storage/NSSUE9KR/oxfordhb-9780198568971-e-030.pdf:application/pdf}
}

@article{goldrick-connectionist-2007,
	title = {Connectionist principles in theories of speech production},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780198568971.001.0001/oxfordhb-9780198568971-e-031},
	urldate = {2016-05-28},
	author = {Goldrick, Matthew},
	year = {2007},
	file = {oxfordhb-9780198568971-e-031:/home/user/Zotero/storage/BV7DBSQX/oxfordhb-9780198568971-e-031.pdf:application/pdf}
}

@article{goldrick-connectionist-2007-1,
	title = {Connectionist principles in theories of speech production},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780198568971.001.0001/oxfordhb-9780198568971-e-031},
	urldate = {2016-05-28},
	author = {Goldrick, Matthew},
	year = {2007},
	file = {oxfordhb-9780198568971-e-031:/home/user/Zotero/storage/T4XVRU52/oxfordhb-9780198568971-e-031.pdf:application/pdf}
}

@misc{noauthor-[1602.05473]-nodate,
	title = {[1602.05473] {Auxiliary} {Deep} {Generative} {Models}},
	url = {https://arxiv.org/abs/1602.05473},
	urldate = {2016-05-28},
	file = {[1602.05473] Auxiliary Deep Generative Models:/home/user/Zotero/storage/EMWTQQRD/1602.html:text/html}
}

@misc{noauthor-[1505.07818]-nodate,
	title = {[1505.07818] {Domain}-{Adversarial} {Training} of {Neural} {Networks}},
	url = {https://arxiv.org/abs/1505.07818},
	urldate = {2016-05-28},
	file = {[1505.07818] Domain-Adversarial Training of Neural Networks:/home/user/Zotero/storage/BSFVF6J7/1505.html:text/html}
}

@article{xu-concept-2007,
	title = {Concept formation and language development: {Count} nouns and object kinds},
	shorttitle = {Concept formation and language development},
	url = {https://www.researchgate.net/profile/Fei\_Xu18/publication/228770218\_Concept\_Formation\_and\_Language\_Development\_Count\_Nouns\_and\_Object\_Kinds/links/0046352ef9849f2b4b000000.pdf},
	urldate = {2016-05-28},
	journal = {Oxford handbook of psycholinguistics},
	author = {Xu, Fei},
	year = {2007},
	pages = {627--634},
	file = {oxfordhb-9780198568971-e-038:/home/user/Zotero/storage/FIIGBUQJ/oxfordhb-9780198568971-e-038.pdf:application/pdf}
}

@article{treiman-learning-2007,
	title = {Learning to read},
	url = {http://128.252.27.189/TreimanGaskellLearnRead/TreimanGaskellLearnRead.pdf},
	urldate = {2016-05-28},
	journal = {Oxford handbook of psycholinguistics},
	author = {Treiman, Rebecca and Kessler, Brett},
	year = {2007},
	pages = {657--666},
	file = {oxfordhb-9780198568971-e-040:/home/user/Zotero/storage/2GF5E8NI/oxfordhb-9780198568971-e-040.pdf:application/pdf}
}

@inproceedings{snoek-practical-2012,
	title = {Practical bayesian optimization of machine learning algorithms},
	url = {http://papers.nips.cc/paper/4522-practical},
	urldate = {2016-05-28},
	booktitle = {Advances in neural information processing systems},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	year = {2012},
	pages = {2951--2959},
	file = {4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf:/home/user/Zotero/storage/RJEMDBCN/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf:application/pdf}
}

@incollection{miikkulainen-topology-2011,
	title = {Topology of a {Neural} {Network}},
	copyright = {©2010 Springer Science+Business Media, LLC},
	isbn = {978-0-387-30768-8 978-0-387-30164-8},
	url = {http://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8\_837},
	language = {en},
	urldate = {2016-03-29},
	booktitle = {Encyclopedia of {Machine} {Learning}},
	publisher = {Springer US},
	author = {Miikkulainen, Risto},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2011},
	doi = {10.1007/978-0-387-30164-8\_837},
	keywords = {Artificial Intelligence (incl. Robotics), Computing Methodologies, Data Mining and Knowledge Discovery, Pattern Recognition, Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences},
	pages = {988--989},
	file = {Miikkulainen_2011_Topology of a Neural Network.pdf:/home/user/Zotero/storage/Q9DQFSDD/Miikkulainen_2011_Topology of a Neural Network.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/58QARVIG/10.html:text/html}
}

@article{kingma-adam:-2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2016-03-29},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2PA2S2KW/1412.html:text/html;Kingma_Ba_2014_Adam.pdf:/home/user/Zotero/storage/3U3ZXH6U/Kingma_Ba_2014_Adam.pdf:application/pdf}
}

@article{shaham-provable-2015,
	title = {Provable approximation properties for deep neural networks},
	url = {http://arxiv.org/abs/1509.07385},
	abstract = {We discuss approximation of functions using deep neural nets. Given a function \$f\$ on a \$d\$-dimensional manifold \$\Gamma \subset \mathbb{R}{\textasciicircum}m\$, we construct a sparsely-connected depth-4 neural network and bound its error in approximating \$f\$. The size of the network depends on dimension and curvature of the manifold \$\Gamma\$, the complexity of \$f\$, in terms of its wavelet description, and only weakly on the ambient dimension \$m\$. Essentially, our network computes wavelet functions, which are computed from Rectified Linear Units (ReLU)},
	urldate = {2016-03-29},
	journal = {arXiv:1509.07385 [cs, stat]},
	author = {Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R.},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.07385},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NBBGBIR9/1509.html:text/html;Shaham et al_2015_Provable approximation properties for deep neural networks.pdf:/home/user/Zotero/storage/T84F7AQS/Shaham et al_2015_Provable approximation properties for deep neural networks.pdf:application/pdf}
}

@article{gehrke-schutzenberger-2016,
	title = {The {Sch}\"utzenberger product for syntactic spaces},
	url = {http://arxiv.org/abs/1603.08264},
	abstract = {Starting from Boolean algebras of languages closed under quotients and using duality theoretic insights, we derive the notion of Boolean spaces with internal monoids as recognisers for arbitrary formal languages of finite words over finite alphabets. This leads to a setting that is well-suited for applying existing tools from Stone duality as applied in semantics. The main focus of the paper is the development of topo-algebraic constructions pertinent to the treatment of languages given by logic formulas. In particular, using the standard semantic view of quantification as projection, we derive a notion of Sch\"utzenberger product for Boolean spaces with internal monoids. This makes heavy use of the Vietoris construction, and its dual functor, which is central to the coalgebraic treatment of classical modal logic. We show that the unary Sch\"utzenberger product for spaces, when applied to a recogniser for the language associated to a formula with a free first-order variable, yields a recogniser for the language of all models of the corresponding existentially quantified formula. Further, we generalise global and local versions of the theorems of Sch\"utzenberger and Reutenauer characterising the languages recognised by the binary Sch\"utzenberger product. Finally, we provide an equational characterisation of Boolean algebras obtained by local Sch\"utzenberger product with the one element space based on an Egli-Milner type condition on generalised factorisations of ultrafilters on words.},
	urldate = {2016-03-29},
	journal = {arXiv:1603.08264 [cs, math]},
	author = {Gehrke, Mai and Petrisan, Daniela and Reggio, Luca},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08264},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, F.1.1, F.4.1, F.4.3, Mathematics - General Topology, Mathematics - Logic},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9ERM2NSS/1603.html:text/html;Gehrke et al_2016_The Sch-utzenberger product for syntactic spaces.pdf:/home/user/Zotero/storage/NXCKIZK2/Gehrke et al_2016_The Sch-utzenberger product for syntactic spaces.pdf:application/pdf}
}

@article{gulcehre-pointing-2016,
	title = {Pointing the {Unknown} {Words}},
	url = {http://arxiv.org/abs/1603.08148},
	abstract = {The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional count based models and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.},
	urldate = {2016-03-29},
	journal = {arXiv:1603.08148 [cs]},
	author = {Gulcehre, Caglar and Ahn, Sungjin and Nallapati, Ramesh and Zhou, Bowen and Bengio, Yoshua},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08148},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BAB6MVZN/1603.html:text/html;Gulcehre et al_2016_Pointing the Unknown Words.pdf:/home/user/Zotero/storage/FVHUZA44/Gulcehre et al_2016_Pointing the Unknown Words.pdf:application/pdf}
}

@article{prabhavalkar-compression-2016,
	title = {On the {Compression} of {Recurrent} {Neural} {Networks} with an {Application} to {LVCSR} acoustic modeling for {Embedded} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1603.08042},
	abstract = {We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.},
	urldate = {2016-03-29},
	journal = {arXiv:1603.08042 [cs]},
	author = {Prabhavalkar, Rohit and Alsharif, Ouais and Bruguier, Antoine and McGraw, Ian},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08042},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RRE9CAXE/1603.html:text/html;Prabhavalkar et al_2016_On the Compression of Recurrent Neural Networks with an Application to LVCSR.pdf:/home/user/Zotero/storage/GEH27IC8/Prabhavalkar et al_2016_On the Compression of Recurrent Neural Networks with an Application to LVCSR.pdf:application/pdf}
}

@article{coke-classifying-2016,
	title = {Classifying {Syntactic} {Regularities} for {Hundreds} of {Languages}},
	url = {http://arxiv.org/abs/1603.08016},
	abstract = {This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label pre- diction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family.},
	urldate = {2016-03-29},
	journal = {arXiv:1603.08016 [cs]},
	author = {Coke, Reed and King, Ben and Radev, Dragomir},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08016},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UMXAZNNN/1603.html:text/html;Coke et al_2016_Classifying Syntactic Regularities for Hundreds of Languages.pdf:/home/user/Zotero/storage/E82FUVW3/Coke et al_2016_Classifying Syntactic Regularities for Hundreds of Languages.pdf:application/pdf}
}

@article{gruter-manuscript-nodate,
	title = {Manuscript accepted for publication in {Linguistic} {Approaches} to {Bilingualism}},
	url = {http://theresgruter.homestead.com/GruterRohdeSchafer\_inpress\_LAB.pdf},
	urldate = {2016-03-28},
	author = {Grüter, Theres and Rohde, Hannah and Schafer, Amy J.},
	file = {Microsoft Word - GruterRohdeSchafer_inpress_LAB.docx - GruterRohdeSchafer.2015.pdf:/home/user/Zotero/storage/673NBF7E/GruterRohdeSchafer.2015.pdf:application/pdf}
}

@article{rohde-markers-2014,
	title = {Markers of {Topical} {Discourse} in {Child}-{Directed} {Speech}},
	volume = {38},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12121/full},
	number = {8},
	urldate = {2016-03-28},
	journal = {Cognitive science},
	author = {Rohde, Hannah and Frank, Michael C.},
	year = {2014},
	pages = {1634--1661},
	file = {RohdeFrank.inpress.pdf:/home/user/Zotero/storage/B9VKZ6QK/RohdeFrank.inpress.pdf:application/pdf}
}

@article{rohde-grammatical-2014,
	title = {Grammatical and information-structural influences on pronoun production},
	volume = {29},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2013.854918},
	number = {8},
	urldate = {2016-03-28},
	journal = {Language, Cognition and Neuroscience},
	author = {Rohde, Hannah and Kehler, Andrew},
	year = {2014},
	pages = {912--927},
	file = {RohdeKehler.2014.pdf:/home/user/Zotero/storage/ZETGHMQB/RohdeKehler.2014.pdf:application/pdf}
}

@article{kehler-probabilistic-2013,
	title = {A probabilistic reconciliation of coherence-driven and centering-driven theories of pronoun interpretation},
	volume = {39},
	url = {http://www.degruyter.com/view/j/thli.2013.39.issue-1-2/tl-2013-0001/tl-2013-0001.xml},
	number = {1-2},
	urldate = {2016-03-28},
	journal = {Theoretical Linguistics},
	author = {Kehler, Andrew and Rohde, Hannah},
	year = {2013},
	pages = {1--37},
	file = {A Probabilistic Reconciliation of Coherence-Driven and Centering-Driven Theories of Pronoun Interpretation - KehlerRohde.2013.pdf:/home/user/Zotero/storage/D9KKK72M/KehlerRohde.2013.pdf:application/pdf}
}

@article{rohde-integration-2012,
	title = {Integration of pragmatic and phonetic cues in spoken word recognition.},
	volume = {38},
	url = {http://psycnet.apa.org/journals/xlm/38/4/967/},
	number = {4},
	urldate = {2016-03-28},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Rohde, Hannah and Ettlinger, Marc},
	year = {2012},
	pages = {967},
	file = {final - RohdeEttlinger.2012.pdf:/home/user/Zotero/storage/RFZUFGVI/RohdeEttlinger.2012.pdf:application/pdf}
}

@article{rohde-anticipating-2011,
	title = {Anticipating explanations in relative clause processing},
	volume = {118},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027710002532},
	number = {3},
	urldate = {2016-03-28},
	journal = {Cognition},
	author = {Rohde, Hannah and Levy, Roger and Kehler, Andrew},
	year = {2011},
	pages = {339--358},
	file = {RohdeLevyKehler.2011.pdf:/home/user/Zotero/storage/8RCDJ28A/RohdeLevyKehler.2011.pdf:application/pdf}
}

@article{kehler-coherence-2008,
	title = {Coherence and coreference revisited},
	volume = {25},
	url = {https://jos.oxfordjournals.org/content/25/1/1.full},
	number = {1},
	urldate = {2016-03-28},
	journal = {Journal of Semantics},
	author = {Kehler, Andrew and Kertz, Laura and Rohde, Hannah and Elman, Jeffrey L.},
	year = {2008},
	pages = {1--44},
	file = {KehlerKertzRohdeElman.2008.pdf:/home/user/Zotero/storage/MMT65RMZ/KehlerKertzRohdeElman.2008.pdf:application/pdf}
}

@article{stevens-modeling-2015,
	title = {Modeling {Referential} {Coordination} as a {Particle} {Swarm} {Optimization} {Task}},
	url = {http://flov.gu.se/digitalAssets/1537/1537281\_semdial2015\_godial\_proceedings.pdf#page=217},
	urldate = {2016-03-28},
	journal = {SEMDIAL 2015 goDIAL},
	author = {Stevens, H. Chase and Rohde, Hannah},
	year = {2015},
	pages = {210},
	file = {1537599_semdial2015_godial_proceedings - StevensRohde.2015.pdf:/home/user/Zotero/storage/4INPZQXU/StevensRohde.2015.pdf:application/pdf}
}

@article{clarke-wheres-2013,
	title = {Where's {Wally}: the influence of visual salience on referring expression generation},
	volume = {4},
	shorttitle = {Where's {Wally}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00329/abstract},
	doi = {10.3389/fpsyg.2013.00329},
	abstract = {Referring expression generation (REG) presents the converse problem to visual search: given a scene and a specified target, how does one generate a description which would allow somebody else to quickly and accurately locate the target?Previous work in psycholinguistics and natural language processing has failed to find an important and integrated role for vision in this task. That previous work, which relies largely on simple scenes, tends to treat vision as a pre-process for extracting feature categories that are relevant to disambiguation. However, the visual search literature suggests that some descriptions are better than others at enabling listeners to search efficiently within complex stimuli. This paper presents a study testing whether participants are sensitive to visual features that allow them to compose such “good” descriptions. Our results show that visual properties (salience, clutter, area, and distance) influence REG for targets embedded in images from the Where's Wally? books. Referring expressions for large targets are shorter than those for smaller targets, and expressions about targets in highly cluttered scenes use more words. We also find that participants are more likely to mention non-target landmarks that are large, salient, and in close proximity to the target. These findings identify a key role for visual salience in language production decisions and highlight the importance of scene complexity for REG.},
	urldate = {2016-03-28},
	journal = {Perception Science},
	author = {Clarke, Alasdair Daniel Francis and Elsner, Micha and Rohde, Hannah},
	year = {2013},
	keywords = {referring expression generation, visual clutter, visual salience},
	pages = {329}
}

@article{cummins-evoking-2015,
	title = {Evoking {Context} with {Contrastive} {Stress}: {Effects} on {Pragmatic} {Enrichment}},
	shorttitle = {Evoking {Context} with {Contrastive} {Stress}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01779/full},
	doi = {10.3389/fpsyg.2015.01779},
	abstract = {Although it is widely acknowledged that context influences a variety of pragmatic phenomena, it is not clear how best to articulate this notion of context and thereby explain the nature of its influence. In this paper, we target contextual alternatives that are evoked via focus placement and test how the same contextual manipulation can influence three different phenomena that involve pragmatic enrichment: scalar implicature, presupposition, and coreference. We argue that focus placement influences these three phenomena indirectly by providing the listener with information about the likely question under discussion (QUD) that a particular utterance answers (Roberts, 1996/2012). In three listening experiments, we find that the predicted interpretations are indeed made more available when focus placement is added to the final element (to the scalar adjective, to an entity embedded under the negated presupposition trigger, and to the predicate of a pronoun). These findings bring together several distinct strands of work on the effect of focus placement on interpretation all in the domain of pragmatic enrichment. Together they advance our empirical understanding of the relation between focus placement and QUD and highlight commonalities between implicature, presupposition, and coreference.},
	urldate = {2016-03-28},
	journal = {Language Sciences},
	author = {Cummins, Chris and Rohde, Hannah},
	year = {2015},
	keywords = {scalar implicature, coreference, focus placement, presupposition projection, question under discussion (QUD)},
	pages = {1779}
}

@article{clarke-giving-2015,
	title = {Giving {Good} {Directions}: {Order} of {Mention} {Reflects} {Visual} {Salience}},
	shorttitle = {Giving {Good} {Directions}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01793/full},
	doi = {10.3389/fpsyg.2015.01793},
	abstract = {In complex stimuli, there are many different possible ways to refer to a specified target. Previous studies have shown that when people are faced with such a task, the content of their referring expression reflects visual properties such as size, salience, and clutter. Here, we extend these findings and present evidence that (i) the influence of visual perception on sentence construction goes beyond content selection and in part determines the order in which different objects are mentioned and (ii) order of mention influences comprehension. Study 1 (a corpus study of reference productions) shows that when a speaker uses a relational description to mention a salient object, that object is treated as being in the common ground and is more likely to be mentioned first. Study 2 (a visual search study) asks participants to listen to referring expressions and find the specified target; in keeping with the above result, we find that search for easy-to-find targets is faster when the target is mentioned first, while search for harder-to-find targets is facilitated by mentioning the target later, after a landmark in a relational description. Our findings show that seemingly low-level and disparate mental “modules” like perception and sentence planning interact at a high level and in task-dependent ways.},
	urldate = {2016-03-28},
	journal = {Language Sciences},
	author = {Clarke, Alasdair D. F. and Elsner, Micha and Rohde, Hannah},
	year = {2015},
	keywords = {visual salience, referring expressions, visual search},
	pages = {1793}
}

@article{hale-information-theoretical-2016,
	title = {Information-theoretical complexity metrics},
	url = {https://courses.cit.cornell.edu/jth99/review\_v4.pdf},
	urldate = {2016-03-28},
	author = {Hale, John},
	year = {2016},
	file = {review_v5.pdf:/home/user/Zotero/storage/4ASSTMPC/review_v5.pdf:application/pdf}
}

@article{hale-modeling-2015,
	title = {Modeling {fMRI} time courses with linguistic structure at various grain sizes},
	url = {http://www.aclweb.org/anthology/W/W15/W15-11.pdf#page=99},
	urldate = {2016-03-28},
	journal = {Proceedings of CMCL},
	author = {Hale, John T. and Lutz, David E. and Luh, Wen-Ming and Brennan, Jonathan R.},
	year = {2015},
	pages = {89--97},
	file = {Modeling fMRI time courses with linguistic structure at various grain sizes - W15-1110.pdf:/home/user/Zotero/storage/6P7K63VM/W15-1110.pdf:application/pdf}
}

@inproceedings{sutskever-importance-2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013\_sutskever13},
	urldate = {2016-03-28},
	booktitle = {Proceedings of the 30th international conference on machine learning ({ICML}-13)},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	year = {2013},
	pages = {1139--1147},
	file = {On the importance of initialization and momentum in deep learning - 1051_2.pdf:/home/user/Zotero/storage/DPT6976W/1051_2.pdf:application/pdf}
}

@article{germain-made:-2015,
	title = {{MADE}: {Masked} {Autoencoder} for {Distribution} {Estimation}},
	shorttitle = {{MADE}},
	url = {http://arxiv.org/abs/1502.03509},
	urldate = {2016-03-28},
	journal = {arXiv preprint arXiv:1502.03509},
	author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	year = {2015},
	file = {MADE\: Masked Autoencoder for Distribution Estimation - made.pdf:/home/user/Zotero/storage/7XVTSAEE/made.pdf:application/pdf}
}

@article{macke-estimation-2013,
	title = {Estimation {Bias} in {Maximum} {Entropy} {Models}},
	volume = {15},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/15/8/3109/},
	doi = {10.3390/e15083109},
	language = {en},
	number = {8},
	urldate = {2016-03-28},
	journal = {Entropy},
	author = {Macke, Jakob and Murray, Iain and Latham, Peter},
	month = aug,
	year = {2013},
	pages = {3109--3129},
	file = {Estimation Bias in Maximum Entropy Models - maxent.pdf:/home/user/Zotero/storage/JHEXJCZB/maxent.pdf:application/pdf}
}

@inproceedings{gutmann-estimation-2013,
	title = {Estimation of unnormalized statistical models without numerical integration},
	url = {http://www.me.inf.kyushu-u.ac.jp/witmse2013/proceeding\_files/Th1.pdf},
	urldate = {2016-03-28},
	booktitle = {Proc {Workshop} on {Information} {Theoretic} {Methods} in {Science} and {Engineering}},
	author = {Gutmann, Michael U. and Hyvärinen, Aapo},
	year = {2013},
	file = {paper.dvi - Gutmann2013b.pdf:/home/user/Zotero/storage/K4BWIP3T/Gutmann2013b.pdf:application/pdf}
}

@article{gutmann-three-layer-2013,
	title = {A three-layer model of natural image statistics},
	volume = {107},
	issn = {09284257},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0928425713000028},
	doi = {10.1016/j.jphysparis.2013.01.001},
	language = {en},
	number = {5},
	urldate = {2016-03-28},
	journal = {Journal of Physiology-Paris},
	author = {Gutmann, Michael U. and Hyvärinen, Aapo},
	month = nov,
	year = {2013},
	pages = {369--398},
	file = {paper.dvi - Gutmann2013.pdf:/home/user/Zotero/storage/NSHN8KRD/Gutmann2013.pdf:application/pdf}
}

@article{kingma-variational-2015,
	title = {Variational dropout and the local reparameterization trick},
	url = {http://arxiv.org/abs/1506.02557},
	urldate = {2016-03-28},
	journal = {arXiv preprint arXiv:1506.02557},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	year = {2015},
	file = {VariationalDropout.pdf:/home/user/Zotero/storage/95VVI4IF/VariationalDropout.pdf:application/pdf}
}

@article{li-scalable-2015,
	title = {Scalable {MCMC} for {Mixed} {Membership} {Stochastic} {Blockmodels}},
	url = {http://arxiv.org/abs/1510.04815},
	urldate = {2016-03-28},
	journal = {arXiv preprint arXiv:1510.04815},
	author = {Li, Wenzhe and Ahn, Sungjin and Welling, Max},
	year = {2015},
	file = {() - SGLD_MMSB.pdf:/home/user/Zotero/storage/E6MVR2NC/SGLD_MMSB.pdf:application/pdf}
}

@article{gutmann-statistical-2014,
	title = {Statistical {Inference} of {Intractable} {Generative} {Models} via {Classification}},
	url = {http://arxiv.org/abs/1407.4981},
	abstract = {Increasingly complex generative models are being used across the disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to perform likelihood-based statistical inference. We consider here a likelihood-free framework where inference is done by identifying parameter values which generate simulated data adequately resembling the observed data. A major difficulty is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models.},
	urldate = {2016-03-28},
	journal = {arXiv:1407.4981 [stat]},
	author = {Gutmann, Michael U. and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.4981},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Q5HGC4XS/1407.html:text/html;Gutmann et al_2014_Statistical Inference of Intractable Generative Models via Classification.pdf:/home/user/Zotero/storage/E9VE82XX/Gutmann et al_2014_Statistical Inference of Intractable Generative Models via Classification.pdf:application/pdf}
}

@article{gutmann-bayesian-2015,
	title = {Bayesian {Optimization} for {Likelihood}-{Free} {Inference} of {Simulator}-{Based} {Statistical} {Models}},
	url = {http://arxiv.org/abs/1501.03291},
	abstract = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
	urldate = {2016-03-28},
	journal = {arXiv:1501.03291 [stat]},
	author = {Gutmann, Michael U. and Corander, Jukka},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.03291},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IF8989D3/1501.html:text/html;Gutmann_Corander_2015_Bayesian Optimization for Likelihood-Free Inference of Simulator-Based.pdf:/home/user/Zotero/storage/EEZEIIKG/Gutmann_Corander_2015_Bayesian Optimization for Likelihood-Free Inference of Simulator-Based.pdf:application/pdf}
}

@article{chalk-attention-2013,
	title = {Attention as {Reward}-{Driven} {Optimization} of {Sensory} {Processing}},
	volume = {25},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/NECO\_a\_00494},
	doi = {10.1162/NECO\_a\_00494},
	abstract = {Attention causes diverse changes to visual neuron responses, including alterations in receptive field structure, and firing rates. A common theoretical approach to investigate why sensory neurons behave as they do is based on the efficient coding hypothesis: that sensory processing is optimized toward the statistics of the received input. We extend this approach to account for the influence of task demands, hypothesizing that the brain learns a probabilistic model of both the sensory input and reward received for performing different actions. Attention-dependent changes to neural responses reflect optimization of this internal model to deal with changes in the sensory environment (stimulus statistics) and behavioral demands (reward statistics). We use this framework to construct a simple model of visual processing that is able to replicate a number of attention-dependent changes to the responses of neurons in the midlevel visual cortices. The model is consistent with and provides a normative explanation for recent divisive normalization models of attention (Reynolds \& Heeger, 2009).},
	number = {11},
	urldate = {2016-03-28},
	journal = {Neural Computation},
	author = {Chalk, Matthew and Murray, Iain and Seriès, Peggy},
	month = jun,
	year = {2013},
	pages = {2904--2933},
	file = {Chalk et al_2013_Attention as Reward-Driven Optimization of Sensory Processing.pdf:/home/user/Zotero/storage/WZP95IVF/Chalk et al_2013_Attention as Reward-Driven Optimization of Sensory Processing.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/JINBJ7CP/NECO_a_00494.html:text/html}
}

@article{srivastava-clustering-2016,
	title = {Clustering with a {Reject} {Option}: {Interactive} {Clustering} as {Bayesian} {Prior} {Elicitation}},
	shorttitle = {Clustering with a {Reject} {Option}},
	url = {http://arxiv.org/abs/1602.06886},
	abstract = {A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when they see one. We present a new approach to interactive clustering for data exploration called TINDER, based on a particularly simple feedback mechanism, in which an analyst can reject a given clustering and request a new one, which is chosen to be different from the previous clustering while fitting the data well. We formalize this interaction in a Bayesian framework as a method for prior elicitation, in which each different clustering is produced by a prior distribution that is modified to discourage previously rejected clusterings. We show that TINDER successfully produces a diverse set of clusterings, each of equivalent quality, that are much more diverse than would be obtained by randomized restarts.},
	urldate = {2016-03-28},
	journal = {arXiv:1602.06886 [cs, stat]},
	author = {Srivastava, Akash and Zou, James and Adams, Ryan P. and Sutton, Charles},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.06886},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/8ZSBJP32/1602.html:text/html;Srivastava et al_2016_Clustering with a Reject Option.pdf:/home/user/Zotero/storage/6M7V9QSI/Srivastava et al_2016_Clustering with a Reject Option.pdf:application/pdf}
}

@inproceedings{matthies-blinkers-2013,
	title = {With {Blinkers} on: {Robust} {Prediction} of {Eye} {Movements} across {Readers}.},
	shorttitle = {With {Blinkers} on},
	url = {http://www.aclweb.org/website/old\_anthology/D/D13/D13-1075.pdf},
	urldate = {2016-03-28},
	booktitle = {{EMNLP}},
	author = {Matthies, Franz and Søgaard, Anders},
	year = {2013},
	pages = {803--807},
	file = {0.pdf:/home/user/Zotero/storage/QGIBJI9P/0.pdf:application/pdf}
}

@misc{noauthor-nadam-nodate,
	title = {nadam},
	url = {http://cs229.stanford.edu/proj2015/054\_report.pdf},
	urldate = {2016-03-27},
	file = {054_report.pdf:/home/user/Zotero/storage/HMC72WE2/054_report.pdf:application/pdf}
}

@article{howes-predicting-2015,
	title = {Predicting {Short}-{Term} {Remembering} as {Boundedly} {Optimal} {Strategy} {Choice}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12271/full},
	urldate = {2016-03-27},
	journal = {Cognitive science},
	author = {Howes, Andrew and Duggan, Geoffrey B. and Kalidindi, Kiran and Tseng, Yuan-Chi and Lewis, Richard L.},
	year = {2015},
	file = {Howes-2015-EmailCal - howes-duggan-kalidindi-tseng-2015-cogsci.pdf:/home/user/Zotero/storage/HKNNM68E/howes-duggan-kalidindi-tseng-2015-cogsci.pdf:application/pdf}
}

@inproceedings{jiang-dependence-2015,
	title = {The dependence of effective planning horizon on model accuracy},
	url = {http://dl.acm.org/citation.cfm?id=2773300},
	urldate = {2016-03-27},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
	year = {2015},
	pages = {1181--1189},
	file = {jiang-et-al-2015-aamas.pdf:/home/user/Zotero/storage/ED5V3X9K/jiang-et-al-2015-aamas.pdf:application/pdf}
}

@inproceedings{oh-action-conditional-2015,
	title = {Action-conditional video prediction using deep networks in atari games},
	url = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games},
	urldate = {2016-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L. and Singh, Satinder},
	year = {2015},
	pages = {2845--2853},
	file = {oh-et-al-2015-NIPS.pdf:/home/user/Zotero/storage/CZR4SK5I/oh-et-al-2015-NIPS.pdf:application/pdf}
}

@inproceedings{guo-deep-2014,
	title = {Deep learning for real-time {Atari} game play using offline {Monte}-{Carlo} tree search planning},
	url = {http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging},
	urldate = {2016-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard L. and Wang, Xiaoshi},
	year = {2014},
	pages = {3338--3346},
	file = {guo-singh-lee-lewis-wang-2014-nips.pdf:/home/user/Zotero/storage/WCKN7V6I/guo-singh-lee-lewis-wang-2014-nips.pdf:application/pdf}
}

@article{howes-utility-2014,
	title = {Utility {Maximization} and {Bounds} on {Human} {Information} {Processing}},
	volume = {6},
	issn = {17568757},
	url = {http://doi.wiley.com/10.1111/tops.12089},
	doi = {10.1111/tops.12089},
	language = {en},
	number = {2},
	urldate = {2016-03-27},
	journal = {Topics in Cognitive Science},
	author = {Howes, Andrew and Lewis, Richard L. and Singh, Satinder},
	month = apr,
	year = {2014},
	pages = {198--203},
	file = {REV_ISS_WEB_TOPS_12089_6-2 198..203 - howes-lewis-singh-2014-topics.pdf:/home/user/Zotero/storage/USINW937/howes-lewis-singh-2014-topics.pdf:application/pdf}
}

@inproceedings{jiang-improving-2014,
	title = {Improving {UCT} planning via approximate homomorphisms},
	url = {http://dl.acm.org/citation.cfm?id=2617453},
	urldate = {2016-03-27},
	booktitle = {Proceedings of the 2014 international conference on {Autonomous} agents and multi-agent systems},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Jiang, Nan and Singh, Satinder and Lewis, Richard},
	year = {2014},
	pages = {1289--1296},
	file = {jian-singh-lewis-2014-aamas.pdf:/home/user/Zotero/storage/JR5G2RWT/jian-singh-lewis-2014-aamas.pdf:application/pdf}
}

@article{lewis-computational-2014,
	title = {Computational {Rationality}: {Linking} {Mechanism} and {Behavior} {Through} {Bounded} {Utility} {Maximization}},
	volume = {6},
	issn = {17568757},
	shorttitle = {Computational {Rationality}},
	url = {http://doi.wiley.com/10.1111/tops.12086},
	doi = {10.1111/tops.12086},
	language = {en},
	number = {2},
	urldate = {2016-03-27},
	journal = {Topics in Cognitive Science},
	author = {Lewis, Richard L. and Howes, Andrew and Singh, Satinder},
	month = apr,
	year = {2014},
	pages = {279--311},
	file = {REV_ISS_WEB_TOPS_12086_6-2 279..311 - lewis-howes-singh-2014-topics.pdf:/home/user/Zotero/storage/UBG45UGZ/ lewis-howes-singh-2014-topics.pdf:application/pdf}
}

@article{shvartsman-computationally-2014,
	title = {Computationally rational saccadic control: {An} explanation of spillover effects based on sampling from noisy perception and memory},
	shorttitle = {Computationally rational saccadic control},
	url = {http://acl2014.org/acl2014/W14-20/W14-20-2014.pdf#page=11},
	urldate = {2016-03-27},
	journal = {ACL 2014},
	author = {Shvartsman, Michael and Lewis, Richard L. and Singh, Satinder},
	year = {2014},
	pages = {1},
	file = {shvartsman-et-al-2014-cmcl.pdf:/home/user/Zotero/storage/4XWG98TI/shvartsman-et-al-2014-cmcl.pdf:application/pdf}
}

@incollection{feary-linking-2013,
	title = {Linking context to evaluation in the design of safety critical interfaces},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-39232-0\_22},
	urldate = {2016-03-27},
	booktitle = {Human-{Computer} {Interaction}. {Human}-{Centred} {Design} {Approaches}, {Methods}, {Tools}, and {Environments}},
	publisher = {Springer},
	author = {Feary, Michael and Billman, Dorrit and Chen, Xiuli and Howes, Andrew and Lewis, Richard and Sherry, Lance and Singh, Satinder},
	year = {2013},
	pages = {193--202},
	file = {Microsoft Word - HCII2013_feary_et_al.docx - feary-et-al-2013-hci.pdf:/home/user/Zotero/storage/BD7XIHEV/feary-et-al-2013-hci.pdf:application/pdf}
}

@inproceedings{guo-reward-2013,
	title = {Reward {Mapping} for {Transfer} in {Long}-{Lived} {Agents}},
	url = {http://papers.nips.cc/paper/5191-reward-mapping-for-transfer-in-long-lived-agents},
	urldate = {2016-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Xiaoxiao and Singh, Satinder and Lewis, Richard L.},
	year = {2013},
	pages = {2130--2138},
	file = {xiao-singh-lewis-2013-nips.pdf:/home/user/Zotero/storage/PRWWC9BM/xiao-singh-lewis-2013-nips.pdf:application/pdf}
}

@article{lewis-adaptive-2013,
	title = {The {Adaptive} {Nature} of {Eye} {Movements} in {Linguistic} {Tasks}: {How} {Payoff} and {Architecture} {Shape} {Speed}-{Accuracy} {Trade}-{Offs}},
	volume = {5},
	issn = {17568757},
	shorttitle = {The {Adaptive} {Nature} of {Eye} {Movements} in {Linguistic} {Tasks}},
	url = {http://doi.wiley.com/10.1111/tops.12032},
	doi = {10.1111/tops.12032},
	language = {en},
	number = {3},
	urldate = {2016-03-27},
	journal = {Topics in Cognitive Science},
	author = {Lewis, Richard L. and Shvartsman, Michael and Singh, Satinder},
	month = jul,
	year = {2013},
	pages = {581--610},
	file = {untitled - lewis-shvartsman-singh-2013-topics.pdf:/home/user/Zotero/storage/AJBEKHA8/lewis-shvartsman-singh-2013-topics.pdf:application/pdf}
}

@inproceedings{myers-bounded-2013,
	title = {Bounded optimal state estimation and control in visual search: {Explaining} distractor ratio effects},
	shorttitle = {Bounded optimal state estimation and control in visual search},
	url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2013/papers/0206/paper0206.pdf},
	urldate = {2016-03-27},
	booktitle = {Proc. {CogSci}},
	author = {Myers, Christopher W. and Lewis, Richard L. and Howes, Andrew},
	year = {2013},
	file = {myers-et-al-2013-cogsciconf.pdf:/home/user/Zotero/storage/AITTNWKB/myers-et-al-2013-cogsciconf.pdf:application/pdf}
}

@inproceedings{liu-optimal-2012,
	title = {Optimal rewards in multiagent teams},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6400862},
	urldate = {2016-03-27},
	booktitle = {Development and {Learning} and {Epigenetic} {Robotics} ({ICDL}), 2012 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Liu, Bingyao and Singh, Sushil and Lewis, Richard L. and Qin, Shuang},
	year = {2012},
	pages = {1--8},
	file = {liu-singh-lewis-qin-2012-icdl.pdf:/home/user/Zotero/storage/E43AU8MS/liu-singh-lewis-qin-2012-icdl.pdf:application/pdf}
}

@inproceedings{sorg-reward-2010,
	title = {Reward design via online gradient ascent},
	url = {http://papers.nips.cc/paper/4146-reward-design-via-online-gradient-ascent},
	urldate = {2016-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sorg, Jonathan and Lewis, Richard L. and Singh, Satinder P.},
	year = {2010},
	pages = {2190--2198},
	file = {sorg-singh-lewis-2010-nips.pdf:/home/user/Zotero/storage/BS69CCQI/sorg-singh-lewis-2010-nips.pdf:application/pdf}
}

@inproceedings{naradowsky-grammarless-2012,
	title = {Grammarless {Parsing} for {Joint} {Inference}.},
	url = {http://people.cs.umass.edu/~narad/\_papers/narad\_coling2012.pdf},
	urldate = {2016-03-27},
	booktitle = {{COLING}},
	author = {Naradowsky, Jason and Vieira, Tim and Smith, David A. and {others}},
	year = {2012},
	pages = {1995--2010},
	file = {grammarless-coling2012.pdf:/home/user/Zotero/storage/TJ2GAZ87/grammarless-coling2012.pdf:application/pdf}
}

@article{roy-reasoning-2015,
	title = {Reasoning about quantities in natural language},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/452},
	urldate = {2016-03-27},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Roy, Subhro and Vieira, Tim and Roth, Dan},
	year = {2015},
	pages = {1--13},
	file = {2015-tacl-quantities.pdf:/home/user/Zotero/storage/JB5B2TUQ/2015-tacl-quantities.pdf:application/pdf}
}

@article{daume-iii-learning-2014,
	title = {Learning to {Search} in {Branch}-and-{Bound} {Algorithms}},
	url = {http://www.umiacs.umd.edu/~hhe/ilp-bb.pdf},
	urldate = {2016-03-27},
	author = {Daumé III, He He Hal and Eisner, Jason},
	year = {2014},
	file = {he+al.nips14.pdf:/home/user/Zotero/storage/CGRV9UW4/he+al.nips14.pdf:application/pdf}
}

@article{hong-deriving-2015,
	title = {Deriving {Multi}-{Headed} {Planar} {Dependency} {Parses} from {Link} {Grammar} {Parses}},
	url = {http://tlt13.sfs.uni-tuebingen.de/tlt13-proceedings.pdf#page=83},
	urldate = {2016-03-27},
	author = {Hong, Juneki and Eisner, Jason},
	year = {2015},
	file = {hong+eisner.tlt14.paper.pdf:/home/user/Zotero/storage/G76V94WW/hong+eisner.tlt14.paper.pdf:application/pdf}
}

@inproceedings{andrews-robust-2014,
	title = {Robust {Entity} {Clustering} via {Phylogenetic} {Inference}.},
	url = {https://www.cs.jhu.edu/~jason/papers/andrews+al.acl14.pdf},
	urldate = {2016-03-27},
	booktitle = {{ACL} (1)},
	author = {Andrews, Nicholas and Eisner, Jason and Dredze, Mark},
	year = {2014},
	pages = {775--785},
	file = {andrews+al.acl14.pdf:/home/user/Zotero/storage/B6VK44UM/andrews+al.acl14.pdf:application/pdf}
}

@inproceedings{he-dynamic-2013,
	title = {Dynamic {Feature} {Selection} for {Dependency} {Parsing}.},
	url = {http://www.aclweb.org/anthology/D13-1152.pdf},
	urldate = {2016-03-27},
	booktitle = {{EMNLP}},
	author = {He, He and Daumé III, Hal and Eisner, Jason},
	year = {2013},
	pages = {1455--1464},
	file = {Dynamic Feature Selection for Dependency Parsing - he+al.emnlp13.pdf:/home/user/Zotero/storage/TZPNMSDR/he+al.emnlp13.pdf:application/pdf}
}

@article{ferraro-virtual-2013,
	title = {A {Virtual} {Manipulative} for {Learning} {Log}-{Linear} {Models}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.377.2087&rep=rep1&type=pdf#page=76},
	urldate = {2016-03-27},
	journal = {ACL 2013},
	author = {Ferraro, Francis and {Jason Eisner}},
	year = {2013},
	pages = {66},
	file = {ferraro+eisner.tnlp13.pdf:/home/user/Zotero/storage/FB7FIAJG/ferraro+eisner.tnlp13.pdf:application/pdf}
}

@inproceedings{matthew-r.-gormley-nonconvex-2013,
	title = {Nonconvex {Global} {Optimization} for {Latent}-{Variable} {Models}.},
	url = {http://www.aclweb.org/anthology/P13-1044},
	urldate = {2016-03-27},
	booktitle = {{ACL} (1)},
	author = {{Matthew R. Gormley} and Eisner, Jason},
	year = {2013},
	pages = {444--454},
	file = {gormley+eisner.acl13.pdf:/home/user/Zotero/storage/QTQ387QA/gormley+eisner.acl13.pdf:application/pdf}
}

@inproceedings{tom-schaul-universal-2015,
	title = {Universal value function approximators},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/icml2015\_schaul15.pdf},
	urldate = {2016-03-27},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning} ({ICML}-15)},
	author = {{Tom Schaul} and Horgan, Daniel and Gregor, Karol and Silver, David},
	year = {2015},
	pages = {1312--1320},
	file = {Universal Value Function Approximators - uvfa.pdf:/home/user/Zotero/storage/ZBQZWTSI/uvfa.pdf:application/pdf}
}

@book{steinwart-support-2008,
	address = {New York},
	edition = {1st ed},
	series = {Information science and statistics},
	title = {Support vector machines},
	isbn = {978-0-387-77241-7 978-0-387-77242-4},
	publisher = {Springer},
	author = {Steinwart, Ingo and Christmann, Andreas},
	year = {2008},
	keywords = {Support vector machines},
	file = {A Kernel Test of Goodness of Fit - 1602.02964v3.pdf:/home/user/Zotero/storage/2QT6TRWV/1602.02964v3.pdf:application/pdf}
}

@article{solch-variational-2016,
	title = {Variational {Inference} for {On}-line {Anomaly} {Detection} in {High}-{Dimensional} {Time} {Series}},
	url = {http://arxiv.org/abs/1602.07109},
	urldate = {2016-03-27},
	journal = {arXiv preprint arXiv:1602.07109},
	author = {Sölch, Maximilian and Bayer, Justin and Ludersdorfer, Marvin and van der Smagt, Patrick},
	year = {2016},
	file = {1602.07109v2.pdf:/home/user/Zotero/storage/I8RVJ6DT/1602.07109v2.pdf:application/pdf}
}

@article{tran-variational-2016,
	title = {{THE} {VARIATIONAL} {GAUSSIAN} {PROCESS}},
	volume = {1050},
	url = {https://pdfs.semanticscholar.org/516b/b8965b56acec632bd0bf18b58e36f582343e.pdf},
	urldate = {2016-03-27},
	journal = {stat},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	year = {2016},
	pages = {4},
	file = {1511.06499v3.pdf:/home/user/Zotero/storage/N8HQRE94/1511.06499v3.pdf:application/pdf}
}

@article{racine-consistent-1997,
	title = {Consistent {Significance} {Testing} for {Nonparametric} {Regression}},
	volume = {15},
	issn = {0735-0015},
	url = {http://www.jstor.org/stable/1392340},
	doi = {10.2307/1392340},
	abstract = {This article presents a framework for individual and joint tests of significance employing nonparametric estimation procedures. The proposed test is based on nonparametric estimates of partial derivatives, is robust to functional misspecification for general classes of models, and employs nested pivotal bootstrapping procedures. Two simulations and one application are considered to examine size and power relative to misspecified parametric models, and to test for the linear unpredictability of exchange-rate movements for G7 currencies.},
	number = {3},
	urldate = {2016-03-27},
	journal = {Journal of Business \& Economic Statistics},
	author = {Racine, Jeff},
	year = {1997},
	pages = {369--378},
	file = {Consistent Significance Testing for Nonparametric Regression - 1392340.pdf:/home/user/Zotero/storage/82VPRXEB/1392340.pdf:application/pdf}
}

@article{duchi-adaptive-2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	urldate = {2016-03-27},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:/home/user/Zotero/storage/24V8FEIV/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CRBF5X5V/duchi11a.html:text/html}
}

@article{li-mutual-2016,
	title = {Mutual {Information} and {Diverse} {Decoding} {Improve} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1601.00372},
	abstract = {Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., \$p(y{\textbar}x)\$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.},
	urldate = {2016-03-25},
	journal = {arXiv:1601.00372 [cs]},
	author = {Li, Jiwei and Jurafsky, Dan},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00372},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2DXIDIPU/1601.html:text/html;Li_Jurafsky_2016_Mutual Information and Diverse Decoding Improve Neural Machine Translation.pdf:/home/user/Zotero/storage/G59FVVNJ/Li_Jurafsky_2016_Mutual Information and Diverse Decoding Improve Neural Machine Translation.pdf:application/pdf}
}

@article{cheng-neural-2016,
	title = {Neural {Summarization} by {Extracting} {Sentences} and {Words}},
	url = {http://arxiv.org/abs/1603.07252},
	abstract = {Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.},
	urldate = {2016-03-25},
	journal = {arXiv:1603.07252 [cs]},
	author = {Cheng, Jianpeng and Lapata, Mirella},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07252},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/M3BGR9G5/1603.html:text/html;Cheng_Lapata_2016_Neural Summarization by Extracting Sentences and Words.pdf:/home/user/Zotero/storage/T3KWAW9T/Cheng_Lapata_2016_Neural Summarization by Extracting Sentences and Words.pdf:application/pdf}
}

@misc{noauthor-cotterell+.acl14.pdf-nodate,
	title = {cotterell+al.acl14.pdf},
	url = {http://www.cs.jhu.edu/~jason/papers/cotterell+al.acl14.pdf},
	urldate = {2016-03-24}
}

@article{culbertson-cognitive-2013,
	title = {Cognitive {Biases}, {Linguistic} {Universals}, and {Constraint}-{Based} {Grammar} {Learning}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12027/abstract},
	doi = {10.1111/tops.12027},
	abstract = {According to classical arguments, language learning is both facilitated and constrained by cognitive biases. These biases are reflected in linguistic typology—the distribution of linguistic patterns across the world's languages—and can be probed with artificial grammar experiments on child and adult learners. Beginning with a widely successful approach to typology (Optimality Theory), and adapting techniques from computational approaches to statistical learning, we develop a Bayesian model of cognitive biases and show that it accounts for the detailed pattern of results of artificial grammar experiments on noun-phrase word order (Culbertson, Smolensky, \& Legendre, 2012). Our proposal has several novel properties that distinguish it from prior work in the domains of linguistic theory, computational cognitive science, and machine learning. This study illustrates how ideas from these domains can be synthesized into a model of language learning in which biases range in strength from hard (absolute) to soft (statistical), and in which language-specific and domain-general biases combine to account for data from the macro-level scale of typological distribution to the micro-level scale of learning by individuals.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Culbertson, Jennifer and Smolensky, Paul and Wilson, Colin},
	month = jul,
	year = {2013},
	keywords = {Learning biases, Artificial language learning, Bayesian modeling, Optimality theory, Typology, word order},
	pages = {392--424},
	file = {Culbertson et al_2013_Cognitive Biases, Linguistic Universals, and Constraint-Based Grammar Learning.pdf:/home/user/Zotero/storage/GE5TDVXW/Culbertson et al_2013_Cognitive Biases, Linguistic Universals, and Constraint-Based Grammar Learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EUPW3HFH/abstract.html:text/html}
}

@article{dubey-probabilistic-2013,
	title = {Probabilistic {Modeling} of {Discourse}-{Aware} {Sentence} {Processing}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12023/abstract},
	doi = {10.1111/tops.12023},
	abstract = {Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This restriction is unrealistic in light of experimental results suggesting interactions between syntax and other forms of linguistic information in human sentence processing. To address this limitation, this article introduces two sentence processing models that augment a syntactic component with information about discourse co-reference. The novel combination of probabilistic syntactic components with co-reference classifiers permits them to more closely mimic human behavior than existing models. The first model uses a deep model of linguistics, based in part on probabilistic logic, allowing it to make qualitative predictions on experimental data; the second model uses shallow processing to make quantitative predictions on a broad-coverage reading-time corpus.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Dubey, Amit and Keller, Frank and Sturt, Patrick},
	month = jul,
	year = {2013},
	keywords = {Co-reference resolution, Cognitive modeling, Discourse/syntax interactions, Markov logic, Sentence processing},
	pages = {425--451},
	file = {Dubey et al_2013_Probabilistic Modeling of Discourse-Aware Sentence Processing.pdf:/home/user/Zotero/storage/EWEW5KWU/Dubey et al_2013_Probabilistic Modeling of Discourse-Aware Sentence Processing.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JVS4X7UB/abstract.html:text/html}
}

@article{engelmann-framework-2013-1,
	title = {A {Framework} for {Modeling} the {Interaction} of {Syntactic} {Processing} and {Eye} {Movement} {Control}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12026/abstract},
	doi = {10.1111/tops.12026},
	abstract = {We explore the interaction between oculomotor control and language comprehension on the sentence level using two well-tested computational accounts of parsing difficulty. Previous work (Boston, Hale, Vasishth, \& Kliegl, 2011) has shown that surprisal (Hale, 2001; Levy, 2008) and cue-based memory retrieval (Lewis \& Vasishth, 2005) are significant and complementary predictors of reading time in an eyetracking corpus. It remains an open question how the sentence processor interacts with oculomotor control. Using a simple linking hypothesis proposed in Reichle, Warren, and McConnell (2009), we integrated both measures with the eye movement model EMMA (Salvucci, 2001) inside the cognitive architecture ACT-R (Anderson et al., 2004). We built a reading model that could initiate short “Time Out regressions” (Mitchell, Shen, Green, \& Hodgson, 2008) that compensate for slow postlexical processing. This simple interaction enabled the model to predict the re-reading of words based on parsing difficulty. The model was evaluated in different configurations on the prediction of frequency effects on the Potsdam Sentence Corpus. The extension of EMMA with postlexical processing improved its predictions and reproduced re-reading rates and durations with a reasonable fit to the data. This demonstration, based on simple and independently motivated assumptions, serves as a foundational step toward a precise investigation of the interaction between high-level language processing and eye movement control.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
	month = jul,
	year = {2013},
	keywords = {working memory, Computational Modeling, Eye Movements, Parsing difficulty, Reading, Sentence comprehension, Surprisal},
	pages = {452--474},
	file = {Engelmann et al_2013_A Framework for Modeling the Interaction of Syntactic Processing and Eye.pdf:/home/user/Zotero/storage/SBTTKV6N/Engelmann et al_2013_A Framework for Modeling the Interaction of Syntactic Processing and Eye.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/E5VEPX25/abstract.html:text/html}
}

@article{frank-uncertainty-2013,
	title = {Uncertainty {Reduction} as a {Measure} of {Cognitive} {Load} in {Sentence} {Comprehension}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12025/abstract},
	doi = {10.1111/tops.12025},
	abstract = {The entropy-reduction hypothesis claims that the cognitive processing difficulty on a word in sentence context is determined by the word's effect on the uncertainty about the sentence. Here, this hypothesis is tested more thoroughly than has been done before, using a recurrent neural network for estimating entropy and self-paced reading for obtaining measures of cognitive processing load. Results show a positive relation between reading time on a word and the reduction in entropy due to processing that word, supporting the entropy-reduction hypothesis. Although this effect is independent from the effect of word surprisal, we find no evidence that these two measures correspond to cognitively distinct processes.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Frank, Stefan L.},
	month = jul,
	year = {2013},
	keywords = {Sentence comprehension, Surprisal, Cognitive load, Entropy reduction, Recurrent neural network, Self-paced reading, Word information},
	pages = {475--494},
	file = {Frank_2013_Uncertainty Reduction as a Measure of Cognitive Load in Sentence Comprehension.pdf:/home/user/Zotero/storage/W673N6SU/Frank_2013_Uncertainty Reduction as a Measure of Cognitive Load in Sentence Comprehension.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/FM862VIG/abstract.html:text/html}
}

@article{frank-adding-2013,
	title = {Adding {Sentence} {Types} to a {Model} of {Syntactic} {Category} {Acquisition}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12030/abstract},
	doi = {10.1111/tops.12030},
	abstract = {The acquisition of syntactic categories is a crucial step in the process of acquiring syntax. At this stage, before a full grammar is available, only surface cues are available to the learner. Previous computational models have demonstrated that local contexts are informative for syntactic categorization. However, local contexts are affected by sentence-level structure. In this paper, we add sentence type as an observed feature to a model of syntactic category acquisition, based on experimental evidence showing that pre-syntactic children are able to distinguish sentence type using prosody and other cues. The model, a Bayesian Hidden Markov Model, allows for adding sentence type in a few different ways; we find that sentence type can aid syntactic category acquisition if it is used to characterize the differences in word order between sentence types. In these models, knowledge of sentence type permits similar gains to those found by extending the local context.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Frank, Stella and Goldwater, Sharon and Keller, Frank},
	month = jul,
	year = {2013},
	keywords = {Bayesian modeling, Computational Modeling, language acquisition, Sentence prosody, Syntactic categories},
	pages = {495--521},
	file = {Frank et al_2013_Adding Sentence Types to a Model of Syntactic Category Acquisition.pdf:/home/user/Zotero/storage/CTMUR9IQ/Frank et al_2013_Adding Sentence Types to a Model of Syntactic Category Acquisition.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GAIZSMFQ/abstract.html:text/html}
}

@article{van-schijndel-model-2013,
	title = {A {Model} of {Language} {Processing} as {Hierarchic} {Sequential} {Prediction}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12034/abstract},
	doi = {10.1111/tops.12034},
	abstract = {Computational models of memory are often expressed as hierarchic sequence models, but the hierarchies in these models are typically fairly shallow, reflecting the tendency for memories of superordinate sequence states to become increasingly conflated. This article describes a broad-coverage probabilistic sentence processing model that uses a variant of a left-corner parsing strategy to flatten sentence processing operations in parsing into a similarly shallow hierarchy of learned sequences. The main result of this article is that a broad-coverage model with constraints on hierarchy depth can process large newspaper corpora with the same accuracy as a state-of-the-art parser not defined in terms of sequential working memory operations.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {van Schijndel, Marten and Exley, Andy and Schuler, William},
	month = jul,
	year = {2013},
	keywords = {working memory, Computational Linguistics, Memory models, Parsing, Sequence models},
	pages = {522--540},
	file = {Snapshot:/home/user/Zotero/storage/D7AHR65Z/abstract.html:text/html;van Schijndel et al_2013_A Model of Language Processing as Hierarchic Sequential Prediction.pdf:/home/user/Zotero/storage/GCQ8KU35/van Schijndel et al_2013_A Model of Language Processing as Hierarchic Sequential Prediction.pdf:application/pdf}
}

@article{van-rij-how-2013,
	title = {How {WM} {Load} {Influences} {Linguistic} {Processing} in {Adults}: {A} {Computational} {Model} of {Pronoun} {Interpretation} in {Discourse}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	shorttitle = {How {WM} {Load} {Influences} {Linguistic} {Processing} in {Adults}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12029/abstract},
	doi = {10.1111/tops.12029},
	abstract = {This paper presents a study of the effect of working memory load on the interpretation of pronouns in different discourse contexts: stories with and without a topic shift. We discuss a computational model (in ACT-R, Anderson, 2007) to explain how referring expressions are acquired and used. On the basis of simulations of this model, it is predicted that WM constraints only affect adults' pronoun resolution in stories with a topic shift, but not in stories without a topic shift. This latter prediction was tested in an experiment. The results of this experiment confirm that WM load reduces adults' sensitivity to discourse cues signaling a topic shift, thus influencing their interpretation of subsequent pronouns.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {van Rij, Jacolien and van Rijn, Hedderik and Hendriks, Petra},
	month = jul,
	year = {2013},
	keywords = {Cognitive modeling, Language processing, Pronouns, Working memory load},
	pages = {564--580},
	file = {Snapshot:/home/user/Zotero/storage/4QN86X2N/abstract.html:text/html;van Rij et al_2013_How WM Load Influences Linguistic Processing in Adults.pdf:/home/user/Zotero/storage/5HBKN4F3/van Rij et al_2013_How WM Load Influences Linguistic Processing in Adults.pdf:application/pdf}
}

@article{lewis-adaptive-2013-1,
	title = {The {Adaptive} {Nature} of {Eye} {Movements} in {Linguistic} {Tasks}: {How} {Payoff} and {Architecture} {Shape} {Speed}-{Accuracy} {Trade}-{Offs}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	shorttitle = {The {Adaptive} {Nature} of {Eye} {Movements} in {Linguistic} {Tasks}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12032/abstract},
	doi = {10.1111/tops.12032},
	abstract = {We explore the idea that eye-movement strategies in reading are precisely adapted to the joint constraints of task structure, task payoff, and processing architecture. We present a model of saccadic control that separates a parametric control policy space from a parametric machine architecture, the latter based on a small set of assumptions derived from research on eye movements in reading (Engbert, Nuthmann, Richter, \& Kliegl, 2005; Reichle, Warren, \& McConnell, 2009). The eye-control model is embedded in a decision architecture (a machine and policy space) that is capable of performing a simple linguistic task integrating information across saccades. Model predictions are derived by jointly optimizing the control of eye movements and task decisions under payoffs that quantitatively express different desired speed-accuracy trade-offs. The model yields distinct eye-movement predictions for the same task under different payoffs, including single-fixation durations, frequency effects, accuracy effects, and list position effects, and their modulation by task payoff. The predictions are compared to—and found to accord with—eye-movement data obtained from human participants performing the same task under the same payoffs, but they are found not to accord as well when the assumptions concerning payoff optimization and processing architecture are varied. These results extend work on rational analysis of oculomotor control and adaptation of reading strategy (Bicknell \& Levy, ; McConkie, Rayner, \& Wilson, 1973; Norris, 2009; Wotschack, 2009) by providing evidence for adaptation at low levels of saccadic control that is shaped by quantitatively varying task demands and the dynamics of processing architecture.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Lewis, Richard L. and Shvartsman, Michael and Singh, Satinder},
	month = jul,
	year = {2013},
	keywords = {psycholinguistics, Eye Movements, Reading, Bounded optimal control, Speed-accuracy trade-off, Task effects},
	pages = {581--610},
	file = {Lewis et al_2013_The Adaptive Nature of Eye Movements in Linguistic Tasks.pdf:/home/user/Zotero/storage/M8X2N5AH/Lewis et al_2013_The Adaptive Nature of Eye Movements in Linguistic Tasks.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/5ND7GPKM/abstract.html:text/html}
}

@article{stabler-two-2013,
	title = {Two {Models} of {Minimalist}, {Incremental} {Syntactic} {Analysis}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12031/abstract},
	doi = {10.1111/tops.12031},
	abstract = {Minimalist grammars (MGs) and multiple context-free grammars (MCFGs) are weakly equivalent in the sense that they define the same languages, a large mildly context-sensitive class that properly includes context-free languages. But in addition, for each MG, there is an MCFG which is strongly equivalent in the sense that it defines the same language with isomorphic derivations. However, the structure-building rules of MGs but not MCFGs are defined in a way that generalizes across categories. Consequently, MGs can be exponentially more succinct than their MCFG equivalents, and this difference shows in parsing models too. An incremental, top-down beam parser for MGs is defined here, sound and complete for all MGs, and hence also capable of parsing all MCFG languages. But since the parser represents its grammar transparently, the relative succinctness of MGs is again evident. Although the determinants of MG structure are narrowly and discretely defined, probabilistic influences from a much broader domain can influence even the earliest analytic steps, allowing frequency and context effects to come early and from almost anywhere, as expected in incremental models.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Stabler, Edward P.},
	month = jul,
	year = {2013},
	keywords = {Parsing, grammar, Minimalist grammar, Multiple context-free grammar, Succinctness},
	pages = {611--633},
	file = {Snapshot:/home/user/Zotero/storage/7TDFK2WP/abstract.html:text/html;Stabler_2013_Two Models of Minimalist, Incremental Syntactic Analysis.pdf:/home/user/Zotero/storage/463D59DC/Stabler_2013_Two Models of Minimalist, Incremental Syntactic Analysis.pdf:application/pdf}
}

@article{tabor-fractal-2013,
	title = {Fractal {Analysis} {Illuminates} the {Form} of {Connectionist} {Structural} {Gradualness}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12036/abstract},
	doi = {10.1111/tops.12036},
	abstract = {We examine two connectionist networks—a fractal learning neural network (FLNN) and a Simple Recurrent Network (SRN)—that are trained to process center-embedded symbol sequences. Previous work provides evidence that connectionist networks trained on infinite-state languages tend to form fractal encodings. Most such work focuses on simple counting recursion cases (e.g., anbn), which are not comparable to the complex recursive patterns seen in natural language syntax. Here, we consider exponential state growth cases (including mirror recursion), describe a new training scheme that seems to facilitate learning, and note that the connectionist learning of these cases has a continuous metamorphosis property that looks very different from what is achievable with symbolic encodings. We identify a property—ragged progressive generalization—which helps make this difference clearer. We suggest two conclusions. First, the fractal analysis of these more complex learning cases reveals the possibility of comparing connectionist networks and symbolic models of grammatical structure in a principled way—this helps remove the black box character of connectionist networks and indicates how the theory they support is different from symbolic approaches. Second, the findings indicate the value of future, linked mathematical and empirical work on these models—something that is more possible now than it was 10 years ago.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Tabor, Whitney and Cho, Pyeong Whan and Szkudlarek, Emily},
	month = jul,
	year = {2013},
	keywords = {Dynamical systems, Fractal grammars, Generalization, Neural (connectionist) networks, recursion, Self-organization, Simple Recurrent Network (SRN), Symbolic models},
	pages = {634--667},
	file = {Snapshot:/home/user/Zotero/storage/DJWBZAMN/abstract.html:text/html;Tabor et al_2013_Fractal Analysis Illuminates the Form of Connectionist Structural Gradualness.pdf:/home/user/Zotero/storage/3IBHPC4T/Tabor et al_2013_Fractal Analysis Illuminates the Form of Connectionist Structural Gradualness.pdf:application/pdf}
}

@article{hale-information-2003,
	title = {The {Information} {Conveyed} by {Words} in {Sentences}},
	volume = {32},
	issn = {0090-6905, 1573-6555},
	url = {http://link.springer.com/article/10.1023/A%3A1022492123056},
	doi = {10.1023/A:1022492123056},
	abstract = {A method is presented for calculating the amount of information conveyed to a hearer by a speaker emitting a sentence generated by a probabilistic grammar known to both parties. The method applies the work of Grenander (1967) to the intermediate states of a top-down parser. This allows the uncertainty about structural ambiguity to be calculated at each point in a sentence. Subtracting these values at successive points gives the information conveyed by a word in a sentence. Word-by-word information conveyed is calculated for several small probabilistic grammars, and it is suggested that the number of bits conveyed per word is a determinant of reading times and other measures of cognitive load.},
	language = {en},
	number = {2},
	urldate = {2016-03-24},
	journal = {Journal of Psycholinguistic Research},
	author = {Hale, John},
	month = mar,
	year = {2003},
	keywords = {psycholinguistics, Entropy reduction, Cognitive Psychology, computational psycholinguistics},
	pages = {101--123},
	file = {Hale_2003_The Information Conveyed by Words in Sentences.pdf:/home/user/Zotero/storage/K47A9VH3/Hale_2003_The Information Conveyed by Words in Sentences.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/X5F5XG9N/10.html:text/html}
}

@article{hale-uncertainty-2006,
	title = {Uncertainty {About} the {Rest} of the {Sentence}},
	volume = {30},
	copyright = {© 2006 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0000\_64/abstract},
	doi = {10.1207/s15516709cog0000\_64},
	abstract = {A word-by-word human sentence processing complexity metric is presented. This metric formalizes the intuition that comprehenders have more trouble on words contributing larger amounts of information about the syntactic structure of the sentence as a whole. The formalization is in terms of the conditional entropy of grammatical continuations, given the words that have been heard so far. To calculate the predictions of this metric, Wilson and Carroll's (1954) original entropy reduction idea is extended to infinite languages. This is demonstrated with a mildly context-sensitive language that includes relative clauses formed on a variety of grammatical relations across the Accessibility Hierarchy of Keenan and Comrie (1977). Predictions are derived that correlate significantly with repetition accuracy results obtained in a sentence-memory experiment (Keenan \& Hawkins, 1987).},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {Cognitive Science},
	author = {Hale, John},
	month = jul,
	year = {2006},
	keywords = {Linguistics, Syntax, Entropy reduction, Accessibility hierarchy, Computer science, computer simulation, Information, Language understanding, mathematical modeling, Minimalist grammars, Probabilistic grammars, Psychology, Relative clauses},
	pages = {643--672},
	file = {Hale_2006_Uncertainty About the Rest of the Sentence.pdf:/home/user/Zotero/storage/T73RVVZJ/Hale_2006_Uncertainty About the Rest of the Sentence.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ABCZPEZ9/abstract.html:text/html}
}

@article{nilsson-proportional-2013,
	title = {Proportional {Hazards} {Modeling} of {Saccadic} {Response} {Times} {During} {Reading}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12033/abstract},
	doi = {10.1111/tops.12033},
	abstract = {In this article we use proportional hazards models to examine how low-level processes affect the probability of making a saccade over time, through the period of fixation, during reading. We apply the Cox proportional hazards model to investigate how launch distance (relative to word beginning), fixation location (relative to word center), and word frequency affect the hazard of a saccadic response. This model requires that covariates have a constant impact on the hazard over time, the assumption of proportional hazards. We show that this assumption is not supported. The impact of the covariates changes with the time passed since fixation onset. To account for the non-proportional hazards we fit step functions of time, resulting in a model with time-varying effects on the hazard. We evaluate the ability to predict the timing of saccades on held-out fixation data. The model with time-varying effects performs better in predicting the timing of saccades for fixations as short as 100 ms and as long as 500 ms, when compared both to a baseline model without covariates and a model which assumes constant covariate effects. This result suggests that the time-varying effects model better recovers the time course of low-level processes that influence the decision to move the eyes.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Nilsson, Mattias and Nivre, Joakim},
	month = jul,
	year = {2013},
	keywords = {Eye Movements, Reading, Cox proportional hazards model, Fixation duration, Time-varying effects},
	pages = {541--563},
	file = {Nilsson_Nivre_2013_Proportional Hazards Modeling of Saccadic Response Times During Reading.pdf:/home/user/Zotero/storage/4JS4RDWV/Nilsson_Nivre_2013_Proportional Hazards Modeling of Saccadic Response Times During Reading.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EZK49C99/abstract.html:text/html}
}

@article{hale-introduction-2013,
	title = {Introduction to the {Issue} on {Computational} {Models} of {Natural} {Language}},
	volume = {5},
	copyright = {Copyright © 2013 Cognitive Science Society, Inc.},
	issn = {1756-8765},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12038/abstract},
	doi = {10.1111/tops.12038},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Topics in Cognitive Science},
	author = {Hale, John and Reitter, David},
	month = jul,
	year = {2013},
	pages = {388--391},
	file = {Hale_Reitter_2013_Introduction to the Issue on Computational Models of Natural Language.pdf:/home/user/Zotero/storage/QA8JS2G6/Hale_Reitter_2013_Introduction to the Issue on Computational Models of Natural Language.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/4A7NF9Q4/abstract.html:text/html}
}

@misc{noauthor-notitle-nodate-1,
	url = {https://mindmodeling.org/cogsci2014/papers/322/},
	urldate = {2016-03-24},
	file = {:/home/user/Zotero/storage/ZHNSGVP5/322.html:text/html}
}

@article{yun-uncertainty-2014,
	title = {Uncertainty in processing relative clauses across {East} {Asian} languages},
	volume = {24},
	issn = {0925-8558, 1572-8560},
	url = {http://link.springer.com/article/10.1007/s10831-014-9126-6},
	doi = {10.1007/s10831-014-9126-6},
	abstract = {The processing difficulty profile for relative clauses in Chinese, Japanese and Korean represents a challenge for theories of human parsing. We address this challenge using a grammar-based complexity metric, one that reflects a minimalist analysis of relative clauses for all three languages as well as structure-dependent corpus distributions. Together, these define a comprehender’s degree of uncertainty at each point in a sentence. We use this idea to quantify the intuition that people do comprehension work as they incrementally resolve ambiguity, word by word. We find that downward changes to this quantitative measure of uncertainty derive observed processing contrasts between Subject- and Object-extracted relative clauses. This demonstrates that the complexity metric, in conjunction with a minimalist grammar and corpus-based weights, accounts for the widely-observed Subject Advantage.},
	language = {en},
	number = {2},
	urldate = {2016-03-24},
	journal = {Journal of East Asian Linguistics},
	author = {Yun, Jiwon and Chen, Zhong and Hunter, Tim and Whitman, John and Hale, John},
	month = dec,
	year = {2014},
	keywords = {Chinese, Comparative Linguistics, Information theory, Japanese, Korean, Languages and Literature, minimalism, Relative clause, Theoretical Languages},
	pages = {113--148},
	file = {Snapshot:/home/user/Zotero/storage/72S7ZHR9/10.html:text/html;Yun et al_2014_Uncertainty in processing relative clauses across East Asian languages.pdf:/home/user/Zotero/storage/ZS6DT35I/Yun et al_2014_Uncertainty in processing relative clauses across East Asian languages.pdf:application/pdf}
}

@article{potts-embedded-2015,
	title = {Embedded {Implicatures} as {Pragmatic} {Inferences} under {Compositional} {Lexical} {Uncertainty}},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/lookup/doi/10.1093/jos/ffv012},
	doi = {10.1093/jos/ffv012},
	language = {en},
	urldate = {2016-03-24},
	journal = {Journal of Semantics},
	author = {Potts, Christopher and Lassiter, Daniel and Levy, Roger and Frank, Michael C.},
	month = dec,
	year = {2015},
	pages = {ffv012},
	file = {OP-SEMA150024 1..48 - jos.ffv012.full.pdf:/home/user/Zotero/storage/H6VXWWMM/jos.ffv012.full.pdf:application/pdf}
}

@article{goodman-probabilistic-2014,
	title = {Probabilistic semantics and pragmatics: {Uncertainty} in language and thought},
	shorttitle = {Probabilistic semantics and pragmatics},
	url = {http://cocolab.stanford.edu/papers/GoodmanLassiter2015-Chapter.pdf},
	urldate = {2016-03-24},
	journal = {Handbook of Contemporary Semantic Theory. Wiley-Blackwell},
	author = {Goodman, Noah D. and Lassiter, Daniel},
	year = {2014},
	file = {Probabilistic Semantics and Pragmatics\: Uncertainty in Language and Thought - Goodman-HCS-final.pdf:/home/user/Zotero/storage/V3SU3TD2/Goodman-HCS-final.pdf:application/pdf}
}

@article{lassiter-adjectival-2015,
	title = {Adjectival modification and gradation},
	url = {https://books.google.com/books?hl=en&lr=&id=zAhQCgAAQBAJ&oi=fnd&pg=PA143&dq=%22Assuming+that+the+copula+is+semantically+vacuous,+the+result+is%22+%22from+individuals+to+truth-values,+we+cannot+account+for+this+use%22+%22use+italicized+words+and+phrases+to+refer+to+natural+language+expressions,%22+&ots=PKe5TY\_opl&sig=yj-Tic02o5f0HNuEqvsjiZVj0Uo},
	urldate = {2016-03-24},
	journal = {Handbook of contemporary semantic theory},
	author = {Lassiter, Daniel},
	year = {2015},
	file = {Adjectival modification and gradation - Lassiter-Blackwell-Adjectives-final.pdf:/home/user/Zotero/storage/BKAV8CNQ/Lassiter-Blackwell-Adjectives-final.pdf:application/pdf}
}

@article{lassiter-epistemic-2015,
	title = {Epistemic {Comparison}, {Models} of {Uncertainty}, and the {Disjunction} {Puzzle}},
	volume = {32},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/cgi/doi/10.1093/jos/ffu008},
	doi = {10.1093/jos/ffu008},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {Journal of Semantics},
	author = {Lassiter, D.},
	month = nov,
	year = {2015},
	pages = {649--684},
	file = {OP-SEMA140017 649..684 - 649.full.pdf:/home/user/Zotero/storage/RKGPQV2X/649.full.pdf:application/pdf}
}

@article{potts-embedded-2015-1,
	title = {Embedded {Implicatures} as {Pragmatic} {Inferences} under {Compositional} {Lexical} {Uncertainty}},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/lookup/doi/10.1093/jos/ffv012},
	doi = {10.1093/jos/ffv012},
	language = {en},
	urldate = {2016-03-24},
	journal = {Journal of Semantics},
	author = {Potts, Christopher and Lassiter, Daniel and Levy, Roger and Frank, Michael C.},
	month = dec,
	year = {2015},
	pages = {ffv012},
	file = {OP-SEMA150024 1..48 - jos.ffv012.full.pdf:/home/user/Zotero/storage/Q35HJTPB/jos.ffv012.full.pdf:application/pdf}
}

@article{lassiter-modality-2014,
	title = {Modality, {Scale} {Structure}, and {Scalar} {Reasoning}},
	volume = {95},
	copyright = {© 2014 The Author. Pacific Philosophical Quarterly © 2014 University of Southern California and John Wiley \& Sons Ltd},
	issn = {1468-0114},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/papq.12045/abstract},
	doi = {10.1111/papq.12045},
	abstract = {Epistemic and deontic comparatives differ in how they interact with disjunction. I argue that this difference provides a compelling empirical argument against the semantics of Kratzer, which predicts that all modal comparatives should interact with disjunction in the same way. Interestingly, an identical distinction is found in the semantics of non-modal adjectives: additive adjectives like ‘heavy’ behave logically like epistemic comparatives, and intermediate adjectives like ‘hot’ behave like deontic comparatives. I characterize this distinction formally and argue that the divergence between epistemic and deontic modals explained if we structure their semantics around scalar concepts: epistemic modals should be analyzed using probability (an additive scale), and deontic modals using expected value (an intermediate scale).},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {Pacific Philosophical Quarterly},
	author = {Lassiter, Daniel},
	month = dec,
	year = {2014},
	pages = {461--490},
	file = {Lassiter_2014_Modality, Scale Structure, and Scalar Reasoning.pdf:/home/user/Zotero/storage/PPN8IW85/Lassiter_2014_Modality, Scale Structure, and Scalar Reasoning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JKDM7KP3/abstract.html:text/html}
}

@article{lassiter-how-2015,
	title = {How many kinds of reasoning? {Inference}, probability, and natural language semantics},
	volume = {136},
	issn = {0010-0277},
	shorttitle = {How many kinds of reasoning?},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027714002170},
	doi = {10.1016/j.cognition.2014.10.016},
	abstract = {The “new paradigm” unifying deductive and inductive reasoning in a Bayesian framework (Oaksford \&amp; Chater, 2007; Over, 2009) has been claimed to be falsified by results which show sharp differences between reasoning about necessity vs. plausibility (Heit \&amp; Rotello, 2010; Rips, 2001; Rotello \&amp; Heit, 2009). We provide a probabilistic model of reasoning with modal expressions such as “necessary” and “plausible” informed by recent work in formal semantics of natural language, and show that it predicts the possibility of non-linear response patterns which have been claimed to be problematic. Our model also makes a strong monotonicity prediction, while two-dimensional theories predict the possibility of reversals in argument strength depending on the modal word chosen. Predictions were tested using a novel experimental paradigm that replicates the previously-reported response patterns with a minimal manipulation, changing only one word of the stimulus between conditions. We found a spectrum of reasoning “modes” corresponding to different modal words, and strong support for our model’s monotonicity prediction. This indicates that probabilistic approaches to reasoning can account in a clear and parsimonious way for data previously argued to falsify them, as well as new, more fine-grained, data. It also illustrates the importance of careful attention to the semantics of language employed in reasoning experiments.},
	urldate = {2016-03-24},
	journal = {Cognition},
	author = {Lassiter, Daniel and Goodman, Noah D.},
	month = mar,
	year = {2015},
	keywords = {Deduction, Induction, Natural language semantics, Probabilistic model, Reasoning},
	pages = {123--134},
	file = {Lassiter_Goodman_2015_How many kinds of reasoning.pdf:/home/user/Zotero/storage/MTN796QQ/Lassiter_Goodman_2015_How many kinds of reasoning.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DNX4TD75/S0010027714002170.html:text/html}
}

@misc{noauthor-notitle-nodate-2,
	url = {http://mindmodeling.org/cogsci2012/papers/0125/index.html},
	urldate = {2016-03-24},
	file = {:/home/user/Zotero/storage/JD9FQ64E/index.html:text/html}
}

@article{lassiter-quantificational-2012,
	title = {Quantificational and modal interveners in degree constructions},
	volume = {22},
	copyright = {Copyright (c)},
	issn = {2163-5951},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/2649},
	number = {0},
	urldate = {2016-03-24},
	journal = {Semantics and Linguistic Theory},
	author = {Lassiter, Daniel},
	month = sep,
	year = {2012},
	keywords = {Intervention effects, modality, quantification, scalar semantics, weak islands},
	pages = {565--583},
	file = {Lassiter_2012_Quantificational and modal interveners in degree constructions.pdf:/home/user/Zotero/storage/ZCPN9IKX/Lassiter_2012_Quantificational and modal interveners in degree constructions.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/T6BGU6QD/2395.html:text/html}
}

@article{lassiter-presuppositions-2012,
	title = {Presuppositions, provisos, and probability},
	volume = {5},
	issn = {1937-8912},
	url = {http://semprag.org/article/view/1571},
	doi = {10.3765/sp.5.2},
	number = {0},
	journal = {Semantics and Pragmatics},
	author = {Lassiter, Daniel},
	month = may,
	year = {2012},
	pages = {2:1--37},
	file = {Presuppositions, provisos, and probability | Lassiter | Semantics and Pragmatics:/home/user/Zotero/storage/BM9J8E7S/sp.5.html:text/html}
}

@article{luana-distributional-nodate,
	title = {From distributional semantics to feature norms : grounding semantic models in human perceptual data},
	author = {Luana, F and Vecchi, Eva Maria},
	pages = {1--6},
	file = {26:/home/user/Zotero/storage/NHT86K4K/26.pdf:application/pdf}
}

@article{mariano-language-2015,
	title = {Language discrimination and clustering via a neural network approach},
	url = {http://arxiv.org/abs/1507.04116},
	abstract = {We classify twenty-one Indo-European languages starting from written text. We use neural networks in order to define a distance among different languages, construct a dendrogram and analyze the ultrametric structure that emerges. Four or five subgroups of languages are identified, according to the "cut" of the dendrogram, drawn with an entropic criterion. The results and the method are discussed.},
	author = {Mariano, Angelo and Parisi, Giorgio and Pascazio, Saverio},
	year = {2015},
	pages = {1--10},
	file = {1507.04116v1:/home/user/Zotero/storage/IDVDJ2X4/1507.04116v1.pdf:application/pdf}
}

@article{manning-case-2015,
	title = {The case for universal dependencies},
	volume = {14},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:The+Case+for+Universal+Symbol+Files#0},
	number = {Depling 2015},
	journal = {Structured Programming},
	author = {{Manning}},
	year = {2015},
	pages = {136--147},
	file = {W15-2101:/home/user/Zotero/storage/WRA5JQF8/W15-2101.pdf:application/pdf}
}

@inproceedings{maccartney-natural-2007,
	series = {{RTE} '07},
	title = {Natural logic for textual inference},
	url = {http://portal.acm.org/citation.cfm?id=1654536.1654575},
	booktitle = {Proceedings of the {ACL}-{PASCAL} {Workshop} on {Textual} {Entailment} and {Paraphrasing}},
	publisher = {Association for Computational Linguistics},
	author = {MacCartney, Bill and Manning, Christopher D},
	year = {2007},
	keywords = {Semantics-computational-special-logic, Textual-Inference},
	pages = {193--200}
}

@phdthesis{maccartney-natural-2009,
	title = {Natural language inference},
	author = {MacCartney, Bill},
	year = {2009},
	file = {Unknown - 2009 - Natural Language Inference:/home/user/Zotero/storage/H6AF8CGK/Unknown - 2009 - Natural Language Inference.pdf:application/pdf}
}

@inproceedings{maccartney-phrase-based-2008,
	title = {A phrase-based alignment model for natural language inference},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {MacCartney, B and Galley, M and Manning, C D},
	year = {2008},
	pages = {802--811}
}

@inproceedings{lin-dirt-2001,
	title = {{DIRT}@ {SBT}@ discovery of inference rules from text},
	booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Lin, D and Pantel, P},
	year = {2001},
	pages = {323--328}
}

@inproceedings{lin-automatic-2004,
	series = {{ACL} '04},
	title = {Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics},
	url = {http://dx.doi.org/10.3115/1218955.1219032},
	doi = {10.3115/1218955.1219032},
	booktitle = {Proceedings of the 42nd {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew and Och, Franz Josef},
	year = {2004}
}

@article{levy-teaching-2012,
	title = {Teaching {Machines} to {Learn} by {Metaphors}.},
	issn = {9781577355687},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/4842/5511},
	journal = {Aaai},
	author = {Levy, Omer and Markovitch, Shaul},
	year = {2012},
	keywords = {Machine Learning (Main Track)},
	pages = {991--997},
	file = {4842-22634-1-PB(1):/home/user/Zotero/storage/5XXCXJEV/4842-22634-1-PB(1).pdf:application/pdf}
}

@article{levy-teaching-2012-1,
	title = {Teaching {Machines} to {Learn} by {Metaphors}.},
	issn = {9781577355687},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/4842/5511},
	journal = {Aaai},
	author = {Levy, Omer and Markovitch, Shaul},
	year = {2012},
	keywords = {Machine Learning (Main Track)},
	pages = {991--997},
	file = {4842-22634-1-PB(1):/home/user/Zotero/storage/D87C6SKD/4842-22634-1-PB(1).pdf:application/pdf}
}

@inproceedings{lin-rouge:-2004,
	title = {Rouge: a package for automatic evaluation of summaries},
	booktitle = {Proceedings of the {Workshop} on {Text} {Summarization} {Branches} {Out} ({WAS} 2004)},
	author = {Lin, Chin-Yew},
	year = {2004},
	pages = {25--26}
}

@article{levy-improving-nodate,
	title = {Improving {Distributional} {Similarity} with {Lessons} {Learned} from {Word} {Embeddings}},
	abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
	file = {ISCOL2015_submission25_e_2:/home/user/Zotero/storage/2XXDDCXF/ISCOL2015_submission25_e_2.pdf:application/pdf}
}

@article{levy-dependency-based-2014,
	title = {Dependency-{Based} {Word} {Embeddings}},
	abstract = {While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary con- texts. In particular, we perform exper- iments with dependency-based contexts, and show that they produce markedly different embeddings. The dependency- based embeddings are less topical and ex- hibit more functional similarity than the original skip-gram embeddings. 1},
	journal = {Acl},
	author = {Levy, Omer and Goldberg, Yoav},
	year = {2014},
	pages = {302--308},
	file = {dependency-based-word-embeddings-acl-2014:/home/user/Zotero/storage/BGI8W467/dependency-based-word-embeddings-acl-2014.pdf:application/pdf}
}

@article{levy-neural-2014,
	title = {Neural {Word} {Embedding} as {Implicit} {Matrix} {Factorization}},
	url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
	journal = {Advances in Neural Information Processing Systems},
	author = {Levy, Omer and Goldberg, Yoav},
	year = {2014},
	pages = {2177--2185},
	file = {5477-neural-word-embedding-as-implicit-matrix-factorization:/home/user/Zotero/storage/GER5T43Q/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf:application/pdf}
}

@article{levy-learning-2012,
	title = {Learning to {Exploit} {Structured} {Resources} for {Lexical} {Inference}},
	author = {Levy, Omer and Dagan, Ido and Goldberger, Jacob},
	year = {2012},
	file = {ISCOL2015_submission4_a_1:/home/user/Zotero/storage/TFZ8QW9P/ISCOL2015_submission4_a_1.pdf:application/pdf}
}

@incollection{levine-head-driven-2006,
	address = {Oxford},
	edition = {2},
	title = {Head-{Driven} {Phrase} {Structure} {Grammar}: {Linguistic} {Approach}, {Formal} {Foundations}, and {Computational} {Realization}},
	booktitle = {Encyclopedia of {Language} and {Linguistics}},
	publisher = {Elsevier},
	author = {Levine, Robert D and Meurers, W Detmar},
	editor = {Brown, Keith},
	year = {2006}
}

@article{lazaridou-unveiling-nodate,
	title = {Unveiling the {Dreams} of {Word} {Embeddings}: {Towards} {Language}-{Driven} {Image} {Generation}},
	abstract = {We introduce language-driven image generation, the task of generating an im-age visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general vi-sual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.},
	author = {Lazaridou, Angeliki and Nguyen, Dat Tien and Bernardi, Raffaella and Baroni, Marco},
	pages = {1--11},
	file = {1506.03500v1:/home/user/Zotero/storage/FSJIMZSH/1506.03500v1.pdf:application/pdf}
}

@article{levy-linguistic-2014,
	title = {Linguistic {Regularities} in {Sparse} and {Explicit} {Word} {Representations}},
	issn = {9781937284473},
	abstract = {Recent work has shown that neural-embedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.’s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
	journal = {Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL 2014)},
	author = {Levy, Omer and Goldberg, Yoav},
	year = {2014},
	file = {linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014:/home/user/Zotero/storage/XBXW27R2/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf:application/pdf}
}

@article{leacock-c-rater:-2003,
	title = {C-rater: {Automated} scoring of short-answer questions},
	volume = {37},
	number = {4},
	journal = {Computers and the Humanities},
	author = {Leacock, C and Chodorow, M},
	year = {2003},
	pages = {389--405}
}

@article{lazaridou-combining-nodate,
	title = {Combining {Language} and {Vision} with a {Multimodal} {Skip}-gram {Model}},
	author = {Lazaridou, Angeliki},
	pages = {1--1},
	file = {21:/home/user/Zotero/storage/DN5GQIT2/21.pdf:application/pdf}
}

@article{lassiter-how-2014,
	title = {How many kinds of reasoning ? {Inference} , probability , and natural language semantics},
	doi = {10.1016/j.cognition.2014.10.016},
	journal = {Submitted},
	author = {Lassiter, Daniel and Goodman, Noah D},
	year = {2014},
	keywords = {Reasoning, deduction, induction, natural language se-, probabilistic model},
	pages = {1--22},
	file = {Lassiter, Goodman - 2014 - How many kinds of reasoning Inference , probability , and natural language semantics:/home/user/Zotero/storage/72QXFVBZ/Lassiter, Goodman - 2014 - How many kinds of reasoning Inference , probability , and natural language semantics.pdf:application/pdf}
}

@article{lapata-acquisition-2000,
	title = {The {Acquisition} and {Modeling} of {Lexical} {Knowledge}},
	url = {papers2://publication/uuid/0824D6CD-7BFB-48E4-B7CE-30A890ECD0C3},
	abstract = {This thesis deals with the acquisition and probabilistic modeling of lexical knowledge. A considerable body of work in lexical semantics concentrates on describing and representing sys- tematic polysemy, i.e., the regular and predictable meaning alternations certain classes of words are subject to. Although the prevalence of the phenomenon has been long recognized, systematic empirical studies of regular polysemy are largely absent, both with respect to the acquisition of systematic polysemous lexical units and the disambiguation of their meaning.\nThe present thesis addresses both tasks. First, we use insights from linguistic theory to guide and structure the acquisition of systematically polysemous units from domain independent wide-coverage text. Second, we constrain ambiguity by developing a probabilistic framework which provides a ranking on the range of meanings for systematically polysemous words in the absence of discourse context.\nWe focus on meaning alternations with syntactic effects and exploit the correspondence between meaning and syntax to inform the acquisition process. The acquired information is useful for empirically testing and validating linguistic generalizations, extending their coverage and quantifying the degree to which they are productive. We acquire lexical seman- tic information automatically using partial parsing and a heuristic approach which exploits fixed correspondences between surface syntactic cues and lexical meaning. We demonstrate the generality of our proposal by applying it to verbs and their complements, adjective-noun combinations, and noun-noun compounds. For each phenomenon we rely on insights from linguistic theory: for verbs we exploit Levin’s (1993) influential classification of verbs on the basis of their meaning and syntactic behavior; for compound nouns we make use of Levi’s (1978) classification of semantic relations, and finally we look at Vendler’s (1968) and Pustejovsky’s (1995) generalizations about adjectival meaning.\nWe present a simple probabilistic model that uses the acquired distributions to select the dominant meaning from a set of meanings arising from syntactically related word combinations. Default meaning—the dominant meaning of polysemous words in the absence of explicit contextual information to the contrary—is modeled probabilistically in a Bayesian framework which combines observed linguistic dependencies (in the form of conditional probabilities) with linguistic generalizations (in the form of prior probabilities derived from classifications such as Levin 1993). Our studies explore a range of model properties: (a) its generality, (b) the representation of the phenomenon under consideration (i.e., the choice of the model variables), (c) the simplification of its parameter space through independence assumptions, and (d) the estimation of the model parameters. Our findings show that the model is general enough to account for different types of lexical units (verbs and their complements, adjective-noun combinations, and noun-noun compounds) under varying assumptions about data requirements (sufficient versus sparse data) and meaning representations (corpus internal or corpus external).},
	journal = {PhD Thesis},
	author = {Lapata, M},
	year = {2000},
	file = {Lapata - 2000 - The Acquisition and Modeling of Lexical Knowledge:/home/user/Zotero/storage/2HZHFG25/Lapata - 2000 - The Acquisition and Modeling of Lexical Knowledge.pdf:application/pdf}
}

@article{lai-how-2015,
	title = {How to {Generate} a {Good} {Word} {Embedding}?},
	url = {http://arxiv.org/abs/1507.05523},
	abstract = {We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.},
	author = {Lai, Siwei and Liu, Kang and Xu, Liheng and Zhao, Jun},
	year = {2015},
	keywords = {distributed representation, neural network, word embedding},
	file = {1507.05523v1:/home/user/Zotero/storage/DKSIP4BG/1507.05523v1.pdf:application/pdf}
}

@article{kusner-word-2010,
	title = {From {Word} {Embeddings} {To} {Document} {Distances}},
	abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text docu-ments. Our work is based on recent results in word embeddings that learn semantically mean-ingful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text doc-uments as the minimum amount of distance that the embedded words of one document need to " travel " to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Dis-tance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classi-fication data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor docu-ment classification error rates.},
	author = {Kusner, Matt J and Sun, Yu and Kolkin, Nicholas I and Weinberger, Kilian Q},
	year = {2010},
	file = {kusnerb15:/home/user/Zotero/storage/629E4QZQ/kusnerb15.pdf:application/pdf}
}

@article{kuhn-sentence-1998,
	title = {Sentence {Interpretation}},
	volume = {44},
	number = {0},
	journal = {Spoken Dialogues with Computers},
	author = {Kuhn, Roland and de Mori, Renato},
	year = {1998},
	pages = {485--522},
	file = {scalars LCP web version:/home/user/Zotero/storage/9VENJA6U/scalars LCP web version.pdf:application/pdf}
}

@article{krishnamurthy-learning-2015,
	title = {Learning a {Compositional} {Semantics} for {Freebase} with an {Open} {Predicate} {Vocabulary}},
	volume = {3},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Krishnamurthy, Jayant and Mitchell, Tom M},
	year = {2015},
	pages = {257--270},
	file = {548-1675-1-PB:/home/user/Zotero/storage/9KBPUZJU/548-1675-1-PB.pdf:application/pdf}
}

@article{konstas-incremental-2014,
	title = {Incremental {Semantic} {Role} {Labeling} with {Tree} {Adjoining} {Grammar}},
	author = {Konstas, Ioannis and Keller, Frank and Demberg, Vera and Lapata, Mirella},
	year = {2014},
	file = {Konstas et al. - 2014 - Incremental Semantic Role Labeling with Tree Adjoining Grammar:/home/user/Zotero/storage/Z63658ES/Konstas et al. - 2014 - Incremental Semantic Role Labeling with Tree Adjoining Grammar.pdf:application/pdf}
}

@article{kohne-discourse-2013,
	title = {Discourse {Connectives} give {Rise} to {Lexical} {Predictions}},
	journal = {Workshop DETEC-2013},
	author = {Köhne, Judith and Demberg, Vera},
	year = {2013},
	file = {Köhne, Demberg - 2013 - Discourse Connectives give Rise to Lexical Predictions:/home/user/Zotero/storage/FDX8ZM5I/Köhne, Demberg - 2013 - Discourse Connectives give Rise to Lexical Predictions.pdf:application/pdf}
}

@article{kleinschmidt-robust-2014,
	title = {Robust speech perception : {Recognize} the familiar , generalize to the similar , and adapt to the novel},
	volume = {122},
	number = {2},
	author = {Kleinschmidt, Dave F and Jaeger, T Florian},
	year = {2014},
	keywords = {Generalization, 10, 1037, a0038695, adaptation, doi, dx, http, lack of invariance, org, speech perception, statistical learning, supp, supplemental materials},
	file = {Kleinschmidt, Jaeger - 2014 - Robust speech perception Recognize the familiar , generalize to the similar , and adapt to the novel:/home/user/Zotero/storage/CSPZ5P6D/Kleinschmidt, Jaeger - 2014 - Robust speech perception Recognize the familiar , generalize to the similar , and adapt to the novel.pdf:application/pdf}
}

@article{kleinschmidt-belief-updating-2012,
	title = {A belief-updating model of adaptation and cue combination in syntactic comprehension},
	abstract = {Kleinschmidt, D.F., Fine, A.B. \& Jaeger, T.F. (2012). A belief-updating model of adaptation and cue combination in syntactic comprehension. (talk) In Miyake, N., Peebles, D. \& Cooper, R.P. , editors, Proceedings of the 34th Annual Conference of the Cognitive Science Society. Sapporo, Japan. Cognitive Science Society. August, 2012.},
	journal = {Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci12)},
	author = {Kleinschmidt, Dave F and Fine, Alex B and Jaeger, T Florian},
	year = {2012},
	keywords = {adaptation, bayesian model-, cue combination, ing, rational analysis, sentence processing},
	pages = {599--604},
	file = {Kleinschmidt, Fine, Jaeger - 2012 - A belief-updating model of adaptation and cue combination in syntactic comprehension:/home/user/Zotero/storage/FHJ3U8PM/Kleinschmidt, Fine, Jaeger - 2012 - A belief-updating model of adaptation and cue combination in syntactic comprehension.pdf:application/pdf}
}

@article{kassner-harvesting-2012,
	title = {Harvesting paraphrases from recurring sequences in aligned bilingual corpora {From} data-induced units to linguistically interpretable categories},
	author = {Kassner, Laura Bernadette},
	year = {2012},
	file = {Kassner - 2012 - Harvesting paraphrases from recurring sequences in aligned bilingual corpora From data-induced units to linguistically:/home/user/Zotero/storage/C3E8T2W8/Kassner - 2012 - Harvesting paraphrases from recurring sequences in aligned bilingual corpora From data-induced units to linguistically.pdf:application/pdf}
}

@article{kao-computational-2012,
	title = {A {Computational} {Analysis} of {Style}, {Affect}, and {Imagery} in {Contemporary} {Poetry}},
	url = {http://www.aclweb.org/anthology/W12-2502},
	journal = {Proceedings of the NAACL-HLT 2012 Workshop on Computational Linguistics for Literature},
	author = {Kao, Justine and Jurafsky, Dan},
	year = {2012},
	pages = {8--17},
	file = {W12-2502:/home/user/Zotero/storage/ZBCTH59C/W12-2502.pdf:application/pdf}
}

@article{kiefel-probabilistic-nodate,
	title = {Probabilistic {Progress} {Bars}},
	author = {Kiefel, Martin and Schuler, Christian and Hennig, Philipp},
	file = {Kiefel, Schuler, Hennig - Unknown - Probabilistic Progress Bars:/home/user/Zotero/storage/FAIWRQCU/Kiefel, Schuler, Hennig - Unknown - Probabilistic Progress Bars.pdf:application/pdf}
}

@article{kao-discriminative-2011,
	title = {{DISCRIMINATIVE} {DURATION} {MODELING} {FOR} {SPEECH} {RECOGNITION} {WITH} {SEGMENTAL} {CONDITIONAL} {RANDOM} {FIELDS} {Symbolic} {Systems} {Program} {Microsoft} {Research} {One} {Microsoft} {Way} , {Redmond} , {WA} 98052 , {USA}},
	issn = {9781457705397},
	journal = {Word Journal Of The International Linguistic Association},
	author = {Kao, Justine T},
	year = {2011},
	pages = {4476--4479},
	file = {icassp2011_duration:/home/user/Zotero/storage/2RXIIVJ2/icassp2011_duration.pdf:application/pdf}
}

@article{kaipio-statistical-2007,
	title = {Statistical inverse problems: {Discretization}, model reduction and inverse crimes},
	volume = {198},
	issn = {0377-0427},
	doi = {10.1016/j.cam.2005.09.027},
	abstract = {The article discusses the discretization of linear inverse problems. When an inverse problem is formulated in terms of infinite-dimensional function spaces and then discretized for computational purposes, a discretization error appears. Since inverse problems are typically ill-posed, neglecting this error may have serious consequences to the quality of the reconstruction. The Bayesian paradigm provides tools to estimate the statistics of the discretization error that is made part of the measurement and modelling errors of the estimation problem. This approach also provides tools to reduce the dimensionality of inverse problems in a controlled manner. The ideas are demonstrated with a computed example. ?? 2005 Elsevier B.V. All rights reserved.},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Kaipio, Jari and Somersalo, Erkki},
	year = {2007},
	keywords = {Bayesian statistics, Discretization, Inverse problems, Modelling error},
	pages = {493--504},
	file = {Kaipio, Somersalo - 2007 - Statistical inverse problems Discretization, model reduction and inverse crimes:/home/user/Zotero/storage/7D6R73ZK/Kaipio, Somersalo - 2007 - Statistical inverse problems Discretization, model reduction and inverse crimes.pdf:application/pdf}
}

@article{ju-unified-nodate,
	title = {A {Unified} {Semantic} {Embedding} : {Relating} {Taxonomies} and {Attributes}},
	author = {Ju, Sung and Unist, Hwang},
	pages = {1--1},
	file = {4:/home/user/Zotero/storage/6IZ5RTWA/4.pdf:application/pdf}
}

@article{ji-wordrank-2015,
	title = {{WordRank} : {Learning} {Word} {Embeddings} via {Robust} {Ranking}},
	journal = {arXiv},
	author = {Ji, Shihao},
	year = {2015},
	pages = {1--12},
	file = {1506.02761v1:/home/user/Zotero/storage/I4SIGV5P/1506.02761v1.pdf:application/pdf}
}

@inproceedings{jakob-mapping-2010,
	title = {Mapping between {Dependency} {Structures} and {Compositional} {Semantic} {Representations}},
	booktitle = {Proceedings of {LREC} 2010},
	author = {Jakob, Max and Lopatkova, Marketa and Kordoni, Valia},
	year = {2010},
	keywords = {Semantic-Parsing}
}

@article{iyyer-generating-nodate,
	title = {Generating {Sentences} from {Semantic} {Vector} {Space} {Representations}},
	author = {Iyyer, Mohit and Boyd-Graber, Jordan and Daumé Iii, Hal},
	pages = {1--5},
	file = {17:/home/user/Zotero/storage/DFFXV7JX/17.pdf:application/pdf}
}

@article{iwata-warped-2012,
	title = {Warped {Mixtures} for {Nonparametric} {Cluster} {Shapes}},
	url = {http://arxiv.org/abs/1206.1846},
	abstract = {A mixture of Gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters. To produce more appropriate clusterings, we introduce a model which warps a latent mixture of Gaussians to produce nonparametric cluster shapes. The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data. The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets.},
	author = {Iwata, Tomoharu and Duvenaud, David and Ghahramani, Zoubin},
	year = {2012},
	pages = {10--10},
	file = {Iwata, Duvenaud, Ghahramani - 2012 - Warped Mixtures for Nonparametric Cluster Shapes:/home/user/Zotero/storage/MAEJAEC2/Iwata, Duvenaud, Ghahramani - 2012 - Warped Mixtures for Nonparametric Cluster Shapes.pdf:application/pdf}
}

@article{israel-formal-nodate,
	title = {On {Formal} {Versus} {Commonsense}},
	author = {Israel, David},
	pages = {134--139},
	file = {Israel - Unknown - On Formal Versus Commonsense:/home/user/Zotero/storage/GXAJZI2V/Israel - Unknown - On Formal Versus Commonsense.pdf:application/pdf}
}

@article{huang-tree-based-2014,
	title = {Tree-based {Convolution} for {Sentence} {Modeling} ∗},
	number = {1995},
	author = {Huang, Liang and Xiang, Bing and Zhou, Bowen},
	year = {2014},
	pages = {1--6},
	file = {1507.01839v1:/home/user/Zotero/storage/SBNCQPR7/1507.01839v1.pdf:application/pdf}
}

@article{hu-convolutional-nodate,
	title = {Convolutional {Neural} {Network} {Architectures} for {Matching} {Natural} {Language} {Sentences}},
	author = {Hu, Baotian and Lu, Zhengdong},
	pages = {1--9},
	file = {5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences:/home/user/Zotero/storage/AENXCJ5R/5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences.pdf:application/pdf}
}

@article{hill-not-2014,
	title = {Not {All} {Neural} {Embeddings} are {Born} {Equal}},
	author = {Hill, Felix and Cho, KyungHyun and Jean, Sebastien and Devin, Coline and Bengio, Yoshua},
	year = {2014},
	pages = {1--5},
	file = {Hill et al. - 2014 - Not All Neural Embeddings are Born Equal:/home/user/Zotero/storage/2GXPSW46/Hill et al. - 2014 - Not All Neural Embeddings are Born Equal.pdf:application/pdf}
}

@article{hahn-medsyndikate-2002,
	title = {{MedsynDikate} - a natural language system for the extraction of medical information from findings reports},
	volume = {67},
	url = {http://www.sciencedirect.com/science/article/pii/S1386505602000539},
	doi = {10.1016/S1386-5056(02)00053-9},
	number = {1-3},
	journal = {International Journal of Medical Informatics},
	author = {Hahn, Udo and Romacker, Martin and Schulz, Stefan},
	year = {2002},
	keywords = {Information-extraction},
	pages = {63--74}
}

@inproceedings{hahn-evaluating-2012,
	title = {Evaluating the {Meaning} of {Answers} to {Reading} {Comprehension} {Questions}: {A} {Semantics}-{Based} {Approach}},
	url = {http://aclweb.org/anthology/W12-2039.pdf},
	booktitle = {Proceedings of the 7th {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications} ({BEA}7)},
	publisher = {Association for Computational Linguistics},
	author = {Hahn, Michael and Meurers, Detmar},
	year = {2012}
}

@article{hern-learning-nodate,
	title = {Learning the {Semantics} of {Discrete} {Random} {Variables} : {Ordinal} or {Categorical} ?},
	author = {Hern, Daniel and Lloyd, James Robert},
	pages = {1--5},
	file = {14:/home/user/Zotero/storage/BD2AQQPS/14.pdf:application/pdf}
}

@incollection{hahn-knowledge-based-1999,
	address = {Cambridge, MA, USA},
	title = {Knowledge-{Based} {Text} {Summarization}: {Salience} and {Generalization} {Operators} for {Knowledge} {Base} {Abstraction}},
	isbn = {0-262-13359-8},
	booktitle = {Advances in {Automatic} {Text} {Summarization}},
	publisher = {MIT Press},
	author = {Hahn, Udo and Reimer, Ulrich},
	editor = {Maybury, Mark T},
	year = {1999},
	keywords = {Summarization}
}

@inproceedings{hahn-deriving-2011,
	title = {On deriving semantic representations from dependencies: {A} practical approach for evaluating meaning in learner corpora},
	url = {http://purl.org/dm/papers/hahn-meurers-11.html},
	booktitle = {Proceedings of the {Int}. {Conference} on {Dependency} {Linguistics} ({Depling} 2011)},
	author = {Hahn, Michael and Meurers, Detmar},
	year = {2011},
	pages = {94--103}
}

@article{guo-tunable-nodate,
	title = {A {Tunable} {Language} {Model} for {Statistical} {Machine} {Translation}},
	number = {1996},
	author = {Guo, Junfei},
	file = {guoliuhanmal14:/home/user/Zotero/storage/XAXF5R6Q/guoliuhanmal14.pdf:application/pdf}
}

@article{graves-speech-2013,
	title = {Speech {Recognition} {With} {Deep} {Recurrent} {Neural} {Networks}},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	number = {3},
	journal = {Icassp},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	pages = {6645--6649},
	file = {RNN13:/home/user/Zotero/storage/UES3CQ2W/RNN13.pdf:application/pdf}
}

@article{hahn-applying-2013,
	title = {Applying {First}-{Order} {Automated} {Theorem} {Proving} and {Model} {Generation} to {Semantic} {Representations} in {Higher}-{Order} {Logic}},
	url = {files/hahn-ba-thesis.pdf},
	author = {Hahn, Michael},
	year = {2013}
}

@article{grefenstette-new-2014,
	title = {New {Directions} in {Vector} {Space} {Models} of {Meaning}},
	journal = {ACL2014 Tutorials},
	author = {Grefenstette, Edward and Moritz, Karl and Dinu, Georgiana and Blunsom, Phil},
	year = {2014},
	file = {aclVectorTutorial:/home/user/Zotero/storage/K3JPQZ8M/aclVectorTutorial.pdf:application/pdf}
}

@article{graf-evaluating-2014,
	title = {Evaluating {Evaluation} {Metrics} for {Minimalist} {Parsing}},
	journal = {Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics},
	author = {Graf, Thomas and Marcinek, Bradley},
	year = {2014},
	pages = {28--36},
	file = {cmcl2014:/home/user/Zotero/storage/B9MHU9SQ/cmcl2014.pdf:application/pdf}
}

@article{gouws-bilbowa:-2013,
	title = {{BilBOWA}: {Fast} {Bilingual} {Distributed} {Representations} without {Word} {Alignments}},
	abstract = {We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel train-ing data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regular-ize two noise-contrastive language models for ef-ficient cross-lingual feature learning. We show that bilingual embeddings learned using the pro-posed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.},
	author = {Gouws, Stephan and Bengio, Yoshua},
	year = {2013},
	file = {gouws15:/home/user/Zotero/storage/DTUK2HIC/gouws15.pdf:application/pdf}
}

@article{gormley-factor-based-nodate,
	title = {Factor-based {Compositional} {Embedding} {Models}},
	number = {Table 1},
	author = {Gormley, Matthew R and Dredze, Mark},
	pages = {1--5},
	file = {5:/home/user/Zotero/storage/TMUNIXWC/5.pdf:application/pdf}
}

@article{goodman-learning-2007,
	title = {Learning grounded causal models},
	url = {http://web.mit.edu/ndg/www/papers/CVcogsci\_submitted.pdf},
	doi = {10.1.1.72.5484},
	abstract = {We address the problem of learning grounded causal models: systems of concepts that are connected by causal relations and explicitly grounded in perception. We present a Bayesian framework for learning these models—both a causal Bayesian network structure over variables and the consequential region of each variable in perceptual space—from dynamic perceptual evidence. Using a novel experimental paradigm we show that humans are able to learn grounded causal models, and that the Bayesian model accounts well for human performance.},
	journal = {Proceedings of the Twenty- …},
	author = {Goodman, Nd},
	year = {2007},
	keywords = {causal learning, causal model, causal variable, concept grounding, observation},
	pages = {305--310},
	file = {Goodman - 2007 - Learning grounded causal models:/home/user/Zotero/storage/6UCZ26Z2/Goodman - 2007 - Learning grounded causal models.pdf:application/pdf}
}

@article{goldberg-training-2014,
	title = {Training {Deterministic} {Parsers} with {Non}-{Deterministic} {Oracles}},
	volume = {1},
	journal = {Acl2014},
	author = {Goldberg, Yoav and Nivre, Joakim},
	year = {2014},
	pages = {403--414},
	file = {tacl2013dynamic:/home/user/Zotero/storage/XVKC8EZZ/tacl2013dynamic.pdf:application/pdf}
}

@article{geurts-reasoning-2003,
	title = {Reasoning with quantifiers},
	volume = {86},
	issn = {0010-0277, Print},
	doi = {10.1016/S0010-0277(02)00180-4},
	abstract = {In the semantics of natural language, quantification may have received more attention than any other subject, and one of the main topics in psychological studies on deductive reasoning is syllogistic inference, which is just a restricted form of reasoning with quantifiers. But thus far the semantical and psychological enterprises have remained disconnected. This paper aims to show how our understanding of syllogistic reasoning may benefit from semantical research on quantification. I present a very simple logic that pivots on the monotonicity properties of quantified statements - properties that are known to be crucial not only to quantification but to a much wider range of semantical phenomena. This logic is shown to account for the experimental evidence available in the literature as well as for the data from a new experiment with cardinal quantifiers ("at least n" and "at most n"), which cannot be explained by any other theory of syllogistic reasoning. ?? 2002 Elsevier Science B.V. All rights reserved.},
	journal = {Cognition},
	author = {Geurts, Bart},
	year = {2003},
	keywords = {generalized quantifiers, Quantification, Semantics, syllogistic reasoning},
	pages = {223--251},
	file = {Geurts - 2003 - Reasoning with quantifiers:/home/user/Zotero/storage/UX7IHFJ4/Geurts - 2003 - Reasoning with quantifiers.pdf:application/pdf}
}

@article{ganesalingam-fully-2013,
	title = {A fully automatic problem solver with human-style output},
	abstract = {This paper describes a program that solves elementary mathematical problems, mostly in metric space theory, and presents solutions that are hard to distinguish from solutions that might be written by human mathematicians. The program is part of a more general project, which we also discuss.},
	journal = {{\tt arXiv:1309.4501 [cs.AI]}},
	author = {Ganesalingam, M and Gowers, W T},
	year = {2013},
	pages = {1--42},
	file = {1309.4501v1:/home/user/Zotero/storage/GW4R2WMW/1309.4501v1.pdf:application/pdf}
}

@inproceedings{freiburg-terminological-1996,
	title = {A {Terminological} {Qualification} {Calculus} {For} {Preferential} {Reasoning} {Under} {Uncertainty}},
	booktitle = {In {KI}'96 - {Proc}. 20th {Annual} {German} {Conf}. on {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Freiburg, Albert-ludwigs-universitat and Schnattinger, Klemens and Hahn, Udo},
	year = {1996},
	keywords = {AI-Reasoning},
	pages = {349--362}
}

@inproceedings{frank-automatic-2000,
	title = {Automatic {F}-structure {Annotation} of {Treebank} {Trees}},
	booktitle = {Proceedings of the {LFG}00 {Conference}},
	publisher = {CSLI Publications},
	author = {Frank, Anette},
	year = {2000}
}

@book{form-coling-2014,
	title = {{COLING} 2014: {The} 25th {International} {Conference} on {Computational} {Linguistics}},
	isbn = {978-1-941643-27-3},
	author = {Form, Review},
	year = {2014},
	file = {Coling14-1:/home/user/Zotero/storage/6UGNW85J/Coling14-1.pdf:application/pdf}
}

@article{fischer-phrasal-2005,
	title = {Phrasal or lexical constructions?* ¨},
	author = {Fischer, Kerstin and Kay, Paul and Michaelis, Laura and Meurers, Detmar and Richter, Frank and Sag, Ivan and Stefanowitsch, Anatol and Bateman, John and Beermann, Dorothee and Fanselow, Gisbert and Krieger, Hans-ulrich and Mcintyre, Andrew and Dyvik, Helge and Svenonius, Peter},
	year = {2005},
	pages = {850--883},
	file = {phrasal-or-lexical:/home/user/Zotero/storage/K8SNEHWK/phrasal-or-lexical.pdf:application/pdf}
}

@inproceedings{erk-structured-2008,
	title = {A structured vector space model for word meaning in context},
	url = {http://newdesign.aclweb.org/anthology-new/D/D08/D08-1094.pdf},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Erk, Katrin and Padó, Sebastion},
	year = {2008},
	pages = {897--906}
}

@inproceedings{engelmann-empirically-2014,
	title = {An {Empirically} {Grounded} {Approach} to {Extend} the {Linguistic} {Coverage} and {Lexical} {Diversity} of {Verbal} {Probabilities}},
	booktitle = {{CogSci}},
	author = {Engelmann, Christine},
	year = {2014},
	keywords = {corpus-based, criterion for the selection, empir-, epistemic modality, ical semantics, of relevant items, our study introduces a, rigorous empirical, thus balancing the, uncertainty in language comprehension, verbal probabilities},
	file = {paper087:/home/user/Zotero/storage/N4T9HXDB/paper087.pdf:application/pdf}
}

@article{donoho-arxiv-2002,
	title = {{arXiv} : math / 0212395v1 [ math . {ST} ] 1 {Dec} 2002 {Emerging} {Applications} of {Geometric} {Multiscale} {Analysis}},
	volume = {I},
	author = {Donoho, David L},
	year = {2002},
	keywords = {and phrases, curvelets, directional wavelets, harmonic analysis, multiscale analysis, ridgelets, wavelets},
	file = {Donoho - 2002 - arXiv math 0212395v1 math . ST 1 Dec 2002 Emerging Applications of Geometric Multiscale Analysis:/home/user/Zotero/storage/ASK6ST4B/Donoho - 2002 - arXiv math 0212395v1 math . ST 1 Dec 2002 Emerging Applications of Geometric Multiscale Analysis.pdf:application/pdf}
}

@article{ferraro-available-2015,
	title = {On {Available} {Corpora} for {Empirical} {Methods} in {Vision} \& {Language}},
	author = {Ferraro, Francis and Mostafazadeh, Nasrin and Huang, Ting-hao and Vanderwende, Lucy and Devlin, Jacob and Galley, Michel and Mitchell, Margaret},
	year = {2015},
	file = {1506.06833v1:/home/user/Zotero/storage/QZ3M4AXM/1506.06833v1.pdf:application/pdf}
}

@article{dredze-learning-2015,
	title = {Learning {Composition} {Models} for {Phrase} {Embeddings}},
	volume = {3},
	author = {Dredze, Mark},
	year = {2015},
	pages = {227--242},
	file = {586-1672-1-PB:/home/user/Zotero/storage/D5CF2TWF/586-1672-1-PB.pdf:application/pdf}
}

@article{donoho-arxiv-2002-1,
	title = {{arXiv} : math / 0212395v1 [ math . {ST} ] 1 {Dec} 2002 {Emerging} {Applications} of {Geometric} {Multiscale} {Analysis}},
	volume = {I},
	author = {Donoho, David L},
	year = {2002},
	keywords = {and phrases, curvelets, directional wavelets, harmonic analysis, multiscale analysis, ridgelets, wavelets},
	file = {Donoho - 2002 - arXiv math 0212395v1 math . ST 1 Dec 2002 Emerging Applications of Geometric Multiscale Analysis:/home/user/Zotero/storage/KEF9MSAP/Donoho - 2002 - arXiv math 0212395v1 math . ST 1 Dec 2002 Emerging Applications of Geometric Multiscale Analysis.pdf:application/pdf}
}

@article{deng-new-2013,
	title = {New {Types} of {Deep} {Neural} {Network} {Learning} for {Speech} {Recognition} and {Related} {Applications} : an {Overview}},
	issn = {9781479903566},
	author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
	year = {2013},
	pages = {8599--8603},
	file = {overview13:/home/user/Zotero/storage/5UQGMA6N/overview13.pdf:application/pdf}
}

@article{demberg-measuring-2013,
	title = {Measuring linguistically-induced cognitive load during driving using the {ConTRe} task},
	issn = {9781450324786},
	doi = {10.1145/2516540.2516546},
	abstract = {This paper shows that fine-grained linguistic complexity has measurable effects on cognitive load with consequences for the design of in-car spoken dialogue systems. We used synthesized German sentences with grammatical ambiguities to test the additional workload caused by human sentence processing during driving. For the driving task, we used the Continuous Tracking and Reaction (ConTRe) task, which we believe is suitable for the measurement of the fine-grained effects of linguistically-related workload phenomena in automotive environments, as it provides millisecond-level driving deviation measurements on a continuous course. We applied the task in an eye-tracking environment, using a pupillometric measure of cognitive workload called the Index of Cognitive Activity (ICA).},
	journal = {Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
	author = {Demberg, Vera and Sayeed, Asad and Mahr, Angela and Müller, Christian},
	year = {2013},
	keywords = {Language processing, ambiguity, cognitive load, index of cognitive activity, pupillometry, relative clause, simulated driving, steering, tracking task},
	pages = {176--183},
	file = {Demberg et al. - 2013 - Measuring linguistically-induced cognitive load during driving using the ConTRe task:/home/user/Zotero/storage/WQJXDDAU/Demberg et al. - 2013 - Measuring linguistically-induced cognitive load during driving using the ConTRe task.pdf:application/pdf}
}

@article{demberg-data-2008,
	title = {Data from eye-tracking corpora as evidence for theories of syntactic processing complexity},
	volume = {109},
	issn = {0010-0277},
	doi = {10.1016/j.cognition.2008.07.008},
	abstract = {We evaluate the predictions of two theories of syntactic processing complexity, dependency locality theory (DLT) and surprisal, against the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text. Our results show that DLT integration cost is not a significant predictor of reading times for arbitrary words in the corpus. However, DLT successfully predicts reading times for nouns. We also find evidence for integration cost effects at auxiliaries, not predicted by DLT. For surprisal, we demonstrate that an unlexicalized formulation of surprisal can predict reading times for arbitrary words in the corpus. Comparing DLT integration cost and surprisal, we find that the two measures are uncorrelated, which suggests that a complete theory will need to incorporate both aspects of processing complexity. We conclude that eye-tracking corpora, which provide reading time data for naturally occurring, contextualized sentences, can complement experimental evidence as a basis for theories of processing complexity. ?? 2008 Elsevier B.V. All rights reserved.},
	journal = {Cognition},
	author = {Demberg, Vera and Keller, Frank},
	year = {2008},
	keywords = {Surprisal, Corpus data, Dependency locality theory, eye-tracking, processing complexity},
	pages = {193--210},
	file = {Demberg, Keller - 2008 - Data from eye-tracking corpora as evidence for theories of syntactic processing complexity:/home/user/Zotero/storage/G36C24NA/Demberg, Keller - 2008 - Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.pdf:application/pdf}
}

@article{dellert-compiling-2014,
	title = {Compiling the {Uralic} {Dataset} for {NorthEuraLex} , a {Lexicostatistical} {Database} of {Northern} {Eurasia}},
	author = {Dellert, Johannes},
	year = {2014},
	pages = {1--11},
	file = {jdellert-2015:/home/user/Zotero/storage/K36MPUIG/jdellert-2015.pdf:application/pdf}
}

@inproceedings{debili-voyellation-1998,
	series = {Semitic '98},
	title = {Voyellation automatique de l'arabe},
	url = {http://dl.acm.org/citation.cfm?id=1621753.1621761},
	booktitle = {Proceedings of the {Workshop} on {Computational} {Approaches} to {Semitic} {Languages}},
	publisher = {Association for Computational Linguistics},
	author = {Debili, Fathi and Achour, Hadhémi},
	year = {1998},
	pages = {42--49}
}

@article{dagan-recognizing-2009,
	title = {Recognizing textual entailment: {Rational}, evaluation and approaches},
	volume = {15},
	number = {4},
	journal = {Natural Language Engineering},
	author = {Dagan, Ido and Dolan, Bill and Magnini, Bernardo and Roth, Dan},
	year = {2009},
	pages = {i--xvii}
}

@article{davis-hilberts-2013,
	title = {hilberts tenth},
	volume = {80},
	number = {3},
	author = {Davis, Martin},
	year = {2013},
	pages = {233--269},
	file = {Davis - 2013 - hilberts tenth:/home/user/Zotero/storage/BDXEWGIU/Davis - 2013 - hilberts tenth.pdf:application/pdf}
}

@article{cooper-probabilistic-2014,
	title = {A {Probabilistic} {Rich} {Type} {Theory} for {Semantic} {Interpretation}},
	issn = {978-1-937284-74-9},
	abstract = {We propose a probabilistic type theory in which a situation s is judged to be of a type T with probability p. In addition to basic and functional types it includes, inter alia, record types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language.},
	journal = {Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS)},
	author = {Cooper, Robin and Dobnik, Simon and Lappin, Shalom and Larsson, Staffan},
	year = {2014},
	pages = {72--79},
	file = {cdll_ttnls14:/home/user/Zotero/storage/8EJMFTQK/cdll_ttnls14.pdf:application/pdf}
}

@techreport{cooper-using-1996,
	title = {Using the framework},
	author = {Cooper, Robin and Crouch, Dick and Van Eijck, Jan and Fox, Chris and Van Genabith, Johan and Jaspars, Jan and Kamp, Hans and Milward, David and Pinkal, Manfred and Poesio, Massimo and {others}},
	year = {1996}
}

@article{com-phrase-based-2015,
	title = {Phrase-based {Image} {Captioning}},
	volume = {37},
	author = {Com, Ronan Collobert},
	year = {2015},
	file = {lebret15:/home/user/Zotero/storage/UJU576IK/lebret15.pdf:application/pdf}
}

@article{cohen-variational-2009,
	title = {Variational {Inference} for {Grammar} {Induction} with {Prior} {Knowledge}},
	number = {August},
	author = {Cohen, Shay B and Smith, Noah a},
	year = {2009},
	pages = {1--4},
	file = {aclshort09constraints:/home/user/Zotero/storage/W5XR998T/aclshort09constraints.pdf:application/pdf}
}

@inproceedings{com-neural-2015,
	title = {A {Neural} {Conversational} {Model}},
	volume = {37},
	booktitle = {{ICML}},
	author = {Com, Q V L Google},
	year = {2015},
	keywords = {chatbots, dialog systems, neural networks},
	file = {1506.05869v3:/home/user/Zotero/storage/FGHV5DCN/1506.05869v3.pdf:application/pdf}
}

@article{cohen-spectral-2012,
	title = {Spectral {Learning} of {Latent}-{Variable} {PCFGs}},
	volume = {15},
	issn = {9781937284244},
	url = {http://www.cs.columbia.edu/~scohen/acl12spectral.pdf},
	abstract = {We introduce a spectral learning algorithmfor latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condi- tion, we prove that the method provides con- sistent parameter estimates.},
	journal = {Acl},
	author = {Cohen, Shay B and Stratos, Karl and Collins, Michael and Foster, Dean P and Ungar, Lyle},
	year = {2012},
	keywords = {latent-variable pcfgs, spectral learning algorithms},
	pages = {1--31},
	file = {Cohen et al. - 2012 - Spectral Learning of Latent-Variable PCFGs:/home/user/Zotero/storage/GQZSUXN5/Cohen et al. - 2012 - Spectral Learning of Latent-Variable PCFGs.pdf:application/pdf}
}

@article{coecke-mathematical-2010,
	title = {Mathematical {Foundations} for a {Compositional} {Distributional} {Model} of {Meaning}},
	volume = {abs/1003.4},
	journal = {CoRR},
	author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
	year = {2010},
	keywords = {Semantics-Distributional-Compositional}
}

@article{clarke-global-2008,
	title = {Global {Inference} for {Sentence} {Compression} : {An} {Integer} {Linear} {Programming} {Approach} {Doctor} of {Philosophy} {School} of {Informatics} {University} of {Edinburgh}},
	author = {Clarke, James},
	year = {2008},
	file = {Clarke - 2008 - Global Inference for Sentence Compression An Integer Linear Programming Approach Doctor of Philosophy School of Informa:/home/user/Zotero/storage/5RKDVFTK/Clarke - 2008 - Global Inference for Sentence Compression An Integer Linear Programming Approach Doctor of Philosophy School of Informa.pdf:application/pdf}
}

@article{clark-composition-2014,
	title = {Composition in {Distributed} {Semantics}},
	number = {December},
	author = {Clark, Stephen},
	year = {2014},
	file = {StephenClark:/home/user/Zotero/storage/38BA3BCM/StephenClark.pdf:application/pdf}
}

@article{chkrebtii-bayesian-2014,
	title = {Bayesian uncertainty quantification for differential equations},
	url = {http://arxiv.org/abs/1306.2365},
	abstract = {This paper advocates expansion of the role of Bayesian statistical inference when formally quantifying uncertainty in computer models de ned by systems of ordinary or partial di erential equations. We adopt the perspective that implicitly de ned in nite dimensional functions representing model states are objects to be inferred probabilistically. We develop a general methodology for the probabilistic integration of di erential equations via model based updating of a joint prior measure on the space of functions and their temporal and spatial derivatives. This results in a posterior measure over functions re ecting how well they satisfy the system of di erential equations and corresponding initial and boundary values. We show how this posterior measure can be naturally incorporated within the Kennedy and O'Hagan framework for uncertainty quanti cation and provides a fully Bayesian approach to model calibration. By taking this probabilistic viewpoint, the full force of Bayesian inference can be exploited when seeking to coherently quantify and propagate epistemic uncertainty in computer models of complex natural and physical systems. A broad variety of examples are provided to illustrate the potential of this framework for characterising discretization uncertainty, including initial value, delay, and boundary value di erential equations, as well as partial di erential equations. We also demonstrate our methodology on a large scale system, by modeling discretization uncertainty in the solution of the Navier-Stokes equations of uid ow, reduced to over 16,000 coupled and sti ordinary di erential equations. Finally, we discuss the wide range of open research themes that follow from the work presented.},
	journal = {arXiv preprint},
	author = {Chkrebtii, Oksana a. and Campbell, David a. and Girolami, Mark a. and Calderhead, Ben},
	year = {2014},
	keywords = {bayesian numerical analysis, differential, equation models, gaussian processes, uncertainty in computer models, uncertainty quantification},
	pages = {1--36},
	file = {Chkrebtii et al. - 2014 - Bayesian uncertainty quantification for differential equations:/home/user/Zotero/storage/U9I58N2E/Chkrebtii et al. - 2014 - Bayesian uncertainty quantification for differential equations.pdf:application/pdf}
}

@article{cheng-investigating-nodate,
	title = {Investigating the {Role} of {Prior} {Disambiguation} in {Deep}-learning {Compositional} {Models} of {Meaning}},
	volume = {2},
	number = {1},
	author = {Cheng, Jianpeng and Kartsaklis, Dimitri and Grefenstette, Edward},
	pages = {1--5},
	file = {16:/home/user/Zotero/storage/BSQMTIPK/16.pdf:application/pdf}
}

@article{chen-learning-nodate,
	title = {Learning a {Recurrent} {Visual} {Representation} for {Image} {Caption} {Generation}},
	author = {Chen, Xinlei and Lawrence Zitnick, C.},
	file = {LarryZitnick:/home/user/Zotero/storage/2A4WR4H3/LarryZitnick.pdf:application/pdf}
}

@article{chen-fast-nodate,
	title = {A {Fast} and {Accurate} {Dependency} {Parser} using {Neural} {Networks}},
	abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2\% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2\% unlabeled attachment score on the English Penn Treebank.},
	number = {i},
	author = {Chen, Danqi and Manning, Christopher D},
	file = {Chen, Manning - Unknown - A Fast and Accurate Dependency Parser using Neural Networks:/home/user/Zotero/storage/C7A5UWBF/Chen, Manning - Unknown - A Fast and Accurate Dependency Parser using Neural Networks.pdf:application/pdf}
}

@book{carlsson-topology-2009,
	title = {Topology and data},
	volume = {46},
	isbn = {0-01-105100-0},
	abstract = {An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered ... \n},
	author = {Carlsson, Gunnar},
	year = {2009},
	file = {Carlsson - 2009 - Topology and data:/home/user/Zotero/storage/H5PWNPMK/Carlsson - 2009 - Topology and data.pdf:application/pdf}
}

@article{chang-text-2014,
	title = {Text to 3D {Scene} {Generation} with {Rich} {Lexical} {Grounding}},
	author = {Chang, Angel and Monroe, Will and Savva, Manolis and Potts, Christopher and Manning, Christopher D},
	year = {2014},
	file = {chang-acl2015-lexground:/home/user/Zotero/storage/ZB2STXM6/chang-acl2015-lexground.pdf:application/pdf}
}

@article{callison-burch-parametric-2008,
	title = {{ParaMetric} : {An} {Automatic} {Evaluation} {Metric} for {Paraphrasing}},
	issn = {978-1-905593-44-6},
	doi = {10.3115/1599081.1599094},
	abstract = {... 1 Introduction Paraphrasing is useful in a variety of natural lan- guage processing applications including natural language generation, question answering, multi- document summarization and machine translation evaluation. ...},
	number = {August},
	journal = {In Proceedings of the 22nd International Conference on Computational Linguistics (Coling)},
	author = {Callison-burch, Chris and Cohn, Trevor and Lapata, Mirella},
	year = {2008},
	pages = {97--104},
	file = {p97-callison-burch:/home/user/Zotero/storage/3WPV9VER/p97-callison-burch.pdf:application/pdf}
}

@inproceedings{cahill-parsing-2002,
	title = {Parsing with {PCFGs} and {Automatic} {F}-{Structure} {Annotation}},
	booktitle = {Proceedings of the {LFG}02 {Conference}},
	publisher = {CSLI Publications},
	author = {Cahill, Aoife and McCarthy, Mair??ead and van Genabith, Josef and Way, Andy},
	year = {2002},
	keywords = {Semantic\_Parsing}
}

@article{buz-evaluating-nodate,
	title = {Evaluating {Systematicity} in {Neural} {Networks} through {Transformation} {Combination}},
	journal = {Training},
	author = {Buz, Esteban and Buz, Esteban and Frank, Robert and Frank, Robert},
	keywords = {neural networks, rule learning, systematicity},
	pages = {1081--1086},
	file = {Buz et al. - Unknown - Evaluating Systematicity in Neural Networks through Transformation Combination:/home/user/Zotero/storage/DWQPII27/Buz et al. - Unknown - Evaluating Systematicity in Neural Networks through Transformation Combination.pdf:application/pdf}
}

@article{burges-relations-2014,
	title = {Relations {World} : {A} {Possibilistic} {Graphical} {Model} 1},
	number = {3},
	author = {Burges, Christopher J C and Renshaw, Erin and Pastusiak, Andrzej},
	year = {2014},
	pages = {1--10},
	file = {1411.4618v1:/home/user/Zotero/storage/CCC72CWN/1411.4618v1.pdf:application/pdf}
}

@article{bowman-natural-nodate,
	title = {Natural {Logic} {Reasoning}},
	author = {Bowman, Samuel R},
	pages = {1--4},
	file = {Bowman - Unknown - Natural Logic Reasoning:/home/user/Zotero/storage/U6H53DWQ/Bowman - Unknown - Natural Logic Reasoning.pdf:application/pdf}
}

@article{bos-computational-2004,
	title = {Computational {{S}}emantics in {{D}}iscourse: {{U}}nderspecification, {{R}}esolution, and {{I}}nference},
	volume = {13},
	journal = {Journal of Logic, Language and Information},
	author = {Bos, Johan},
	year = {2004},
	pages = {139--157}
}

@inproceedings{bos-towards-2005,
	title = {Towards {Wide}-{Coverage} {Semantic} {Interpretation}},
	booktitle = {Proceedings of {Sixth} {International} {Workshop} on {Computational} {Semantics} {IWCS}-6},
	author = {Bos, Johan},
	year = {2005},
	pages = {42--53}
}

@inproceedings{bos-wide-coverage-2008,
	series = {Research in {Computational} {Semantics}},
	title = {Wide-{Coverage} {Semantic} {Analysis} with {Boxer}},
	url = {http://dl.acm.org/citation.cfm?id=1626503},
	abstract = {Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C\&C tools, Boxer reachesmore than 95\%coverage on newswire texts. The semantic repre- sentations produced by Boxer, known as Discourse Representation Struc- tures (DRSs), incorporate a neo-Davidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer’s performance on the shared task for comparing semantic represtations was promising. It was able to produce complete DRSs for all seven texts. Manually in- specting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall cor- rectly computed; (c) some measure and time expressions are correctly analysed, others aren’t; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C\&C tools and freely available for research purposes.},
	booktitle = {Semantics in {Text} {Processing}. {STEP} 2008 {Conference} {Proceedings}},
	publisher = {College Publications},
	author = {Bos, Johan},
	editor = {Bos, Johan and Delmonte, Rodolfo},
	year = {2008},
	pages = {277--286},
	file = {Bos - 2008 - Wide-coverage semantic analysis with Boxer:/home/user/Zotero/storage/3KC4Q34F/Bos - 2008 - Wide-coverage semantic analysis with Boxer.pdf:application/pdf;Bos - 2008 - Wide-coverage semantic analysis with Boxer(2):/home/user/Zotero/storage/W9GF9TBJ/Bos - 2008 - Wide-coverage semantic analysis with Boxer(2).pdf:application/pdf}
}

@techreport{myhill-finite-1957,
	title = {Finite automata and the representation of events},
	author = {Myhill, John},
	year = {1957},
	pages = {112--137}
}

@inproceedings{murlak-mathematical-2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mathematical {Foundations} of {Computer} {Science} 2011 - 36th {International} {Symposium}, {MFCS} 2011, {Warsaw}, {Poland}, {August} 22-26, 2011. {Proceedings}},
	volume = {6907},
	isbn = {978-3-642-22992-3},
	booktitle = {{MFCS}},
	publisher = {Springer},
	editor = {Murlak, Filip and Sankowski, Piotr},
	year = {2011}
}

@article{mulmuley-p-2011,
	title = {On {P} vs. {NP} and geometric complexity theory},
	volume = {58},
	url = {http://portal.acm.org/citation.cfm?doid=1944345.1944346},
	doi = {10.1145/1944345.1944346},
	number = {2},
	journal = {Journal of the ACM},
	author = {Mulmuley, Ketan D.},
	month = apr,
	year = {2011},
	pages = {1--26},
	file = {Mulmuley - 2011 - On P vs. NP and geometric complexity theory:/home/user/Zotero/storage/76BPUFVQ/Mulmuley - 2011 - On P vs. NP and geometric complexity theory.pdf:application/pdf}
}

@article{moller-counting-2003,
	title = {Counting on {CTL}*: {On} the expressive power of monadic path logic},
	volume = {184},
	doi = {10.1016/S0890-5401(03)00104-4},
	abstract = {Monadic second-order logic (MSOL) provides a general framework for expressing properties of reactive systems as modelled by trees. Monadic path logic (MPL) is obtained by restricting second-order quantification to paths reflecting computation sequences. In this paper we show that the expressive power of MPL over trees coincides with the usual branching time logic CTL* embellished with a simple form of counting. As a corollary, we derive an earlier result that CTL* coincides with the bisimulation-invariant properties of MPL. In order to prove the main result, we first prove a new Composition Theorem for trees.},
	journal = {Information and Computation},
	author = {Moller, Faron and Rabinovich, Alexander},
	year = {2003},
	pages = {147--159},
	file = {count:/home/user/Zotero/storage/96FH72IZ/count.pdf:application/pdf}
}

@article{moller-expressive-1999,
	title = {On the expressive power of {CTL}},
	volume = {1999},
	issn = {0-7695-0158-3},
	doi = {10.1109/LICS.1999.782631},
	abstract = {We show that the expressive power of the branching time logic CTL
coincides with that of the class of bisimulation invariant properties
expressible in so-called monadic path logic: monadic second order logic
in which set quantification is restricted to paths. In order to prove
this result, we first prove a new composition theorem for trees. This
approach is adapted from the approach of Hafer and Thomas in their proof
that CTL coincides with the whole of monadic path logic over the class
of full binary trees},
	number = {157},
	journal = {Proceedings. 14th Symposium on Logic in Computer Science (Cat. No. PR00158)},
	author = {Moller, F. and Rabinovich, L.},
	year = {1999},
	file = {0157:/home/user/Zotero/storage/FHVJA3Z6/0157.pdf:application/pdf}
}

@article{mironov-theory-2015,
	title = {A theory of probabilistic automata, part 1},
	url = {http://arxiv.org/abs/1507.05164},
	abstract = {In the book we present main concepts of probabilistic automata theory.},
	author = {Mironov, Andrew M.},
	year = {2015},
	file = {1507.05164v1:/home/user/Zotero/storage/QJQWB56M/1507.05164v1.pdf:application/pdf}
}

@inproceedings{miller-proceedings-1980,
	title = {Proceedings of the 12th {Annual} {{ACM}} {Symposium} on {Theory} of {Computing}, {April} 28-30, 1980, {Los} {Angeles}, {California}, {{USA}}},
	publisher = {ACM},
	editor = {Miller, Raymond E and Ginsburg, Seymour and Burkhard, Walter A and Lipton, Richard J},
	year = {1980}
}

@article{micha-descriptive-2014,
	title = {Descriptive set theoretic methods in automata theory},
	number = {December},
	author = {Micha, Mechanics},
	year = {2014},
	file = {Micha - 2014 - Descriptive set theoretic methods in automata theory:/home/user/Zotero/storage/3MP544D3/Micha - 2014 - Descriptive set theoretic methods in automata theory.pdf:application/pdf}
}

@article{naturwissenschaften-complexity-2011,
	title = {On the {Complexity} of {Modal} {Logic} {Variants} and their {Fragments}},
	author = {Naturwissenschaften, Doktor Der},
	year = {2011},
	file = {Naturwissenschaften - 2011 - On the Complexity of Modal Logic Variants and their Fragments:/home/user/Zotero/storage/JHRVWCUN/Naturwissenschaften - 2011 - On the Complexity of Modal Logic Variants and their Fragments.pdf:application/pdf}
}

@inproceedings{murlak-mathematical-2011-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mathematical {Foundations} of {Computer} {Science} 2011 - 36th {International} {Symposium}, {MFCS} 2011, {Warsaw}, {Poland}, {August} 22-26, 2011. {Proceedings}},
	volume = {6907},
	isbn = {978-3-642-22992-3},
	booktitle = {{MFCS}},
	publisher = {Springer},
	editor = {Murlak, Filip and Sankowski, Piotr},
	year = {2011}
}

@article{mulmuley-gct-nodate,
	title = {The {GCT} program towards the {P} vs . {NP} problem {Dedicated} to {Sri} {Ramakrishna}},
	author = {Mulmuley, Ketan D},
	keywords = {geometric complexity theory, np, p},
	file = {Mulmuley - Unknown - The GCT program towards the P vs . NP problem Dedicated to Sri Ramakrishna:/home/user/Zotero/storage/D2ESVCZA/Mulmuley - Unknown - The GCT program towards the P vs . NP problem Dedicated to Sri Ramakrishna.pdf:application/pdf}
}

@inproceedings{mosses-tapsoft95:-1995,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{TAPSOFT}'95: {Theory} and {Practice} of {Software} {Development}, 6th {International} {Joint} {Conference} {CAAP}/{FASE}, {Aarhus}, {Denmark}, {May} 22-26, 1995, {Proceedings}},
	volume = {915},
	isbn = {3-540-59293-8},
	publisher = {Springer},
	editor = {Mosses, Peter D and Nielsen, Mogens and Schwartzbach, Michael I},
	year = {1995}
}

@article{mix-first-order-nodate,
	title = {First-{Order} {Expressibility} of {Languages} with {Neutral} {Letters} {Or} : {The} {Crane} {Beach} {Conjecture}},
	number = {March 2004},
	author = {Mix, David A and Immerman, Neil and Lautemann, Clemens and Th, Denis},
	pages = {1--35},
	file = {Mix et al. - Unknown - First-Order Expressibility of Languages with Neutral Letters Or The Crane Beach Conjecture:/home/user/Zotero/storage/DCWBMJ9N/Mix et al. - Unknown - First-Order Expressibility of Languages with Neutral Letters Or The Crane Beach Conjecture.pdf:application/pdf}
}

@article{mix-barrington-non-uniform-1990,
	title = {Non-uniform automata over groups},
	volume = {89},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0890540190900075},
	doi = {10.1016/0890-5401(90)90007-5},
	number = {2},
	journal = {Information and Computation},
	author = {Mix Barrington, David a. and Straubing, Howard and Thérien, Denis},
	month = dec,
	year = {1990},
	pages = {109--132},
	file = {Mix Barrington, Straubing, Thérien - 1990 - Non-uniform automata over groups:/home/user/Zotero/storage/VE9CB76U/Mix Barrington, Straubing, Thérien - 1990 - Non-uniform automata over groups.pdf:application/pdf}
}

@book{mcnaughton-counter-free-1971,
	title = {Counter-free {Automata}},
	publisher = {MIT Press},
	author = {McNaughton, Robert and Papert, Seymour},
	year = {1971}
}

@book{mcnaughton-counter-free-1971-1,
	title = {Counter-free automata. {With} an appendix by {William} {Henneman}.},
	publisher = {Research Monograph No.65. Cambridge, Massachusetts, and London, England: The M. I. T. Press. XIX, 163 p.},
	author = {McNaughton, Robert and Papert, Seymour},
	year = {1971}
}

@article{mckenzie-extensional-2010,
	title = {Extensional {Uniformity} for {Boolean} {Circuits}},
	volume = {39},
	number = {7},
	journal = {SIAM J. Comput.},
	author = {McKenzie, Pierre and Thomas, Michael and Vollmer, Heribert},
	year = {2010},
	pages = {3186--3206}
}

@article{mckenzie-extensional-2010-1,
	title = {Extensional {Uniformity} for {Boolean} {Circuits}},
	volume = {39},
	url = {http://epubs.siam.org/doi/abs/10.1137/080741811},
	doi = {10.1137/080741811},
	number = {7},
	journal = {SIAM Journal on Computing},
	author = {McKenzie, Pierre and Thomas, Michael and Vollmer, Heribert},
	month = jan,
	year = {2010},
	keywords = {03b70, 1, 1 can differ from, 1 of boolean circuits, 68q05, 68q15, 68q19, a family, ams subject classifications, boolean circuits, c n, c n is restricted, descriptive complexity, generally, in which c n, introduction, is uniform if the, n, uniformity, uniformity is imposed by, way},
	pages = {3186--3206},
	file = {McKenzie, Thomas, Vollmer - 2010 - Extensional Uniformity for Boolean Circuits:/home/user/Zotero/storage/AHUJNZAU/McKenzie, Thomas, Vollmer - 2010 - Extensional Uniformity for Boolean Circuits.pdf:application/pdf}
}

@inproceedings{mckenzie-extensional-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Extensional {Uniformity} for {Boolean} {Circuits}},
	volume = {5213},
	isbn = {978-3-540-87530-7},
	booktitle = {{CSL}},
	publisher = {Springer},
	author = {McKenzie, Pierre and Thomas, Michael and Vollmer, Heribert},
	editor = {Kaminski, Michael and Martini, Simone},
	year = {2008},
	pages = {64--78}
}

@article{mckenzie-nc^1:-1991,
	title = {{{NC}{\textasciicircum}1}: {The} {Automata}-{Theoretic} {Viewpoint}},
	volume = {1},
	journal = {Computational Complexity},
	author = {McKenzie, Pierre and Péladeau, Pierre and Thérien, Denis},
	year = {1991},
	pages = {330--359}
}

@article{mckenzie-nci-1991,
	title = {{NCI} : {T} {H} {E} {A} {U} {T} {O} {M} {A} {T} {A} - {T} {H} {E} {O} {R} {E} {T} {I} {C} {VIEWPOINT}},
	volume = {1},
	author = {Mckenzie, Pierre},
	year = {1991},
	pages = {330--359},
	file = {Mckenzie - 1991 - NCI T H E A U T O M A T A - T H E O R E T I C VIEWPOINT:/home/user/Zotero/storage/FCSG26NM/Mckenzie - 1991 - NCI T H E A U T O M A T A - T H E O R E T I C VIEWPOINT.pdf:application/pdf}
}

@article{maroti-guide-nodate,
	title = {A guide for mortals to tame congruence theory ´},
	author = {Maroti, Miklos},
	pages = {1--15},
	file = {Maroti - Unknown - A guide for mortals to tame congruence theory ´:/home/user/Zotero/storage/REJIBPZV/Maroti - Unknown - A guide for mortals to tame congruence theory ´.pdf:application/pdf}
}

@article{maroti-variety-2002,
	title = {variety generated by tournaments},
	author = {Maroti, Miklos},
	year = {2002},
	file = {Maroti - 2002 - variety generated by tournaments:/home/user/Zotero/storage/S3U79JC6/Maroti - 2002 - variety generated by tournaments.pdf:application/pdf}
}

@article{maletti-bakkalaureatsarbeit-nodate,
	title = {Bakkalaureatsarbeit},
	author = {Maletti, Andreas},
	file = {mal01:/home/user/Zotero/storage/3MTN6F5K/mal01.pdf:application/pdf}
}

@article{maletti-diplomarbeit-nodate,
	title = {Diplomarbeit},
	author = {Maletti, Andreas},
	file = {mal02:/home/user/Zotero/storage/BSP3PIXV/mal02.pdf:application/pdf}
}

@article{maletti-power-2006,
	title = {The {Power} of {Tree} {Series} {Transducers}},
	url = {http://www.ims.uni-stuttgart.de/~maletti/pub/slides/mal06c.pdf},
	author = {Maletti, Andreas},
	year = {2006},
	pages = {1--26},
	file = {mal06c:/home/user/Zotero/storage/68G8X7TJ/mal06c.pdf:application/pdf}
}

@article{maurer-property-1965,
	title = {A {Property} of {Finite} {Simple} {Non}-{Abelian} {Groups}},
	volume = {16},
	number = {3},
	journal = {Proc. Am. Math. Soc.},
	author = {Maurer, W. D. and Rhodes, John L.},
	year = {1965},
	pages = {552--554},
	file = {Maurer, Rhodes - 1965 - A Property of Finite Simple Non-Abelian Groups:/home/user/Zotero/storage/DDJUC96V/Maurer, Rhodes - 1965 - A Property of Finite Simple Non-Abelian Groups.pdf:application/pdf}
}

@article{marcolli-seiberg-witten-1999,
	title = {Seiberg-{Witten} {Gauge} {Theory}},
	issn = {8185931224},
	url = {http://www.its.caltech.edu/~matilde/work.html},
	abstract = {{The newly developed field of Seiberg-Witten gauge theory has become a well-established part of the differential topology of four-manifolds and three-manifolds. This book offers an introduction and an up-to-date review of the state of current research. The first part of the book collects some preliminary notions and then gives an introduction of Seiberg-Witten theory of four- dimensional manifolds. In the second part, the author introduces the dimensional reduction and uses it to describe Seiberg-Witten in three-dimensional manifolds. In both parts, the Seiberg-Witten equations are derived, the moduli spaces of solutions are constructed, and the corresponding invariants of manifolds are introduced. In the third part, the author gives an overview of geometric and topological results obtained via Seiberg-Witten theory.  Through all these parts of the book, Seiberg-Witten gauge theory is considered as a completely self-contained subject and no a priori knowledge of Donaldson theory is assumed.  In fact, all the sections that refer to Donaldson theory can be skipped, and this will not affect the comprehension of the remaining sections. In the final part of the book, the author describes physical theories that are responsible for the emergence of this new piece of mathematics, the Seiberg-Witten theory.}},
	author = {Marcolli, Matilde},
	year = {1999},
	pages = {205--205},
	file = {swcosi:/home/user/Zotero/storage/X828P7I7/swcosi.pdf:application/pdf}
}

@article{malod-lower-2015,
	title = {Lower bounds for non-commutative skew circuits ∗},
	volume = {22},
	number = {22},
	author = {Malod, Guillaume},
	year = {2015},
	pages = {1--35},
	file = {Malod - 2015 - Lower bounds for non-commutative skew circuits ∗:/home/user/Zotero/storage/IE5WCPE3/Malod - 2015 - Lower bounds for non-commutative skew circuits ∗.pdf:application/pdf}
}

@article{maler-krohn-rhodes-2010,
	title = {On the {Krohn}-{Rhodes} cascaded decomposition theorem},
	volume = {6200 LNCS},
	issn = {3642137539},
	doi = {10.1007/978-3-642-13754-9\_12},
	abstract = {The Krohn-Rhodes theorem states that any deterministic automaton is a homomorphic image of a cascade of very simple automata which realize either resets or permutations. Moreover, if the automaton is counter-free, only reset automata are needed. In this paper we give a very constructive proof of a variant of this theorem due to Eilenberg.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Maler, Oded},
	year = {2010},
	pages = {260--278},
	file = {Maler - 2010 - On the Krohn-Rhodes cascaded decomposition theorem:/home/user/Zotero/storage/5UX5HA8M/Maler - 2010 - On the Krohn-Rhodes cascaded decomposition theorem.pdf:application/pdf}
}

@article{mahajan-counting-nodate,
	title = {Counting paths in planar width 2 branching programs},
	author = {Mahajan, Meena and Saurabh, Nitin and Sreenivasaiah, Karteek},
	file = {Mahajan, Saurabh, Sreenivasaiah - Unknown - Counting paths in planar width 2 branching programs:/home/user/Zotero/storage/HARK5V76/Mahajan, Saurabh, Sreenivasaiah - Unknown - Counting paths in planar width 2 branching programs.pdf:application/pdf}
}

@book{mac-lane-categories-1998,
	title = {Categories for the {Working} {Mathematician}},
	publisher = {Springer},
	author = {Mac Lane, Saunders},
	year = {1998}
}

@article{luck-parameterized-2014,
	title = {Parameterized {Complexity} of {CTL}: {A} {Generalization} of {Courcelle}'s {Theorem}},
	url = {http://arxiv.org/abs/1410.4044},
	abstract = {We present an almost complete classification of the parameterized complexity of all operator fragments of the satisfiability problem in computation tree logic CTL. The investigated parameterization is the sum of temporal depth and structural pathwidth. The classification shows a dichotomy between W[1]-hard and fixed-parameter tractable fragments. The only real operator fragment which is confirmed to be in FPT is the fragment containing solely AX. Also we prove a generalization of Courcelle's theorem to infinite signatures which will be used to proof the FPT-membership case.},
	author = {Lück, Martin and Meier, Arne and Schindler, Irina},
	month = oct,
	year = {2014},
	pages = {1--15},
	file = {Lück, Meier, Schindler - 2014 - Parameterized Complexity of CTL A Generalization of Courcelle's Theorem:/home/user/Zotero/storage/7UR7EH8X/Lück, Meier, Schindler - 2014 - Parameterized Complexity of CTL A Generalization of Courcelle's Theorem.pdf:application/pdf}
}

@book{lothaire-algebraic-2002,
	address = {Cambridge},
	title = {Algebraic {Combinatorics} on {Words}},
	publisher = {Cambridge University Press},
	author = {Lothaire, M.},
	year = {2002}
}

@article{lohrey-rational-2013,
	title = {Rational subsets and submonoids of wreath products ⋆},
	author = {Lohrey, Markus and Steinberg, Benjamin and Zetzsche, Georg},
	year = {2013},
	pages = {1--19},
	file = {Lohrey, Steinberg, Zetzsche - 2013 - Rational subsets and submonoids of wreath products:/home/user/Zotero/storage/MMAPH4BI/Lohrey, Steinberg, Zetzsche - 2013 - Rational subsets and submonoids of wreath products.pdf:application/pdf;Lohrey, Steinberg, Zetzsche - Unknown - Rational subsets and submonoids of wreath products ⋆:/home/user/Zotero/storage/9NS59X9M/Lohrey, Steinberg, Zetzsche - Unknown - Rational subsets and submonoids of wreath products ⋆.pdf:application/pdf}
}

@article{lohrey-rational-2013-1,
	title = {Rational subsets in groups {Rational} sets in arbitrary monoids : {Definition} 1},
	author = {Lohrey, Markus},
	year = {2013},
	file = {Lohrey - 2013 - Rational subsets in groups Rational sets in arbitrary monoids Definition 1:/home/user/Zotero/storage/BXBH6CMM/Lohrey - 2013 - Rational subsets in groups Rational sets in arbitrary monoids Definition 1.pdf:application/pdf}
}

@article{lovasz-finitely-2011,
	title = {Finitely forcible graphons},
	volume = {101},
	doi = {10.1016/j.jctb.2011.03.005},
	abstract = {We investigate families of graphs and graphons (graph limits) that are determined by a finite number of prescribed subgraph densities. Our main focus is the case when the family contains only one element, i.e., a unique structure is forced by finitely many subgraph densities. Generalizing results of Turán, Erd"s-Simonovits and Chung-Graham-Wilson, we construct numerous finitely forcible graphons. Most of these fall into two categories: one type has an algebraic structure and the other type has an iterated (fractal-like) structure. We also give some necessary conditions for forcibility, which imply that finitely forcible graphons are "rare", and exhibit simple and explicit non-forcible graphons. © 2011 Elsevier Inc.},
	number = {67867},
	journal = {Journal of Combinatorial Theory. Series B},
	author = {Lovász, L. and Szegedy, B.},
	year = {2011},
	keywords = {Extremal graph, Finitely forcible graphon, Graph limit, Graphon},
	pages = {269--301},
	file = {0901.0929v2:/home/user/Zotero/storage/BM9N9ZPZ/0901.0929v2.pdf:application/pdf}
}

@phdthesis{lohrey-computational-2003,
	title = {computational and logical aspects of infinite monoids},
	author = {Lohrey, Markus},
	year = {2003},
	file = {Lohrey - 2003 - computational and logical aspects of infinite monoids:/home/user/Zotero/storage/BB5439QN/Lohrey - 2003 - computational and logical aspects of infinite monoids.pdf:application/pdf}
}

@article{logic-hauptsatz-2013,
	title = {Hauptsatz for {Higher} {Order} {Logic} {Author} ( s ): {Dag} {Prawitz} {Source} : {The} {Journal} of {Symbolic} {Logic} , {Vol} . 33 , {No} . 3 ( {Sep} ., 1968 ), pp . 452-457 {Published} by : {Association} for {Symbolic} {Logic} {Stable} {URL} : http://www.jstor.org/stable/2270331 .},
	volume = {33},
	number = {3},
	author = {Logic, Symbolic},
	year = {2013},
	pages = {452--457},
	file = {Logic - 2013 - Hauptsatz for Higher Order Logic Author ( s ) Dag Prawitz Source The Journal of Symbolic Logic , Vol . 33 , No . 3 ( Sep:/home/user/Zotero/storage/B8HPVTSF/Logic - 2013 - Hauptsatz for Higher Order Logic Author ( s ) Dag Prawitz Source The Journal of Symbolic Logic , Vol . 33 , No . 3 ( Sep.pdf:application/pdf}
}

@article{loeding-visibly-2004,
	title = {Visibly {Pushdown} {Games}},
	journal = {FSTTCS},
	author = {Loeding, Christof and Madhusudan, P and Serre, Olivier},
	year = {2004},
	pages = {408--420},
	file = {Loeding, Madhusudan, Serre - 2004 - Visibly Pushdown Games:/home/user/Zotero/storage/F86TC36R/Loeding, Madhusudan, Serre - 2004 - Visibly Pushdown Games.pdf:application/pdf}
}

@article{limaye-complexity-2008,
	title = {On the complexity of membership and counting in height-deterministic pushdown automata},
	issn = {9783540797098},
	author = {Limaye, Nutan and Mahajan, Meena and Meyer, Antoine},
	year = {2008},
	file = {Limaye, Mahajan, Meyer - 2008 - On the complexity of membership and counting in height-deterministic pushdown automata:/home/user/Zotero/storage/BJRGMGW6/Limaye, Mahajan, Meyer - 2008 - On the complexity of membership and counting in height-deterministic pushdown automata.pdf:application/pdf}
}

@book{libkin-elements-2004,
	title = {Elements of {Finite} {Model} {Theory}},
	isbn = {3-540-21202-7},
	publisher = {Springer},
	author = {Libkin, Leonid},
	year = {2004}
}

@article{loding-methods-1997,
	title = {Methods for the {Transformation} of ω-{Automata}: {Complexity} and {Connection} to {Second} {Order} {Logic}},
	author = {Löding, Christof and Löding, Christof},
	year = {1997},
	file = {diploma_loeding:/home/user/Zotero/storage/IE9WD8QM/diploma_loeding.pdf:application/pdf}
}

@article{liu-doubly-2015,
	title = {Doubly infinite separation of quantum information and communication},
	url = {http://arxiv.org/abs/1507.03546},
	abstract = {We prove the existence of (one-way) communication tasks with a vanishing vs. diverging type of asymptotic gap, which we call "doubly infinite", between quantum information and communication complexities. We do so by showing the following: As the size of the task \$n\$ increases, the quantum communication complexity of a certain regime of the exclusion game, recently introduced by Perry, Jain, and Oppenheim, scales at least logarithmically in \$n\$, while the information cost of a winning quantum strategy may tend to zero. The logarithmic lower bound on the quantum communication complexity is shown to hold even if we allow a small probability of error, although the \$n\$-qubit quantum message of the zero-error strategy can then be compressed polynomially. We leave open the problems of whether the quantum communication complexity of the specified regime scales polynomially in \$n\$, and whether the gap between quantum and classical communication complexities can be superexponential beyond this regime.},
	author = {Liu, Zi-Wen and Perry, Christopher and Zhu, Yechao and Koh, Dax Enshan and Aaronson, Scott},
	year = {2015},
	pages = {1--15},
	file = {1507.03546v1:/home/user/Zotero/storage/PN4GPRF9/1507.03546v1.pdf:application/pdf}
}

@article{litow-boolean-2002,
	title = {On the {Boolean} {Closure} of {Semilinear} {Sets} - {I}.},
	journal = {Technical Report 2002-1, James Cook University, School of Information and Technology},
	author = {Litow, Bruce E and Hartmann, Klaas},
	year = {2002}
}

@article{leroy-asymptotic-nodate,
	title = {Asymptotic properties of free monoid morphisms ´},
	author = {Leroy, Julien and Rigo, Michel},
	pages = {1--25},
	file = {1507.00206v1:/home/user/Zotero/storage/QH2XNICR/1507.00206v1.pdf:application/pdf}
}

@inproceedings{lenzerini-proceedings-2008,
	title = {Proceedings of the {Twenty}-{Seventh} {{ACM}} {{SIGMOD}-{SIGACT}-{SIGART}} {Symposium} on {Principles} of {Database} {Systems}, {{PODS}} 2008, {June} 9-11, 2008, {Vancouver}, {BC}, {Canada}},
	isbn = {978-1-60558-108-8},
	url = {http://dl.acm.org/citation.cfm?id=1376916},
	publisher = {ACM},
	editor = {Lenzerini, Maurizio and Lembo, Domenico},
	year = {2008}
}

@article{leivant-propositional-2008,
	title = {Propositional {Dynamic} {Logic} with {Program} {Quantifiers}},
	volume = {218},
	doi = {10.1016/j.entcs.2008.10.014},
	abstract = {We consider an extension QPDL of Segerberg-Pratt's Propositional Dynamic Logic PDL, with program quantification, and study its expressive power and complexity. A mild form of program quantification is obtained in the calculus ??PDL, extending PDL with recursive procedures (i.e. context free programs), which is known to be ??11-complete. The unrestricted program quantification we consider leads to complexity equivalent to that of second-order logic (and second-order arithmetic), i.e. outside the analytical hierarchy. However, the deterministic variant of QPDL has complexity ??11. ?? 2008.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Leivant, Daniel},
	year = {2008},
	keywords = {??PDL, Propositional dynamic logic with program quantific, QPDL},
	pages = {231--240},
	file = {lose06:/home/user/Zotero/storage/6HE6PV6R/lose06.pdf:application/pdf}
}

@article{lee-quantum-2010,
	title = {Quantum query complexity of state conversion},
	url = {http://arxiv.org/abs/1011.3020},
	doi = {10.1109/FOCS.2011.75},
	abstract = {State conversion generalizes query complexity to the problem of converting between two input-dependent quantum states by making queries to the input. We characterize the complexity of this problem by introducing a natural information-theoretic norm that extends the Schur product operator norm. The complexity of converting between two systems of states is given by the distance between them, as measured by this norm.   In the special case of function evaluation, the norm is closely related to the general adversary bound, a semi-definite program that lower-bounds the number of input queries needed by a quantum algorithm to evaluate a function. We thus obtain that the general adversary bound characterizes the quantum query complexity of any function whatsoever. This generalizes and simplifies the proof of the same result in the case of boolean input and output. Also in the case of function evaluation, we show that our norm satisfies a remarkable composition property, implying that the quantum query complexity of the composition of two functions is at most the product of the query complexities of the functions, up to a constant. Finally, our result implies that discrete and continuous-time query models are equivalent in the bounded-error setting, even for the general state-conversion problem.},
	author = {Lee, Troy and Mittal, Rajat and Reichardt, Ben W. and Spalek, Robert and Szegedy, Mario},
	month = nov,
	year = {2010},
	pages = {19--19},
	file = {Lee et al. - 2010 - Quantum query complexity of state conversion:/home/user/Zotero/storage/ZXZVUHV5/Lee et al. - 2010 - Quantum query complexity of state conversion.pdf:application/pdf}
}

@article{lee-no-nodate,
	title = {No {Title}},
	author = {Lee, Troy},
	file = {Lee - Unknown - No Title:/home/user/Zotero/storage/AGUEXPID/Lee - Unknown - No Title.pdf:application/pdf}
}

@book{lawson-finite-2004,
	title = {Finite automata},
	publisher = {Chapman \& Hall/CRC, Boca Raton London New York Washington, D.C.},
	author = {Lawson, Mark V},
	year = {2004}
}

@article{lautemann-descriptive-2001,
	title = {The {Descriptive} {Complexity} {Approach} to {{LOGCFL}}},
	volume = {62},
	number = {4},
	journal = {J. Comput. Syst. Sci.},
	author = {Lautemann, Clemens and McKenzie, Pierre and Schwentick, Thomas and Vollmer, Heribert},
	year = {2001},
	pages = {629--652}
}

@incollection{lange-boolean-2012,
	address = {Heidelberg},
	series = {{LNCS}},
	title = {The {Boolean} {Formula} {Value} {Problem} as {Formal} {Language}},
	booktitle = {Languages {Alive} -- {Essays} {Dedicated} to {J}{ü}rgen {Dassow} on the {Occasion} of {His} 65th {Birthday}},
	publisher = {Springer},
	author = {Lange, Klaus-Jörn},
	year = {2012},
	pages = {138--144}
}

@inproceedings{lange-complexity-1993,
	title = {Complexity and {Structure} in {Formal} {Language} {Theory}},
	booktitle = {Structure in {Complexity} {Theory} {Conference}},
	author = {Lange, Klaus-Jörn},
	year = {1993},
	pages = {224--238}
}

@article{lange-some-results-on-majority-quantifiers-over-words.pdf-nodate,
	title = {some\_results\_on\_majority\_quantifiers\_over\_words.pdf},
	author = {Lange, Klaus-Jörn},
	file = {Lange - Unknown - some_results_on_majority_quantifiers_over_words.pdf:/home/user/Zotero/storage/6NF9RUPE/Lange - Unknown - some_results_on_majority_quantifiers_over_words.pdf.pdf:application/postscript}
}

@article{lange-note-2010,
	title = {A {Note} on the {P}-completeness of {Deterministic} {One}-way {Stack} {Language}},
	volume = {16},
	number = {5},
	journal = {Journal of Universal Computer Science},
	author = {Lange, K.-J.},
	year = {2010},
	pages = {795--799}
}

@article{ladner-structure-1975,
	title = {On the {Structure} of {Polynomial} {Time} {Reducibility}},
	volume = {22},
	url = {http://doi.acm.org/10.1145/321864.321877},
	doi = {10.1145/321864.321877},
	number = {1},
	journal = {J. {ACM}},
	author = {Ladner, Richard E},
	year = {1975},
	pages = {155--171}
}

@article{kunc-structure-nodate,
	title = {Structure of {Finite} {Semigroups} and {Language} {Equations}},
	author = {Kunc, Michal},
	file = {Kunc - Unknown - Structure of Finite Semigroups and Language Equations:/home/user/Zotero/storage/WN8MNIMR/Kunc - Unknown - Structure of Finite Semigroups and Language Equations.pdf:application/pdf}
}

@article{kufleitner-lattice-2010,
	title = {On the lattice of sub-pseudovarieties of {DA}},
	volume = {81},
	url = {http://link.springer.com/10.1007/s00233-010-9258-6},
	doi = {10.1007/s00233-010-9258-6},
	number = {2},
	journal = {Semigroup Forum},
	author = {Kufleitner, Manfred and Weil, Pascal},
	month = aug,
	year = {2010},
	pages = {243--254},
	file = {Kufleitner, Weil - 2010 - On the lattice of sub-pseudovarieties of DA:/home/user/Zotero/storage/FUATZZ6Z/Kufleitner, Weil - 2010 - On the lattice of sub-pseudovarieties of DA.pdf:application/pdf}
}

@inproceedings{lautemann-algebraic-2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Algebraic} {Point} of {View} on the {Crane} {Beach} {Property}},
	volume = {4207},
	isbn = {3-540-45458-6},
	booktitle = {{CSL}},
	publisher = {Springer},
	author = {Lautemann, Clemens and Tesson, Pascal and Thérien, Denis},
	editor = {Ésik, Zoltán},
	year = {2006},
	pages = {426--440}
}

@article{lautemann-algebraic-nodate,
	title = {An {Algebraic} {Point} of {View} on the {Crane}-{Beach} {Conjecture}},
	author = {Lautemann, Clemens and Tesson, Pascal and Thérien, Denis},
	file = {Lautemann, Tesson, Thérien - Unknown - An Algebraic Point of View on the Crane-Beach Conjecture:/home/user/Zotero/storage/BA9B9IKT/Lautemann, Tesson, Thérien - Unknown - An Algebraic Point of View on the Crane-Beach Conjecture.pdf:application/pdf}
}

@article{lautemann-descriptive-1998,
	title = {The descriptive complexity approach to {LOGCFL}},
	url = {http://arxiv.org/abs/cs/9809114},
	abstract = {Building upon the known generalized-quantifier-based first-order characterization of LOGCFL, we lay the groundwork for a deeper investigation. Specifically, we examine subclasses of LOGCFL arising from varying the arity and nesting of groupoidal quantifiers. Our work extends the elaborate theory relating monoidal quantifiers to NC1 and its subclasses. In the absence of the BIT predicate, we resolve the main issues: we show in particular that no single outermost unary groupoidal quantifier with FO can capture all the context-free languages, and we obtain the surprising result that a variant of Greibach's ``hardest context-free language'' is LOGCFL-complete under quantifier-free BIT-free projections. We then prove that FO with unary groupoidal quantifiers is strictly more expressive with the BIT predicate than without. Considering a particular groupoidal quantifier, we prove that first-order logic with majority of pairs is strictly more expressive than first-order with majority of individuals. As a technical tool of independent interest, we define the notion of an aperiodic nondeterministic finite automaton and prove that FO translations are precisely the mappings computed by single-valued aperiodic nondeterministic finite transducers.},
	author = {Lautemann, Clemens and McKenzie, Pierre and Schwentick, Thomas and Vollmer, Heribert},
	month = sep,
	year = {1998},
	keywords = {descriptive complexity, automata and formal languages, computational com-, finite model theory, plexity},
	pages = {10--10},
	file = {Lautemann et al. - 1998 - The descriptive complexity approach to LOGCFL:/home/user/Zotero/storage/AUHDDXXS/Lautemann et al. - 1998 - The descriptive complexity approach to LOGCFL.pdf:application/pdf}
}

@article{lange-reversible-2000,
	title = {Reversible {Space} {Equals} {Deterministic} {Space}},
	volume = {60},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Lange, Klaus-Jörn and McKenzie, Pierre and Tapp, Alain},
	year = {2000},
	pages = {354--367}
}

@inproceedings{lange-results-2004,
	title = {Some {Results} on {Majority} {Quantifiers} over {Words}},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Lange, Klaus-Jörn},
	year = {2004},
	pages = {123--129}
}

@book{lallement-semigroups-1979,
	address = {New York},
	title = {Semigroups and {Combinatorial} {Applications}},
	publisher = {Wiley-Interscience},
	author = {Lallement, Gerard},
	year = {1979}
}

@article{ladner-circuit-1975,
	title = {The circuit value problem is log space complete for {P}},
	volume = {7},
	url = {http://doi.acm.org/10.1145/990518.990519},
	doi = {10.1145/990518.990519},
	number = {1},
	journal = {SIGACT News},
	author = {Ladner, Richard E},
	month = jan,
	year = {1975},
	pages = {18--20}
}

@article{kumar-visibly-2007,
	title = {Visibly {Pushdown} {Automata} for {Streaming} {XML}},
	issn = {9781595936547},
	url = {http://portal.acm.org/citation.cfm?doid=1242572.1242714},
	doi = {10.1145/1242572.1242714},
	abstract = {We propose the study of visibly pushdown automata (VPA) for processing XML documents. VPAs are pushdown automata where the input determines the stack operation, and XML documents are naturally visibly pushdown with the VPA pushing onto the stack on open-tags and popping the stack on close-tags. In this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for XML documents. We study the problems of type-checking streaming XML documents against SDTD schemas, and the problem of typing tags in a streaming XML document according to an SDTD schema. For the latter problem, we consider both pre-order typing and post-order typing of a document, which dynamically determines types at open-tags and close-tags respectively as soon as they are met. We also generalize the problems of pre-order and post-order typing to prefix querying. We show that a deterministic VPA yields an algorithm to the problem of answering in one pass the set of all answers to any query that has the property that a node satisfying the query is determined solely by the prefix leading to the node. All the streaming algorithms we develop in this paper are based on the construction of deterministic VPAs, and hence, for any fixed problem, the algorithms process each element of the input in constant time, and use space (d), where d is the depth of the document.},
	journal = {Proceedings of the 16th international conference on World Wide Web - WWW '07},
	author = {Kumar, Viraj and Madhusudan, P. and Viswanathan, Mahesh},
	year = {2007},
	keywords = {push-, query, schema, streaming algorithms, typing, xml},
	pages = {1053--1053},
	file = {Kumar, Madhusudan, Viswanathan - 2007 - Visibly Pushdown Automata for Streaming XML:/home/user/Zotero/storage/CJDKWZZ7/Kumar, Madhusudan, Viswanathan - 2007 - Visibly Pushdown Automata for Streaming XML.pdf:application/pdf}
}

@article{kufleitner-fo2-2009,
	title = {On {FO}2 quantifier alternation over words {To} cite this version :},
	author = {Kufleitner, Manfred and Weil, Pascal and Fo, On},
	year = {2009},
	file = {Kufleitner, Weil, Fo - 2009 - On FO2 quantifier alternation over words To cite this version:/home/user/Zotero/storage/2TPBTS4R/Kufleitner, Weil, Fo - 2009 - On FO2 quantifier alternation over words To cite this version.pdf:application/pdf}
}

@article{kufleitner-quantifier-nodate,
	title = {Quantifier {Alternation} in {Two}-{Variable} {First}-{Order} {Logic} with {Successor} {Is} {Decidable} ∗},
	author = {Kufleitner, Manfred and Lauser, Alexander},
	keywords = {2013, 305, 4230, and phrases automata theory, digital object identifier 10, first-order logic, lipics, regular languages, semigroups, stacs},
	pages = {305--316},
	file = {Kufleitner, Lauser - Unknown - Quantifier Alternation in Two-Variable First-Order Logic with Successor Is Decidable ∗:/home/user/Zotero/storage/KTAC536Z/Kufleitner, Lauser - Unknown - Quantifier Alternation in Two-Variable First-Order Logic with Successor Is Decidable ∗.pdf:application/pdf}
}

@article{kufleitner-survey-nodate,
	title = {A {Survey} on the {Local} {Divisor} {Technique}},
	author = {Kufleitner, Manfred},
	pages = {1--18},
	file = {Kufleitner - Unknown - A Survey on the Local Divisor Technique:/home/user/Zotero/storage/FVZCKGTM/Kufleitner - Unknown - A Survey on the Local Divisor Technique.pdf:application/pdf}
}

@article{kufleitner-star-free-nodate,
	title = {Star-{Free} {Languages} and {Local} {Divisors}},
	author = {Kufleitner, Manfred},
	pages = {1--6},
	file = {Kufleitner - Unknown - Star-Free Languages and Local Divisors:/home/user/Zotero/storage/4X8Z2IRZ/Kufleitner - Unknown - Star-Free Languages and Local Divisors.pdf:application/pdf}
}

@article{kubat-identities-2015,
	title = {Identities of the plactic monoid},
	issn = {0023301496099},
	doi = {10.1007/s00233-014-9609-9},
	author = {Kubat, Łukasz and Okni, Jan},
	year = {2015},
	keywords = {bicyclic monoid, identity, plactic monoid},
	pages = {100--112},
	file = {Kubat, Okni - 2015 - Identities of the plactic monoid:/home/user/Zotero/storage/R4WHPT28/Kubat, Okni - 2015 - Identities of the plactic monoid.pdf:application/pdf}
}

@article{krebs-universal-nodate,
	title = {Universal covers , color refinement , and two-variable counting logic : {Lower} bounds for the depth},
	author = {Krebs, Andreas and Verbitsky, Oleg},
	pages = {1--25},
	file = {journal:/home/user/Zotero/storage/VZ992FGM/journal.pdf:application/pdf}
}

@article{krokhin-complexity-2005,
	title = {The complexity of constraint satisfaction : an algebraic approach.},
	issn = {978-1-4020-3817-4},
	url = {http://dx.doi.org/10.1007/1-4020-3817-8\_8},
	doi = {10.1007/1-4020-3817-8\_8},
	abstract = {Many computational problems arising in artificial intelligence, computer science and elsewhere can be represented as constraint satisfaction and optimization problems. In this survey paper we discuss an algebraic approach that has proved to be very successful in studying the complexity of constraint problems.},
	number = {257039},
	author = {Krokhin, a. and Bulatov, a. and Jeavons, P.},
	year = {2005},
	keywords = {4230, digital object identifier 10, lipics, stacs, 2, 2015, and phrases constraint satisfaction, and spatial reasoning, category invited talk, clones, model theory, temporal, universal algebra},
	pages = {2--9},
	file = {Krokhin, Bulatov, Jeavons - 2005 - The complexity of constraint satisfaction an algebraic approach:/home/user/Zotero/storage/Z69K3R6Q/Krokhin, Bulatov, Jeavons - 2005 - The complexity of constraint satisfaction an algebraic approach.pdf:application/pdf}
}

@article{krohn-algebraic-1965,
	title = {Algebraic {Theory} of {Machines}. {I}: {Decomposition} {Theorem} for {Finite} {Semigroups} and {Machines}},
	volume = {116},
	journal = {Trans. Am. Math. Soc.},
	author = {Krohn, Kenneth and Rhodes, John},
	year = {1965},
	pages = {450--464}
}

@article{krohn-algebraic-1965-1,
	title = {The {Algebraic} {Theory} of {Machines}},
	volume = {116},
	journal = {Transactions of the AMS},
	author = {Krohn, Kenneth and Rhodes, John},
	year = {1965},
	pages = {450--464}
}

@article{krohn-methods-1967,
	title = {Methods of the {Algebraic} {Theory} of {Machines}. {I}: {Decomposition} {Theorem} for {Generalized} {Machines}; {Properties} {Preserved} under {Series} and {Parallel} {Compositions} of {Machines}},
	volume = {1},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Krohn, Kenneth and Mateosian, Richard and Rhodes, John},
	year = {1967},
	pages = {55--85}
}

@article{krebs-universal-nodate-1,
	title = {Universal covers , color refinement , and two-variable counting logic : {Lower} bounds for the depth},
	author = {Krebs, Andreas and Verbitsky, Oleg},
	pages = {1--25},
	file = {Krebs, Verbitsky - Unknown - Universal covers , color refinement , and two-variable counting logic Lower bounds for the depth:/home/user/Zotero/storage/HAN4KEKD/Krebs, Verbitsky - Unknown - Universal covers , color refinement , and two-variable counting logic Lower bounds for the depth.pdf:application/pdf}
}

@article{krebs-ef-2014,
	title = {{EF} + {EX} {Forest} {Algebras} {arXiv} : 1408 . 0809v1 [ cs . {LO} ] 4 {Aug} 2014},
	volume = {0},
	author = {Krebs, Andreas and {straubing}},
	year = {2014},
	pages = {1--27},
	file = {Krebs, straubing - 2014 - EF EX Forest Algebras arXiv 1408 . 0809v1 cs . LO 4 Aug 2014:/home/user/Zotero/storage/UZIC6SK2/Krebs, straubing - 2014 - EF EX Forest Algebras arXiv 1408 . 0809v1 cs . LO 4 Aug 2014.pdf:application/pdf}
}

@article{krebs-characterizing-2005,
	title = {Characterizing {TC}0 in {Terms} of {Infinite} {Groups}},
	volume = {40},
	url = {http://link.springer.com/article/10.1007/s00224-006-1310-2},
	number = {4},
	journal = {Theory of Computing Systems},
	author = {Krebs, Andreas and Reifferscheid, Stephanie and Lange, KJ},
	year = {2005},
	pages = {303--325},
	file = {Krebs, Reifferscheid - 2014 - Characterizing TC 0 in Terms of Infinite Groups:/home/user/Zotero/storage/GP9KASMP/Krebs, Reifferscheid - 2014 - Characterizing TC 0 in Terms of Infinite Groups.pdf:application/pdf}
}

@article{krebs-parallel-nodate,
	title = {Parallel {Computational} {Tree} {Logic}},
	author = {Krebs, Andreas and Meier, Arne and Virtema, Jonni},
	file = {Krebs, Meier, Virtema - Unknown - Parallel Computational Tree Logic:/home/user/Zotero/storage/HEZNDFKQ/Krebs, Meier, Virtema - Unknown - Parallel Computational Tree Logic.pdf:application/pdf}
}

@article{krebs-model-2015,
	title = {The model checking fingerprints of {CTL} operators},
	url = {http://arxiv.org/abs/1504.04708},
	abstract = {The aim of this study is to understand the inherent expressive power of CTL operators. We investigate the complexity of model checking for all CTL fragments with one CTL operator and arbitrary Boolean operators. This gives us a fingerprint of each CTL operator. The comparison between the fingerprints yields a hierarchy of the operators that mirrors their strength with respect to model checking.},
	author = {Krebs, Andreas and Meier, Arne and Mundhenk, Martin},
	year = {2015},
	pages = {25--27},
	file = {1504.04708v2:/home/user/Zotero/storage/VBFJS8EX/1504.04708v2.pdf:application/pdf}
}

@inproceedings{krebs-streaming-2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Streaming {Algorithms} for {Recognizing} {Nearly} {Well}-{Parenthesized} {Expressions}},
	volume = {6907},
	isbn = {978-3-642-22992-3},
	booktitle = {{MFCS}},
	publisher = {Springer},
	author = {Krebs, Andreas and Limaye, Nutan and Srinivasan, Srikanth},
	editor = {Murlak, Filip and Sankowski, Piotr},
	year = {2011},
	pages = {412--423}
}

@inproceedings{krebs-characterizing-2005-1,
	title = {Characterizing {{TC}{\textasciicircum}{\mbox{0}}} in {Terms} of {Infinite} {Groups}},
	booktitle = {{STACS}},
	author = {Krebs, Andreas and Lange, Klaus-Jörn and Reifferscheid, Stephanie},
	year = {2005},
	pages = {496--507}
}

@article{krebs-characterizing-2007,
	title = {Characterizing {{TC}{\textasciicircum}{\mbox{0}}} in {Terms} of {Infinite} {Groups}},
	volume = {40},
	number = {4},
	journal = {Theory Comput. Syst.},
	author = {Krebs, Andreas and Lange, Klaus-Jörn and Reifferscheid, Stephanie},
	year = {2007},
	pages = {303--325}
}

@inproceedings{krebs-dense-2012,
	title = {Dense {Completeness}},
	url = {http://dx.doi.org/10.1007/978-3-642-31653-1\_17},
	doi = {10.1007/978-3-642-31653-1\_17},
	booktitle = {Developments in {Language} {Theory} - 16th {International} {Conference}, {{DLT}} 2012, {Taipei}, {Taiwan}, {August} 14-17, 2012. {Proceedings}},
	author = {Krebs, Andreas and Lange, Klaus-Jörn},
	year = {2012},
	pages = {178--189}
}

@phdthesis{krebs-typed-2008,
	title = {{{T}}yped {{S}}emigroups, {{M}}ajority {{L}}ogic, and {{T}}hreshold {{C}}ircuits.},
	author = {Krebs, Andreas},
	year = {2008}
}

@phdthesis{krebs-typed-2008-1,
	title = {Typed semigroups, majority logic, and threshold circuits},
	url = {http://tobias-lib.ub.uni-tuebingen.de/volltexte/2008/3624/},
	author = {Krebs, Andreas},
	year = {2008}
}

@inproceedings{koucky-bounded-depth-2005,
	title = {Bounded-depth circuits: separating wires from gates},
	booktitle = {{STOC}},
	author = {Koucký, Michal and Pudlák, Pavel and Thérien, Denis},
	year = {2005},
	pages = {257--265}
}

@inproceedings{koucky-circuit-2006,
	title = {Circuit {Lower} {Bounds} via {Ehrenfeucht}-{Fraisse} {Games}},
	booktitle = {21st {Annual} {IEEE} {Conference} on {Computational} {Complexity} ({CCC} 2006), 16-20 {July} 2006, {Prague}, {Czech} {Republic}},
	publisher = {IEEE Computer Society},
	author = {Koucký, Michal and Lautemann, Clemens and Poloczek, Sebastian and Thérien, Denis},
	year = {2006},
	pages = {190--201}
}

@article{korvin-approximation-1968,
	title = {Approximation theorems on some classes of automata},
	author = {Korvin, Andre De},
	year = {1968},
	file = {Korvin - 1968 - Approximation theorems on some classes of automata:/home/user/Zotero/storage/5ABPMHPK/Korvin - 1968 - Approximation theorems on some classes of automata.pdf:application/pdf}
}

@article{kontinen-complexity-2011,
	title = {Complexity of two-variable dependence logic and {IF}-logic},
	issn = {9780769544120},
	doi = {10.1109/LICS.2011.14},
	abstract = {We study the two-variable fragments D{\textasciicircum}2 and IF{\textasciicircum}2 of dependence logic and independence-friendly logic. We consider the satisfiability and finite satisfiability problems of these logics and show that for D{\textasciicircum}2, both problems are NEXPTIME-complete, whereas for IF{\textasciicircum}2, the problems are undecidable. We also show that D{\textasciicircum}2 is strictly less expressive than IF{\textasciicircum}2 and that already in D{\textasciicircum}2, equicardinality of two unary predicates and infinity can be expressed (the latter in the presence of a constant symbol).An extended version of this publication can be found at arxiv.org.},
	journal = {Proceedings - Symposium on Logic in Computer Science},
	author = {Kontinen, Juha and Kuusisto, Antti and Lohmann, Peter and Virtema, Jonni},
	year = {2011},
	keywords = {Complexity, Decidability, Dependence logic, Expressivity, Independence-friendly logic, Satisfiability, Two-variable logic},
	pages = {289--298},
	file = {1104.3148v1:/home/user/Zotero/storage/BEXDWZ5Q/1104.3148v1.pdf:application/pdf}
}

@article{krebs-ef+ex-2014,
	title = {{EF}+{EX} {Forest} {Algebras}.},
	volume = {abs/1408.0},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1408.html#KrebsS14},
	journal = {CoRR},
	author = {Krebs, Andreas and Straubing, Howard},
	year = {2014}
}

@article{krebs-effective-2012,
	title = {An effective characterization of the alternation hierarchy in two-variable logic},
	author = {Krebs, Andreas and Straubing, Howard},
	year = {2012},
	pages = {1--18},
	file = {Krebs, Straubing - 2012 - An effective characterization of the alternation hierarchy in two-variable logic:/home/user/Zotero/storage/HQ35A9XA/Krebs, Straubing - 2012 - An effective characterization of the alternation hierarchy in two-variable logic.pdf:application/pdf}
}

@inproceedings{krebs-non-definability-2012,
	title = {Non-definability of {Languages} by {Generalized} {First}-order {Formulas} over ({N}, +)},
	booktitle = {Proceedings of the 27th {Annual} {IEEE} {Symposium} on {Logic} in {Computer} {Science}, {LICS} 2012, {Dubrovnik}, {Croatia}, {June} 25-28, 2012},
	publisher = {IEEE},
	author = {Krebs, Andreas and Sreejith, A V},
	year = {2012},
	pages = {451--460}
}

@article{krebs-counting-2012,
	title = {Counting paths in {VPA} is complete for \#{NC}1},
	volume = {64},
	issn = {3642140300},
	doi = {10.1007/s00453-011-9501-x},
	number = {March 2011},
	journal = {Algorithmica},
	author = {Krebs, Andreas and Limaye, Nutan and Mahajan, Meena},
	year = {2012},
	pages = {279--294}
}

@article{krebs-counting-2012-1,
	title = {Counting {Paths} in {VPA} {Is} {Complete} for \#{NC} 1},
	volume = {64},
	number = {2},
	journal = {Algorithmica},
	author = {Krebs, Andreas and Limaye, Nutan and Mahajan, Meena},
	year = {2012},
	pages = {279--294}
}

@inproceedings{krebs-visibly-2015,
	title = {Visibly {Counter} {Languages} and {Constant} {Depth} {Circuits}},
	booktitle = {{{STACS}} 2015},
	author = {Krebs, Andreas and Lange, Klaus-Jörn and Ludwig, Michael},
	year = {2015},
	keywords = {ac0, and phrases visibly counter, automata, constant depth circuits, fo},
	file = {Krebs, Lange, Ludwig - 2015 - Visibly Counter Languages and Constant Depth Circuits:/home/user/Zotero/storage/2TUJMMKU/Krebs, Lange, Ludwig - 2015 - Visibly Counter Languages and Constant Depth Circuits.pdf:application/pdf}
}

@phdthesis{krebs-typed-2008-2,
	title = {Typed {Semigroups}, {Majority} {Logic}, and {Threshold} {Circuits}},
	author = {Krebs, Andreas},
	year = {2008},
	file = {Krebs - 2008 - Typed Semigroups , Majority Logic , and Threshold Circuits:/home/user/Zotero/storage/9VVRBJ79/Krebs - 2008 - Typed Semigroups , Majority Logic , and Threshold Circuits.pdf:application/pdf}
}

@article{koucky-circuit-2006-1,
	title = {Circuit lower bounds via {Ehrenfeucht}-{Fraisse} games},
	issn = {0-7695-2596-2},
	doi = {10.1109/CCC.2006.12},
	abstract = {In this paper we prove that the class of functions expressible by first order formulas with only two variables coincides with the class of functions computable by AC0 circuits with a linear number of gates. We then investigate the feasibility of using Ehrenfeucht-Fraisse games to prove lower bounds for that class of circuits, as well as for general AC0 circuits},
	journal = {21st Annual IEEE Conference on Computational Complexity (CCC'06)},
	author = {Koucky, M. and Poloczek, S. and Lautemann, C. and Therien, D.},
	year = {2006},
	file = {Koucky et al. - 2006 - Circuit lower bounds via Ehrenfeucht-Fraisse games:/home/user/Zotero/storage/7ADPHE3P/Koucky et al. - 2006 - Circuit lower bounds via Ehrenfeucht-Fraisse games.pdf:application/pdf}
}

@article{zhu-cross-linguistic-2014,
	title = {Cross-linguistic {Evidence} for {Cognitive} {Foundations} of {Polysemy}},
	abstract = {Existing discussions of polysemy describe the relations that extended senses may have to the most central sense of a word, but they do not explain in more detail how particular senses are generated for a given word. We propose that extended senses are initially built on the salient features of referents of core senses (and further senses may be generated from those). We provide evidence for the role of salient features of core senses in generating extended senses through three studies. These studies use speakers of English and Chinese, historically unrelated languages.},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society (CogSci 2014)},
	author = {Zhu, Huichun and Malt, Barbara C},
	year = {2014},
	keywords = {cross-linguistic comparison, polysemy, word meaning, word senses},
	pages = {934--939},
	file = {paper168:/home/user/Zotero/storage/NDZDJUNQ/paper168.pdf:application/pdf}
}

@article{yildirim-linguistic-2013,
	title = {Linguistic {Variability} and {Adaptation} in {Quantifier} {Meanings}},
	journal = {Proceedings of the 35th Annual Meeting of the Cognitive Science Society (CogSci13)},
	author = {Yildirim, Ilker and Degen, Judith and Tanenhaus, Michael K and Jaeger, T Florian},
	year = {2013},
	keywords = {Language processing, Generalization, adaptation, Semantics, quantifiers},
	pages = {3835--3840},
	file = {YildirimDegenTanenhausJaeger2013:/home/user/Zotero/storage/KH34Z867/YildirimDegenTanenhausJaeger2013.pdf:application/pdf}
}

@article{youn-universal-2015,
	title = {On the universal structure of human lexical semantics},
	url = {http://arxiv.org/abs/1504.07843},
	author = {Youn, Hyejin and Sutton, Logan and Smith, Eric and Moore, Cristopher and Wilkins, Jon F and Maddieson, Ian and Croft, William and Bhattacharya, Tanmoy},
	year = {2015},
	pages = {1--32},
	file = {1504.07843v1:/home/user/Zotero/storage/2X4A4N77/1504.07843v1.pdf:application/pdf}
}

@article{yasunaga-is-2015,
	title = {Is the subject-before-object preference universal? {An} event-related potential study in the {Kaqchikel} {Mayan} language},
	volume = {3798},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1080372},
	doi = {10.1080/23273798.2015.1080372},
	number = {September},
	journal = {Language, Cognition and Neuroscience},
	author = {Yasunaga, Daichi and Yano, Masataka and Yasugi, Yoshiho and Koizumi, Masatoshi},
	year = {2015},
	keywords = {basic word order, field-based psycholinguistics, guatemala, processing load, syntactic complexity},
	pages = {1--21}
}

@article{vinyals-grammar-nodate,
	title = {Grammar as a {Foreign} {Language}},
	author = {Vinyals, Oriol and Koo, Terry and Hinton, Geoffrey},
	pages = {1--10},
	file = {1412.7449v3:/home/user/Zotero/storage/F22HJQKC/1412.7449v3.pdf:application/pdf}
}

@article{wieling-quantitative-2011,
	title = {Quantitative social dialectology: {Explaining} linguistic variation geographically and socially},
	volume = {6},
	doi = {10.1371/journal.pone.0023613},
	abstract = {In this study we examine linguistic variation and its dependence on both social and geographic factors. We follow dialectometry in applying a quantitative methodology and focusing on dialect distances, and social dialectology in the choice of factors we examine in building a model to predict word pronunciation distances from the standard Dutch language to 424 Dutch dialects. We combine linear mixed-effects regression modeling with generalized additive modeling to predict the pronunciation distance of 559 words. Although geographical position is the dominant predictor, several other factors emerged as significant. The model predicts a greater distance from the standard for smaller communities, for communities with a higher average age, for nouns (as contrasted with verbs and adjectives), for more frequent words, and for words with relatively many vowels. The impact of the demographic variables, however, varied from word to word. For a majority of words, larger, richer and younger communities are moving towards the standard. For a smaller minority of words, larger, richer and younger communities emerge as driving a change away from the standard. Similarly, the strength of the effects of word frequency and word category varied geographically. The peripheral areas of the Netherlands showed a greater distance from the standard for nouns (as opposed to verbs and adjectives) as well as for high-frequency words, compared to the more central areas. Our findings indicate that changes in pronunciation have been spreading (in particular for low-frequency words) from the Hollandic center of economic power to the peripheral areas of the country, meeting resistance that is stronger wherever, for well-documented historical reasons, the political influence of Holland was reduced. Our results are also consistent with the theory of lexical diffusion, in that distances from the Hollandic norm vary systematically and predictably on a word by word basis.},
	number = {9},
	journal = {PLoS ONE},
	author = {Wieling, Martijn and Nerbonne, John and Baayen, R. Harald},
	year = {2011},
	file = {WielingNerbonneBaayenPLOS1-1:/home/user/Zotero/storage/JSQ2HQQC/WielingNerbonneBaayenPLOS1-1.pdf:application/pdf}
}

@article{wieling-cognitively-2014,
	title = {A cognitively grounded measure of pronunciation distance},
	volume = {9},
	doi = {10.1371/journal.pone.0075734},
	abstract = {In this study we develop pronunciation distances based on naive discriminative learning (NDL). Measures of pronunciation distance are used in several subfields of linguistics, including psycholinguistics, dialectology and typology. In contrast to the commonly used Levenshtein algorithm, NDL is grounded in cognitive theory of competitive reinforcement learning and is able to generate asymmetrical pronunciation distances. In a first study, we validated the NDL-based pronunciation distances by comparing them to a large set of native-likeness ratings given by native American English speakers when presented with accented English speech. In a second study, the NDL-based pronunciation distances were validated on the basis of perceptual dialect distances of Norwegian speakers. Results indicated that the NDL-based pronunciation distances matched perceptual distances reasonably well with correlations ranging between 0.7 and 0.8. While the correlations were comparable to those obtained using the Levenshtein distance, the NDL-based approach is more flexible as it is also able to incorporate acoustic information other than sound segments.},
	number = {1},
	journal = {PLoS ONE},
	author = {Wieling, Martijn and Nerbonne, John and Bloem, Jelke and Gooskens, Charlotte and Heeringa, Wilbert and Baayen, R. Harald},
	year = {2014},
	file = {journal.pone.0075734:/home/user/Zotero/storage/HVXHU99E/journal.pone.0075734.pdf:application/pdf}
}

@article{xue-chineseenglish-2014,
	title = {Chinese–{English} bilinguals processing temporal–spatial metaphor},
	volume = {15},
	url = {http://link.springer.com/10.1007/s10339-014-0621-5},
	doi = {10.1007/s10339-014-0621-5},
	journal = {Cognitive Processing},
	author = {Xue, Jin and Yang, Jie and Zhao, Qian},
	year = {2014},
	keywords = {bilingual, erp, metaphor, spatial order, temporal sequence},
	pages = {269--281},
	file = {Xue, Yang, Zhao - 2014 - Chinese–English bilinguals processing temporal–spatial metaphor:/home/user/Zotero/storage/9JWKDFA3/Xue, Yang, Zhao - 2014 - Chinese–English bilinguals processing temporal–spatial metaphor.pdf:application/pdf}
}

@article{wieling-investigating-nodate,
	title = {Investigating dialectal differences using articulography},
	author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Baayen, R Harald},
	keywords = {articulography, dialectology, generalized additive, modeling},
	file = {WielingEtAlICPHS2015:/home/user/Zotero/storage/2X3D9TFG/WielingEtAlICPHS2015.pdf:application/pdf}
}

@article{wieling-lexical-nodate,
	title = {Lexical differences between {Tuscan} dialects and standard {Italian}: {Accounting} for geographic and socio-demographic variation using generalized additive mixed modeling},
	url = {http://www.martijnwieling.nl/files/WielingMontemagniNerbonneBaayen2013.pdf},
	doi = {10.1353/lan.2014.0064},
	journal = {Language},
	author = {Wieling, Martijn and Montemagni, Simonetta and Nerbonne, John and Baayen., R. Harald},
	pages = {0--47},
	file = {WielingMontemagniNerbonneBaayen2013:/home/user/Zotero/storage/UNQ87XXE/WielingMontemagniNerbonneBaayen2013.pdf:application/pdf}
}

@article{willits-language-2015,
	title = {Language knowledge and event knowledge in language use},
	volume = {78},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028515000080},
	doi = {10.1016/j.cogpsych.2015.02.002},
	journal = {Cognitive Psychology},
	author = {Willits, Jon a. and Amato, Michael S. and MacDonald, Maryellen C.},
	year = {2015},
	keywords = {language comprehension},
	pages = {1--27},
	file = {Willits, Amato, MacDonald - 2015 - Language knowledge and event knowledge in language use:/home/user/Zotero/storage/VTNM33N2/Willits, Amato, MacDonald - 2015 - Language knowledge and event knowledge in language use.pdf:application/pdf}
}

@article{wagner-brian-2013,
	title = {{BRIAN} {SKYRMS} {Signals}: {Evolution}, {Learning}, and {Information}},
	volume = {64},
	issn = {9780199580828},
	url = {http://bjps.oxfordjournals.org/cgi/doi/10.1093/bjps/axt004},
	doi = {10.1093/bjps/axt004},
	number = {4},
	journal = {The British Journal for the Philosophy of Science},
	author = {Wagner, E. O. and Franke, M.},
	year = {2013},
	pages = {883--887},
	file = {Br J Philos Sci-2013-Wagner-883-7:/home/user/Zotero/storage/NBICQ4JF/Br J Philos Sci-2013-Wagner-883-7.pdf:application/pdf}
}

@article{vinson-faster-2015,
	title = {A faster path between meaning and form? {Iconicity} facilitates sign recognition and production in {British} {Sign} {Language}},
	volume = {82},
	url = {http://dx.doi.org/10.1016/j.jml.2015.03.002},
	doi = {10.1016/j.jml.2015.03.002},
	journal = {Journal of Memory and Language},
	author = {Vinson, David and Thompson, Robin L and Skinner, Robert and Vigliocco, Gabriella},
	year = {2015},
	pages = {56--85},
	file = {Vinson et al. - 2015 - A faster path between meaning and form Iconicity facilitates sign recognition and production in British Sign Lang:/home/user/Zotero/storage/IQ6Q7IKM/Vinson et al. - 2015 - A faster path between meaning and form Iconicity facilitates sign recognition and production in British Sign Lang.pdf:application/pdf}
}

@article{twomey-as-2014,
	title = {Do as {I} say, not as {I} do: {A} lexical distributional account of {English} locative verb class acquisition},
	volume = {73},
	url = {http://dx.doi.org/10.1016/j.cogpsych.2014.05.001},
	doi = {10.1016/j.cogpsych.2014.05.001},
	abstract = {Children overgeneralise verbs to ungrammatical structures early in acquisition, but retreat from these overgeneralisations as they learn semantic verb classes. In a large corpus of English locative utterances (e.g., the woman sprayed water onto the wall/. wall with water), we found structural biases which changed over development and which could explain overgeneralisation behaviour. Children and adults had similar verb classes and a correspondence analysis suggested that lexical distributional regularities in the adult input could help to explain the acquisition of these classes. A connectionist model provided an explicit account of how structural biases could be learned over development and how these biases could be reduced by learning verb classes from distributional regularities. © 2014 Elsevier Inc.},
	journal = {Cognitive Psychology},
	author = {Twomey, Katherine E. and Chang, Franklin and Ambridge, Ben},
	year = {2014},
	keywords = {Connectionist model, Corpus analysis, Distributional learning, Language acquisition, Verb semantics},
	pages = {41--71},
	file = {Twomey, Chang, Ambridge - 2014 - Do as I say, not as I do A lexical distributional account of English locative verb class acquisition:/home/user/Zotero/storage/ITS24Q3G/Twomey, Chang, Ambridge - 2014 - Do as I say, not as I do A lexical distributional account of English locative verb class acquisition.pdf:application/pdf}
}

@article{tily-syntactic-2009,
	title = {Syntactic probabilities affect pronunciation variation in spontaneous speech},
	volume = {1},
	issn = {1866-9808},
	doi = {10.1515/LANGCOG.2009.008},
	number = {2009},
	journal = {Language and Cognition},
	author = {Tily, Harry and Gahl, Susanne and Arnon, Inbal and Snider, Neal and Kothari, Anubha and Bresnan, Joan},
	year = {2009},
	keywords = {Linguistics, correspondence address, disfluency, ditransitive, gradience, harry tily, margaret jacks hall, pronunciation variation, speech production, stanford univer-, syntactic alternation, word duration},
	pages = {147--165},
	file = {langcog.2009.008:/home/user/Zotero/storage/PSBTJZ2D/langcog.2009.008.pdf:application/pdf}
}

@article{tomaschek-vowel-2014,
	title = {Vowel articulation affected by word frequency},
	author = {Tomaschek, Fabian and Tucker, Benjamin V and Wieling, Martijn and Baayen, R Harald},
	year = {2014},
	keywords = {articulography, Learning, vowels, Word frequency},
	pages = {5--8},
	file = {TomaschekEtAl2014:/home/user/Zotero/storage/XRPXIUIK/TomaschekEtAl2014.pdf:application/pdf}
}

@article{the-for-1998,
	title = {For the},
	issn = {0780344715},
	author = {The, F O R},
	year = {1998},
	pages = {473--476},
	file = {Roberts_influences(1):/home/user/Zotero/storage/SUQ2XI86/Roberts_influences(1).pdf:application/pdf;The - 1998 - For the(2):/home/user/Zotero/storage/VJPNTSZF/The - 1998 - For the(2).pdf:application/pdf}
}

@article{trettenbrein-book-2015,
	title = {Book {Review}: “{Cognitive} neuroscience of language”},
	volume = {6},
	issn = {9781848726215},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00553},
	doi = {10.3389/fpsyg.2015.00553},
	number = {May},
	journal = {Frontiers in Psychology},
	author = {Trettenbrein, Patrick C.},
	year = {2015},
	keywords = {Linguistics, aphasiology, cognitive neuroscience, language and the brain, linguistics, cognitive neuroscience, neurolinguist, neuroimaging, Neurolinguistics, textbook},
	pages = {1--2},
	file = {fpsyg-06-00553:/home/user/Zotero/storage/CRUHU9ZP/fpsyg-06-00553.pdf:application/pdf}
}

@article{tomaschek-word-2013,
	title = {Word frequency, vowel length and vowel quality in speech production: {An} {EMA} study of the importance of experience},
	journal = {Proceedings of Interspeech 2013},
	author = {Tomaschek, Fabian and Wieling, Martijn and Arnold, Denis and Baayen, R. Harald},
	year = {2013},
	pages = {1302--1306},
	file = {TomaschekWielingArnoldBaayen2013:/home/user/Zotero/storage/PB42C4H5/TomaschekWielingArnoldBaayen2013.pdf:application/pdf}
}

@article{tillman-learning-2010,
	title = {Learning the language of time : {Children} ’ s acquisition of duration words},
	volume = {78},
	url = {http://dx.doi.org/10.1016/j.cogpsych.2015.03.001},
	doi = {10.1016/j.cogpsych.2015.03.001},
	journal = {Cognitive Psychology},
	author = {Tillman, Katharine a},
	year = {2010},
	keywords = {language, abstract word learning, acquisition, number-line estimation, time perception},
	pages = {1462--1467},
	file = {Tillman - 2010 - Learning the language of time Children ’ s acquisition of duration words:/home/user/Zotero/storage/BUNFHXB5/Tillman - 2010 - Learning the language of time Children ’ s acquisition of duration words.pdf:application/pdf}
}

@article{the-for-1998-1,
	title = {For the},
	issn = {0780344715},
	author = {The, F O R},
	year = {1998},
	pages = {473--476},
	file = {Roberts_influences(1):/home/user/Zotero/storage/4TACQWM2/Roberts_influences(1).pdf:application/pdf;The - 1998 - For the:/home/user/Zotero/storage/R7UN676A/The - 1998 - For the.pdf:application/pdf}
}

@article{tagliamonte-models-2012,
	title = {Models, forests, and trees of {York} {English}: {Was}/were variation as a case study for statistical practice},
	volume = {24},
	issn = {0954394512000},
	doi = {10.1017/S0954394512000129},
	abstract = {What is the explanation for vigorous variation between was and were in plural existential constructions and what is the optimal tool for analyzing it? The standard variationist tool — the variable rule program — is a generalized linear model; however, recent developments in statistics have introduced new tools, including mixed-effects models, random forests and conditional infer- ence trees. In a step-by-step demonstration, we show how this well known variable benefits from these complementary techniques. Mixed-effects models provide a principled way of assessing the importance of random-effect factors such as the individuals in the sample. Random forests provide information about the importance of predictors, whether factorial or continuous, and do so also for unbalanced designs with high multicollinearity, cases for which the family of linear models is less appropriate. Conditional inference trees straightforwardly visualize how multiple predictors operate in tandem. Taken together the results confirm that polarity, distance from verb to plural element and the nature of the DP are significant predictors. Ongoing linguistic change and so- cial reallocation via morphologization are operational. Furthermore, the results make predictions that can be tested in future research. We conclude that variationist research can be substantially enriched by an expanded tool kit.},
	journal = {Language Variation and Change},
	author = {Tagliamonte, Sali a. and Baayen, R. Harald},
	year = {2012},
	pages = {135--178},
	file = {TagliamonteBaayen2012:/home/user/Zotero/storage/BIBV3P2N/TagliamonteBaayen2012.pdf:application/pdf}
}

@article{steedman-connectionist-1999,
	title = {Connectionist sentence processing in perspective},
	volume = {23},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog2304\_10},
	doi = {10.1207/s15516709cog2304\_10},
	abstract = {The emphasis in the connectionist sentence-processing literature on distributed representation and emergence of grammar from such systems can easily obscure the often close relations between connectionist and symbolist systems. This paper argues that the Simple Recurrent Network (SRN) models proposed by and are more directly related to stochastic Part-of-Speech (POS) Taggers than to parsers or grammars as such, while auto-associative memory models of the kind pioneered by Longuet-Higgins, Willshaw, Pollack and others may be useful for grammar induction from a network-based conceptual structure as well as for structure-building. These observations suggest some interesting new directions for specifically connectionist sentence processing research, including more efficient representations for finite state machines, and acquisition devices based on a distinctively connectionist basis for grounded symbolist conceptual structure.},
	number = {4},
	journal = {Cognitive Science},
	author = {Steedman, Mark},
	year = {1999},
	keywords = {connectionism, connectionist, connectionist networks, semantic networks},
	pages = {615--634},
	file = {10.1.1.399.7246:/home/user/Zotero/storage/CPFCGG8M/10.1.1.399.7246.pdf:application/pdf;s15516709cog2304_10:/home/user/Zotero/storage/A7FMB5NT/s15516709cog2304_10.pdf:application/pdf}
}

@article{staub-influence-2015,
	title = {The influence of cloze probability and item constraint on cloze task response time},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000236},
	doi = {10.1016/j.jml.2015.02.004},
	journal = {Journal of Memory and Language},
	author = {Staub, Adrian and Grant, Margaret and Astheimer, Lori and Cohen, Andrew},
	year = {2015},
	pages = {1--17},
	file = {Staub et al. - 2015 - The influence of cloze probability and item constraint on cloze task response time:/home/user/Zotero/storage/JM84BKZM/Staub et al. - 2015 - The influence of cloze probability and item constraint on cloze task response time.pdf:application/pdf}
}

@article{sirts-pos-2014,
	title = {{POS} induction with distributional and morphological information using a distance-dependent {Chinese} restaurant process},
	issn = {9781937284732},
	url = {http://www.aclweb.org/anthology/P/P14/P14-2044},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Sirts, Kairit and Eisenstein, Jacob and Elsner, Micha and Goldwater, Sharon},
	year = {2014},
	pages = {265--271},
	file = {Sirts et al. - 2014 - POS induction with distributional and morphological information using a distance-dependent Chinese restaurant proc:/home/user/Zotero/storage/X9VIUUI4/Sirts et al. - 2014 - POS induction with distributional and morphological information using a distance-dependent Chinese restaurant proc.pdf:application/pdf}
}

@article{schreuder-how-1997,
	title = {How simplex complex words can be},
	volume = {37},
	journal = {Journal of Memory and Language},
	author = {Schreuder, Robert and Baayen, R. Harald},
	year = {1997},
	pages = {118--139},
	file = {SchreuderBaayenJML1997:/home/user/Zotero/storage/XDFHMMK8/SchreuderBaayenJML1997.pdf:application/pdf}
}

@article{schreuder-no-2010,
	title = {No {Title}},
	author = {Schreuder, Robert},
	year = {2010},
	pages = {1--38},
	file = {TabakSchreuderBaayen2010:/home/user/Zotero/storage/7XBIGH7C/TabakSchreuderBaayen2010.pdf:application/pdf}
}

@article{smith-comprehensive-1986,
	title = {A comprehensive model of spoken word recognition must be multimodal : {Evidence} from studies of language mediated visual attention {Simulating} multimodal competition in},
	author = {Smith, Alastair C and Monaghan, Padraic},
	year = {1986},
	keywords = {modeling, connectionist, spoken word comprehension, the visual world paradigm, visual attention},
	pages = {1449--1454},
	file = {paper254:/home/user/Zotero/storage/CXH8S2N5/paper254.pdf:application/pdf}
}

@article{shaoul-no-2014,
	title = {No {Title}},
	author = {Shaoul, Cyrus and Baayen, R Harald},
	year = {2014},
	pages = {1--43},
	file = {ShaoulBaayenWestburyML2014:/home/user/Zotero/storage/U2TCH3WQ/ShaoulBaayenWestburyML2014.pdf:application/pdf}
}

@article{scott-emotion-2014,
	title = {Emotion words and categories: {Evidence} from lexical decision},
	volume = {15},
	doi = {10.1007/s10339-013-0589-6},
	abstract = {We examined the categorical nature of emotion word recognition. Positive, negative, and neutral words were presented in lexical decision tasks. Word frequency was additionally manipulated. In Experiment 1, "positive" and "negative" categories of words were implicitly indicated by the blocked design employed. A significant emotion-frequency interaction was obtained, replicating past research. While positive words consistently elicited faster responses than neutral words, only low frequency negative words demonstrated a similar advantage. In Experiments 2a and 2b, explicit categories ("positive," "negative," and "household" items) were specified to participants. Positive words again elicited faster responses than did neutral words. Responses to negative words, however, were no different than those to neutral words, regardless of their frequency. The overall pattern of effects indicates that positive words are always facilitated, frequency plays a greater role in the recognition of negative words, and a "negative" category represents a somewhat disparate set of emotions. These results support the notion that emotion word processing may be moderated by distinct systems.},
	journal = {Cognitive Processing},
	author = {Scott, Graham G. and O'Donnell, Patrick J. and Sereno, Sara C.},
	year = {2014},
	keywords = {Arousal, Word frequency, category, Emotion, lexical decision, Valence},
	pages = {209--215},
	file = {Scott, O'Donnell, Sereno - 2014 - Emotion words and categories Evidence from lexical decision:/home/user/Zotero/storage/P6FQW2KZ/Scott, O'Donnell, Sereno - 2014 - Emotion words and categories Evidence from lexical decision.pdf:application/pdf}
}

@article{rohde-grammatical-2013,
	title = {Grammatical and information-structural influences on pronoun production},
	volume = {29},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2013.854918},
	doi = {10.1080/01690965.2013.854918},
	abstract = {A standard assumption in psycholinguistic research on pronoun interpretation is that production and interpretation are guided by the same set of contextual factors. A line of recent research has suggested otherwise, however, arguing instead that pronoun production is insensitive to a class of semantically driven contextual biases that have been shown to influence pronoun interpretation. The work reported in this paper addresses three fundamental questions that have been left unresolved by this research. First, research demonstrating the insensitivity of production to semantic biases has relied on referentially unambiguous settings in which the comprehender's ability to resolve the pronoun is not actually at stake. Experiment 1, a story continuation study, demonstrates that pronoun production is also insensitive to semantic biases in settings in which a pronoun would be referentially ambiguous. Second, previous research has not distinguished between accounts in which production biases are driven by grammatical properties of intended referents (e.g., subject position) or by information-structural factors (specifically, topichood) that are inherently pragmatic in nature. Experiment 2 examines this question with a story continuation study that manipulates the likelihood of potential referents being the topic while keeping grammatical role constant. A significant effect of the manipulation on rate of pronominalisation supports the claim that pronoun production is influenced by the likelihood that the referent is the current topic. Lastly, the predictions of Kehler et al.'s Bayesian analysis of the relationship between production and interpretation have never been quantitatively examined. The results of both experiments are shown to support the analysis over two competing models.\nA standard assumption in psycholinguistic research on pronoun interpretation is that production and interpretation are guided by the same set of contextual factors. A line of recent research has suggested otherwise, however, arguing instead that pronoun production is insensitive to a class of semantically driven contextual biases that have been shown to influence pronoun interpretation. The work reported in this paper addresses three fundamental questions that have been left unresolved by this research. First, research demonstrating the insensitivity of production to semantic biases has relied on referentially unambiguous settings in which the comprehender's ability to resolve the pronoun is not actually at stake. Experiment 1, a story continuation study, demonstrates that pronoun production is also insensitive to semantic biases in settings in which a pronoun would be referentially ambiguous. Second, previous research has not distinguished between accounts in which production biases are driven by grammatical properties of intended referents (e.g., subject position) or by information-structural factors (specifically, topichood) that are inherently pragmatic in nature. Experiment 2 examines this question with a story continuation study that manipulates the likelihood of potential referents being the topic while keeping grammatical role constant. A significant effect of the manipulation on rate of pronominalisation supports the claim that pronoun production is influenced by the likelihood that the referent is the current topic. Lastly, the predictions of Kehler et al.'s Bayesian analysis of the relationship between production and interpretation have never been quantitatively examined. The results of both experiments are shown to support the analysis over two competing models.},
	journal = {Language, Cognition and Neuroscience},
	author = {Rohde, Hannah and Kehler, Andrew},
	year = {2013},
	keywords = {implicit causality, information structure, pronoun production},
	pages = {912--927},
	file = {RohdeKehler.2014:/home/user/Zotero/storage/5GGBIISV/RohdeKehler.2014.pdf:application/pdf}
}

@article{riordan-grammatical-2015,
	title = {Grammatical number processing and anticipatory eye movements are not tightly coordinated in {English} spoken language comprehension},
	volume = {6},
	url = {http://www.frontiersin.org/Language\_Sciences/10.3389/fpsyg.2015.00590/abstract},
	doi = {10.3389/fpsyg.2015.00590},
	number = {May},
	journal = {Frontiers in Psychology},
	author = {Riordan, Brian and Dye, Melody and Jones, Michael N.},
	year = {2015},
	keywords = {Eye movements, grammatical number, grammatical number, eye movements, sentence compre, sentence comprehension, Spoken word recognition, visual},
	pages = {1--11},
	file = {fpsyg-06-00590:/home/user/Zotero/storage/JXR3B4RU/fpsyg-06-00590.pdf:application/pdf}
}

@article{riley-long-lasting-2015,
	title = {Long-lasting semantic interference effects in object naming are not necessarily conceptually mediated},
	volume = {6},
	url = {http://www.frontiersin.org/Language\_Sciences/10.3389/fpsyg.2015.00578/abstract},
	doi = {10.3389/fpsyg.2015.00578},
	number = {May},
	journal = {Frontiers in Psychology},
	author = {Riley, Emma and McMahon, Katie L. and de Zubicaray, Greig},
	year = {2015},
	keywords = {language production, language production, lexical retrieval, semantic i, lexical retrieval, semantic interference},
	pages = {1--14},
	file = {fpsyg-06-00578:/home/user/Zotero/storage/S8886GB9/fpsyg-06-00578.pdf:application/pdf}
}

@article{scheutz-embodied-2012,
	title = {An {Embodied} {Real}-{Time} {Model} of {Language}-{Guided} {Incremental} {Visual} {Search}},
	author = {Scheutz, Matthias and Krause, Evan and Sadeghi, Sepideh},
	year = {2012},
	keywords = {embodied, incremental interactive processing, interaction, natural language and vision, real-time model},
	pages = {1365--1370},
	file = {paper240:/home/user/Zotero/storage/M4DZNZG9/paper240.pdf:application/pdf}
}

@article{rooij-promise-2012,
	title = {Promise and {Threat} with {Conditionals} and {Disjunctions}},
	url = {http://books.google.nl/books?hl=en&lr=&id=SroMz7KdiOgC&oi=fnd&pg=PA69&ots=8Zdv9zlLoF&sig=h6QNBz0IWPJyD8EsJoP0I4VoQGI},
	number = {1},
	journal = {…  and Grammar: From Sentence Types to  …},
	author = {Rooij, R Van and Franke, M},
	year = {2012},
	pages = {1--17},
	file = {joint_PIs2010:/home/user/Zotero/storage/U5XNZPUB/joint_PIs2010.pdf:application/pdf}
}

@article{reisenauer-stochastic-2013,
	title = {Stochastic dynamics of lexicon learning in an uncertain and nonuniform world},
	volume = {110},
	doi = {10.1103/PhysRevLett.110.258701},
	abstract = {We study the time taken by a language learner to correctly identify the meaning of all words in a lexicon under conditions where many plausible meanings can be inferred whenever a word is uttered. We show that the most basic form of cross-situational learning—whereby information from multiple episodes is combined to eliminate incorrect meanings—can perform badly when words are learned independently and meanings are drawn from a nonuniform distribution. If learners further assume that no two words share a common meaning, we find a phase transition between a maximally efficient learning regime, where the learning time is reduced to the shortest it can possibly be, and a partially efficient regime where incorrect candidate meanings for words persist at late times.We obtain exact results for the word-learning process through an equivalence to a statistical mechanical problem of enumerating loops in the space of word- meaning mappings. DOI:},
	journal = {Physical Review Letters},
	author = {Reisenauer, Rainer and Smith, Kenny and Blythe, Richard a.},
	year = {2013},
	file = {1302.5526v2:/home/user/Zotero/storage/VPWWJAUZ/1302.5526v2.pdf:application/pdf}
}

@article{ramscar-learning-nodate,
	title = {Learning and {Cognitive} {Maturation}},
	author = {Ramscar, Michael and Baayen, Harald},
	file = {RamscarBaayenEAA2015:/home/user/Zotero/storage/H9PX4M38/RamscarBaayenEAA2015.pdf:application/pdf}
}

@article{rego-when-2014,
	title = {When a text is translated does the complexity of its vocabulary change? {Translations} and target readerships.},
	volume = {9},
	doi = {10.1371/journal.pone.0110213},
	abstract = {In linguistic studies, the academic level of the vocabulary in a text can be described in terms of statistical physics by using a "temperature" concept related to the text's word-frequency distribution. We propose a "comparative thermo-linguistic" technique to analyze the vocabulary of a text to determine its academic level and its target readership in any given language. We apply this technique to a large number of books by several authors and examine how the vocabulary of a text changes when it is translated from one language to another. Unlike the uniform results produced using the Zipf law, using our "word energy" distribution technique we find variations in the power-law behavior. We also examine some common features that span across languages and identify some intriguing questions concerning how to determine when a text is suitable for its intended readership.},
	number = {10},
	journal = {PloS one},
	author = {Rêgo, Hênio Henrique Aragão and Braunstein, Lidia A and D Agostino, Gregorio and Stanley, H Eugene and Miyazima, Sasuke},
	month = jan,
	year = {2014},
	pages = {e110213--e110213},
	file = {pone.0110213:/home/user/Zotero/storage/4T79TJ9T/pone.0110213.pdf:application/pdf}
}

@article{rasin-learnability-2014,
	title = {A learnability argument for constraints on},
	number = {1967},
	author = {Rasin, Ezer and Katzir, Roni},
	year = {2014},
	pages = {1--14},
	file = {Rasin, Katzir - 2014 - A learnability argument for constraints on:/home/user/Zotero/storage/XRXFC9QA/Rasin, Katzir - 2014 - A learnability argument for constraints on.pdf:application/pdf}
}

@article{ramscar-learning-2013,
	title = {Learning is not decline: {The} mental lexicon as a window into cognition across the lifespan},
	volume = {8},
	doi = {10.1075/ml.8.3.08ram},
	journal = {The Mental Lexicon},
	author = {Ramscar, Michael and Hendrix, Peter and Love, B and Baayen, R Harald},
	year = {2013},
	pages = {450--481},
	file = {RamscarEtAlMentLex2013:/home/user/Zotero/storage/HJXSN6KU/RamscarEtAlMentLex2013.pdf:application/pdf}
}

@article{ramscar-production-2013,
	title = {Production, comprehension, and synthesis: {A} communicative perspective on language},
	volume = {4},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2013.00233},
	number = {May},
	journal = {Frontiers in Psychology},
	author = {Ramscar, Michael and Baayen, Harald},
	year = {2013},
	pages = {2011--2014},
	file = {RamscarBaayen2013:/home/user/Zotero/storage/6QR7IEPP/RamscarBaayen2013.pdf:application/pdf}
}

@article{rama-fiorini-representing-2014,
	title = {Representing part-whole relations in conceptual spaces},
	volume = {15},
	doi = {10.1007/s10339-013-0585-x},
	abstract = {In this paper, we propose a cognitive semantic approach to represent part-whole relations. We base our proposal on the theory of conceptual spaces, focusing on prototypical structures in part-whole relations. Prototypical structures are not accounted for in traditional mereological formalisms. In our account, parts and wholes are represented in distinct conceptual spaces; parts are joined to form wholes in a structure space. The structure space allows systematic similarity judgments between wholes, taking into consideration shared parts and their configurations. A point in the structure space denotes a particular part structure; regions in the space represent different general types of part structures. We argue that the structural space can represent prototype effects: structural types are formed around typical arrangements of parts. We also show how structure space captures the variations in part structure of a given concept across different domains. In addition, we discuss how some taxonomies of part-whole relations can be understood within our framework.},
	journal = {Cognitive Processing},
	author = {Rama Fiorini, Sandro and Gärdenfors, Peter and Abel, Mara},
	year = {2014},
	keywords = {Conceptual spaces, Context, Part-whole relation, Partonomy, Prototype},
	pages = {127--142},
	file = {Rama Fiorini, Gärdenfors, Abel - 2014 - Representing part-whole relations in conceptual spaces:/home/user/Zotero/storage/2PVM663Q/Rama Fiorini, Gärdenfors, Abel - 2014 - Representing part-whole relations in conceptual spaces.pdf:application/pdf}
}

@article{qing-gradable-2014,
	title = {Gradable adjectives , vagueness , and optimal language use :},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2014},
	keywords = {Linguistics, language production, absolute and relative adjectives, evolutionary, gradable adjectives, probabilistic models, vagueness},
	pages = {23--41},
	file = {Attachment:/home/user/Zotero/storage/KWKQUAGC/QingFranke_2014_Gradable_AdjectivesSALT:application/pdf}
}

@article{prieto-velasco-embodied-2014,
	title = {The embodied nature of medical concepts: image schemas and language for pain},
	doi = {10.1007/s10339-013-0594-9},
	abstract = {Cognitive linguistics assumes that knowledge is both embodied and situated as far as it is acquired through our bodily interaction with the world in a specific environment (e.g. Barsalou in Lang Cogn Process 18:513-562, 2003; Connell et al. in PLoS One 7:3, 2012). Therefore, embodiment provides an explanation to the mental representation and linguistic expression of concepts. Among the first, we find multimodal conceptual structures, like image schemas, which are schematic representations of embodied experiences resulting from our conceptualization of the surrounding environment (Tercedor Sánchez et al. in J Spec Transl 18:187-205, 2012). Furthermore, the way we interact with the environment and its objects is dynamic and configures how we refer to concepts both by means of images and lexicalizations. In this article, we investigate how image schemas underlie verbal and visual representations. They both evoke concepts based on exteroception, interoception and proprioception which can be lexicalized through language. More specifically, we study (1) a multimodal corpus of medical texts to examine how image schemas lexicalize in the language of medicine to represent specialized concepts and (2) medical pictures to explore the depiction of image-schematic concepts, in order to account for the verbal and visual representation of embodied concepts. We explore the concept PAIN, a sensory and emotional experience associated with actual or potential tissue damage, using corpus analysis tools (Sketch Engine) to extract information about the lexicalization of underlying image schemas in definitions and defining contexts. Then, we use the image schemas behind medical concepts to consistently select images which depict our experience of pain and the way we understand it. Finally, such lexicalizations and visualizations will help us assess how we refer to PAIN both verbally and visually.},
	journal = {Cognitive Processing},
	author = {Prieto Velasco, Juan Antonio and Tercedor Sánchez, Maribel},
	year = {2014},
	keywords = {Embodiment, Image schemas, Knowledge visualization, Medical concepts},
	pages = {1--14},
	file = {Prieto Velasco, Tercedor Sánchez - 2014 - The embodied nature of medical concepts image schemas and language for pain:/home/user/Zotero/storage/E278QAKH/Prieto Velasco, Tercedor Sánchez - 2014 - The embodied nature of medical concepts image schemas and language for pain.pdf:application/pdf}
}

@article{ragni-theory-2006,
	title = {Theory {Comparison} for {Generalized} {Quantifiers}},
	journal = {Cognitive Science Conference 2014},
	author = {Ragni, Marco and Singmann, Henrik and Steinlein, Eva-Maria},
	year = {2006},
	keywords = {generalized quantifiers, syllogistic reasoning, model selection, mpts},
	pages = {1228--1233},
	file = {paper217:/home/user/Zotero/storage/RG7VEC38/paper217.pdf:application/pdf}
}

@article{qing-variations-2013,
	title = {Variations on a {{B}}ayesian {Theme}: {{C}}omparing {{B}}ayesian {Models} of {Referential} {Reasoning}},
	journal = {Proceedings of Workshop on {B}ayesian Natural Language Semantics and Pragmatics ({BNLSP} 13)},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2013},
	file = {QingFranke_2013_Variations_on_Bayes:/home/user/Zotero/storage/4FF9X6A7/QingFranke_2013_Variations_on_Bayes.pdf:application/pdf}
}

@article{qing-meaning-2014,
	title = {Meaning and {Use} of {Gradable} {Adjectives}: {Formal} {Modeling} {Meets} {Empirical} {Data}},
	abstract = {The meaning of gradable adjectives is highly context-dependent, and is notoriously difficult to capture precisely. Recent work in theoretical linguistics suggests that the way we use gradable adjectives can be explained in terms of optimal language use. To test this hypothesis we formulate a probabilistic speaker model that combines ideas from Bayesian approaches to pragmatic reasoning as social cognition with broader optimality considerations, as suggested by evolutionary linguistics. We demonstrate that, despite its simplicity, the model explains empirical data on the applicability of adjectives in context astonishingly well.},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society (CogSci 2014)},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2014},
	keywords = {gradable adjectives, bayesian cognitive modeling, context-dependence, natural language production/generation},
	pages = {1204--1209},
	file = {QingFranke_2014_Meaning and Use of Gradable Adjectives Formal Modeling Meets Empirical:/home/user/Zotero/storage/QMJFIWQM/QingFranke_2014_Meaning and Use of Gradable Adjectives Formal Modeling Meets Empirical.pdf:application/pdf}
}

@article{pluymaekers-morphological-nodate,
	title = {Morphological effects on fine phonetic detail : {The} case of {Dutch} -igheid},
	author = {Pluymaekers, Mark and Ernestus, Mirjam and Baayen, R Harald and Booij, Geert},
	file = {PluymaekersErnestusBaayenBooijLabPhon10:/home/user/Zotero/storage/JG9H8Q84/PluymaekersErnestusBaayenBooijLabPhon10.pdf:application/pdf}
}

@article{pham-semantic-2013,
	title = {Semantic relations and compound transparency: {A} regression study in {CARIN} theory},
	volume = {46},
	doi = {10.2298/PSI1304455P},
	number = {4},
	journal = {Psihologija},
	author = {Pham, Hien and Baayen, R. Harald},
	year = {2013},
	keywords = {CARIN theory, compounds, Conceptual relations, Morphological processing, Relative entropy, Semantic transparency},
	pages = {455--478},
	file = {0048-57051304455P:/home/user/Zotero/storage/PC24FSZW/0048-57051304455P.pdf:application/pdf}
}

@article{pham--2008,
	title = {( 1 , 2 ) −},
	journal = {Solutions},
	author = {Pham, Hien and Baayen, Harald},
	year = {2008},
	pages = {1--6},
	file = {PhamBaayenLCN2015:/home/user/Zotero/storage/BMWH4QCV/PhamBaayenLCN2015.pdf:application/pdf}
}

@article{ottl-does-2015,
	title = {Does {Formal} {Complexity} {Reflect} {Cognitive} {Complexity}? {Investigating} {Aspects} of the {Chomsky} {Hierarchy} in an {Artificial} {Language} {Learning} {Study}},
	volume = {10},
	url = {http://dx.plos.org/10.1371/journal.pone.0123059},
	doi = {10.1371/journal.pone.0123059},
	journal = {Plos One},
	author = {Öttl, Birgit and Jäger, Gerhard and Kaup, Barbara},
	year = {2015},
	pages = {e0123059--e0123059},
	file = {journal.pone.0123059:/home/user/Zotero/storage/DDCVU2HT/journal.pone.0123059.pdf:application/pdf}
}

@article{opitz-concurrence-2015,
	title = {Concurrence of rule- and similarity-based mechanisms in artificial grammar learning},
	volume = {77},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028515000146},
	doi = {10.1016/j.cogpsych.2015.02.003},
	journal = {Cognitive Psychology},
	author = {Opitz, Bertram and Hofmann, Juliane},
	year = {2015},
	keywords = {agl, artificial grammar learning},
	pages = {77--99},
	file = {Opitz, Hofmann - 2015 - Concurrence of rule- and similarity-based mechanisms in artificial grammar learning:/home/user/Zotero/storage/PS5GU6RA/Opitz, Hofmann - 2015 - Concurrence of rule- and similarity-based mechanisms in artificial grammar learning.pdf:application/pdf}
}

@article{paperno-corpus-based-2014,
	title = {Corpus-based estimates of word association predict biases in judgment of word co-occurrence likelihood.},
	volume = {74},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/25151368},
	doi = {10.1016/j.cogpsych.2014.07.001},
	abstract = {This paper draws a connection between statistical word association measures used in linguistics and confirmation measures from epistemology. Having theoretically established the connection, we replicate, in the new context of the judgments of word co-occurrence, an intriguing finding from the psychology of reasoning, namely that confirmation values affect intuitions about likelihood. We show that the effect, despite being based in this case on very subtle statistical insights about thousands of words, is stable across three different experimental settings. Our theoretical and empirical results suggest that factors affecting traditional reasoning tasks are also at play when linguistic knowledge is probed, and they provide further evidence for the importance of confirmation in a new domain.},
	journal = {Cognitive psychology},
	author = {Paperno, Denis and Marelli, Marco and Tentori, Katya and Baroni, Marco},
	year = {2014},
	pages = {66--83},
	file = {Paperno et al. - 2014 - Corpus-based estimates of word association predict biases in judgment of word co-occurrence likelihood:/home/user/Zotero/storage/S3TQ5GTD/Paperno et al. - 2014 - Corpus-based estimates of word association predict biases in judgment of word co-occurrence likelihood.pdf:application/pdf}
}

@article{palmiero-representation-2014,
	title = {The representation of conceptual knowledge: {Visual}, auditory, and olfactory imagery compared with semantic processing},
	volume = {15},
	issn = {1612-4790 (Electronic)},
	doi = {10.1007/s10339-013-0586-9},
	abstract = {Two experiments comparing imaginative processing in different modalities and semantic processing were carried out to investigate the issue of whether conceptual knowledge can be represented in different format. Participants were asked to judge the similarity between visual images, auditory images, and olfactory images in the imaginative block, if two items belonged to the same category in the semantic block. Items were verbally cued in both experiments. The degree of similarity between the imaginative and semantic items was changed across experiments. Experiment 1 showed that the semantic processing was faster than the visual and the auditory imaginative processing, whereas no differentiation was possible between the semantic processing and the olfactory imaginative processing. Experiment 2 revealed that only the visual imaginative processing could be differentiated from the semantic processing in terms of accuracy. These results showed that the visual and auditory imaginative processing can be differentiated from the semantic processing, although both visual and auditory images strongly rely on semantic representations. On the contrary, no differentiation is possible within the olfactory domain. Results are discussed in the frame of the imagery debate.},
	journal = {Cognitive Processing},
	author = {Palmiero, Massimiliano and Di Matteo, Rosalia and Belardinelli, Marta Olivetti},
	year = {2014},
	keywords = {Imagery, Semantic representation, Sensory modality},
	pages = {143--157},
	file = {Palmiero, Di Matteo, Belardinelli - 2014 - The representation of conceptual knowledge Visual, auditory, and olfactory imagery compared w:/home/user/Zotero/storage/5ERKPRDH/Palmiero, Di Matteo, Belardinelli - 2014 - The representation of conceptual knowledge Visual, auditory, and olfactory imagery compared w.pdf:application/pdf}
}

@article{olofsson-muted-2015,
	title = {The muted sense: neurocognitive limitations of olfactory language},
	volume = {19},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661315000960},
	doi = {10.1016/j.tics.2015.04.007},
	number = {6},
	journal = {Trends in Cognitive Sciences},
	author = {Olofsson, Jonas K. and Gottfried, Jay a.},
	year = {2015},
	keywords = {language, olfaction, perception},
	pages = {314--321},
	file = {Olofsson, Gottfried - 2015 - The muted sense neurocognitive limitations of olfactory language:/home/user/Zotero/storage/MZAREJ42/Olofsson, Gottfried - 2015 - The muted sense neurocognitive limitations of olfactory language.pdf:application/pdf}
}

@article{oliveira-morphological-2008,
	title = {Morphological development of},
	volume = {48},
	author = {Oliveira, Edinbergh Caldas D E and Assakawa, Luciana Fugimoto},
	year = {2008},
	keywords = {amazon, larval development, mapará, morphological description, rio negro},
	pages = {37--48},
	file = {RamscarDyeBlevinsBaayenHCD2015:/home/user/Zotero/storage/X7ACRX38/RamscarDyeBlevinsBaayenHCD2015.pdf:application/pdf}
}

@article{mulder-cross-language-2015,
	title = {Cross-language activation of morphological relatives in cognates: the role of orthographic overlap and task-related processing},
	volume = {9},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2015.00016/abstract},
	doi = {10.3389/fnhum.2015.00016},
	number = {February},
	journal = {Frontiers in Human Neuroscience},
	author = {Mulder, Kimberley and Dijkstra, Ton and Baayen, R. Harald},
	year = {2015},
	keywords = {bilingual word recognition, cognates, Morphological family size, morphological family size, bilingual word recognit, response competition, spreading},
	pages = {1--18},
	file = {fnhum-09-00016:/home/user/Zotero/storage/GZZK8B87/fnhum-09-00016.pdf:application/pdf}
}

@article{mulder-effects-2014,
	title = {Effects of primary and secondary morphological family size in monolingual and bilingual word processing},
	volume = {72},
	journal = {Journal of Memory and Language},
	author = {Mulder, Kimberley and Dijkstra, T and Schreuder, R and Baayen, R Harald},
	year = {2014},
	file = {MulderDijkstraSchreuderBaayeninpress:/home/user/Zotero/storage/5TF3PGA3/MulderDijkstraSchreuderBaayeninpress.pdf:application/pdf}
}

@article{muhlenbernd-meaning-nodate,
	title = {Meaning , {Evolution} , and the {Structure} of {Society}},
	author = {Mühlenbernd, Roland and Franke, Michael},
	file = {MuhlenberndFranke_2014_Meaning_Evolution_Structure:/home/user/Zotero/storage/USFCIZH2/MuhlenberndFranke_2014_Meaning_Evolution_Structure.pdf:application/pdf}
}

@article{muhlenbernd-signaling-2012,
	title = {Signaling conventions: who learns what where and when in a social network},
	url = {http://www.sfs.uni-tuebingen.de/~roland/downloads/SignalingConventions.pdf},
	number = {2005},
	journal = {The Evolution of Language-Proceedings of the 9th International Conference (EVOLANG 2012)},
	author = {Mühlenbernd, R and Franke, Michael},
	year = {2012},
	pages = {242--249},
	file = {networks:/home/user/Zotero/storage/4K9DVMWF/networks.pdf:application/pdf}
}

@article{mintz-word-2014,
	title = {Word categorization from distributional information: {Frames} confer more than the sum of their ({Bigram}) parts},
	volume = {75},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028514000577},
	doi = {10.1016/j.cogpsych.2014.07.003},
	journal = {Cognitive Psychology},
	author = {Mintz, Toben H. and Wang, Felix Hao and Li, Jia},
	year = {2014},
	keywords = {grammatical category},
	pages = {1--27},
	file = {Mintz, Wang, Li - 2014 - Word categorization from distributional information Frames confer more than the sum of their (Bigram) parts:/home/user/Zotero/storage/ABVGP46A/Mintz, Wang, Li - 2014 - Word categorization from distributional information Frames confer more than the sum of their (Bigram) parts.pdf:application/pdf}
}

@article{modeling-reasoning-nodate,
	title = {Reasoning in {Reference} {Games}},
	author = {Modeling, Individual- Population-level Probabilistic and Franke, Michael and Degen, Judith and Hall, Jordan},
	pages = {1--49},
	file = {FrankeDegen_2015_Reasoning in Reference Games Individual- vs.~Population-Level Probabilistic Modelinga:/home/user/Zotero/storage/32M6E8WQ/FrankeDegen_2015_Reasoning in Reference Games Individual- vs.~Population-Level Probabilistic Modelinga.pdf:application/pdf}
}

@article{miwa-time-course-nodate,
	title = {time-course of lexical activation},
	volume = {1},
	number = {780},
	author = {{Miwa} and {Libben} and {Dijkstra} and {Baayen}},
	pages = {1--93},
	file = {MiwaLibbenDijkstraBaayen2014QJEP:/home/user/Zotero/storage/3UAVKKQW/MiwaLibbenDijkstraBaayen2014QJEP.pdf:application/pdf}
}

@article{mitchell-predicting-2008,
	title = {Predicting human brain activity associated with the meanings of nouns},
	volume = {320},
	journal = {Science},
	author = {Mitchell, Tom and Shinkareva, Svetlana and Carlson, Andrew and Chiang, Kai-Min and Malave, Vicente L and Mason, Robert A and Just, Marcel Adam},
	year = {2008},
	pages = {1191--1195}
}

@article{mccarthy-effects-2010,
	title = {The {Effects} of {Semantic} {Priming} on {Novel} {Verb} {Inflection}},
	number = {1993},
	author = {Mccarthy, Molly A and Kershaw, Trina C},
	year = {2010},
	keywords = {grammar, Semantics, past tense verb production},
	pages = {994--999},
	file = {paper178:/home/user/Zotero/storage/7ZBED8QG/paper178.pdf:application/pdf}
}

@article{martins-representing-2015,
	title = {Representing visual recursion does not require verbal or motor resources},
	volume = {77},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028515000055},
	doi = {10.1016/j.cogpsych.2015.01.004},
	journal = {Cognitive Psychology},
	author = {Martins, Maurício De Jesus Dias and Muršič, Zarja and Oh, Jinook and Fitch, W. Tecumseh},
	year = {2015},
	pages = {20--41},
	file = {Martins et al. - 2015 - Representing visual recursion does not require verbal or motor resources:/home/user/Zotero/storage/FXUTQ7BI/Martins et al. - 2015 - Representing visual recursion does not require verbal or motor resources.pdf:application/pdf}
}

@article{manuscript-attention-2012,
	title = {attention durig natural vision warps {Semantic} {Representation}},
	volume = {29},
	issn = {2122633255},
	doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
	number = {6},
	journal = {Nat Neurosci},
	author = {Manuscript, Author},
	year = {2012},
	keywords = {visual search, 1, attention, attention is thought to, fmri, increase information processing efficiency, movie, natural stimuli, neurophysiology studies in early, representation, through several convergent mechanisms, throughout the brain, tuning change, visual areas},
	pages = {997--1003},
	file = {nihms457964:/home/user/Zotero/storage/NWJPISVQ/nihms457964.pdf:application/pdf}
}

@article{milin-paradigms-2009,
	title = {Paradigms bit by bit: an information-theoretic approachto the processing of paradigmatic structure in inflectionand derivation},
	volume = {381},
	issn = {9780199547548},
	abstract = {[No abstract given, so paragraph from introduction in stead.]},
	number = {21},
	journal = {Analogy in grammar: Form and acquisition},
	author = {Milin, Petar and Kuperman, Victor and Kostić, Aleksandar and Baayen, R Harald},
	year = {2009},
	pages = {214--252},
	file = {milinKupermanKosticBaayen2009:/home/user/Zotero/storage/JTAXBIJ2/milinKupermanKosticBaayen2009.pdf:application/pdf}
}

@article{mcmahan-bayesian-2015,
	title = {A {Bayesian} {Model} of {Grounded} {Color} {Semantics}},
	volume = {3},
	journal = {Transactions of the ACL},
	author = {Mcmahan, Brian and Stone, Matthew},
	year = {2015},
	pages = {103--115},
	file = {276-1527-1-PB:/home/user/Zotero/storage/M6ZR286M/276-1527-1-PB.pdf:application/pdf}
}

@article{marghetis-spatial-2014,
	title = {Spatial reasoning in bilingual {Mexico} : {Delimiting} the influence of language {Fieldsite} and {Languages}},
	journal = {Proceedings of the 36th Annual Meeting of the Cognitive Science Society},
	author = {Marghetis, Tyler and Mccomsey, Melanie and Cooperrider, Kensy},
	year = {2014},
	keywords = {cognitive diversity, linguistic relativity, space, spatial frames of reference},
	pages = {940--945},
	file = {paper169:/home/user/Zotero/storage/CD3UCEAF/paper169.pdf:application/pdf}
}

@article{malt-bidirectional-2015,
	title = {Bidirectional lexical interaction in late immersed {Mandarin}-{English} bilinguals},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000248},
	doi = {10.1016/j.jml.2015.03.001},
	journal = {Journal of Memory and Language},
	author = {Malt, Barbara C. and Li, Ping and Pavlenko, Aneta and Zhu, Huichun and Ameel, Eef},
	year = {2015},
	pages = {86--104},
	file = {Malt et al. - 2015 - Bidirectional lexical interaction in late immersed Mandarin-English bilinguals:/home/user/Zotero/storage/2E2TSTI8/Malt et al. - 2015 - Bidirectional lexical interaction in late immersed Mandarin-English bilinguals.pdf:application/pdf}
}

@article{malaia-neural-2015,
	title = {Neural bases of syntax–semantics interface processing},
	volume = {9},
	issn = {1157101593},
	url = {http://link.springer.com/10.1007/s11571-015-9328-2},
	doi = {10.1007/s11571-015-9328-2},
	journal = {Cognitive Neurodynamics},
	author = {Malaia, Evguenia and Newman, Sharlene},
	year = {2015},
	keywords = {Semantics, Neurolinguistics, binding, Interface, neurolinguistics á syntax á, semantics á, syntax},
	pages = {317--329}
}

@article{lau-unsupervised-nodate,
	title = {Unsupervised {Prediction} of {Acceptability} {Judgements}},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	file = {lcl_acl15:/home/user/Zotero/storage/PEKCK2RI/lcl_acl15.pdf:application/pdf}
}

@article{lau-measuring-2007,
	title = {Measuring {Gradience} in {Speakers} ’ {Grammaticality} {Judgements}},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	year = {2007},
	keywords = {acceptability, binary categories, gradient classifiers, grammaticality, judgements, speakers},
	pages = {821--826},
	file = {lcl_cogsci14:/home/user/Zotero/storage/Z7PS4BBV/lcl_cogsci14.pdf:application/pdf}
}

@article{lappin-using-2005,
	title = {Using {Machine} {Learning} for {Non}-{Sentential} {Utterance} {Classification}},
	url = {http://hdl.handle.net/10065/854},
	abstract = {Article},
	author = {Lappin, Shalom and Fernandez, R and Ginzburg, J},
	year = {2005},
	keywords = {machine learning, Non-sentential utterances, Philosophy of Language},
	file = {NSU-sigdial:/home/user/Zotero/storage/3BAN8Z2H/NSU-sigdial.pdf:application/pdf}
}

@article{lappin-classifying-2007,
	title = {Classifying {Non}-{Sentential} {Utterances} in {Dialogue}: {A} {Machine} {Learning} {Approach}},
	url = {http://hdl.handle.net/10065/849},
	doi = {10.1162/coli.2007.33.3.397},
	abstract = {Article},
	number = {September 2004},
	author = {Lappin, Shalom and Fernandez, R and Ginzburg, J},
	year = {2007},
	keywords = {Non-sentential utterances, Philosophy of Language, Computational linguistics, Dialogue},
	file = {fernandez-ginzburg-lappin_cl07:/home/user/Zotero/storage/K5ES62RR/fernandez-ginzburg-lappin_cl07.pdf:application/pdf}
}

@article{lappin-machine-2005,
	title = {Machine {Learning} and the {Cognitive} {Basis} of {Natural} {Language}},
	url = {http://hdl.handle.net/10065/853},
	abstract = {Article},
	journal = {Challenge},
	author = {Lappin, Shalom},
	year = {2005},
	keywords = {machine learning, natural language, philosophy language},
	pages = {1--12},
	file = {clin04:/home/user/Zotero/storage/6UBS6GPA/clin04.pdf:application/pdf}
}

@article{lake-one-shot-2014,
	title = {One-shot learning of generative speech concepts},
	journal = {{Proceedings of the 36th Annual Conference of the Cognitive Science Society}},
	author = {Lake, Brenden M and Lee, Chia-ying and Glass, James R and Tenenbaum, Joshua B},
	year = {2014},
	keywords = {Learning, category, exemplar generation, one-shot learning, speech recognition},
	pages = {803--808},
	file = {paper146:/home/user/Zotero/storage/7QI5TEZD/paper146.pdf:application/pdf}
}

@article{lappin-probabilistic-2011,
	title = {Probabilistic {Semantics} for {Natural} {Language}},
	author = {Lappin, Shalom and Eijck, Jan Van},
	year = {2011},
	file = {vaneijck-lappinLIRA12:/home/user/Zotero/storage/FW9V3SSF/vaneijck-lappinLIRA12.pdf:application/pdf}
}

@article{lappin-curry-2014,
	title = {Curry {Typing}, {Polymorphism}, and {Fine}-{Grained} {Intensionality}},
	number = {June},
	journal = {Handbook of Contemporary Semantics, 2nd Edition},
	author = {Lappin, Shalom},
	year = {2014},
	pages = {1--33},
	file = {lappin_semantics_handbook_chapter:/home/user/Zotero/storage/NHRUPK6B/lappin_semantics_handbook_chapter.pdf:application/pdf}
}

@article{lago-agreement-2015,
	title = {Agreement attraction in {Spanish} comprehension},
	volume = {82},
	url = {http://dx.doi.org/10.1016/j.jml.2015.02.002},
	doi = {10.1016/j.jml.2015.02.002},
	journal = {Journal of Memory and Language},
	author = {Lago, Sol and Shalom, Diego E and Sigman, Mariano and Lau, Ellen F and Phillips, Colin},
	year = {2015},
	keywords = {agreement attraction},
	pages = {133--149},
	file = {Lago et al. - 2015 - Agreement attraction in Spanish comprehension:/home/user/Zotero/storage/NGG6HV9K/Lago et al. - 2015 - Agreement attraction in Spanish comprehension.pdf:application/pdf}
}

@article{kuperman-processing-2010,
	title = {Processing trade-offs in the reading of {Dutch} derived words},
	volume = {62},
	issn = {0749596X},
	doi = {10.1016/j.jml.2009.10.001},
	abstract = {This eye-tracking study explores visual recognition of Dutch suffixed words (e.g., plaats+ing "placing") embedded in sentential contexts, and provides new evidence on the interplay between storage and computation in morphological processing. We show that suffix length crucially moderates the use of morphological properties. In words with shorter suffixes, we observe a stronger effect of full-forms (derived word frequency) on reading times than in words with longer suffixes. Also, processing times increase if the base word (plaats) and the suffix (-ing) differ in the amount of information carried by their morphological families (sets of words that share the base or the suffix). We model this imbalance of informativeness in the morphological families with the information-theoretical measure of relative entropy and demonstrate its predictivity for the processing times. The observed processing trade-offs are discussed in the context of current models of morphological processing. ?? 2009 Elsevier Inc. All rights reserved.},
	journal = {Journal of Memory and Language},
	author = {Kuperman, Victor and Bertram, Raymond and Baayen, R. Harald},
	year = {2010},
	keywords = {Information theory, Eye movements, Derived words, Lexical processing, morphology},
	pages = {83--97},
	file = {KupermanBertramBaayenJML2010:/home/user/Zotero/storage/SZDEZG6G/KupermanBertramBaayenJML2010.pdf:application/pdf}
}

@article{kush-relation-sensitive-2015,
	title = {Relation-sensitive retrieval: {Evidence} from bound variable pronouns},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000224},
	doi = {10.1016/j.jml.2015.02.003},
	journal = {Journal of Memory and Language},
	author = {Kush, Dave and Lidz, Jeffrey and Phillips, Colin},
	year = {2015},
	pages = {18--40},
	file = {Kush, Lidz, Phillips - 2015 - Relation-sensitive retrieval Evidence from bound variable pronouns:/home/user/Zotero/storage/RE2MTFKE/Kush, Lidz, Phillips - 2015 - Relation-sensitive retrieval Evidence from bound variable pronouns.pdf:application/pdf}
}

@article{kuperman-reading-2009,
	title = {Reading polymorphemic {Dutch} compounds: toward a multiple route model of lexical processing.},
	volume = {35},
	issn = {0096-1523 (Print)\r0096-1523 (Linking)},
	doi = {10.1037/a0013484},
	abstract = {This article reports an eye-tracking experiment with 2,500 polymorphemic Dutch compounds presented in isolation for visual lexical decision while readers' eye movements were registered. The authors found evidence that both full forms of compounds (dishwasher) and their constituent morphemes (e.g., dish, washer) and morphological families of constituents (sets of compounds with a shared constituent) played a role in compound processing. They observed simultaneous effects of compound frequency, left constituent frequency, and family size early (i.e., before the whole compound has been scanned) and also observed effects of right constituent frequency and family size that emerged after the compound frequency effect. The temporal order of these and other observed effects goes against assumptions of many models of lexical processing. The authors propose specifications for a new multiple-route model of polymorphemic compound processing that is based on time-locked, parallel, and interactive use of all morphological cues as soon as they become even partly available to the visual uptake system.},
	journal = {Journal of experimental psychology. Human perception and performance},
	author = {Kuperman, Victor and Schreuder, Robert and Bertram, Raymond and Baayen, R Harald},
	year = {2009},
	pages = {876--895},
	file = {kupermanSchreuderBertramBaayenJEP2009:/home/user/Zotero/storage/4SIM95BJ/kupermanSchreuderBertramBaayenJEP2009.pdf:application/pdf}
}

@article{kryuchkova-danger-2012,
	title = {Danger and usefulness are detected early in auditory lexical processing: {Evidence} from electroencephalography},
	volume = {122},
	issn = {1090-2155 (Electronic) 0093-934X (Linking)},
	doi = {10.1016/j.bandl.2012.05.005},
	abstract = {Visual emotionally charged stimuli have been shown to elicit early electrophysiological responses (e.g., Ihssen, Heim, \& Keil, 2007; Schupp, Jungh??fer, Weike, \& Hamm, 2003; Stolarova, Keil, \& Moratti, 2006). We presented isolated words to listeners, and observed, using generalized additive modeling, oscillations in the upper part of the delta range, the theta range (Bastiaansen \& Hagoort, 2003), and the lower part of the alpha range related to degree of (rated) danger and usefulness (Wurm, 2007) starting around 150. ms and continuing to 350. ms post stimulus onset. A negative deflection in the oscillations tied to danger around 250-300. ms fits well with a similar negativity observed in the same time interval for visual emotion processing. Frequency and competitor effects emerged or reached maximal amplitude later, around or following the uniqueness point. The early effect of danger, long before the words' uniqueness points, is interpreted as evidence for the dual pathway theory of LeDoux (1996). ?? 2012 Elsevier Inc.},
	journal = {Brain and Language},
	author = {Kryuchkova, Tatiana and Tucker, Benjamin V. and Wurm, Lee H. and Baayen, R. Harald},
	year = {2012},
	keywords = {Auditory comprehension, Danger, Frequency, Theta oscillations, Uniqueness point, Usefulness},
	pages = {81--91},
	file = {Kryuchkova-Tucker-Wurm-Baayen-2012:/home/user/Zotero/storage/4BBHW6UN/Kryuchkova-Tucker-Wurm-Baayen-2012.pdf:application/pdf}
}

@article{konopka-how-2015,
	title = {How message similarity shapes the timecourse of sentence formulation},
	volume = {84},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000467},
	doi = {10.1016/j.jml.2015.04.003},
	journal = {Journal of Memory and Language},
	author = {Konopka, Agnieszka E. and Kuchinsky, Stefanie E.},
	year = {2015},
	pages = {1--23},
	file = {Konopka, Kuchinsky - 2015 - How message similarity shapes the timecourse of sentence formulation:/home/user/Zotero/storage/79EHX7DS/Konopka, Kuchinsky - 2015 - How message similarity shapes the timecourse of sentence formulation.pdf:application/pdf}
}

@article{krott-analogical-2007,
	title = {Analogical effects on linking elements in {German} compound words},
	volume = {22},
	issn = {01690965},
	doi = {10.1080/01690960500343429},
	abstract = {This paper examines whether the selection of linking elements for novel German compounds can be better explained in terms of a single or a dual-route model. Previous studies had focused on the predictability of linking elements by rules. We investigate a single-route model by focusing on the paradigmatic analogical effect of the compounds sharing the left (right) constituent with the target compound, i.e., the left (right) constituent family. A production experiment reveals an effect of the left, but not of the right constituent family. Simulation studies of the responses, using a computational model of paradigmatic analogy, show that the left constituent and its phonological and morphological properties (rime, gender, and inflectional class) simultaneously codetermine the selection of linking elements. We show how these results can be accounted for by a single-route approach, and we outline a symbolic interactive activation model that merges the factors into one psycholinguistically motivated processing mechanism.},
	journal = {Language and Cognitive Processes},
	author = {Krott, Andrea and Schreuder, Robert and Harald Baayen, R. and Dressler, Wolfgang U.},
	year = {2007},
	pages = {25--57},
	file = {KrottEtAlLCP:/home/user/Zotero/storage/MQGSBXCT/KrottEtAlLCP.pdf:application/pdf}
}

@article{kosling-prominence-2013,
	title = {Prominence in triconstituent compounds: pitch contours and linguistic theory.},
	volume = {56},
	issn = {0023830913},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24597276},
	doi = {10.1177/0023830913478914},
	abstract = {According to the widely accepted Lexical Category Prominence Rule (LCPR), prominence assignment to triconstituent compounds depends on the branching direction. Left-branching compounds, that is, compounds with a left-hand complex constituent, are held to have highest prominence on the left-most constituent, whereas right-branching compounds have highest prominence on the second of the three constituents. The LCPR is, however, only poorly empirically supported. The present paper tests a new hypothesis concerning the prominence of triconstituent compounds and suggests a new methodology for the empirical investigation of compound prominence. According to this hypothesis, the prominence pattern of the embedded compound has a decisive influence on the prominence of the whole compound. Using a mixed-effects generalized additive model for the analysis of the pitch movements, it is shown that all triconstituent compounds have an accent on the first constituent irrespective of branching, and that the placement of a second, or even a third, accent is dependent on the prominence pattern of the embedded compound. The LCPR is wrong.},
	number = {0},
	journal = {Language and speech},
	author = {Kösling, Kristina and Kunter, Gero and Baayen, Harald and Plag, Ingo},
	year = {2013},
	keywords = {Humans, Linguistics, Adolescent, Adult, Female, Male, Models, Psychological, Models, Statistical, Pattern Recognition, Physiological, Pitch Discrimination, Pitch Perception, Recognition (Psychology), Signal Processing, Computer-Assisted, Sound Spectrography, Speech Acoustics, Speech Production Measurement, Time Factors, Young Adult},
	pages = {529--54},
	file = {KoeslingEtAl2013:/home/user/Zotero/storage/EH63VMCW/KoeslingEtAl2013.pdf:application/pdf}
}

@article{konopka-priming-2014,
	title = {Priming sentence planning.},
	volume = {73},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24838190},
	doi = {10.1016/j.cogpsych.2014.04.001},
	abstract = {Sentence production requires mapping preverbal messages onto linguistic structures. Because sentences are normally built incrementally, the information encoded in a sentence-initial increment is critical for explaining how the mapping process starts and for predicting its timecourse. Two experiments tested whether and when speakers prioritize encoding of different types of information at the outset of formulation by comparing production of descriptions of transitive events (e.g., A dog is chasing the mailman) that differed on two dimensions: the ease of naming individual characters and the ease of apprehending the event gist (i.e., encoding the relational structure of the event). To additionally manipulate ease of encoding, speakers described the target events after receiving lexical primes (facilitating naming; Experiment 1) or structural primes (facilitating generation of a linguistic structure; Experiment 2). Both properties of the pictured events and both types of primes influenced the form of target descriptions and the timecourse of formulation: character-specific variables increased the probability of speakers encoding one character with priority at the outset of formulation, while the ease of encoding event gist and of generating a syntactic structure increased the likelihood of early encoding of information about both characters. The results show that formulation is flexible and highlight some of the conditions under which speakers might employ different planning strategies.},
	journal = {Cognitive psychology},
	author = {Konopka, Agnieszka E and Meyer, Antje S},
	year = {2014},
	pages = {1--40},
	file = {Konopka, Meyer - 2014 - Priming sentence planning:/home/user/Zotero/storage/I86X4QFJ/Konopka, Meyer - 2014 - Priming sentence planning.pdf:application/pdf}
}

@article{kleinman-single-word-2015,
	title = {Single-word predictions of upcoming language during comprehension: {Evidence} from the cumulative semantic interference task},
	volume = {79},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S001002851500033X},
	doi = {10.1016/j.cogpsych.2015.04.001},
	journal = {Cognitive Psychology},
	author = {Kleinman, Daniel and Runnqvist, Elin and Ferreira, Victor S.},
	year = {2015},
	pages = {68--101},
	file = {Kleinman, Runnqvist, Ferreira - 2015 - Single-word predictions of upcoming language during comprehension Evidence from the cumulative se:/home/user/Zotero/storage/3PQFKW6K/Kleinman, Runnqvist, Ferreira - 2015 - Single-word predictions of upcoming language during comprehension Evidence from the cumulative se.pdf:application/pdf}
}

@article{kierstead-1.-2015,
	title = {1. {Introduction} {Some} of the most interesting studies on the relationship between semantics and pragmatics involve simple function words—members of closed classes, with a very high frequency of},
	number = {1},
	author = {Kierstead, Greg and Simons, Mandy and Gillies, Thoni and Hoeksma, Jack and Lepore, Ernie and Maier, Emar and Moltmann, Friederike},
	year = {2015},
	pages = {1--59},
	file = {Kierstead et al. - 2015 - 1. Introduction Some of the most interesting studies on the relationship between semantics and pragmatics in:/home/user/Zotero/storage/ZTCB68GG/Kierstead et al. - 2015 - 1. Introduction Some of the most interesting studies on the relationship between semantics and pragmatics in.pdf:application/pdf;Roberts.EpistemicModality:/home/user/Zotero/storage/JG4INTBR/Roberts.EpistemicModality.pdf:application/pdf}
}

@article{kao-funny-2013,
	title = {The {Funny} {Thing} {About} {Incongruity}: {A} {Computational} {Model} of {Humor} in {Puns}},
	volume = {1},
	journal = {Proceedings of the 35th Conference of the Cognitive Science Society},
	author = {Kao, Justine T and Levy, Roger and Goodman, Noah D},
	year = {2013},
	keywords = {humor, language understanding, probabilistic},
	file = {pun_KaoLevyGoodman:/home/user/Zotero/storage/KW9S5DT4/pun_KaoLevyGoodman.pdf:application/pdf}
}

@article{katzir-cognitively-2010,
	title = {A {Cognitively} {Plausible} {Model} for {Grammar} {Induction}},
	volume = {2},
	number = {2},
	author = {Katzir, Roni},
	year = {2010},
	pages = {1--17},
	file = {Katzir - 2010 - A Cognitively Plausible Model for Grammar Induction:/home/user/Zotero/storage/TSKPE8QU/Katzir - 2010 - A Cognitively Plausible Model for Grammar Induction.pdf:application/pdf}
}

@article{kao-acquired-1993,
	title = {An acquired taste : {How} reading literature affects sensitivity to word distributions when judging literary texts {The} {Probabilistic} {Nature} of {Natural} {Languages} {Reading} {Habits} and {Judgment} : {Experiment}},
	journal = {Journal of English Linguistics},
	author = {Kao, Justine and Ryan, Robert and Dye, Melody and Ramscar, Michael and Hall, Jordan},
	year = {1993},
	keywords = {psycholinguistics, anderson et, corpus linguistics, discourse, distributions, genre differences, literary studies, processes, reading comprehension and vocabulary, reading habits, tests, word},
	pages = {990--995},
	file = {cogsci2010_literary:/home/user/Zotero/storage/39TMVR5Z/cogsci2010_literary.pdf:application/pdf}
}

@article{kao-nonliteral-2014,
	title = {Nonliteral understanding of number words.},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/25092304},
	doi = {10.1073/pnas.1407479111},
	abstract = {One of the most puzzling and important facts about communication is that people do not always mean what they say; speakers often use imprecise, exaggerated, or otherwise literally false descriptions to communicate experiences and attitudes. Here, we focus on the nonliteral interpretation of number words, in particular hyperbole (interpreting unlikely numbers as exaggerated and conveying affect) and pragmatic halo (interpreting round numbers imprecisely). We provide a computational model of number interpretation as social inference regarding the communicative goal, meaning, and affective subtext of an utterance. We show that our model predicts humans' interpretation of number words with high accuracy. Our model is the first to our knowledge to incorporate principles of communication and empirically measured background knowledge to quantitatively predict hyperbolic and pragmatic halo effects in number interpretation. This modeling framework provides a unified approach to nonliteral language understanding more generally.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kao, Justine T and Wu, Jean Y and Bergen, Leon and Goodman, Noah D},
	year = {2014},
	pages = {1--6},
	file = {PNAS2014_kaoetal_hyperbole:/home/user/Zotero/storage/6I7FNWKE/PNAS2014_kaoetal_hyperbole.pdf:application/pdf}
}

@article{kao-formalizing-2014,
	title = {Formalizing the {Pragmatics} of {Metaphor} {Understanding}},
	volume = {1},
	abstract = {While the ubiquity and importance of nonliteral language are clear, people’s ability to use and understand it remains a mystery. Metaphor in particular has been studied extensively across many disciplines in cognitive science. One approach focuses on the pragmatic principles that listeners utilize to infer meaning from metaphorical utterances. While this approach has generated a number of insights about how people understand metaphor, to our knowledge there is no formal model showing that effects in metaphor understanding can arise from basic principles of communication. Building upon recent advances in formal models of pragmatics, we describe a computational model that uses pragmatic reasoning to interpret metaphorical utterances. We conduct behavioral experiments to evaluate the model’s performance and show that our model produces metaphorical interpretations that closely fit behavioral data. We discuss implications of the model for metaphor understanding, principles of communication, and formal models of language understanding.},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society (CogSci 2014)},
	author = {Kao, Justine T and Bergen, Leon and Goodman, Noah D},
	year = {2014},
	keywords = {metaphor, language understanding, computational models, Pragmatics},
	pages = {719--724},
	file = {kaoetal-metaphors-2014(1):/home/user/Zotero/storage/T5CJP6NV/kaoetal-metaphors-2014(1).pdf:application/pdf}
}

@article{kao-computational-2011,
	title = {a {Computational} {Analysis} of {Poetic} {Craft} in {Contemporary} {Professional} and {Amateur} {Poetry} a {Thesis} {Submitted} {To} the {Department} of {Symbolic} {Systems} of {Stanford} {University} in {Partial} {Fulfillment} of the {Requirements} for the {Degree} of},
	number = {June},
	author = {Kao, Justine T},
	year = {2011},
	file = {Kao - 2011 - a Computational Analysis of Poetic Craft in Contemporary Professional and Amateur Poetry a Thesis Submitted To the Departme:/home/user/Zotero/storage/MG3SH94U/Kao - 2011 - a Computational Analysis of Poetic Craft in Contemporary Professional and Amateur Poetry a Thesis Submitted To the Departme.pdf:application/pdf;poetry_thesis:/home/user/Zotero/storage/7NXJRQNP/poetry_thesis.pdf:application/pdf}
}

@article{jouravlev-lexical-2015,
	title = {Lexical stress assignment as a problem of probabilistic inference},
	url = {http://link.springer.com/10.3758/s13423-015-0802-y},
	doi = {10.3758/s13423-015-0802-y},
	journal = {Psychonomic Bulletin \& Review},
	author = {Jouravlev, Olessia and Lupker, Stephen J.},
	year = {2015},
	keywords = {bayesian, lexical stress assignment, nonword naming, probabilities, reading, Russian, word naming},
	pages = {1174--1192}
}

@article{kao-when-nodate,
	title = {When “ all ” means not all : {Nonliteral} interpretations of universal quantifiers},
	volume = {0287},
	author = {Kao, Justine T and Degen, Judith and Goodman, Noah D},
	pages = {287--287},
	file = {CUNY2015-KaoDegenGoodman:/home/user/Zotero/storage/TMDTCS8E/CUNY2015-KaoDegenGoodman.pdf:application/pdf}
}

@article{janda-capturing-2010,
	title = {Capturing correlational structure in {Russian} paradigms: {A} case study in logistic mixed-effects modeling},
	volume = {6},
	doi = {10.1515/CLLT.2010.002},
	abstract = {This study addresses the statistical analysis of a phenomenon in Russian verbal paradigms, a suffix shift that is spreading through the paradigm and making it more regular. A problem that arises in the analysis of data collected from the Russian National Corpus is that counts documenting this phenomenon are based on repeated observations of the same verbs, and, moreover, on counts for different parts of the paradigms of these same verbs. Unsurprisingly, individual verbs display consistent (although variable) behavior with respect to the suffix shift. The non-independence of the elementary observations in our data has to be taken into account in the statistical evaluation of the patterns in the data. We show how mixed-effects modeling can be used to do this in a principled way, and that it is also necessary to do so in order to avoid anti-conservative evaluation of significance. 1},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Janda, Laura a. and Nesset, Tore and Baayen, R. Harald},
	year = {2010},
	keywords = {morphology, Russian, logistic mixed-effects modeling, paradigms},
	pages = {29--48},
	file = {jandaNessetBaayenCLLT2010:/home/user/Zotero/storage/4NURP4W7/jandaNessetBaayenCLLT2010.pdf:application/pdf}
}

@article{kao-formalizing-2014-1,
	title = {Formalizing the {Pragmatics} of {Metaphor} {Understanding}},
	volume = {1},
	abstract = {While the ubiquity and importance of nonliteral language are clear, people’s ability to use and understand it remains a mystery. Metaphor in particular has been studied extensively across many disciplines in cognitive science. One approach focuses on the pragmatic principles that listeners utilize to infer meaning from metaphorical utterances. While this approach has generated a number of insights about how people understand metaphor, to our knowledge there is no formal model showing that effects in metaphor understanding can arise from basic principles of communication. Building upon recent advances in formal models of pragmatics, we describe a computational model that uses pragmatic reasoning to interpret metaphorical utterances. We conduct behavioral experiments to evaluate the model’s performance and show that our model produces metaphorical interpretations that closely fit behavioral data. We discuss implications of the model for metaphor understanding, principles of communication, and formal models of language understanding.},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society (CogSci 2014)},
	author = {Kao, Justine T and Bergen, Leon and Goodman, Noah D},
	year = {2014},
	keywords = {metaphor, language understanding, computational models, Pragmatics},
	pages = {719--724},
	file = {kaoetal-metaphors-2014(1):/home/user/Zotero/storage/ZCN8H4N2/kaoetal-metaphors-2014(1).pdf:application/pdf}
}

@article{jager-game-2012,
	title = {Game theory in semantics and pragmatics},
	journal = {Semantics},
	author = {Jager, Gehard},
	year = {2012},
	pages = {1--32},
	file = {hskArticleGJ-2013:/home/user/Zotero/storage/KQK7UIZZ/hskArticleGJ-2013.pdf:application/pdf}
}

@article{ingo-plag-suffix-2009,
	title = {Suffix {Ordering} and {Morphological} {Processing}},
	volume = {85},
	issn = {0097-8507},
	doi = {10.1353/lan.0.0087},
	abstract = {There is a long-standing debate about the principles constraining the combinatorial properties of suffixes. Hay 2002 and Hay \& Plag 2004 proposed a model in which suffixes can be ordered along a hierarchy of processing complexity. We show that this model generalizes to a larger set of suffixes, and we provide independent evidence supporting the claim that a higher rank in the ordering correlates with increased productivity. Behavioral data from lexical decision and word naming show, however, that this model has been one-sided in its exclusive focus on the importance of constituent-driven processing, and that it requires supplementation by a second and equally important focus on the role of memory. Finally, using concepts from graph theory, we show that the space of existing suffix combinations can be conceptualized as a directed graph, which, with surprisingly few exceptions, is acyclic. This acyclicity is hypothesized to be functional for lexical processing.},
	journal = {Language},
	author = {{Ingo Plag} and {Harald Baayen}},
	year = {2009},
	pages = {109--152},
	file = {plagBaayenLanguage2009:/home/user/Zotero/storage/RKBH5X8H/plagBaayenLanguage2009.pdf:application/pdf}
}

@article{huth-continuous-2012,
	title = {A {Continuous} {Semantic} {Space} {Describes} the {Representation} of {Thousands} of {Object} and {Action} {Categories} across the {Human} {Brain}},
	volume = {76},
	issn = {1097-4199 (Electronic)\n0896-6273 (Linking)},
	doi = {10.1016/j.neuron.2012.10.014},
	abstract = {Humans can see and name thousands of distinct object and action categories, so it is unlikely that each category is represented in a distinct brain area. A more efficient scheme would be to represent categories as locations in a continuous semantic space mapped smoothly across the cortical surface. To search for such a space, we used fMRI to measure human brain activity evoked by natural movies. We then used voxelwise models to examine the cortical representation of 1,705 object and action categories. The first few dimensions of the underlying semantic space were recovered from the fit models by principal components analysis. Projection of the recovered semantic space onto cortical flat maps shows that semantic selectivity is organized into smooth gradients that cover much of visual and nonvisual cortex. Furthermore, both the recovered semantic space and the cortical organization of the space are shared across different individuals.},
	number = {6},
	journal = {Neuron},
	author = {Huth, Alexander G. and Nishimoto, Shinji and Vu, An T. and Gallant, Jack L.},
	year = {2012},
	pages = {1210--1224},
	file = {1-s2.0-S0896627312009348-main:/home/user/Zotero/storage/VPJGKGU4/1-s2.0-S0896627312009348-main.pdf:application/pdf}
}

@article{hobbs-coherence-1979,
	title = {Coherence and coreference},
	volume = {3},
	issn = {2951740867},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021379800439},
	doi = {10.1207/s15516709cog0301\_4},
	abstract = {Coherence in conversations and in texts can be partially characterized by a set of coherence relations, motivated ultimately by the speaker's or writer's need to be understood. In this paper, formal definitions are given for several coherence relations, based on the operations of an inference system; that is, the relations between successive portions of a discourse are characterized in terms of the inferences that can be drawn from each. In analyzing a discourse, it is frequently the case that we would recognize it as coherent, in that it would satisfy the formal definition of some coherence relation, if only we could assume certain noun phrases to be coreferential. In such cases, we will simply assume the identity of the entities referred to, in what might be called a "petty conversational implicature," thereby solving the coherence and coreference problems simultaneously. Three examples of different kinds of reference problems are presented. In each, it is shown how the coherence of the discourse can be recognized, and how the reference problems are solved, almost as a by-product, by means of these petty conversational implicatures.},
	journal = {Cognitive Science},
	author = {Hobbs, Jerry R},
	year = {1979},
	pages = {67--90},
	file = {KehlerKertzRohdeElman.2008:/home/user/Zotero/storage/PSFRH4NP/KehlerKertzRohdeElman.2008.pdf:application/pdf}
}

@article{jacobs-why-2015,
	title = {Why are repeated words produced with reduced durations? {Evidence} from inner speech and homophone production},
	volume = {84},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000728},
	doi = {10.1016/j.jml.2015.05.004},
	journal = {Journal of Memory and Language},
	author = {Jacobs, Cassandra L. and Yiu, Loretta K. and Watson, Duane G. and Dell, Gary S.},
	year = {2015},
	pages = {37--48},
	file = {Jacobs et al. - 2015 - Why are repeated words produced with reduced durations Evidence from inner speech and homophone production:/home/user/Zotero/storage/3CBQKA7F/Jacobs et al. - 2015 - Why are repeated words produced with reduced durations Evidence from inner speech and homophone production.pdf:application/pdf}
}

@article{hochstein-ignorance-nodate,
	title = {ignorance and inference gricean children scalar implicature},
	journal = {under review},
	author = {{Hochstein} and {Bale} and {Fox} and {Barner}},
	file = {Hochstein, Bale, Fox, & Barner (under review):/home/user/Zotero/storage/N6IDMU38/Hochstein, Bale, Fox, & Barner (under review).pdf:application/pdf}
}

@article{hawthorne-acoustic-2015,
	title = {The acoustic salience of prosody trumps infants’ acquired knowledge of language-specific prosodic patterns},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000388},
	doi = {10.1016/j.jml.2015.03.005},
	journal = {Journal of Memory and Language},
	author = {Hawthorne, Kara and Mazuka, Reiko and Gerken, LouAnn},
	year = {2015},
	pages = {105--117},
	file = {Hawthorne, Mazuka, Gerken - 2015 - The acoustic salience of prosody trumps infants’ acquired knowledge of language-specific prosodic p:/home/user/Zotero/storage/VJ7P7HVW/Hawthorne, Mazuka, Gerken - 2015 - The acoustic salience of prosody trumps infants’ acquired knowledge of language-specific prosodic p.pdf:application/pdf}
}

@article{hartshorne-verb-2012,
	title = {Verb argument structure predicts implicit causality: {The} advantages of finer-grained semantics},
	doi = {10.1080/01690965.2012.689305},
	abstract = {While the referent of a nonreflexive pronoun clearly depends on context, the nature of these contextual restrictions is controversial. The present study seeks to characterise one representation that guides pronoun resolution. Our focus is an effect known as ‘‘implicit causality’’. In causal dependant clauses, the preferred referent of a pronoun varies systematically with the verb in the main clause (contrast Sally frightened Mary because she .. . with Sally feared Mary because she...). A number of researchers have tried to explain and predict such biases with reference to semantic classes of verbs. However, such studies have focused on a small number of specially selected verbs. In Experiment 1, we find that existing taxonomies perform near chance at predicting pronoun-resolution bias on a large set of representative verbs. However, a more fine-grained taxonomy recently proposed in the linguistics literature does significantly better. In Experiment 2, we tested all 264 verbs in two of the narrowly defined verb classes from this new taxonomy, finding that pronoun-resolution biases were categorically different. These findings suggest that the semantic structure of verbs tightly constrains the interpretation of pronouns in causal sentences, raising challenges for theories which posit that implicit causality biases reflect world knowledge or arbitrary lexical features.},
	number = {0623845},
	journal = {Language and Cognitive Processes},
	author = {Hartshorne, Joshua K. and Snedeker, Jesse},
	year = {2012},
	keywords = {implicit causality, a, a proper name like, a third-person pronoun like, always refers to the, cates, catherine the great, catherine the great almost, in contrast, predicate decomposition, pronoun resolution, psych verbs, psychological predi-, same person, she can refer to, thematic roles},
	pages = {1--35},
	file = {HartshorneSnedeker2012:/home/user/Zotero/storage/ZCZA4KF9/HartshorneSnedeker2012.pdf:application/pdf}
}

@article{hartshorne-causes-2015,
	title = {The causes and consequences explicit in verbs},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1008524},
	doi = {10.1080/23273798.2015.1008524},
	number = {April},
	journal = {Language, Cognition and Neuroscience},
	author = {Hartshorne, Joshua K. and O'Donnell, Timothy J. and Tenenbaum, Joshua B.},
	year = {2015},
	keywords = {implicit causality, Pragmatics, implicit consequentiality, pronouns},
	pages = {1--19},
	file = {HartshorneODonnellTenenbaum:/home/user/Zotero/storage/27GEGAVU/HartshorneODonnellTenenbaum.pdf:application/pdf}
}

@article{hartshorne-what-2013,
	title = {What is implicit causality?},
	volume = {29},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2013.796396},
	doi = {10.1080/01690965.2013.796396},
	number = {1974},
	journal = {Language, Cognition and Neuroscience},
	author = {Hartshorne, Joshua K},
	year = {2013},
	keywords = {1, implicit causality, Pragmatics, a, pronoun resolution, thematic roles, caramazza, catherine garvey and alfonso, illustrated below, in 1974, inference, introduced a puzzling phenomenon, sally frightened mary because, she},
	pages = {804--824},
	file = {Hartshorne_LCP_WhatIsIC:/home/user/Zotero/storage/3UMJETXD/Hartshorne_LCP_WhatIsIC.pdf:application/pdf}
}

@article{gwilliams-non-linear-2015,
	title = {Non-linear processing of a linear speech stream: {The} influence of morphological structure on the recognition of spoken {Arabic} words},
	volume = {147},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0093934X15000929},
	doi = {10.1016/j.bandl.2015.04.006},
	journal = {Brain and Language},
	author = {Gwilliams, L. and Marantz, a.},
	year = {2015},
	pages = {1--13},
	file = {1-s2.0-S0093934X15000929-main:/home/user/Zotero/storage/EGZ3F7FV/1-s2.0-S0093934X15000929-main.pdf:application/pdf}
}

@article{gow-lexical-2015,
	title = {Lexical mediation of phonotactic frequency effects on spoken word recognition: {A} {Granger} causality analysis of {MRI}-constrained {MEG}/{EEG} data},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000376},
	doi = {10.1016/j.jml.2015.03.004},
	journal = {Journal of Memory and Language},
	author = {Gow, David W. and Olson, Bruna B.},
	year = {2015},
	keywords = {effective connectivity, magnetoencephalography, phonotactic frequency},
	pages = {41--55},
	file = {Gow, Olson - 2015 - Lexical mediation of phonotactic frequency effects on spoken word recognition A Granger causality analysis of MRI-co:/home/user/Zotero/storage/IA3S9KM4/Gow, Olson - 2015 - Lexical mediation of phonotactic frequency effects on spoken word recognition A Granger causality analysis of MRI-co.pdf:application/pdf}
}

@article{giorgolo-towards-2006,
	title = {Towards a {Statistical} {Model} of {Grammaticality}},
	url = {http://philpapers.org/rec/GIOTAS},
	journal = {Philpapers.Org},
	author = {Giorgolo, Gianluca and Lappin, Shalom and Clark, Alexander},
	year = {2006},
	keywords = {enriched language models, grammaticality judgements, probabilistic syntax, probability distribu-, tion},
	pages = {2064--2069},
	file = {cgl_cogsci13:/home/user/Zotero/storage/KDQNP8II/cgl_cogsci13.pdf:application/pdf}
}

@article{gibson-need-2010,
	title = {The need for quantitative methods in syntax and semantics research},
	issn = {0169-0965 1464-0732},
	doi = {10.1080/01690965.2010.515080},
	abstract = {Sprouse and Almeida (S\&A) present quantitative results that suggest that intuitive judgments utilised in syntax research are generally correct in two-condition comparisons: the sentence type that is presented as “good/grammatical” is usually rated better than the sentence type that is presented as “bad/ungrammatical” in controlled experiments. Although these evaluations of intuitive relative judgments are valuable, they do not justify the use of nonquantitative linguistic methods. We argue that objectivity is a universal value in science that should be adopted by linguistics. In addition, the reliability measures that S\&A report are not sufficient for developing sophisticated linguistic theories. Furthermore, quantitative methods yield two additional benefits: consistency of judgments across many pairs of judgments; and an understanding of the relative effect sizes across sets of judgments. We illustrate these points with an experiment that demonstrates five clear levels of acceptability. Finally, we observe that S\&A's experiments—where only two authors evaluated 10 years' worth of journal articles and one standard textbook within a few months—further emphasise one of our critical original points: conducting behavioural experiments is in many respects easy and fast with the advent of online research tools like Amazon's Mechanical Turk. Given the current ease of performing quantitative experiments (using a platform like Mechanical Turk) and the clear limitations of not doing so, linguistic hypotheses should be evaluated quantitatively whenever it is feasible.},
	number = {July 2015},
	journal = {Language and Cognitive Processes},
	author = {Gibson, Edward and Fedorenko, Evelina},
	year = {2010},
	pages = {1--37},
	file = {Gibson, Fedorenko - 2010 - The need for quantitative methods in syntax and semantics research:/home/user/Zotero/storage/APSUM4D3/Gibson, Fedorenko - 2010 - The need for quantitative methods in syntax and semantics research.pdf:application/pdf}
}

@article{graf-local-2013,
	title = {Local and {Transderivational} {Constraints} in {Syntax} and {Semantics} by},
	author = {Graf, Thomas},
	year = {2013},
	file = {PhDThesis_RollingRelease:/home/user/Zotero/storage/2IUNRWGA/PhDThesis_RollingRelease.pdf:application/pdf}
}

@article{goodman-knowledge-2013,
	title = {Knowledge and {Implicature}: {Modeling} {Language} {Understanding} as {Social} {Cognition}},
	volume = {5},
	issn = {1756-8765 (Electronic)\r1756-8757 (Linking)},
	doi = {10.1111/tops.12007},
	abstract = {Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to "invert" this model of the speaker. We apply this framework to model scalar implicature ("some" implies "not all," and "N" implies "not more than N"). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.},
	journal = {Topics in Cognitive Science},
	author = {Goodman, Noah D. and Stuhlmüller, Andreas},
	year = {2013},
	keywords = {language, scalar implicature, Bayesian model},
	pages = {173--184},
	file = {implicature-topics2013:/home/user/Zotero/storage/JX3ZKATQ/implicature-topics2013.pdf:application/pdf}
}

@article{gerlach-supplemental-2013,
	title = {Supplemental {Material} for : {Stochastic} model for the vocabulary growth in natural languages},
	author = {Gerlach, Martin and Altmann, Eduardo G},
	year = {2013},
	pages = {1--15},
	file = {1212.1362v3:/home/user/Zotero/storage/8PIWUBHZ/1212.1362v3.pdf:application/pdf}
}

@article{galeazzi-smart-2015,
	title = {Smart {Transformations}: {The} {Evolution} of {Choice} {Principles}},
	url = {http://arxiv.org/abs/1505.07054},
	abstract = {Evolutionary game theory classically investigates which behavioral patterns are evolutionarily successful in a single game. More recently, a number of contributions have studied the evolution of preferences instead: which subjective conceptualizations of a game's payoffs give rise to evolutionarily successful behavior in a single game. Here, we want to extend this existing approach even further by asking: which general patterns of subjective conceptualizations of payoff functions are evolutionarily successful across a class of games. In other words, we will look at evolutionary competition of payoff transformations in "meta-games", obtained from averaging over payoffs of single games. Focusing for a start on the class of 2x2 symmetric games, we show that regret minimization can outperform payoff maximization if agents resort to a security strategy in case of radical uncertainty.},
	author = {Galeazzi, Paolo and Franke, Michael},
	year = {2015},
	keywords = {ecological, evolution of preferences, evolutionary dynamics, evolutionary game theory, meta-games, rationality, rationality norms},
	pages = {1--8},
	file = {1505.07054v1:/home/user/Zotero/storage/PBTJQK7X/1505.07054v1.pdf:application/pdf}
}

@article{friederici-grounding-2015,
	title = {Grounding language processing on basic neurophysiological principles},
	volume = {19},
	url = {http://dx.doi.org/10.1016/j.tics.2015.03.012},
	doi = {10.1016/j.tics.2015.03.012},
	number = {6},
	journal = {Trends in Cognitive Sciences},
	author = {Friederici, Angela D and Singer, Wolf},
	year = {2015},
	keywords = {language, neural networks, oscillation},
	pages = {1--10},
	file = {Friederici, Singer - 2015 - Grounding language processing on basic neurophysiological principles:/home/user/Zotero/storage/H45EWG7S/Friederici, Singer - 2015 - Grounding language processing on basic neurophysiological principles.pdf:application/pdf}
}

@article{franke-strategies-nodate,
	title = {Strategies of persuasion, manipulation and propaganda: psychological and social aspects},
	url = {http://staff.science.uva.nl/~mfranke/Papers/Franke-vanRooij\_Opinion-Dynamics.pdf},
	journal = {Staff.Science.Uva.Nl},
	author = {Franke, Michael and Rooij, Robert Van},
	pages = {1--36},
	file = {Franke-vanRooij_Opinion-Dynamics:/home/user/Zotero/storage/2B22FX36/Franke-vanRooij_Opinion-Dynamics.pdf:application/pdf}
}

@article{franke-pragmatic-2013,
	title = {Pragmatic {Back}-and-{Forth} {Reasoning}},
	url = {http://www.sfs.uni-tuebingen.de/~gjaeger/publications/FrankeJaeger-PragmaticBackAndForth.pdf},
	number = {2010},
	journal = {Sfs.Uni-Tuebingen.De},
	author = {Franke, Michael and Jäger, Gerhard},
	year = {2013},
	pages = {1--22},
	file = {FrankeJager_2013_Pragmatic_Back-and-Forth Reasoning:/home/user/Zotero/storage/BQCB285H/FrankeJager_2013_Pragmatic_Back-and-Forth Reasoning.pdf:application/pdf}
}

@article{franke-probabilistic-2011,
	title = {Probabilistic pragmatics , or why {Bayes} ’ rule is probably important for pragmatics},
	author = {Franke, Michael and Gerhard, J},
	year = {2011},
	pages = {1--32},
	file = {FrankeJager_2015_Probabilistic pragmatics, or why Bayes' rule is probably important for pragmatics:/home/user/Zotero/storage/34GB3UWD/FrankeJager_2015_Probabilistic pragmatics, or why Bayes' rule is probably important for pragmatics.pdf:application/pdf}
}

@article{franke-independence-nodate,
	title = {Independence and {Decision}-{Contexts} for {Non}-{Interference}},
	author = {Franke, Michael and Franke, Michael},
	file = {ESSLLI07ProcPaper_Franke:/home/user/Zotero/storage/WIMI6UM7/ESSLLI07ProcPaper_Franke.pdf:application/pdf}
}

@article{franke-bidirectional-2012,
	title = {Bidirectional {Optimization} from {Reasoning} and {Learning} in {Games}},
	volume = {21},
	doi = {10.1007/s10849-011-9151-z},
	abstract = {... Some consider bidirectional optimization as a diachronic, evolutionary force that shapes language use over ... that ot- systems should better be translated into signaling  games ( Lewis 1969), where first ... but given by a probabilistic choice function, the origins and dynamics of which ... \n},
	number = {1},
	journal = {Journal of Logic, Language and Information},
	author = {Franke, Michael and Jäger, Gerhard},
	year = {2012},
	keywords = {Bidirectional optimality theory, game theory, iterated best response, Reinforcement learning, signaling games},
	pages = {117--139}
}

@article{franke-vagueness-nodate,
	title = {Vagueness , {Signaling} \& {Bounded} {Rationality}},
	author = {Franke, Michael and Gerhard, J},
	keywords = {vagueness, signaling games, bounded rational-, categorization, fictitious play, ity, language evolution, quantal response equilibrium},
	file = {post_proc_lenls2010:/home/user/Zotero/storage/Q59AHRW3/post_proc_lenls2010.pdf:application/pdf}
}

@article{franke-quantity-2011,
	title = {Quantity implicatures, exhaustive interpretation, and rational conversation},
	volume = {4},
	url = {http://dx.doi.org/10.3765/sp.4.1},
	doi = {10.3765/sp.4.1},
	abstract = {Quantity implicatures are inferences triggered by an utterance based on what other utterances a speaker could have made instead. Using ideas and formalisms from game theory, I demonstrate that these inferences can be explained in a strictly Gricean sense as *rational behavior*. To this end, I offer a procedure for constructing the context of utterance insofar as it is relevant for quantity reasoning as a game between speaker and hearer. I then give a new solution concept that improves on classical equilibrium approaches in that it uniquely selects the desired "empirically correct" play in these interpretation games by a chain of back-and-forth reasoning about players' behavior. To make this formal approach more accessible to a wider audience, I give a simple algorithm with the help of which the model's solution can be computed without having to do heavy calculations of probabilities, expected utilities and the like. This rationalistic approach subsumes and improves on recent exhaustivity-based approaches. It makes correct and uniform predictions for quantity implicatures of various epistemic varieties, free choice readings of disjunctions, as well as a phenomenon tightly related to the latter, namely so-called "simplification of disjunctive antecedents".\n\ndoi:10.3765/sp.4.1\n\n BibTeX info},
	number = {1},
	journal = {Semantics and Pragmatics},
	author = {Franke, Michael},
	year = {2011},
	keywords = {game theory, exhaustive interpretation, iterated, quantity implicature, quantity implicature, exhaustive interpretation, g},
	pages = {1--82},
	file = {1330-4446-1-PB:/home/user/Zotero/storage/WHKCT343/1330-4446-1-PB.pdf:application/pdf}
}

@article{franke-epistemic-2009,
	title = {An {Epistemic} {Characterization} of {Bidirectional} {Optimality} {Based} on {Signaling} {Games}},
	volume = {51},
	journal = {Papers in Pragmasemantics},
	author = {Franke, Michael},
	year = {2009},
	pages = {111--134},
	file = {Franke_ZAS_ling_paper:/home/user/Zotero/storage/J9HNP57G/Franke_ZAS_ling_paper.pdf:application/pdf}
}

@article{franke-scales-2011,
	title = {Scales, salience and referential safety: the benefit of communicating the extreme},
	author = {Franke, Michael},
	year = {2011},
	file = {extreme:/home/user/Zotero/storage/U5T8AEQH/extreme.pdf:application/pdf}
}

@article{franke-admissibility-2014,
	title = {On admissibility in game theoretic pragmatics},
	volume = {37},
	url = {http://link.springer.com/10.1007/s10988-014-9148-6},
	doi = {10.1007/s10988-014-9148-6},
	number = {3},
	journal = {Linguistics and Philosophy},
	author = {Franke, Michael},
	year = {2014},
	pages = {249--256}
}

@article{franke-;-1986,
	title = {; . 7 8:7},
	author = {Franke, Michael and Franke, Michael},
	year = {1986},
	keywords = {conjunction, discourse relations, disjunction, non-standard uses of, pseudo-imperatives, pseudo-imperatives and a pragmatic, puzzle},
	file = {Attachment:/home/user/Zotero/storage/IVEUXD3I/PIsCCCD_ProofRead1:application/pdf}
}

@article{franke-relevance-2012,
	title = {Relevance in cooperation and conflict},
	volume = {22},
	doi = {10.1093/logcom/exp070},
	abstract = {Linguistic pragmatics assumes that conversation is a by-and-large cooperative endeavour. Although clearly reasonable and helpful, this is an idealization and it pays to ask what happens to natural language interpretation if the presumption of cooperativity is dropped, be that entirely or only to some degree. Game theory suggests itself as a formal tool for modelling the different degrees in which speaker and hearer may or may not have common interests, and it is in this game-theoretic light that this article investigates in particular a notion of speaker-relevance and its impact on the question why we communicate cooperatively in most cases and what happens to pragmatic phenomena such as conversational implicatures if full cooperation cannot be assumed.},
	number = {1},
	journal = {Journal of Logic and Computation},
	author = {Franke, Michael and De Jager, Tikitu and Van Rooij, Robert},
	year = {2012},
	keywords = {Pragmatics, game theory, iterated best response, cooperation, credibility, implicature, relevance, status},
	pages = {23--54},
	file = {J Logic Computation-2012-Franke-23-54:/home/user/Zotero/storage/EPW6MD4V/J Logic Computation-2012-Franke-23-54.pdf:application/pdf}
}

@article{franke-conflicts-nodate,
	title = {Conflicts in {Interpretation} – {A} {Critical} {Review}},
	author = {Franke, Michael and Aloni, Maria},
	pages = {1--9},
	file = {Franke-Aloni_OTReview:/home/user/Zotero/storage/58TGDUM2/Franke-Aloni_OTReview.pdf:application/pdf}
}

@article{franke-pragmatics-2007,
	title = {The {Pragmatics} of {Biscuit} {Conditionals}},
	issn = {9789081264310},
	number = {3},
	journal = {Proceedings of the 16th Amsterdam Colloquium},
	author = {Franke, Michael},
	year = {2007},
	pages = {91--96},
	file = {AC07_Paper:/home/user/Zotero/storage/JQ7N8H76/AC07_Paper.pdf:application/pdf}
}

@article{franke-typical-nodate,
	title = {Typical use of quantifiers: {A} probabilistic speaker model},
	url = {http://staff.science.uva.nl/~mfranke/Papers/Franke\_2014\_Typical use of quantifiers A probabilistic speaker model.pdf},
	journal = {Staff.Science.Uva.Nl},
	author = {Franke, Michael},
	keywords = {ing, bayesian cognitive modeling, game theory, alternatives, gricean reason-, pragmatics of natural language},
	pages = {487--492},
	file = {Franke_2014_Typical use of quantifiers A probabilistic speaker model:/home/user/Zotero/storage/GFEI9SZM/Franke_2014_Typical use of quantifiers A probabilistic speaker model.pdf:application/pdf}
}

@article{franke-adaptationist-nodate,
	title = {An adaptationist criterion for signal meaning},
	author = {Franke, Michael},
	file = {13_Franke:/home/user/Zotero/storage/7UQGMU3Z/13_Franke.pdf:application/pdf}
}

@article{franke-interpretation-2008,
	title = {Interpretation of {Optimal} {Signals}},
	volume = {4},
	journal = {New Perspectives on Games and Interaction},
	author = {Franke, Michael},
	year = {2008},
	pages = {297--310},
	file = {Franke_Interpretation_Optimal_Signals:/home/user/Zotero/storage/UV4BZR59/Franke_Interpretation_Optimal_Signals.pdf:application/pdf}
}

@article{franke-semantic-nodate,
	title = {Semantic {Meaning} \& {Pragmatic} {Inference} in {Non}-{Cooperative} {Conversation} {Semantic} {Meaning} and {Credible} {Information} in},
	number = {m},
	author = {Franke, Michael},
	pages = {1--12},
	file = {Franke10ESSLLI:/home/user/Zotero/storage/9J7G5TAX/Franke10ESSLLI.pdf:application/pdf}
}

@article{franke-pragmatic-2013-1,
	title = {Pragmatic {Reasoning} {About} {Unawareness}},
	doi = {10.1007/s10670-013-9464-1},
	abstract = {Language use and interpretation is heavily contingent on context. But human interlocutors need not always agree what the actual context is. In game theoretic approaches to language use and interpretation, interlocutors’ beliefs about the context are the players’ beliefs about the game that they are playing. Together this entails that we need to consider cases in which interlocutors have different subjective conceptualizations of the game they are in. This paper therefore extends iterated best response reasoning, as an established model for pragmatic reasoning, to games with unawareness. This extension not only leads to more plausible context models for many communicative situations, but also to improved predictions for otherwise problematic cases and an extension of the scope of pragmatic phenomena that can be captured by game theoretic analysis.},
	journal = {Erkenntnis},
	author = {Franke, Michael},
	year = {2013},
	pages = {1--39}
}

@article{franke-admissibility-2014-1,
	title = {On admissibility in game theoretic pragmatics},
	volume = {37},
	url = {http://link.springer.com/10.1007/s10988-014-9148-6},
	doi = {10.1007/s10988-014-9148-6},
	number = {3},
	journal = {Linguistics and Philosophy},
	author = {Franke, Michael},
	year = {2014},
	pages = {249--256}
}

@article{franke-scales-2012,
	title = {On scales, salience and referential language use},
	volume = {7218 LNCS},
	issn = {9783642314810},
	doi = {10.1007/978-3-642-31482-7\_32},
	number = {2007},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Franke, Michael},
	year = {2012},
	keywords = {gradable adjectives, game theory, salience, scale topology},
	pages = {311--320},
	file = {AC-post_proc-2011:/home/user/Zotero/storage/W5QGIN7X/AC-post_proc-2011.pdf:application/pdf}
}

@article{franke-free-2010,
	title = {Free choice from iterated best response},
	volume = {6042 LNAI},
	issn = {3642142869},
	doi = {10.1007/978-3-642-14287-1\_30},
	number = {2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Franke, Michael},
	year = {2010},
	keywords = {iterated best response, conversational implicatures, free choice disjunction, game theoretic pragmatics},
	pages = {295--304},
	file = {Franke_AC09_post-paper:/home/user/Zotero/storage/H4J79UZE/Franke_AC09_post-paper.pdf:application/pdf}
}

@inproceedings{franke-typical-2014,
	title = {Typical use of quantifiers: {A} probabilistic speaker model},
	url = {http://staff.science.uva.nl/~mfranke/Papers/Franke\_2014\_Typical use of quantifiers A probabilistic speaker model.pdf},
	booktitle = {{CogSci}},
	author = {Franke, Michael},
	year = {2014},
	keywords = {ing, bayesian cognitive modeling, game theory, alternatives, gricean reason-, pragmatics of natural language},
	pages = {487--492},
	file = {paper093:/home/user/Zotero/storage/7BCDEFJF/paper093.pdf:application/pdf}
}

@article{franke-signal-2010,
	title = {Signal to {Act}},
	author = {Franke, Michael},
	year = {2010},
	file = {Franke_PhD_thesis:/home/user/Zotero/storage/RMGRZRMJ/Franke_PhD_thesis.pdf:application/pdf}
}

@article{franke-m-1989,
	title = {M \& i c c},
	number = {m},
	author = {Franke, Michael},
	year = {1989},
	pages = {1--10},
	file = {ESSLLI_ Franke_Credibility_V06:/home/user/Zotero/storage/P6NXWZRS/ESSLLI_ Franke_Credibility_V06.pdf:application/pdf}
}

@article{franke-how-2005,
	title = {How and how not to employ discourse relations to account for pseudo-imperatives},
	issn = {9057761467},
	url = {http://odur.let.rug.nl/egg/Papiere/ac05.pdf#page=93},
	number = {2},
	journal = {Fifteenth Amsterdam Colloquium},
	author = {Franke, M},
	year = {2005},
	file = {DRsPIs-AC05-Paper:/home/user/Zotero/storage/J8ZZ2U45/DRsPIs-AC05-Paper.pdf:application/pdf}
}

@article{frank-adding-2013-1,
	title = {Adding {Sentence} {Types} to a {Model} of {Syntactic} {Category} {Acquisition}},
	volume = {5},
	doi = {10.1111/tops.12030},
	abstract = {The acquisition of syntactic categories is a crucial step in the process of acquiring syntax. At this stage, before a full grammar is available, only surface cues are available to the learner. Previous computational models have demonstrated that local contexts are informative for syntactic categorization. However, local contexts are affected by sentence-level structure. In this paper, we add sentence type as an observed feature to a model of syntactic category acquisition, based on experimental evidence showing that pre-syntactic children are able to distinguish sentence type using prosody and other cues. The model, a Bayesian Hidden Markov Model, allows for adding sentence type in a few different ways; we find that sentence type can aid syntactic category acquisition if it is used to characterize the differences in word order between sentence types. In these models, knowledge of sentence type permits similar gains to those found by extending the local context.},
	journal = {Topics in Cognitive Science},
	author = {Frank, Stella and Goldwater, Sharon and Keller, Frank},
	year = {2013},
	keywords = {Bayesian modeling, Sentence prosody, Syntactic categories, Language acquisition, Computational modeling},
	pages = {495--521},
	file = {Frank, Goldwater, Keller - 2013 - Adding Sentence Types to a Model of Syntactic Category Acquisition:/home/user/Zotero/storage/EB83F3AC/Frank, Goldwater, Keller - 2013 - Adding Sentence Types to a Model of Syntactic Category Acquisition.pdf:application/pdf}
}

@article{frank-representing-2012,
	title = {Representing exact number visually using mental abacus.},
	volume = {141},
	issn = {0096-3445},
	doi = {10.1037/a0024427},
	abstract = {Mental abacus (MA) is a system for performing rapid and precise arithmetic by manipulating a mental representation of an abacus, a physical calculation device. Previous work has speculated that MA is based on visual imagery, suggesting that it might be a method of representing exact number nonlinguistically, but given the limitations on visual working memory, it is unknown how MA structures could be stored. We investigated the structure of the representations underlying MA in a group of children in India. Our results suggest that MA is represented in visual working memory by splitting the abacus into a series of columns, each of which is independently stored as a unit with its own detailed substructure. In addition, we show that the computations of practiced MA users (but not those of control participants) are relatively insensitive to verbal interference, consistent with the hypothesis that MA is a nonlinguistic format for exact numerical computation.},
	journal = {Journal of Experimental Psychology: General},
	author = {Frank, Michael C. and Barner, David},
	year = {2012},
	keywords = {abacus, although other creatures are, between small quantities and, can, form exact numerical computations, have the capacity to, human adults, mental arithmetic, number, per-, sensitive to precise differences, unlike other animals, visual cognition},
	pages = {134--149},
	file = {FB-jepg2011:/home/user/Zotero/storage/644BGW7D/FB-jepg2011.pdf:application/pdf;Frank, Barner - 2012 - Representing exact number visually using mental abacus:/home/user/Zotero/storage/CRTJUKET/Frank, Barner - 2012 - Representing exact number visually using mental abacus.pdf:application/pdf}
}

@article{frank-inferring-2009,
	title = {Inferring word meanings by assuming that speakers are informative},
	volume = {75},
	url = {http://langcog.stanford.edu/papers/FG-underreview.pdf},
	doi = {10.1016/j.cogpsych.2014.08.002},
	journal = {Langcog.Stanford.Edu},
	author = {Frank, Mc and Goodman, Nd},
	year = {2009},
	keywords = {Language acquisition},
	pages = {1--26},
	file = {FG-cogpsych2014:/home/user/Zotero/storage/V2DZRMTC/FG-cogpsych2014.pdf:application/pdf;Frank, Goodman - 2009 - Inferring word meanings by assuming that speakers are informative(2):/home/user/Zotero/storage/CVJRZ562/Frank, Goodman - 2009 - Inferring word meanings by assuming that speakers are informative(2).pdf:application/pdf}
}

@article{frank-inferring-2009-1,
	title = {Inferring word meanings by assuming that speakers are informative},
	volume = {75},
	url = {http://langcog.stanford.edu/papers/FG-underreview.pdf},
	doi = {10.1016/j.cogpsych.2014.08.002},
	journal = {Langcog.Stanford.Edu},
	author = {Frank, Mc and Goodman, Nd},
	year = {2009},
	keywords = {Language acquisition},
	pages = {1--26},
	file = {Frank, Goodman - 2009 - Inferring word meanings by assuming that speakers are informative:/home/user/Zotero/storage/ZC7U29UW/Frank, Goodman - 2009 - Inferring word meanings by assuming that speakers are informative.pdf:application/pdf}
}

@article{franke-evolution-nodate,
	title = {The evolution of compositionality and proto-syntax in signaling games},
	author = {Franke, Michael},
	pages = {1--22},
	file = {Franke_2014_The Evolution of Compositionality and Proto-Syntax in Signaling Games:/home/user/Zotero/storage/XBF3B729/Franke_2014_The Evolution of Compositionality and Proto-Syntax in Signaling Games.pdf:application/pdf}
}

@article{franke-teleological-2006,
	title = {Teleological necessity and only},
	journal = {Proceedings of the ESSLLI Student Session},
	author = {Franke, Michael},
	year = {2006},
	pages = {14--26},
	file = {TelNecOnly:/home/user/Zotero/storage/S6VXCQ6P/TelNecOnly.pdf:application/pdf}
}

@article{frank-predicting-2012,
	title = {Predicting {Pragmatic} {Reasoning} in {Language} {Games}},
	volume = {336},
	issn = {978-0-12-373679-6},
	doi = {10.1126/science.1218633},
	abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers' intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.},
	journal = {Science},
	author = {Frank, M. C. and Goodman, N. D.},
	year = {2012},
	pages = {998--998},
	file = {FG-science2012full:/home/user/Zotero/storage/SMIMC4VR/FG-science2012full.pdf:application/pdf;Frank, Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games:/home/user/Zotero/storage/HMVZAP6J/Frank, Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games.pdf:application/pdf}
}

@article{francez-chimerical-2015,
	title = {Chimerical conditionals},
	volume = {8},
	url = {http://semprag.org/article/view/sp.8.2},
	doi = {10.3765/sp.8.2},
	number = {2},
	journal = {Semantics and Pragmatics},
	author = {Francez, Itamar},
	year = {2015},
	keywords = {Pragmatics, and katrin schultz on, biscuit conditionals, brian leahey, careful and insightful criticism, conditional dependence, conditionals, conditionals, definiteness, presupposition, condit, definiteness, familiarity, for extensive discussion, i thank cleo condoravdi, im-, kyle rawlins, of michael franke, plicit arguments, presupposition, previous drafts have, the sharp},
	pages = {1--35},
	file = {2964-6560-1-PB:/home/user/Zotero/storage/U5AUB3V3/2964-6560-1-PB.pdf:application/pdf}
}

@article{fox-type-theoretic-2014,
	title = {Type-theoretic logic with an operational account of intensionality},
	issn = {1122901303901},
	doi = {10.1007/s11229-013-0390-1},
	number = {December 2012},
	journal = {Synthese},
	author = {Fox, Chris and Lappin, Shalom},
	year = {2014},
	keywords = {Semantics, Intensionality, Logic, Operational interpretation, Type theory},
	pages = {1--22},
	file = {fox-lappin_ms_synthese14:/home/user/Zotero/storage/RS3WJ5ZJ/fox-lappin_ms_synthese14.pdf:application/pdf}
}

@article{ferguson-eye-2011,
	title = {Eye movements reveal the time-course of anticipating behaviour based on complex, conflicting desires},
	volume = {119},
	issn = {0010-0277},
	url = {http://dx.doi.org/10.1016/j.cognition.2011.01.005},
	doi = {10.1016/j.cognition.2011.01.005},
	abstract = {The time-course of representing others' perspectives is inconclusive across the currently available models of ToM processing. We report two visual-world studies investigating how knowledge about a character's basic preferences (e.g. Tom's favourite colour is pink) and higher-order desires (his wish to keep this preference secret) compete to influence online expectations about subsequent behaviour. Participants' eye movements around a visual scene were tracked while they listened to auditory narratives. While clear differences in anticipatory visual biases emerged between conditions in Experiment 1, post-hoc analyses testing the strength of the relevant biases suggested a discrepancy in the time-course of predicting appropriate referents within the different contexts. Specifically, predictions to the target emerged very early when there was no conflict between the character's basic preferences and higher-order desires, but appeared to be relatively delayed when comprehenders were provided with conflicting information about that character's desire to keep a secret. However, a second experiment demonstrated that this apparent 'cognitive cost' in inferring behaviour based on higher-order desires was in fact driven by low-level features between the context sentence and visual scene. Taken together, these results suggest that healthy adults are able to make complex higher-order ToM inferences without the need to call on costly cognitive processes. Results are discussed relative to previous accounts of ToM and language processing. © 2011 Elsevier B.V.},
	journal = {Cognition},
	author = {Ferguson, Heather J. and Breheny, Richard},
	year = {2011},
	keywords = {Eye movements, Discourse processing, Theory of Mind, Visual-world paradigm},
	pages = {179--196},
	file = {secrets:/home/user/Zotero/storage/MEUAWQ3D/secrets.pdf:application/pdf}
}

@article{elman-alternative-2004,
	title = {An alternative view of the mental lexicon},
	volume = {8},
	issn = {1364-6613},
	doi = {10.1016/j.tics.2004.05.003},
	abstract = {An essential aspect of knowing language is knowing the words of that language. This knowledge is usually thought to reside in the mental lexicon, a kind of dictionary that contains information regarding a word's meaning, pronunciation, syntactic characteristics, and so on. In this article, a very different view is presented. In this view, words are understood as stimuli that operate directly on mental states. The phonological, syntactic and semantic properties of a word are revealed by the effects it has on those states.},
	number = {7},
	journal = {Trends in Cognitive Sciences},
	author = {Elman, Jeffrey L.},
	year = {2004},
	pages = {301--306},
	file = {elman_tics_opinion_2004(1):/home/user/Zotero/storage/4V922AVA/elman_tics_opinion_2004(1).pdf:application/pdf}
}

@article{elman-alternative-2004-1,
	title = {An alternative view of the mental lexicon},
	volume = {8},
	issn = {1364-6613},
	doi = {10.1016/j.tics.2004.05.003},
	abstract = {An essential aspect of knowing language is knowing the words of that language. This knowledge is usually thought to reside in the mental lexicon, a kind of dictionary that contains information regarding a word's meaning, pronunciation, syntactic characteristics, and so on. In this article, a very different view is presented. In this view, words are understood as stimuli that operate directly on mental states. The phonological, syntactic and semantic properties of a word are revealed by the effects it has on those states.},
	number = {7},
	journal = {Trends in Cognitive Sciences},
	author = {Elman, Jeffrey L.},
	year = {2004},
	pages = {301--306},
	file = {elman_tics_opinion_2004(1):/home/user/Zotero/storage/VA6DW9JI/elman_tics_opinion_2004(1).pdf:application/pdf}
}

@article{duarte-self-organized-2014,
	title = {Self-{Organized} {Artificial} {Grammar} {Learning} in {Spiking} {Neural} {Networks}},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
	author = {Duarte, Renato and Seriès, Peggy and Morrison, Abigail},
	year = {2014},
	keywords = {artificial grammar learning, cesses and assessing the, plasticity, plex computational processes to, properties of the neuronal, self-organization, sequence learning, system re-, the underlying neuronal pro-},
	pages = {427--432},
	file = {paper083:/home/user/Zotero/storage/PAR2WQQG/paper083.pdf:application/pdf}
}

@article{ernestus-corpora-2011,
	title = {Corpora and {Exemplars} in {Phonology}},
	issn = {9781405157681},
	doi = {10.1002/9781444343069.ch12},
	abstract = {This chapter reviews the role of corpora in phonological research, as well as the role of exemplars in phonological theory. We begin with illustrating the importance of corpora for phonological research as a source of data. We then present an overview of speech corpora, and discuss the kinds of problems that arise when corpus data have to be transcribed and analyzed. The enormous variability in the speech signal that emerges from speech corpora, in combination with current experimental evidence, calls for more sophisticated theories of phonology than those developed in the early days of generative grammar. The importance of exemplars for accurate phonological generalization is discussed in detail, as well as the characteristics of and challenges to several types of abstractionist, exemplar, and hybrid models.},
	journal = {The Handbook of Phonological Theory: Second Edition},
	author = {Ernestus, Mirjam and Baayen, R. Harald},
	year = {2011},
	keywords = {Abstractionist, and exemplar-based models - corpus, Corpora and exemplars in phonology, Corpora for phonology - getting the facts right, Dilley and Pitt (2007), and regressive place assim, Hybrid models - abstract representations, or exemp, Pronunciation of homophones, time and thyme - homo, Reduction, and information - phonological structur, Regressive voice assimilation - in Dutch, Role of discourse - and pragmatics in grammar of p, Speech corpora, recent data source - compared to c},
	pages = {374--400},
	file = {ErnestusBaayenPhonHandbook2011:/home/user/Zotero/storage/TG25SZQC/ErnestusBaayenPhonHandbook2011.pdf:application/pdf}
}

@article{degen-wonky-nodate,
	title = {Wonky worlds : {Listeners} revise world knowledge when utterances are odd},
	number = {2},
	author = {Degen, Judith and Tessler, Michael Henry and Goodman, Noah D},
	keywords = {experimental pragmatics, scalar implicature, computational pragmatics, how often do you, if not always, in water, liefs, now imagine read-, prior be-, probably extremely often, think marbles would sink, world knowledge},
	file = {DegenTesslerGoodman2015:/home/user/Zotero/storage/SNHE6ACP/DegenTesslerGoodman2015.pdf:application/pdf}
}

@article{degen-lost-2014,
	title = {Lost your marbles? {The} puzzle of dependent measures in experimental pragmatics},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
	author = {Degen, Judith and Goodman, Noah D},
	year = {2014},
	keywords = {scalar implicature, psycholinguistics, Pragmatics},
	pages = {397--402},
	file = {paper078:/home/user/Zotero/storage/8RSNQHEB/paper078.pdf:application/pdf}
}

@article{degen-lost-2014-1,
	title = {Lost your marbles? {The} puzzle of dependent measures in experimental pragmatics},
	journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
	author = {Degen, Judith and Goodman, Noah D},
	year = {2014},
	keywords = {scalar implicature, psycholinguistics, Pragmatics},
	file = {DegenGoodman2014:/home/user/Zotero/storage/A3DWP72S/DegenGoodman2014.pdf:application/pdf}
}

@article{demberg-incremental-2013,
	title = {Incremental , {Predictive} {Parsing} with {Psycholinguistically} {Motivated}, {Tree}-{Adjoining} {Grammar}},
	volume = {39},
	issn = {9781608459858},
	doi = {10.1162/COLI},
	abstract = {Psycholinguistic research shows that key properties of the human sentence processor are incrementality, connectedness (partial structures contain no unattached nodes), and prediction (upcoming syntactic structure is anticipated). There is currently no broad-coverage parsing model with these properties, however. In this article, we present the first broad-coverage probabilistic parser for PLTAG, a variant of TAG that supports all three requirements. We train our parser on a TAG-transformed version of the Penn Treebank and show that it achieves performance comparable to existing TAG parsers that are incremental but not predictive. We also use our PLTAG model to predict human reading times, demonstrating a better fit on the Dundee eye-tracking corpus than a standard surprisal model.},
	number = {January},
	journal = {Computational Linguistics},
	author = {Demberg, Vera and Keller, Frank and Koller, Alexander},
	year = {2013},
	pages = {1025--1066},
	file = {Demberg, Keller, Koller - 2013 - Incremental , Predictive Parsing with Psycholinguistically Motivated, Tree-Adjoining Grammar:/home/user/Zotero/storage/9ZIPSCEE/Demberg, Keller, Koller - 2013 - Incremental , Predictive Parsing with Psycholinguistically Motivated, Tree-Adjoining Grammar.pdf:application/pdf}
}

@article{dekker-bi-directional-2000,
	title = {Bi-{Directional} {Optimality} {Theory} : {An} {Application} of {Game} {Theory}},
	doi = {10.1093/jos/17.3.217},
	author = {Dekker, Paul and Rooy, Robert V a N},
	year = {2000},
	pages = {217--242},
	file = {dekker-vanRooy_OT-GT:/home/user/Zotero/storage/N7JPI7FD/dekker-vanRooy_OT-GT.pdf:application/pdf}
}

@article{degen-processing-2014,
	title = {Processing {Scalar} {Implicature}: {A} {Constraint}-{Based} {Approach}},
	url = {http://doi.wiley.com/10.1111/cogs.12171},
	doi = {10.1111/cogs.12171},
	journal = {Cognitive Science},
	author = {Degen, Judith and Tanenhaus, Michael K.},
	year = {2014},
	keywords = {scalar implicature, quantifiers, Pragmatics, alternatives},
	pages = {n/a--n/a},
	file = {DegenTanenhaus2014:/home/user/Zotero/storage/XXZFX7R7/DegenTanenhaus2014.pdf:application/pdf}
}

@article{degen-cost-based-2013,
	title = {Cost-{Based} {Pragmatic} {Inference} about {Referential} {Expressions}},
	abstract = {We present data from three experiments addressing how much theory of mind reasoning is involved in production and inter- pretation of ambiguous referential expressions in an artificial language task, and how this interacts with the cost and avail- ability of alternative utterances. When an unambiguous alter- native is not available, listeners tend to draw simple Quantity inferences reminiscent of scalar implicatures (Grice, 1975). When an unambiguous alternative is available, fewer infer- ences are observed, but gradiently more as the cost of unam- biguous alternatives increase. We outline a novel game the- oretic model of pragmatic reasoning based on probabilistic back-and-forth reasoning about interlocutors’ rational choices and beliefs. The model provides a good fit to the data and raises interesting issues for future research.},
	journal = {Proceedings of the 35th Annual Conference of the Cognitive Science Society (CogSci'13)},
	author = {Degen, Judith and Franke, Michael and Jäger, Gerhard},
	year = {2013},
	keywords = {bcs, dept, edu, jdegen, judith degen, of brain and cognitive, referential expressions, rochester, sciences, t-based pragmatic inference about, university of rochester},
	pages = {376--281},
	file = {DegenFrankeJaeger2013:/home/user/Zotero/storage/NDIT2PXW/DegenFrankeJaeger2013.pdf:application/pdf}
}

@article{de-cat-representational-2015,
	title = {Representational deficit or processing effect? {An} electrophysiological study of noun-noun compound processing by very advanced {L}2 speakers of {English}},
	volume = {6},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.00077/abstract},
	doi = {10.3389/fpsyg.2015.00077},
	number = {February},
	journal = {Frontiers in Psychology},
	author = {De Cat, Cecile and Klepousniotou, Ekaterini and Baayen, R. Harald},
	year = {2015},
	keywords = {word order, erp, compounds, compounds, second language, word order, ERP, frequ, frequency effects, generalized additive mixed models, second language},
	pages = {1--17},
	file = {fpsyg-06-00077:/home/user/Zotero/storage/ZB5T8PVF/fpsyg-06-00077.pdf:application/pdf}
}

@article{degen-cost-based-2013-1,
	title = {Cost-{Based} {Pragmatic} {Inference} about {Referential} {Expressions}},
	abstract = {We present data from three experiments addressing how much theory of mind reasoning is involved in production and inter- pretation of ambiguous referential expressions in an artificial language task, and how this interacts with the cost and avail- ability of alternative utterances. When an unambiguous alter- native is not available, listeners tend to draw simple Quantity inferences reminiscent of scalar implicatures (Grice, 1975). When an unambiguous alternative is available, fewer infer- ences are observed, but gradiently more as the cost of unam- biguous alternatives increase. We outline a novel game the- oretic model of pragmatic reasoning based on probabilistic back-and-forth reasoning about interlocutors’ rational choices and beliefs. The model provides a good fit to the data and raises interesting issues for future research.},
	journal = {Proceedings of the 35th Annual Conference of the Cognitive Science Society (CogSci'13)},
	author = {Degen, Judith and Franke, Michael and Jäger, Gerhard},
	year = {2013},
	keywords = {bcs, dept, edu, jdegen, judith degen, of brain and cognitive, referential expressions, rochester, sciences, t-based pragmatic inference about, university of rochester},
	pages = {376--281},
	file = {DegenFranke_2013_Cost-Based-Inference:/home/user/Zotero/storage/5CBC3SSR/DegenFranke_2013_Cost-Based-Inference.pdf:application/pdf}
}

@article{degen-optimal-2012,
	title = {Optimal {Reasoning} {About} {Referential} {Expressions}},
	journal = {Proceedings of The 16th Workshop on the Semantics and Pragmatics of Dialogue},
	author = {Degen, Judith and Franke, Michael},
	year = {2012},
	keywords = {expressions, imal reasoning about referential},
	pages = {2--11},
	file = {DegenFranke-semdial2012:/home/user/Zotero/storage/P7FPEIIQ/DegenFranke-semdial2012.pdf:application/pdf}
}

@article{degen-investigating-2015-1,
	title = {Investigating the distribution of 'some' (but not 'all') implicatures using corpora and web-based methods},
	volume = {8},
	url = {http://dx.doi.org/10.3765/sp.8.11},
	doi = {10.3765/sp.8.11},
	number = {11},
	journal = {Semantics and Pragmatics},
	author = {Degen, Judith},
	year = {2015},
	keywords = {corpora, experimental pragmatics, GCI, scalar implicature, and mike tanenhaus, anton benz, as well as, florian jaeger, greatly from discussion with, including, lab and the zas, many wonderful colleagues, members of the tanenhaus, michael franke, parts of, polly jacobson, semantikzirkel, this paper has benefited, to whom i presented},
	pages = {1--55},
	file = {3022-6702-1-PB:/home/user/Zotero/storage/7NXJNWMA/3022-6702-1-PB.pdf:application/pdf}
}

@article{cole-grammar-2014,
	title = {The {Grammar} of {Binding} in the {Languages} of the {World}: {Innate} or {Learned}?},
	volume = {141},
	url = {http://dx.doi.org/10.1016/j.cognition.2015.04.005},
	doi = {10.1016/j.cognition.2015.04.005},
	journal = {R\&R for Cognition},
	author = {Cole, Peter},
	year = {2014},
	keywords = {binding, syntax, abstract, also be great, anaphora, and in, at the same time, but, each other without limit, indonesian, innate, joos, languages around the world, languages can differ from, learned, nearly identical grammatical, often appear to manifest, properties, s claim that, seeming to support martin, sometimes even, the grammatical differences can},
	pages = {138--160},
	file = {1-s2.0-S0010027715000761-main(1):/home/user/Zotero/storage/IJMD4NG9/1-s2.0-S0010027715000761-main(1).pdf:application/pdf}
}

@article{coco-integrating-nodate,
	title = {Integrating {Mechanisms} of {Visual} {Guidance} in {Naturalistic} {Language} {Production}},
	author = {Coco, Moreno I and Keller, Frank},
	keywords = {language production, cross-, eye-movements, eye-voice span, modal processing, scene understanding, structural guidance},
	file = {Coco, Keller - Unknown - Integrating Mechanisms of Visual Guidance in Naturalistic Language Production:/home/user/Zotero/storage/DAQW52SC/Coco, Keller - Unknown - Integrating Mechanisms of Visual Guidance in Naturalistic Language Production.pdf:application/pdf}
}

@article{coco-interaction-nodate,
	title = {The {Interaction} of {Visual} and {Linguistic} {Saliency} during {Syntactic} {Ambiguity} {Resolution}},
	author = {Coco, Moreno I and Keller, Frank},
	keywords = {adaptive constraint-based architecture, ambiguity resolution, intonational breaks, situated language comprehension, Visual saliency},
	pages = {46--74},
	file = {Coco, Keller - Unknown - The Interaction of Visual and Linguistic Saliency during Syntactic Ambiguity Resolution:/home/user/Zotero/storage/TDPS8ZFW/Coco, Keller - Unknown - The Interaction of Visual and Linguistic Saliency during Syntactic Ambiguity Resolution.pdf:application/pdf}
}

@article{cole-grammar-2014-1,
	title = {The {Grammar} of {Binding} in the {Languages} of the {World}: {Innate} or {Learned}?},
	volume = {141},
	url = {http://dx.doi.org/10.1016/j.cognition.2015.04.005},
	doi = {10.1016/j.cognition.2015.04.005},
	journal = {R\&R for Cognition},
	author = {Cole, Peter},
	year = {2014},
	keywords = {binding, syntax, abstract, also be great, anaphora, and in, at the same time, but, each other without limit, indonesian, innate, joos, languages around the world, languages can differ from, learned, nearly identical grammatical, often appear to manifest, properties, s claim that, seeming to support martin, sometimes even, the grammatical differences can},
	pages = {138--160},
	file = {1-s2.0-S0010027715000761-main(1):/home/user/Zotero/storage/SBXUDG8W/1-s2.0-S0010027715000761-main(1).pdf:application/pdf}
}

@article{coco-no-2014,
	title = {No {Title}},
	number = {Experiment 1},
	author = {Coco, Moreno I and Malcolm, George L and Keller, Frank},
	year = {2014},
	pages = {1096--1120},
	file = {Coco, Malcolm, Keller - 2014 - No Title:/home/user/Zotero/storage/TV9UB74I/Coco, Malcolm, Keller - 2014 - No Title.pdf:application/pdf}
}

@article{cocho-rank-2015,
	title = {Rank {Diversity} of {Languages}: {Generic} {Behavior} in {Computational} {Linguistics}},
	volume = {10},
	url = {http://dx.plos.org/10.1371/journal.pone.0121898},
	doi = {10.1371/journal.pone.0121898},
	number = {4},
	journal = {PLOS ONE},
	author = {Cocho, Germinal and Flores, Jorge and Gershenson, Carlos and Pineda, Carlos and Sánchez, Sergio},
	year = {2015},
	pages = {e0121898--e0121898},
	file = {journal.pone.0121898:/home/user/Zotero/storage/72VT29PQ/journal.pone.0121898.pdf:application/pdf}
}

@article{coco-classification-2014,
	title = {Classification of visual and linguistic tasks using eye-movement features},
	volume = {14},
	doi = {10.1167/14.3.11.doi},
	journal = {Journal of Vision},
	author = {Coco, Moreno I and Keller, Frank},
	year = {2014},
	keywords = {visual attention, active vision, com-, eye-movement features, task classification},
	pages = {1--18},
	file = {Coco, Keller - 2014 - Classification of visual and linguistic tasks using eye-movement features:/home/user/Zotero/storage/PHSTPQC6/Coco, Keller - 2014 - Classification of visual and linguistic tasks using eye-movement features.pdf:application/pdf}
}

@article{clarke-impact-2013,
	title = {The impact of attentional, linguistic, and visual features during object naming},
	volume = {4},
	doi = {10.3389/fpsyg.2013.00927},
	abstract = {Object detection and identification are fundamental to human vision, and there is mounting evidence that objects guide the allocation of visual attention. However, the role of objects in tasks involving multiple modalities is less clear. To address this question, we investigate object naming, a task in which participants have to verbally identify objects they see in photorealistic scenes. We report an eye-tracking study that investigates which features (attentional, visual, and linguistic) influence object naming. We find that the amount of visual attention directed toward an object, its position and saliency, along with linguistic factors such as word frequency, animacy, and semantic proximity, significantly influence whether the object will be named or not. We then ask how features from different modalities are combined during naming, and find significant interactions between saliency and position, saliency and linguistic features, and attention and position. We conclude that when the cognitive system performs tasks such as object naming, it uses input from one modality to constraint or enhance the processing of other modalities, rather than processing each input modality independently.},
	journal = {Frontiers in Psychology},
	author = {Clarke, Alasdair D F and Coco, Moreno I. and Keller, Frank},
	year = {2013},
	keywords = {Eye movements, Visual saliency, naming, Object perception, Overt attention, Scene perception},
	pages = {1--12},
	file = {Clarke, Coco, Keller - 2013 - The impact of attentional, linguistic, and visual features during object naming:/home/user/Zotero/storage/EFHJ4D5C/Clarke, Coco, Keller - 2013 - The impact of attentional, linguistic, and visual features during object naming.pdf:application/pdf}
}

@article{clark-complexity-2013,
	title = {Complexity in {Language} {Acquisition}},
	volume = {5},
	doi = {10.1111/tops.12001},
	abstract = {Learning theory has frequently been applied to language acquisition, but discussion has largely focused on information theoretic problems-in particular on the absence of direct negative evidence. Such arguments typically neglect the probabilistic nature of cognition and learning in general. We argue first that these arguments, and analyses based on them, suffer from a major flaw: they systematically conflate the hypothesis class and the learnable concept class. As a result, they do not allow one to draw significant conclusions about the learner. Second, we claim that the real problem for language learning is the computational complexity of constructing a hypothesis from input data. Studying this problem allows for a more direct approach to the object of study--the language acquisition device-rather than the learnable class of languages, which is epiphenomenal and possibly hard to characterize. The learnability results informed by complexity studies are much more insightful. They strongly suggest that target grammars need to be objective, in the sense that the primitive elements of these grammars are based on objectively definable properties of the language itself. These considerations support the view that language acquisition proceeds primarily through data-driven learning of some form.},
	journal = {Topics in Cognitive Science},
	author = {Clark, Alexander and Lappin, Shalom},
	year = {2013},
	keywords = {Language acquisition, Computational complexity, Computational learning theory},
	pages = {89--110},
	file = {clark-lappin_tics13_proofs:/home/user/Zotero/storage/VJ4ER7FN/clark-lappin_tics13_proofs.pdf:application/pdf}
}

@article{clark-computational-2011,
	title = {Computational {Learning} {Theory} and {Language} {Acquisition}},
	volume = {14},
	number = {June},
	author = {Clark, Alexander and Lappin, Shalom},
	year = {2011},
	pages = {441--471},
	file = {clark-lappin_phil_ling_handbook_chapter:/home/user/Zotero/storage/89Q8G4QJ/clark-lappin_phil_ling_handbook_chapter.pdf:application/pdf}
}

@article{clark-unsupervised-2010,
	title = {Unsupervised learning and grammar induction},
	url = {http://books.google.com/books?hl=en&lr=&id=mv\_FfXjzQ7gC&oi=fnd&pg=PA197&dq=Unsupervised+Learning+and+Grammar+Induction&ots=huXltcRILr&sig=yWcCQX5IwkJAcWfZbSZzzvaD5ss\nhttp://books.google.com/books?hl=en&lr=&id=mv\_FfXjzQ7gC&oi=fnd&pg=PA197&dq=Unsupervised+},
	number = {April},
	journal = {The Handbook of Computational Linguistics and Natural Language Processing},
	author = {Clark, Alex and Lappin, Shalom},
	year = {2010},
	pages = {1--32},
	file = {clark-lappin_handbook_chapter:/home/user/Zotero/storage/4AGXXQE5/clark-lappin_handbook_chapter.pdf:application/pdf}
}

@article{chrusch-tentative-2014,
	title = {A {Tentative} {Role} for {FOXP}2 in the {Evolution} of {Dual} {Processing} {Modes} and {Generative} {Abilities} {The} {Cultural} {Explosion} of the {Middle}-{Upper} {Paleolithic}},
	author = {Chrusch, Courtney and Gabora, Liane},
	year = {2014},
	keywords = {language evolution, associative thought, contextual focus, creativity, divergent thought, dual process, foxp2, human evolution, neural basis of language, paleolithic, processing theories and generative, review literature on dual},
	pages = {499--504},
	file = {paper095:/home/user/Zotero/storage/MM98A86N/paper095.pdf:application/pdf}
}

@article{chesley-predicting-2010,
	title = {Predicting new words from newer words: {Lexical} borrowings in {French}},
	volume = {48},
	issn = {0024-3949},
	doi = {10.1515/LING.2010.043},
	abstract = {This study addresses entrenchment into the lexicon of lexical borrowings. We search for all new lexical borrowings in a corpus of French newspaper texts and examine the frequency with which these borrowings reoccur in a second corpus of newspaper texts from about 10 years later. Lexical entrenchment emerges as depending on a variety of factors, including length in syllables, the original language of the borrowing, and also semantic and contextual factors. The dispersion of a word in the early corpus is found to be a better predictor of its frequency in the later corpus than its frequency, but both measures contribute to predicting the degree of entrenchment of a lexical item. The interaction between these two variables implies that borrowings are penalized for their burstiness},
	journal = {Linguistics},
	author = {Chesley, Paula and Baayen, R. Harald},
	year = {2010},
	pages = {1343--1374},
	file = {ChesleyBaayenLinguistics2010:/home/user/Zotero/storage/G4FC3PV6/ChesleyBaayenLinguistics2010.pdf:application/pdf}
}

@article{cat-electrophysiological-2014,
	title = {Electrophysiological correlates of noun-noun compound processing by non-native speakers of {English}},
	number = {2},
	author = {Cat, Cecile De and Klepousniotou, Ekaterini and Baayen, Harald},
	year = {2014},
	pages = {41--52},
	file = {DeCatKlepousniotouBaayenEACL2014:/home/user/Zotero/storage/E23S5JXK/DeCatKlepousniotouBaayenEACL2014.pdf:application/pdf}
}

@article{casselton-12-2006,
	title = {1,2 2 1},
	volume = {1},
	journal = {Differentiation},
	author = {Casselton, Lorna a and Casselton, Lorna a and Challen, Mike P and Challen, Mike P},
	year = {2006},
	pages = {352--368},
	file = {BlomBaayen2012:/home/user/Zotero/storage/MI45UV9K/BlomBaayen2012.pdf:application/pdf}
}

@article{brouwer-modeling-2010,
	title = {Modeling the {Noun} {Phrase} versus {Sentence} {Coordination} {Ambiguity} in {Dutch} : {Evidence} from {Surprisal} {Theory}},
	number = {July},
	journal = {Computational Linguistics},
	author = {Brouwer, Harm and Fitz, Hartmut and Hoeks, John C J},
	year = {2010},
	pages = {72--80},
	file = {Brouwer2010ModelingNoun:/home/user/Zotero/storage/8FDV829P/Brouwer2010ModelingNoun.pdf:application/pdf}
}

@article{bott-distinguishing-2012,
	title = {Distinguishing speed from accuracy in scalar implicatures},
	volume = {66},
	url = {http://dx.doi.org/10.1016/j.jml.2011.09.005},
	doi = {10.1016/j.jml.2011.09.005},
	abstract = {Scalar implicatures are inferences that arise when a weak expression is used instead of a stronger alternative. For example, when a speaker says, " Some of the children are in the classroom," she often implies that not all of them are. Recent processing studies of scalar implicatures have argued that generating an implicature carries a cost. In this study we investigated this cost using a sentence verification task similar to that of Bott and Noveck (2004) combined with a response deadline procedure to estimate speed and accuracy independently. Experiment 1 compared implicit upper-bound interpretations (some [but not all]) with lower-bound interpretations (some [and possibly all]). Experiment 2 compared an implicit upper-bound meaning of some with the explicit upper-bound meaning of only some. Experiment 3 compared an implicit lower-bound meaning of some with the explicit lower-bound meaning of at least some. Sentences with implicatures required additional processing time that could not be attributed to retrieval probabilities or factors relating to semantic complexity. Our results provide evidence against several different types of processing models, including verification and nonverification default implicature models and cost-free contextual models. More generally, our data are the first to provide evidence of the costs associated with deriving implicatures per se. ?? 2011 Elsevier Inc.},
	number = {1},
	journal = {Journal of Memory and Language},
	author = {Bott, Lewis and Bailey, Todd M. and Grodner, Daniel},
	year = {2012},
	keywords = {Language processing, Speed-accuracy trade-off, Pragmatics, Inferences, Scalar implicatures},
	pages = {123--142},
	file = {1-s2.0-S0749596X11001033-main:/home/user/Zotero/storage/IKPW5ZVZ/1-s2.0-S0749596X11001033-main.pdf:application/pdf}
}

@article{blythe-word-nodate,
	title = {Word learning under infinite uncertainty},
	journal = {arxiv},
	author = {Blythe, Richard A and Smith, Andrew D M and Smith, Kenny},
	keywords = {cross-situational learning, quine, s problem, word learning},
	file = {1412.2487v1:/home/user/Zotero/storage/3RXBAM7P/1412.2487v1.pdf:application/pdf}
}

@article{budisavljevic-age-related-2015,
	title = {Age-{Related} {Differences} and {Heritability} of the {Perisylvian} {Language} {Networks}},
	volume = {35},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1255-14.2015},
	doi = {10.1523/JNEUROSCI.1255-14.2015},
	number = {37},
	journal = {Journal of Neuroscience},
	author = {Budisavljevic, S. and Dell'Acqua, F. and Rijsdijk, F. V. and Kane, F. and Picchioni, M. and McGuire, P. and Toulopoulou, T. and Georgiades, a. and Kalidindi, S. and Kravariti, E. and Murray, R. M. and Murphy, D. G. and Craig, M. C. and Catani, M.},
	year = {2015},
	keywords = {language, adolescence and early adulthood, and frontoparietal, and remain lateralized throughout, and right lateralized, anterior segment, arcuate fasciculus, arcuate fasciculus are left, by early childhood, connections of the, diffusion tensor tractography, frontotemporal, heritability, lateralization, long segment, network asymmetry, our study shows that, respectively, significance statement},
	pages = {12625--12634},
	file = {12625.full:/home/user/Zotero/storage/AAJZH3HU/12625.full.pdf:application/pdf}
}

@article{breu-time-2008,
	title = {The time course of lexical innovation},
	url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf},
	journal = {Vasa},
	author = {Breu, Fx and Guggenbichler, S and Wollmann, Jc},
	year = {2008},
	pages = {1--77},
	file = {BaayenRenoufLanguage1996:/home/user/Zotero/storage/RHH75KKX/BaayenRenoufLanguage1996.pdf:application/pdf}
}

@article{bresnan-predicting-2007,
	title = {Predicting the dative alternation},
	issn = {9069844931},
	abstract = {Theoretical linguists have traditionally relied on linguistic intuitions such as grammaticality judgments for their data. But the massive growth of computer-readable texts and recordings, the availability of cheaper, more powerful computers and software, and the development of new probabilistic models for language have now made the spontaneous use of language in natural settings a rich and easily accessible alternative source of data. Surprisingly, many linguists believe that such ‘usage data’ are irrelevant to the theory of grammar. Four problems are repeatedly brought up in the critiques of usage data— 1. correlated factors seeming to support reductive theories, 2. pooled data invalidating grammatical inference, 3. syntactic choices reducing to lexical biases, and 4. cross-corpus differences undermining corpus studies. Presenting a case study of work on the English dative alternation, we show first, that linguistic intuitions of grammaticality are deeply flawed and seriously underestimate the space of grammatical possibility, and second, that the four problems in the critique of usage data are empirical issues that can be resolved by using modern statistical theory and modelling strategies widely used in other fields. The new models allow linguistic theory to solve more difficult problems than it has in the past, and to build convergent projects with psychology, computer science, and allied fields of cognitive science.},
	journal = {Cognitive Foundations of Interpretation},
	author = {Bresnan, Joan and Cueni, Anna and Nikita, Tatiana and Baayen, R. H.},
	year = {2007},
	pages = {69--94},
	file = {BresnanEtAL:/home/user/Zotero/storage/KIUZ7QM2/BresnanEtAL.pdf:application/pdf}
}

@article{bergmann-modelling-2015,
	title = {Modelling the {Noise}-{Robustness} of {Infants}’ {Word} {Representations}: {The} {Impact} of {Previous} {Experience}},
	volume = {10},
	issn = {0000000000},
	url = {http://dx.plos.org/10.1371/journal.pone.0132245},
	doi = {10.1371/journal.pone.0132245},
	number = {7},
	journal = {Plos One},
	author = {Bergmann, Christina and Bosch, Louis Ten and Fikkert, Paula and Boves, Lou},
	year = {2015},
	pages = {e0132245--e0132245},
	file = {journal.pone.0132245:/home/user/Zotero/storage/P5PBPBXZ/journal.pone.0132245.pdf:application/pdf}
}

@article{blythe-hierarchy-2015,
	title = {Hierarchy of {Scales} in {Language} {Dynamics}},
	url = {http://arxiv.org/abs/1505.00122v1},
	author = {Blythe, Richard A},
	year = {2015},
	file = {1505.00122v1:/home/user/Zotero/storage/RCWB8V4W/1505.00122v1.pdf:application/pdf}
}

@article{bien-frequency-2011,
	title = {Frequency effects in the production of {Dutch} deverbal adjectives and inflected verbs},
	volume = {26},
	doi = {10.1080/01690965.2010.511475},
	abstract = {In two experiments, we studied the role of frequency information in the production of deverbal adjectives and inflected verbs in Dutch. Naming latencies were triggered in a position-response association task and analysed using stepwise mixed-effects modelling, with subject and word as crossed random effects. The production latency of deverbal adjectives was affected by the cumulative frequencies of their verbal stems, arguing for decomposition and against full listing. However, for the inflected verbs, there was an inhibitory effect of Inflectional Entropy, and a nonlinear effect of Lemma Frequency. Additional effects of Position-specific Neighbourhood Density and Cohort Entropy in both types of words underline the importance of paradigmatic relations in the mental lexicon. Taken together, the data suggest that the word-form level does neither contain full forms nor strictly separated morphemes, but rather morphemes with links to phonologically andin case of inflected verbsmorphologically related word forms.},
	journal = {Language and Cognitive Processes},
	author = {Bien, Heidrun and Baayen, R. Harald and Levelt, Willem J. M.},
	year = {2011},
	pages = {683--715},
	file = {BienLeveltBaayenLCP2010:/home/user/Zotero/storage/K55SGN5A/BienLeveltBaayenLCP2010.pdf:application/pdf}
}

@article{bidet-ildei-are-2014,
	title = {Are judgments for action verbs and point-light human actions equivalent?},
	volume = {16},
	url = {http://link.springer.com/10.1007/s10339-014-0634-0},
	doi = {10.1007/s10339-014-0634-0},
	journal = {Cognitive Processing},
	author = {Bidet-Ildei, Christel and Toussaint, Lucette},
	year = {2014},
	keywords = {ability á point-light, action words á judgement, human actions á sensorimotor, representations},
	pages = {57--67},
	file = {Bidet-Ildei, Toussaint - 2014 - Are judgments for action verbs and point-light human actions equivalent:/home/user/Zotero/storage/BCA2JPVK/Bidet-Ildei, Toussaint - 2014 - Are judgments for action verbs and point-light human actions equivalent.pdf:application/pdf}
}

@article{baxter-utterance-2006,
	title = {Utterance selection model of language change},
	volume = {73},
	doi = {10.1103/PhysRevE.73.046118},
	abstract = {We present a mathematical formulation of a theory of language change. The theory is evolutionary in nature and has close analogies with theories of population genetics. The mathematical structure we construct similarly has correspondences with the Fisher-Wright model of population genetics, but there are significant differences. The continuous time formulation of the model is expressed in terms of a Fokker-Planck equation. This equation is exactly soluble in the case of a single speaker and can be investigated analytically in the case of multiple speakers who communicate equally with all other speakers and give their utterances equal weight. Whilst the stationary properties of this system have much in common with the single-speaker case, time-dependent properties are richer. In the particular case where linguistic forms can become extinct, we find that the presence of many speakers causes a two-stage relaxation, the first being a common marginal distribution that persists for a long time as a consequence of ultimate extinction being due to rare fluctuations.},
	journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	author = {Baxter, G. J. and Blythe, R. a. and Croft, W. and McKane, a. J.},
	year = {2006},
	pages = {1--21},
	file = {0512588v1:/home/user/Zotero/storage/324UMT7P/0512588v1.pdf:application/pdf}
}

@article{baumann-dependencies-2008,
	title = {Dependencies and {Hierarchical} {Structure} in {Sentence} {Processing}},
	author = {Baumann, Peter},
	year = {2008},
	keywords = {sentence processing, modeling, syntax, reading, eye-, Memory, tracking},
	pages = {152--157},
	file = {paper037:/home/user/Zotero/storage/C5ZHMD96/paper037.pdf:application/pdf}
}

@article{bartolotti-wordlikeness-2012,
	title = {Wordlikeness and {Novel} {Word} {Learning}},
	author = {Bartolotti, James},
	year = {2012},
	keywords = {neighborhood den-, orthotactic probability, second language acquisition, sity},
	pages = {146--151},
	file = {paper036:/home/user/Zotero/storage/VX77THRQ/paper036.pdf:application/pdf}
}

@article{balling-probability-2012,
	title = {Probability and surprisal in auditory comprehension of morphologically complex words},
	volume = {125},
	issn = {1873-7838 (Electronic) 0010-0277 (Linking)},
	doi = {10.1016/j.cognition.2012.06.003},
	abstract = {Two auditory lexical decision experiments document for morphologically complex words two points at which the probability of a target word given the evidence shifts dramatically. The first point is reached when morphologically unrelated competitors are no longer compatible with the evidence. Adapting terminology from Marslen-Wilson (1984), we refer to this as the word's initial uniqueness point (UP1). The second point is the complex uniqueness point (CUP) introduced by Balling and Baayen (2008), at which morphologically related competitors become incompatible with the input. Later initial as well as complex uniqueness points predict longer response latencies. We argue that the effects of these uniqueness points arise due to the large surprisal (Levy, 2008) carried by the phonemes at these uniqueness points, and provide independent evidence that how cumulative surprisal builds up in the course of the word co-determines response latencies. The presence of effects of surprisal, both at the initial uniqueness point of complex words, and cumulatively throughout the word, challenges the Shortlist B model of Norris and McQueen (2008), and suggests that a Bayesian approach to auditory comprehension requires complementation from information theory in order to do justice to the cognitive cost of updating probability distributions over lexical candidates. ?? 2012 Elsevier B.V.},
	number = {2008},
	journal = {Cognition},
	author = {Balling, Laura Winther and Baayen, R. Harald},
	year = {2012},
	keywords = {Spoken word recognition, Morphological processing, Morphological family size, (cumulative) Surprisal, Kullback-Leibler divergence, Neighborhood measures, Shortlist B, Uniqueness points},
	pages = {80--106},
	file = {BallingBaayen2012:/home/user/Zotero/storage/ZXCU5GT4/BallingBaayen2012.pdf:application/pdf}
}

@article{baayen-demythologizing-2010,
	title = {Demythologizing the word frequency effect: {A} discriminative learning perspective},
	url = {http://www.ingentaconnect.com/content/jbp/ml/2010/00000005/00000003/art00010},
	number = {2004},
	journal = {The Mental Lexicon},
	author = {Baayen, Rh},
	year = {2010},
	file = {BaayenML2011:/home/user/Zotero/storage/BPD38RU8/BaayenML2011.pdf:application/pdf}
}

@article{baayen-analyzing-2010,
	title = {Analyzing {Reaction} {Times}},
	volume = {3},
	issn = {20112084},
	doi = {10.1287/mksc.12.4.395},
	abstract = {Reaction times (rts) are an important source of information in experimental psychology. Classical methodological considerations pertaining to the sta- tistical analysis of rt data are optimized for analyses of aggregated data, based on subject or item means (c.f., Forster \& Dickinson, 1976). Mixed- effects modeling (see, e.g., Baayen, Davidson, \& Bates, 2008) does not re- quire prior aggregation and allows the researcher the more ambitious goal of predicting individual responses. Mixed-modeling calls for a reconsideration of the classical methodological strategies for analysing rts. In this study, we argue for empirical flexibility with respect to the choice of transforma- tion for the rts. We advocate minimal a-priori data trimming, combined with model criticism. We also show how trial-to-trial, longitudinal depen- dencies between individual observations can be brought into the statistical model. These strategies are illustrated for a large dataset with a non-trivial random-effects structure. Special attention is paid to the evaluation of in- teractions involving fixed-effect factors that partition the levels sampled by random-effect factors.},
	journal = {International Journal of Psychological Research},
	author = {Baayen, R. Harold and Milin, Peter},
	year = {2010},
	keywords = {analysis, congition, reaction times},
	pages = {12--28},
	file = {BaayenMilin2010:/home/user/Zotero/storage/6XFXRUT9/BaayenMilin2010.pdf:application/pdf}
}

@article{barr-random-2013,
	title = {Random effects structure for confirmatory hypothesis testing: {Keep} it maximal},
	volume = {68},
	issn = {0749-596X (Print)\r0749-596X (Linking)},
	url = {http://dx.doi.org/10.1016/j.jml.2012.11.001},
	doi = {10.1016/j.jml.2012.11.001},
	abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the 'gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond. ?? 2012 Elsevier Inc.},
	number = {3},
	journal = {Journal of Memory and Language},
	author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
	year = {2013},
	keywords = {Generalization, Linear mixed-effects models, Monte Carlo simulation, Statistics},
	pages = {255--278},
	file = {1-s2.0-S0749596X12001180-main:/home/user/Zotero/storage/R4PUIZB2/1-s2.0-S0749596X12001180-main.pdf:application/pdf}
}

@article{baayen-productivity-1997,
	title = {productivity in context},
	journal = {linguistics},
	author = {{Baayen} and {Neijt}},
	year = {1997},
	file = {BaayenNeijtLinguistics1997:/home/user/Zotero/storage/VUJ3QERH/BaayenNeijtLinguistics1997.pdf:application/pdf}
}

@article{baayen-effects-1995,
	title = {effects},
	number = {1},
	author = {{Baayen} and {Burani} and {Schreuder}},
	year = {1995},
	pages = {1--20},
	file = {BaayenBuraniSchreuder:/home/user/Zotero/storage/IQVQZ7T6/BaayenBuraniSchreuder.pdf:application/pdf}
}

@article{baayen-productivity-1994,
	title = {Productivity in language production},
	volume = {9},
	issn = {0169096940840},
	doi = {10.1080/01690969408402127},
	abstract = {Used lexical statistics and a production experiment to gauge the extent to which the linguistic notion of morphological productivity is relevant for psycholinguistic theories of speech production in languages such as Dutch and English. Lexical statistics of productivity show that despite the relatively poor morphology of Dutch, new words are created often enough for the marginalization of word formation in theories of speech production to be theoretically unattractive. This conclusion is supported by the results of a production experiment in which 36 Ss freely created hundreds of productive neologisms but only a handful of unproductive neologisms. A tentative solution is proposed as to why the opposite pattern has been observed in the speech of jargonaphasics. (PsycINFO Database Record (c) 2002 APA, all rights reserved).},
	journal = {Language and Cognitive Processes},
	author = {Baayen, R. Harald},
	year = {1994},
	pages = {447--469},
	file = {BaayenLCP1994:/home/user/Zotero/storage/5A3EDAEW/BaayenLCP1994.pdf:application/pdf}
}

@article{baayen-mixed-effects-2008,
	title = {Mixed-effects modeling with crossed random effects for subjects and items},
	volume = {59},
	issn = {0897916360},
	url = {http://dx.doi.org/10.1016/j.jml.2007.12.005},
	doi = {10.1016/j.jml.2007.12.005},
	abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed. ?? 2007 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Memory and Language},
	author = {Baayen, R. H. and Davidson, D. J. and Bates, D. M.},
	year = {2008},
	keywords = {By-item, By-subject, Crossed random effects, Mixed-effects models, Quasi-F},
	pages = {390--412},
	file = {1-s2.0-S0749596X07001398-main:/home/user/Zotero/storage/2K4ZE57M/1-s2.0-S0749596X07001398-main.pdf:application/pdf}
}

@article{baayen-derivational-1994,
	title = {Derivational productivity and text typology*},
	volume = {1},
	issn = {0929617940},
	doi = {10.1080/09296179408589996},
	abstract = {The productivity of English derivational affixes is studied as a function of text type. Principal component analyses show that texts can be classified adequately, on the basis of the relative frequencies of the highest frequency words \& on the basis of the productivity of derivational affixes. Stylistically heterogeneous texts are clustered into text types \& stylistically homogeneous texts into the time dimension, allowing diachronic changes in productivity to be traced. Supplementary analyses on the basis of the relative frequencies of function words support the morphology-based clusterings. The role \& marked nature of the nonnative stratum of the lexicon is discussed, along with the way in which the rival affixes -ness, un-, \& in- are put to use. Results show that any theory of morphological productivity that does not take stylistic factors into account is incomplete. 2 Tables, 11 Figures, 23 References. Adapted from the source document},
	journal = {Journal of Quantitative Linguistics},
	author = {Baayen, R. Harald},
	year = {1994},
	keywords = {1976, 1977, arono, booij, derivational morphology, e ortlessly, for it to be, formation rule should have, function words, has focussed predominantly on, markedness, most of the research, of a language to, on morphological productivity, productive, productivity, rule-based polymorphemic words, semantic properties a word, text typology, the formal and, the possibility for speakers, use and understand novel},
	pages = {16--34},
	file = {BaayenJQL1994:/home/user/Zotero/storage/8SPXHBBV/BaayenJQL1994.pdf:application/pdf}
}

@article{baayen-real-2010,
	title = {A real experiment is a factorial experiment?},
	volume = {5},
	doi = {10.1075/ml.5.1.06baa},
	journal = {The Mental Lexicon},
	author = {Baayen, R. Harald},
	year = {2010},
	pages = {149--157},
	file = {baayenML2010matching:/home/user/Zotero/storage/A5WG8JEM/baayenML2010matching.pdf:application/pdf}
}

@article{baayen-frequency-1993,
	title = {On frequency, transparency and productivity},
	issn = {0922-3495},
	doi = {10.1007/978-94-017-3710-4\_7},
	abstract = {Various corpus-based quantitative measures of morphological productivity are studied with respect to their empirical adequacy. A new productivity measure (P*) is proposed that evaluates the productivity of affixes in terms of their contribution to the growth rate of the overall vocabulary. This measure is shown to be the limiting form of a more general measure (A) that can be interpreted as estimating the resting activation levels of representations of affixes in the mental lexicon. This more general measure allows the effects of pseudo-affixation \& semantic transparency to be taken into account. Its application to large corpora of Dutch \& English written texts led to adequate productivity rankings. 5 Tables, 32 References. AA},
	journal = {Yearbook of morphology 1992},
	author = {Baayen, R. H.},
	year = {1993},
	keywords = {Affixes (00750), corpus-based quantitative morphological productivi, Creativity (Linguistics) (15900), Lexicon (47150), Morphology (55500), Text Analysis (89100)},
	pages = {181--208},
	file = {baayenYoM1993:/home/user/Zotero/storage/EH5S5PHK/baayenYoM1993.pdf:application/pdf}
}

@article{baayen-amorphous-2011,
	title = {An amorphous model for morphological processing in visual comprehension based on naive discriminative learning.},
	volume = {118},
	issn = {1939-1471},
	doi = {10.1037/a0023851},
	abstract = {A 2-layer symbolic network model based on the equilibrium equations of the Rescorla-Wagner model (Danks, 2003) is proposed. The study first presents 2 experiments in Serbian, which reveal for sentential reading the inflectional paradigmatic effects previously observed by Milin, Filipović Đurđević, and Moscoso del Prado Martín (2009) for unprimed lexical decision. The empirical results are successfully modeled without having to assume separate representations for inflections or data structures such as inflectional paradigms. In the next step, the same naive discriminative learning approach is pitted against a wide range of effects documented in the morphological processing literature. Frequency effects for complex words as well as for phrases (Arnon \& Snider, 2010) emerge in the model without the presence of whole-word or whole-phrase representations. Family size effects (Moscoso del Prado Martín, Bertram, Häikiö, Schreuder, \& Baayen, 2004; Schreuder \& Baayen, 1997) emerge in the simulations across simple words, derived words, and compounds, without derived words or compounds being represented as such. It is shown that for pseudo-derived words no special morpho-orthographic segmentation mechanism, as posited by Rastle, Davis, and New (2004), is required. The model also replicates the finding of Plag and Baayen (2009) that, on average, words with more productive affixes elicit longer response latencies; at the same time, it predicts that productive affixes afford faster response latencies for new words. English phrasal paradigmatic effects modulating isolated word reading are reported and modeled, showing that the paradigmatic effects characterizing Serbian case inflection have crosslinguistic scope.},
	journal = {Psychological review},
	author = {Baayen, R Harald and Milin, Petar and Đurđević, Dusica Filipović and Hendrix, Peter and Marelli, Marco},
	year = {2011},
	pages = {438--481},
	file = {BaayenEtAlPsychReview:/home/user/Zotero/storage/4E9RAUF2/BaayenEtAlPsychReview.pdf:application/pdf}
}

@article{baayen-quantitative-1992,
	title = {Quantitative aspects of morphological productivity},
	issn = {978-0-7923-1416-5},
	doi = {10.1007/978-94-011-2516-1\_8},
	abstract = {An exploration of the relation between the frequencies with which words are used in some text corpora \& the productivity of the morphological category to which they belong. Two techniques for the quantitative analysis of morphological productivity are developed. (1) The "global" productivity of a morphological category may be gauged by jointly considering the observed vocabulary - the number of different word types observed in the sample - \& the growth rate of the vocabulary - the probability of observing new types when the sample is increased. This growth rate, estimated by the ratio of the number of types occurring once only \& the sample size, is proposed as the statistical formalization of the notion "degree of productivity." (2) By using an appropriate word frequency distribution "law," the number of "potential" word types can be estimated. Hence the extent to which a morphological category is productive may also be evaluated by considering the extent to which the number of potential types exceeds the number of observed types. The statistical results are interpreted in a dual route model of lexical processing, it being argued that the crucial presence of large numbers of the lowest frequency types for a high degree of productivity is linked with the absence of independent memory representations for these types \& hence with some form of productive on-line morphological decomposition. 11 Tables, 4 Figures, 111 References. AA},
	journal = {Yearbook of morphology 1991},
	author = {Baayen, R. H.},
	year = {1992},
	keywords = {Creativity (Linguistics) (15900), Morphology (55500), morphological category productivity-word frequency, quantitative analysis techniques development, Word Frequency (97450)},
	pages = {109--149},
	file = {baayenYoM1992:/home/user/Zotero/storage/VF2J3IRU/baayenYoM1992.pdf:application/pdf}
}

@article{baayen-frequency-2010,
	title = {Frequency {Effects} in {Compound} {Processing}},
	journal = {Compounding},
	author = {Baayen, R Harald and Kuperman, Victor and Bertram, Raymond},
	year = {2010},
	pages = {257--270},
	file = {baayenKuperman2009MS:/home/user/Zotero/storage/FGP8WJXF/baayenKuperman2009MS.pdf:application/pdf}
}

@article{baayen-sidestepping-2013,
	title = {Sidestepping the combinatorial explosion: {Towards} a processing model based on discriminative learning},
	volume = {56},
	abstract = {Arnon and Snider (2010) documented frequency eﬀects for compositional 4-grams independently of the frequen- cies of lower-order n-grams. They argue that compre- henders apparently store frequency information about multi-word units. We show that n-gram frequency eﬀects can emerge in a parameter-free computational model driven by naive discriminative learning, trained on a sample of 300,000 4-word phrases from the British National Corpus. The discriminative learning model is a full decomposition model, associating orthographic input features straightforwardly with meanings. The model does not make use of separate representations for derived or inﬂected words, nor for compounds, nor for phrases. Nevertheless, frequency eﬀects are correctly predicted for all these linguistic units. Naive discrimina- tive learning provides the simplest and most economical explanation for frequency eﬀects in language processing, obviating the need to posit counters in the head for, and the existence of, hundreds of millions of n-gram repre- sentations.},
	journal = {Language and Speech},
	author = {Baayen, R Harald and Hendrix, Peter},
	year = {2013},
	pages = {329--347},
	file = {BaayenHendrixLSA2011:/home/user/Zotero/storage/TZQFQRN9/BaayenHendrixLSA2011.pdf:application/pdf}
}

@article{baayen-43.-nodate,
	title = {43. {Corpus} linguistics in morphology: morphological productivity {R}. {Harald} {Baayen} ({Baayen}, 43) 1},
	author = {Baayen, R Harald},
	pages = {1--51},
	file = {BaayenHSK2009:/home/user/Zotero/storage/ZRMWDBVF/BaayenHSK2009.pdf:application/pdf}
}

@article{baayen-assessing-2010,
	title = {Assessing {The} {Processing} {Consequences} {Of} {Segment} {Reduction} {In} {Dutch} {With} {Naive} {Discriminative} {Learning}},
	volume = {9},
	journal = {Lingue e Linguaggio},
	author = {Baayen, R Harald},
	year = {2010},
	pages = {95--112},
	file = {BaayenLL2010:/home/user/Zotero/storage/28EJAAHM/BaayenLL2010.pdf:application/pdf}
}

@article{baayen-directed-2010,
	title = {The directed compound graph of {English}. {An} exploration of lexical connectivity and its processing consequences.},
	abstract = {This study explores the consequences of morphological connectivity for English compounds, combining tools from graph theory with measures of lexical processing costs as available in the English Lexicon Project (Balota et al., 2007). The directed compound graph reveals a significant trend to acyclicity just as the directed affix graphs of Hay and Plag (2004); Plag and Baayen (2009); Zirkel (2010), and similar correlations of rank and productivity. Rank in the directed graph, however, fails to correlate with measures of processing complexity. In order to understand the high degree of acyclicity, it is hypothesized that the activation of more distant neighbors in the lexical network is disadvantageous. A measure for more distant lexical neighbors, secondary family size, is proposed, and shown to have an inhibitory effect in visual lexical decision and word naming. Furthermore, an inhibitory effect of the shortest path from head to modifier is documented, and shown to depend on a specific time window within which activation reaching the modifier disrupts the process of compound interpreta- tion.},
	number = {2004},
	journal = {New impulses in word-formation (Linguistische Berichte Sonderheft 17)},
	author = {Baayen, R Harald},
	year = {2010},
	keywords = {lexical decision, naming, productivity, complexity-based ordering, directed graph, mediated priming, strongly connected component},
	pages = {383--402},
	file = {BaayenLingBerichte2010:/home/user/Zotero/storage/3GFV8NZ7/BaayenLingBerichte2010.pdf:application/pdf}
}

@article{baayen-cognitive-nodate,
	title = {Cognitive decline ? {Pah} !},
	author = {Baayen, Harald},
	file = {RamscarBaayenNS2014:/home/user/Zotero/storage/K4VPQRCI/RamscarBaayenNS2014.pdf:application/pdf}
}

@article{baayen-mixed-effect-nodate,
	title = {Mixed-effect models {R}. {Harald} {Baayen}},
	author = {Baayen, R Harald},
	keywords = {fixed-effect factor, generalized linear mixed-effect models, index terms, interactions, longitudinal effects, mixed-effect models, random-effect factor, shrinkage},
	pages = {1--19},
	file = {BaayenHandbookLabPhonLMM2012:/home/user/Zotero/storage/EICIMJ2B/BaayenHandbookLabPhonLMM2012.pdf:application/pdf}
}

@article{baayen-multivariate-2008,
	title = {Multivariate {Statistics}},
	volume = {1},
	issn = {9781412934428},
	doi = {10.4135/9781446214565.n11},
	author = {{Baayen}},
	year = {2008},
	pages = {208--218},
	file = {baayenRML2012:/home/user/Zotero/storage/KFH9S4ZQ/baayenRML2012.pdf:application/pdf}
}

@article{baayen-no-nodate,
	title = {No {Title}},
	author = {{Baayen}},
	file = {BaayenDijkstraSchreuderJML1997:/home/user/Zotero/storage/FQFZPDA4/BaayenDijkstraSchreuderJML1997.pdf:application/pdf}
}

@article{baayen--nodate,
	title = { ore xesset @  niversity of  roms ø {D} {toreFnessetduitFnoAD} enn  indresen},
	author = {{Baayen}},
	file = {BaayenJandaNessetEndresenMakarova2013:/home/user/Zotero/storage/9NEEPI39/BaayenJandaNessetEndresenMakarova2013.pdf:application/pdf}
}

@article{amsterdam-relevance-2004,
	title = {The relevance of awareness michael franke and tikitu de jager},
	author = {Amsterdam, Universiteit Van},
	year = {2004},
	file = {Franke-deJager-AC07:/home/user/Zotero/storage/SR3QWMGE/Franke-deJager-AC07.pdf:application/pdf}
}

@article{amsel-close-2015,
	title = {Close, but no garlic: {Perceptuomotor} and event knowledge activation during language comprehension},
	volume = {82},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X1500042X},
	doi = {10.1016/j.jml.2015.03.009},
	journal = {Journal of Memory and Language},
	author = {Amsel, Ben D. and DeLong, Katherine a. and Kutas, Marta},
	year = {2015},
	pages = {118--132},
	file = {Amsel, DeLong, Kutas - 2015 - Close, but no garlic Perceptuomotor and event knowledge activation during language comprehension:/home/user/Zotero/storage/KWGIBC7P/Amsel, DeLong, Kutas - 2015 - Close, but no garlic Perceptuomotor and event knowledge activation during language comprehension.pdf:application/pdf}
}

@article{altmann-statistical-2015,
	title = {Statistical laws in linguistics},
	url = {http://arxiv.org/abs/1502.03296},
	author = {Altmann, Eduardo G. and Gerlach, Martin},
	year = {2015},
	pages = {1--12},
	file = {1502.03296v1:/home/user/Zotero/storage/RZNABPAT/1502.03296v1.pdf:application/pdf}
}

@article{arnold-using-2013,
	title = {Using generalized additive models and random forests to model prosodic prominence in {German}},
	number = {August},
	journal = {Proceedings of Interspeech 2013},
	author = {Arnold, Denis and Wagner, Petra and Baayen, R. Harald},
	year = {2013},
	keywords = {*},
	pages = {272--276},
	file = {ArnoldWagnerBaayen2013:/home/user/Zotero/storage/U5VCRH8V/ArnoldWagnerBaayen2013.pdf:application/pdf}
}

@article{altmann-abstract-nodate,
	title = {Abstract},
	volume = {6},
	doi = {10.1371/journal.pone.0019009.2},
	number = {5},
	author = {Altmann, Eduardo G and Pierrehumbert, Janet B and Motter, Adilson E},
	pages = {1--28},
	file = {1009.3321v2:/home/user/Zotero/storage/GDXEES6U/1009.3321v2.pdf:application/pdf}
}

@article{altmann-origin-2012,
	title = {On the origin of long-range correlations in texts},
	volume = {109},
	issn = {9780735412620},
	doi = {10.1073/pnas.1117723109},
	abstract = {The complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language. In order to retain and convey such a high dimensional information, the statistical properties of our linguistic output has to be highly correlated in time. An example are the robust observations, still largely not understood, of correlations on arbitrary long scales in literary texts. In this paper we explain how long-range correlations flow from highly structured linguistic levels down to the building blocks of a text (words, letters, etc..). By combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text. The mechanisms we identify are fairly general and can be equally applied to other hierarchical settings.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Altmann, E. G. and Cristadoro, G. and Esposti, M. D.},
	year = {2012},
	pages = {11582--11587},
	file = {1207.0658v1:/home/user/Zotero/storage/3GSE9NPN/1207.0658v1.pdf:application/pdf}
}

@article{watters-neuronal-2014,
	title = {Neuronal {Spike} {Train} {Entropy} {Estimation} by {History} {Clustering}},
	volume = {1872},
	doi = {10.1162/NECO},
	abstract = {Neurons send signals to each other by means of sequences of action potentials (spikes). Ignoring variations in spike amplitude and shape that are probably not meaningful to a receiving cell, the information content, or entropy of the signal depends on only the timing of action potentials, and because there is no external clock, only the interspike intervals, and not the absolute spike times, are significant. Estimating spike train entropy is a difficult task, particularly with small data sets, and many methods of entropy estimation have been proposed. Here we present two related model-based methods for estimating the entropy of neural signals and compare them to existing methods. One of the methods is fast and reasonably accurate, and it converges well with short spike time records; the other is impractically time-consuming but apparently very accurate, relying on generating artificial data that are a statistical match to the experimental data. Using the slow, accurate method to generate a best-estimate entropy value, we find that the faster estimator converges to this value more closely and with smaller data sets than many existing entropy estimators.},
	journal = {Neural computation},
	author = {Watters, Nicholas and Reeke, George N},
	year = {2014},
	pages = {1840--1872},
	file = {neco_a_00756:/home/user/Zotero/storage/3A2KU6PR/neco_a_00756.pdf:application/pdf}
}

@article{stott-representations-2015,
	title = {Representations of {Value} in the {Brain}: {An} {Embarrassment} of {Riches}?},
	volume = {13},
	url = {http://dx.plos.org/10.1371/journal.pbio.1002174},
	doi = {10.1371/journal.pbio.1002174},
	journal = {PLOS Biology},
	author = {Stott, Jeffrey J. and Redish, a. David},
	year = {2015},
	pages = {e1002174--e1002174},
	file = {journal.pbio.1002174:/home/user/Zotero/storage/82V7RNXU/journal.pbio.1002174.pdf:application/pdf}
}

@article{weaver-brain-2015,
	title = {Brain {Signature} {Predicts} {Negative} {Emotion} in {Individuals}},
	volume = {13},
	url = {http://dx.plos.org/10.1371/journal.pbio.1002179},
	doi = {10.1371/journal.pbio.1002179},
	journal = {PLOS Biology},
	author = {Weaver, Janelle},
	year = {2015},
	pages = {e1002179--e1002179},
	file = {journal.pbio.1002179:/home/user/Zotero/storage/9VNTSDEA/journal.pbio.1002179.pdf:application/pdf}
}

@article{wang-energy-2014,
	title = {Energy coding in neural network with inhibitory neurons},
	volume = {9},
	url = {http://link.springer.com/10.1007/s11571-014-9311-3},
	doi = {10.1007/s11571-014-9311-3},
	journal = {Cognitive Neurodynamics},
	author = {Wang, Ziyin and Wang, Rubin and Fang, Ruiyan},
	year = {2014},
	pages = {129--144}
}

@article{van-der-velde-necessity-2015,
	title = {The necessity of connection structures in neural models of variable binding},
	issn = {1157101593},
	url = {http://link.springer.com/10.1007/s11571-015-9331-7},
	doi = {10.1007/s11571-015-9331-7},
	journal = {Cognitive Neurodynamics},
	author = {van der Velde, Frank and de Kamps, Marc},
	year = {2015},
	keywords = {binding, Behavior, behavior á frame of, Frame of reference, networks á novel variable, Novel variable binding, reference á small-word, Small-word networks},
	pages = {359--370}
}

@article{schwitzgebel-philosophers-2015,
	title = {Philosophers’ biased judgments persist despite training, expertise and reflection},
	volume = {141},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715000931},
	doi = {10.1016/j.cognition.2015.04.015},
	journal = {Cognition},
	author = {Schwitzgebel, Eric and Cushman, Fiery},
	year = {2015},
	pages = {127--137},
	file = {1-s2.0-S0010027715000931-main:/home/user/Zotero/storage/SK5C3D8M/1-s2.0-S0010027715000931-main.pdf:application/pdf}
}

@article{mendoza-drosophila-2014,
	title = {Drosophila {FoxP} mutants are deficient in operant self-learning},
	volume = {9},
	doi = {10.1371/journal.pone.0100648},
	abstract = {Intact function of the Forkhead Box P2 (FOXP2) gene is necessary for normal development of speech and language. This important role has recently been extended, first to other forms of vocal learning in animals and then also to other forms of motor learning. The homology in structure and in function among the FoxP gene members raises the possibility that the ancestral FoxP gene may have evolved as a crucial component of the neural circuitry mediating motor learning. Here we report that genetic manipulations of the single Drosophila orthologue, dFoxP, disrupt operant self-learning, a form of motor learning sharing several conceptually analogous features with language acquisition. Structural alterations of the dFoxP locus uncovered the role of dFoxP in operant self-learning and habit formation, as well as the dispensability of dFoxP for operant world-learning, in which no motor learning occurs. These manipulations also led to subtle alterations in the brain anatomy, including a reduced volume of the optic glomeruli. RNAi-mediated interference with dFoxP expression levels copied the behavioral phenotype of the mutant flies, even in the absence of mRNA degradation. Our results provide evidence that motor learning and language acquisition share a common ancestral trait still present in extant invertebrates, manifest in operant self-learning. This 'deep' homology probably traces back to before the split between vertebrate and invertebrate animals.},
	number = {6},
	journal = {PLoS ONE},
	author = {Mendoza, Ezequiel and Colomb, Julien and Rybak, Jürgen and Pflüger, Hans Joachim and Zars, Troy and Scharff, Constance and Brembs, Björn},
	year = {2014},
	file = {foxP-2014:/home/user/Zotero/storage/CZZTZTG9/foxP-2014.pdf:application/pdf}
}

@article{manuscript-attention-2012-1,
	title = {attention durig natural vision warps {Semantic} {Representation}},
	volume = {29},
	issn = {2122633255},
	doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
	number = {6},
	journal = {Nat Neurosci},
	author = {Manuscript, Author},
	year = {2012},
	keywords = {visual search, 1, attention, attention is thought to, fmri, increase information processing efficiency, movie, natural stimuli, neurophysiology studies in early, representation, through several convergent mechanisms, throughout the brain, tuning change, visual areas},
	pages = {997--1003},
	file = {nihms457964:/home/user/Zotero/storage/BKRRMG28/nihms457964.pdf:application/pdf}
}

@article{lin-nature-2015,
	title = {The {Nature} of {Shared} {Cortical} {Variability}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S089662731500598X},
	doi = {10.1016/j.neuron.2015.06.035},
	journal = {Neuron},
	author = {Lin, I-Chun and Okun, Michael and Carandini, Matteo and Harris, Kenneth D.},
	year = {2015},
	pages = {1--13},
	file = {PIIS089662731500598X:/home/user/Zotero/storage/JCJ2G6UV/PIIS089662731500598X.pdf:application/pdf}
}

@article{samura-neural-2015,
	title = {A neural network model of reliably optimized spike transmission},
	volume = {9},
	issn = {1157101593},
	url = {http://link.springer.com/10.1007/s11571-015-9329-1},
	doi = {10.1007/s11571-015-9329-1},
	journal = {Cognitive Neurodynamics},
	author = {Samura, Toshikazu and Ikegaya, Yuji and Sato, Yasuomi D.},
	year = {2015},
	keywords = {Log-normally distributed synaptic weights, Power-law-distributed synchrony, Spike transmission, spike transmission á power-law-distributed, Synaptic background activity, synaptic weights á, synchrony á log-normally distributed},
	pages = {265--277}
}

@article{huang-reduction-2006,
	title = {Reduction in {V}1 activation associated with decreased visibility of a visual target},
	volume = {31},
	issn = {1053-8119},
	doi = {10.1016/j.neuroimage.2006.02.020},
	abstract = {The perception of a brief visual target stimulus can be affected by another visual mask stimulus immediately preceding or following the target. The link of this visual masking illusion, with visual cortical activation, offers insights into the neural mechanisms for visual perception. The present study investigated the association of the visibility of a target with cortical activation in humans using psychophysical testing and functional magnetic resonance imaging (fMRI). A visual masking protocol that was suitable for an fMRI study was developed. The event-related fMRI was used to measure activation in primary visual cortex (V1) during visual masking and unmasking stimulation. We found that the visibility of the target stimulus was reduced in the masking condition, due to the presence of mask stimuli, but not in the unmasking condition. We also found that the activation in V1 was modulated by the temporal separation of the mask stimuli from the target and was associated with the visibility of the target that was recorded during psychophysical testing and fMRI. These findings are consistent with what has been observed in the primate visual cortex of monkeys, i.e., the transient on-response and after-discharge of V1 neurons to the target stimulus were suppressed by forward and backward mask stimuli, respectively. ?? 2006 Elsevier Inc. All rights reserved.},
	journal = {NeuroImage},
	author = {Huang, Jie and Xiang, Ming and Cao, Yue},
	year = {2006},
	pages = {1693--1699},
	file = {Neuroimage-paper:/home/user/Zotero/storage/IQ7HM4PD/Neuroimage-paper.pdf:application/pdf}
}

@article{gasque-calcium-dependent-2015,
	title = {A {Calcium}-{Dependent} {Mechanism} of {Neuronal} {Memory}},
	volume = {13},
	url = {http://dx.plos.org/10.1371/journal.pbio.1002182},
	doi = {10.1371/journal.pbio.1002182},
	journal = {PLOS Biology},
	author = {Gasque, Gabriel},
	year = {2015},
	pages = {e1002182--e1002182},
	file = {journal.pbio.1002182:/home/user/Zotero/storage/BIRCW5WD/journal.pbio.1002182.pdf:application/pdf}
}

@article{ison-rapid-2015,
	title = {Rapid {Encoding} of {New} {Memories} by {Individual} {Neurons} in the {Human} {Brain}},
	volume = {87},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627315005590},
	doi = {10.1016/j.neuron.2015.06.016},
	number = {1},
	journal = {Neuron},
	author = {Ison, Matias J. and Quian Quiroga, Rodrigo and Fried, Itzhak},
	year = {2015},
	pages = {220--230},
	file = {PIIS0896627315005590:/home/user/Zotero/storage/WU8KUWWN/PIIS0896627315005590.pdf:application/pdf}
}

@article{fischer-dual-2015,
	title = {Dual serotonergic signals: a key to understanding paradoxical effects?},
	volume = {19},
	url = {http://dx.doi.org/10.1016/j.tics.2014.11.004},
	doi = {10.1016/j.tics.2014.11.004},
	number = {1},
	journal = {Trends in Cognitive Sciences},
	author = {Fischer, Adrian G and Jocham, Gerhard and Ullsperger, Markus},
	year = {2015},
	keywords = {co-release, glutamate, serotonin, ssri},
	pages = {21--26},
	file = {Fischer, Jocham, Ullsperger - 2015 - Dual serotonergic signals a key to understanding paradoxical effects:/home/user/Zotero/storage/VXXTE3TH/Fischer, Jocham, Ullsperger - 2015 - Dual serotonergic signals a key to understanding paradoxical effects.pdf:application/pdf}
}

@article{hu-control-2014,
	title = {Control of absence seizures induced by the pathways connected to {SRN} in corticothalamic system},
	url = {http://link.springer.com/10.1007/s11571-014-9321-1},
	doi = {10.1007/s11571-014-9321-1},
	journal = {Cognitive Neurodynamics},
	author = {Hu, Bing and Guo, Daqing and Wang, Qingyun},
	year = {2014},
	keywords = {Absence seizures, Basal ganglia, basal ganglia á absence, Control, seizures á control},
	pages = {279--289}
}

@article{hayakawa-spatial-2014,
	title = {Spatial information enhanced by non-spatial information in hippocampal granule cells},
	volume = {9},
	url = {http://link.springer.com/10.1007/s11571-014-9309-x},
	doi = {10.1007/s11571-014-9309-x},
	journal = {Cognitive Neurodynamics},
	author = {Hayakawa, Hirofumi and Samura, Toshikazu and Kamijo, Tadanobu Chuyo and Sakai, Yutaka and Aihara, Takeshi},
	year = {2014},
	keywords = {cell model á, dentate gyrus á granule, spatial and non-spatial, temporal pattern discrimination á},
	pages = {1--12}
}

@article{coles-erps:-nodate,
	title = {{ERPS}: introduction},
	author = {{Coles} and {Rugg}},
	file = {Rugg-ColesChp1:/home/user/Zotero/storage/JGR4FW6Z/Rugg-ColesChp1.pdf:application/pdf}
}

@article{ciavarro-rtms-2013,
	title = {{rTMS} of {Medial} {Parieto}-occipital {Cortex} {Interferes} with {Attentional} {Reorienting} during {Attention} and {Reaching} {Tasks}},
	issn = {1530-8898 (Electronic)\r0898-929X (Linking)},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00409\nhttp://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00409},
	doi = {10.1162/jocn},
	abstract = {Unexpected changes in the location of a target for an upcoming action require both attentional reorienting and motor planning update. In both macaque and human brain, the medial posterior parietal cortex is involved in both phenomena but its causal role is still unclear. Here we used on-line rTMS over the putative human V6A (pV6A), a reach-related region in the dorsal part of the anterior bank of the parieto-occipital sulcus, during an attention and a reaching task requiring covert shifts of attention and planning of reaching movements toward cued targets in space. We found that rTMS increased RTs to invalidly cued but not to validly cued targets during both the attention and reaching task. Furthermore, we found that rTMS induced a deviation of reaching endpoints toward visual fixation and that this deviation was larger for invalidly cued targets. The results suggest that reorienting signals are used by human pV6A area to rapidly update the current motor plan or the ongoing action when a behaviorally relevant object unexpectedly occurs in an unattended location. The current findings suggest a direct involvement of the action-related dorso-medial visual stream in attentional reorienting and a more specific role of pV6A area in the dynamic, on-line control of reaching actions.},
	journal = {Journal of Cognitive Neuroscience},
	author = {Ciavarro, Marco and Ambrosini, Ettore and Tosoni, Annalisa and Committeri, Giorgia and Fattori, Patrizia and Galletti, Claudio},
	year = {2013},
	pages = {1--10},
	file = {jocn_a_00836:/home/user/Zotero/storage/JWQ8RX84/jocn_a_00836.pdf:application/pdf}
}

@article{ciavarro-rtms-2013-1,
	title = {{rTMS} of {Medial} {Parieto}-occipital {Cortex} {Interferes} with {Attentional} {Reorienting} during {Attention} and {Reaching} {Tasks}},
	issn = {1530-8898 (Electronic)\r0898-929X (Linking)},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00409\nhttp://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00409},
	doi = {10.1162/jocn},
	abstract = {Unexpected changes in the location of a target for an upcoming action require both attentional reorienting and motor planning update. In both macaque and human brain, the medial posterior parietal cortex is involved in both phenomena but its causal role is still unclear. Here we used on-line rTMS over the putative human V6A (pV6A), a reach-related region in the dorsal part of the anterior bank of the parieto-occipital sulcus, during an attention and a reaching task requiring covert shifts of attention and planning of reaching movements toward cued targets in space. We found that rTMS increased RTs to invalidly cued but not to validly cued targets during both the attention and reaching task. Furthermore, we found that rTMS induced a deviation of reaching endpoints toward visual fixation and that this deviation was larger for invalidly cued targets. The results suggest that reorienting signals are used by human pV6A area to rapidly update the current motor plan or the ongoing action when a behaviorally relevant object unexpectedly occurs in an unattended location. The current findings suggest a direct involvement of the action-related dorso-medial visual stream in attentional reorienting and a more specific role of pV6A area in the dynamic, on-line control of reaching actions.},
	journal = {Journal of Cognitive Neuroscience},
	author = {Ciavarro, Marco and Ambrosini, Ettore and Tosoni, Annalisa and Committeri, Giorgia and Fattori, Patrizia and Galletti, Claudio},
	year = {2013},
	pages = {1--10},
	file = {jocn_a_00832:/home/user/Zotero/storage/BM9GE258/jocn_a_00832.pdf:application/pdf}
}

@article{ciavarro-rtms-2013-2,
	title = {{rTMS} of {Medial} {Parieto}-occipital {Cortex} {Interferes} with {Attentional} {Reorienting} during {Attention} and {Reaching} {Tasks}},
	issn = {1530-8898 (Electronic)\r0898-929X (Linking)},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00409\nhttp://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00409},
	doi = {10.1162/jocn},
	abstract = {Unexpected changes in the location of a target for an upcoming action require both attentional reorienting and motor planning update. In both macaque and human brain, the medial posterior parietal cortex is involved in both phenomena but its causal role is still unclear. Here we used on-line rTMS over the putative human V6A (pV6A), a reach-related region in the dorsal part of the anterior bank of the parieto-occipital sulcus, during an attention and a reaching task requiring covert shifts of attention and planning of reaching movements toward cued targets in space. We found that rTMS increased RTs to invalidly cued but not to validly cued targets during both the attention and reaching task. Furthermore, we found that rTMS induced a deviation of reaching endpoints toward visual fixation and that this deviation was larger for invalidly cued targets. The results suggest that reorienting signals are used by human pV6A area to rapidly update the current motor plan or the ongoing action when a behaviorally relevant object unexpectedly occurs in an unattended location. The current findings suggest a direct involvement of the action-related dorso-medial visual stream in attentional reorienting and a more specific role of pV6A area in the dynamic, on-line control of reaching actions.},
	journal = {Journal of Cognitive Neuroscience},
	author = {Ciavarro, Marco and Ambrosini, Ettore and Tosoni, Annalisa and Committeri, Giorgia and Fattori, Patrizia and Galletti, Claudio},
	year = {2013},
	pages = {1--10},
	file = {jocn_a_00830:/home/user/Zotero/storage/W3VEMURJ/jocn_a_00830.pdf:application/pdf}
}

@article{bestmann-understanding-2015,
	title = {Understanding the behavioural consequences of noninvasive brain stimulation},
	volume = {19},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661314002320},
	doi = {10.1016/j.tics.2014.10.003},
	number = {1},
	journal = {Trends in Cognitive Sciences},
	author = {Bestmann, Sven and de Berker, Archy O. and Bonaiuto, James},
	year = {2015},
	keywords = {cognition, computational neurostimulation, hancement, improvement, modelling, neuroen-, noninvasive brain stimulation},
	pages = {13--20},
	file = {Bestmann, de Berker, Bonaiuto - 2015 - Understanding the behavioural consequences of noninvasive brain stimulation:/home/user/Zotero/storage/PZ2BATSK/Bestmann, de Berker, Bonaiuto - 2015 - Understanding the behavioural consequences of noninvasive brain stimulation.pdf:application/pdf}
}

@article{ciavarro-rtms-2013-3,
	title = {{rTMS} of {Medial} {Parieto}-occipital {Cortex} {Interferes} with {Attentional} {Reorienting} during {Attention} and {Reaching} {Tasks}},
	issn = {1530-8898 (Electronic)\r0898-929X (Linking)},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00409\nhttp://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00409},
	doi = {10.1162/jocn},
	abstract = {Unexpected changes in the location of a target for an upcoming action require both attentional reorienting and motor planning update. In both macaque and human brain, the medial posterior parietal cortex is involved in both phenomena but its causal role is still unclear. Here we used on-line rTMS over the putative human V6A (pV6A), a reach-related region in the dorsal part of the anterior bank of the parieto-occipital sulcus, during an attention and a reaching task requiring covert shifts of attention and planning of reaching movements toward cued targets in space. We found that rTMS increased RTs to invalidly cued but not to validly cued targets during both the attention and reaching task. Furthermore, we found that rTMS induced a deviation of reaching endpoints toward visual fixation and that this deviation was larger for invalidly cued targets. The results suggest that reorienting signals are used by human pV6A area to rapidly update the current motor plan or the ongoing action when a behaviorally relevant object unexpectedly occurs in an unattended location. The current findings suggest a direct involvement of the action-related dorso-medial visual stream in attentional reorienting and a more specific role of pV6A area in the dynamic, on-line control of reaching actions.},
	journal = {Journal of Cognitive Neuroscience},
	author = {Ciavarro, Marco and Ambrosini, Ettore and Tosoni, Annalisa and Committeri, Giorgia and Fattori, Patrizia and Galletti, Claudio},
	year = {2013},
	pages = {1--10},
	file = {jocn_a_00880:/home/user/Zotero/storage/SQPS4DXN/jocn_a_00880.pdf:application/pdf}
}

@article{brembs-spontaneous-2011,
	title = {Spontaneous decisions and operant conditioning in fruit flies},
	volume = {87},
	issn = {1872-8308 (Electronic)\n0376-6357 (Linking)},
	doi = {10.1016/j.beproc.2011.02.005},
	abstract = {Already in the 1930s Skinner, Konorskiand colleagues debated the commonalities, differences and interactions among the processes underlying what was then known as "conditioned reflexes type I and II", but which is today more well-known as classical (Pavlovian) and operant (instrumental) conditioning. Subsequent decades of research have confirmed that the interactions between the various learning systems engaged during operant conditioning are complex and difficult to disentangle. Today, modern neurobiological tools allow us to dissect the biological processes underlying operant conditioning and study their interactions. These processes include initiating spontaneous behavioral variability, world-learning and self-learning. The data suggest that behavioral variability is generated actively by the brain, rather than as a by-product of a complex, noisy input-output system. The function of this variability, in part, is to detect how the environment responds to such actions. World-learning denotes the biological process by which value is assigned to environmental stimuli. Self-learning is the biological process which assigns value to a specific action or movement. In an operant learning situation using visual stimuli for flies, world-learning inhibits self-learning via a prominent neuropil region, the mushroom-bodies. Only extended training can overcome this inhibition and lead to habit formation by engaging the self-learning mechanism. Self-learning transforms spontaneous, flexible actions into stereotyped, habitual responses. © 2011 Elsevier B.V.},
	journal = {Behavioural Processes},
	author = {Brembs, Björn},
	year = {2011},
	keywords = {Learning, Memory, Drosophila, Insect, Multiple memory systems, Operant, Self-learning, World-learning},
	pages = {157--164},
	file = {bproc_2011:/home/user/Zotero/storage/6QQEM575/bproc_2011.pdf:application/pdf}
}

@article{alderson-day-hearing-2015,
	title = {Hearing voices in the resting brain: {A} review of intrinsic functional connectivity research on auditory verbal hallucinations},
	volume = {55},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415001207},
	doi = {10.1016/j.neubiorev.2015.04.016},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Alderson-Day, Ben and McCarthy-Jones, Simon and Fernyhough, Charles},
	year = {2015},
	pages = {78--87},
	file = {Alderson-Day, McCarthy-Jones, Fernyhough - 2015 - Hearing voices in the resting brain A review of intrinsic functional connectivity rese:/home/user/Zotero/storage/3S2TWMQ8/Alderson-Day, McCarthy-Jones, Fernyhough - 2015 - Hearing voices in the resting brain A review of intrinsic functional connectivity rese.pdf:application/pdf}
}

@article{walker-empirical-2015,
	title = {Empirical and computational findings converge in support of the {Hierarchical} {State} {Feedback} {Control} theory},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1096404},
	doi = {10.1080/23273798.2015.1096404},
	number = {0},
	urldate = {2015-10-13},
	journal = {Language, Cognition and Neuroscience},
	author = {Walker, Grant M. and Hickok, Gregory},
	month = oct,
	year = {2015},
	pages = {1--1},
	file = {Full Text PDF:/home/user/Zotero/storage/W4CCBT6A/Walker and Hickok - 2015 - Empirical and computational findings converge in s.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/38TEFXAC/23273798.2015.html:text/html}
}

@article{strijkers-can-2015,
	title = {Can hierarchical models display parallel cortical dynamics? {A} non-hierarchical alternative of brain language theory},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Can hierarchical models display parallel cortical dynamics?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1096403},
	doi = {10.1080/23273798.2015.1096403},
	number = {0},
	urldate = {2015-10-13},
	journal = {Language, Cognition and Neuroscience},
	author = {Strijkers, Kristof},
	month = oct,
	year = {2015},
	pages = {1--5},
	file = {Full Text PDF:/home/user/Zotero/storage/RFHPKKR8/Strijkers - 2015 - Can hierarchical models display parallel cortical .pdf:application/pdf;Snapshot:/home/user/Zotero/storage/R2BKF474/23273798.2015.html:text/html}
}

@article{piai-role-2015,
	title = {The role of electrophysiology in informing theories of word production: a critical standpoint},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {The role of electrophysiology in informing theories of word production},
	url = {http://dx.doi.org/10.1080/23273798.2015.1100749},
	doi = {10.1080/23273798.2015.1100749},
	number = {0},
	urldate = {2015-10-13},
	journal = {Language, Cognition and Neuroscience},
	author = {Piai, Vitória},
	month = oct,
	year = {2015},
	pages = {1--3},
	file = {Full Text PDF:/home/user/Zotero/storage/N7RDVCTV/Piai - 2015 - The role of electrophysiology in informing theorie.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7UF9XP3C/23273798.2015.html:text/html}
}

@article{laganaro-dynamics-2015,
	title = {Dynamics of word production and processing speed},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1096402},
	doi = {10.1080/23273798.2015.1096402},
	number = {0},
	urldate = {2015-10-13},
	journal = {Language, Cognition and Neuroscience},
	author = {Laganaro, Marina},
	month = oct,
	year = {2015},
	pages = {1--2},
	file = {Full Text PDF:/home/user/Zotero/storage/DWVIC2CQ/Laganaro - 2015 - Dynamics of word production and processing speed.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/99QJFRXE/23273798.2015.html:text/html}
}

@article{leibovich-asymmetric-2015,
	title = {Asymmetric {Processing} of {Numerical} and {Nonnumerical} {Magnitudes} in the {Brain}: {An} {fMRI} {Study}},
	issn = {0898-929X},
	shorttitle = {Asymmetric {Processing} of {Numerical} and {Nonnumerical} {Magnitudes} in the {Brain}},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00887},
	doi = {10.1162/jocn\_a\_00887},
	abstract = {It is well established that, when comparing nonsymbolic magnitudes (e.g., dot arrays), adults can use both numerical (i.e., the number of items) and nonnumerical (density, total surface areas, etc.) magnitudes. It is less clear which of these magnitudes is more salient or processed more automatically. In this fMRI study, we used a nonsymbolic comparison task to ask if different brain areas are responsible for the automatic processing of numerical and nonnumerical magnitudes, when participants attend to either the numerical or the nonnumerical magnitudes of the same stimuli. An interaction of task (numerical vs. nonnumerical) and congruity (congruent vs. incongruent) was found in the right TPJ. Specifically, this brain region was more strongly activated during numerical processing when the nonnumerical magnitudes were negatively correlated with numerosity (incongruent trials). In contrast, such an interference effect was not evident during nonnumerical processing when the task-irrelevant numerical magnitude was incongruent. In view of the role of the right TPJ in the control of stimulus-driven attention, we argue that these data demonstrate that the processing of nonnumerical magnitudes is more automatic than that of numerical magnitudes and that, therefore, the influence of numerical and nonnumerical variables on each other is asymmetrical.},
	urldate = {2015-10-13},
	journal = {Journal of Cognitive Neuroscience},
	author = {Leibovich, Tali and Vogel, Stephan E. and Henik, Avishai and Ansari, Daniel},
	month = oct,
	year = {2015},
	pages = {1--11},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/9D35T2VZ/jocn_a_00887.html:text/html}
}

@article{stoianov-prefrontal-2015,
	title = {Prefrontal {Goal} {Codes} {Emerge} as {Latent} {States} in {Probabilistic} {Value} {Learning}},
	issn = {0898-929X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00886},
	doi = {10.1162/jocn\_a\_00886},
	abstract = {pFC supports goal-directed actions and exerts cognitive control over behavior, but the underlying coding and mechanism are heavily debated. We present evidence for the role of goal coding in pFC from two converging perspectives: computational modeling and neuronal-level analysis of monkey data. We show that neural representations of prospective goals emerge by combining a categorization process that extracts relevant behavioral abstractions from the input data and a reward-driven process that selects candidate categories depending on their adaptive value; both forms of learning have a plausible neural implementation in pFC. Our analyses demonstrate a fundamental principle: goal coding represents an efficient solution to cognitive control problems, analogous to efficient coding principles in other (e.g., visual) brain areas. The novel analytical–computational approach is of general interest because it applies to a variety of neurophysiological studies.},
	urldate = {2015-10-13},
	journal = {Journal of Cognitive Neuroscience},
	author = {Stoianov, Ivilin and Genovesio, Aldo and Pezzulo, Giovanni},
	month = oct,
	year = {2015},
	pages = {1--18},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/TXXM97HK/jocn_a_00886.html:text/html}
}

@article{hutchinson-biased-2015,
	title = {Biased {Competition} during {Long}-term {Memory} {Formation}},
	issn = {0898-929X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00889},
	doi = {10.1162/jocn\_a\_00889},
	abstract = {A key task for the brain is to determine which pieces of information are worth storing in memory. To build a more complete representation of the environment, memory systems may prioritize new information that has not already been stored. Here, we propose a mechanism that supports this preferential encoding of new information, whereby prior experience attenuates neural activity for old information that is competing for processing. We evaluated this hypothesis with fMRI by presenting a series of novel stimuli concurrently with repeated stimuli at different spatial locations in Experiment 1 and from different visual categories (i.e., faces and scenes) in Experiment 2. Subsequent memory for the novel stimuli could be predicted from the reduction in activity in ventral temporal cortex for the accompanying repeated stimuli. This relationship was eliminated in control conditions where the competition during encoding came from another novel stimulus. These findings reveal how prior experience adaptively guides learning toward new aspects of the environment.},
	urldate = {2015-10-13},
	journal = {Journal of Cognitive Neuroscience},
	author = {Hutchinson, J. Benjamin and Pak, Sarah S. and Turk-Browne, Nicholas B.},
	month = oct,
	year = {2015},
	pages = {1--11},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/VH5Q4AB5/jocn_a_00889.html:text/html}
}

@article{frank-adding-2013-2,
	title = {Adding sentence types to a model of syntactic category acquisition},
	volume = {5},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12030/full},
	number = {3},
	urldate = {2015-10-13},
	journal = {Topics in cognitive science},
	author = {Frank, Stella and Goldwater, Sharon and Keller, Frank},
	year = {2013},
	pages = {495--521},
	file = {topics13b.pdf:/home/user/Zotero/storage/KJFAHK99/topics13b.pdf:application/pdf}
}

@misc{center-for-history-and-new-media-zotero-nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick\_start\_guide},
	author = {{Center for History and New Media}}
}

@inproceedings{bauland-complexity-2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Complexity} of {Generalized} {Satisfiability} for {Linear} {Temporal} {Logic}},
	volume = {4423},
	isbn = {978-3-540-71388-3},
	booktitle = {{FoSSaCS}},
	publisher = {Springer},
	author = {Bauland, Michael and Schneider, Thomas and Schnoor, Henning and Schnoor, Ilka and Vollmer, Heribert},
	editor = {Seidl, Helmut},
	year = {2007},
	pages = {48--62}
}

@article{bauland-tractability-2011,
	title = {The tractability of model checking for {LTL}: {The} good, the bad, and the ugly fragments},
	volume = {12},
	number = {2},
	journal = {ACM Trans. Comput. Log.},
	author = {Bauland, Michael and Mundhenk, Martin and Schneider, Thomas and Schnoor, Henning and Schnoor, Ilka and Vollmer, Heribert},
	year = {2011},
	pages = {13--13}
}

@article{barrington-regular-1992,
	title = {Regular {Languages} in {NC}\${\textasciicircum}1\$},
	journal = {Journal of Computer and System Sciences},
	author = {{Barrington} and {Compton} and {Straubing} and {Therien}},
	year = {1992},
	file = {Barrington et al. - 1992 - Regular Languages in NC\$1\$:/home/user/Zotero/storage/VZASGTG4/Barrington et al. - 1992 - Regular Languages in NC\$1\$.pdf:application/pdf}
}

@inproceedings{behle-lower-2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Lower} {Reaches} of {Circuit} {Uniformity}},
	volume = {7464},
	isbn = {978-3-642-32588-5},
	url = {http://dx.doi.org/10.1007/978-3-642-32589-2\_52},
	doi = {10.1007/978-3-642-32589-2\_52},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 2012 - 37th {International} {Symposium}, {{MFCS}} 2012, {Bratislava}, {Slovakia}, {August} 27-31, 2012. {Proceedings}},
	publisher = {Springer},
	author = {Behle, Christoph and Krebs, Andreas and Lange, Klaus-Jörn and McKenzie, Pierre},
	editor = {Rovan, Branislav and Sassone, Vladimiro and Widmayer, Peter},
	year = {2012},
	pages = {590--602}
}

@article{behle-lower-2011,
	title = {The lower reaches of circuit uniformity},
	volume = {95},
	number = {95},
	author = {Behle, Christoph and Krebs, Andreas and Lange, Klaus-Jorn and Mckenzie, Pierre},
	year = {2011},
	file = {Behle et al. - 2011 - The lower reaches of circuit uniformity:/home/user/Zotero/storage/W9WEFREK/Behle et al. - 2011 - The lower reaches of circuit uniformity.pdf:application/pdf}
}

@article{bedard-extensions-1993,
	title = {Extensions to {Barrington}'s {M}-{Program} {Model}},
	volume = {107},
	url = {http://dx.doi.org/10.1016/0304-3975(93)90253-P},
	doi = {10.1016/0304-3975(93)90253-P},
	number = {1},
	journal = {Theor. Comput. Sci.},
	author = {Bédard, François and Lemieux, François and McKenzie, Pierre},
	year = {1993},
	pages = {31--61}
}

@article{barrington-finite-1988,
	title = {Finite monoids and the fine structure of {{NC}{\textasciicircum}{\mbox{1}}}},
	volume = {35},
	number = {4},
	journal = {J. ACM},
	author = {Barrington, David A Mix and Thérien, Denis},
	year = {1988},
	pages = {941--952}
}

@article{barrington-finite-1988-1,
	title = {Finite monoids and the fine structure of {NC}{\textasciicircum}{\mbox{1}}},
	volume = {35},
	number = {4},
	journal = {J. ACM},
	author = {Barrington, David A Mix and Thérien, Denis},
	year = {1988},
	pages = {941--952}
}

@article{barrington-non-uniform-1990,
	title = {Non-{Uniform} {Automata} {Over} {Groups}},
	volume = {89},
	number = {2},
	journal = {Inf. Comput.},
	author = {Barrington, David A Mix and Straubing, Howard and Thérien, Denis},
	year = {1990},
	pages = {109--132}
}

@article{barrington-oracle-1991,
	title = {Oracle branching programs and {Logspace} versus {P}},
	volume = {95},
	number = {1},
	journal = {Inf. Comput.},
	author = {Barrington, David A Mix and McKenzie, Pierre},
	year = {1991},
	pages = {96--115}
}

@article{barrington-first-order-2005,
	title = {First-order expressibility of languages with neutral letters or: {The} {Crane} {Beach} conjecture},
	volume = {70},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Immerman, Neil and Lautemann, Clemens and Schweikardt, Nicole and Thérien, Denis},
	year = {2005},
	pages = {101--127}
}

@article{barrington-first-order-2005-1,
	title = {First-order expressibility of languages with neutral letters or: {The} {Crane} {Beach} conjecture},
	volume = {70},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Immerman, Neil and Lautemann, Clemens and Schweikardt, Nicole and Thérien, Denis},
	year = {2005},
	pages = {101--127}
}

@inproceedings{barrington-time-1994,
	title = {Time, {Hardware}, and {Uniformity}},
	booktitle = {Proceedings of the {Ninth} {Annual} {Structure} in {Complexity} {Theory} {Conference}, {Amsterdam}, {The} {Netherlands}, {June} 28 - {July} 1, 1994},
	publisher = {IEEE Computer Society},
	author = {Barrington, David A Mix and Immerman, Neil},
	year = {1994},
	pages = {176--185}
}

@article{barrington-regular-1992-1,
	title = {Regular {Languages} in {NC}1},
	volume = {44},
	url = {http://dx.doi.org/10.1016/0022-0000(92)90014-A},
	doi = {10.1016/0022-0000(92)90014-A},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Compton, Kevin J and Straubing, Howard and Thérien, Denis},
	year = {1992},
	pages = {478--499}
}

@article{barrington-bounded-width-1989,
	title = {Bounded-{Width} {Polynomial}-{Size} {Branching} {Programs} {Recognize} {Exactly} {Those} {Languages} in {NC}{\textasciicircum}1},
	volume = {38},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix},
	year = {1989},
	pages = {150--164}
}

@inproceedings{barany-regularity-2006,
	title = {Regularity {Problems} for {Visibly} {Pushdown} {Languages}},
	url = {http://dx.doi.org/10.1007/11672142\_34},
	doi = {10.1007/11672142\_34},
	booktitle = {{{STACS}} 2006, 23rd {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Marseille}, {France}, {February} 23-25, 2006, {Proceedings}},
	author = {Bárány, Vince and Löding, Christof and Serre, Olivier and Vince, B and Serre, Olivier},
	year = {2006},
	pages = {420--431},
	file = {Vince, Serre - Unknown - Regularity Problems for Visibly Pushdown Languages:/home/user/Zotero/storage/277G86J7/Vince, Serre - Unknown - Regularity Problems for Visibly Pushdown Languages.pdf:application/pdf}
}

@phdthesis{barany-automatic-2007,
	title = {Automatic {Presentations} of {Infinite} {Structures}},
	author = {Barany, Vince},
	year = {2007},
	file = {Barany - Unknown - Automatic Presentations of Infinite Structures:/home/user/Zotero/storage/B6IEBCAF/Barany - Unknown - Automatic Presentations of Infinite Structures.pdf:application/pdf}
}

@article{barrington-superlinear-1995,
	title = {Superlinear {Lower} {Bounds} for {Bounded}-{Width} {Branching} {Programs}},
	volume = {50},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Straubing, Howard},
	year = {1995},
	pages = {374--381}
}

@article{barrington-uniformity-1990,
	title = {On {{U}}niformity within {{NC}{\textasciicircum}1}},
	volume = {41},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Immerman, Neil and Straubing, Howard},
	year = {1990},
	pages = {274--306}
}

@article{barrington-uniformity-1990-1,
	title = {On {Uniformity} within {NC}{\textasciicircum}1},
	volume = {41},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Immerman, Neil and Straubing, Howard},
	year = {1990},
	pages = {274--306}
}

@inproceedings{barrington-crane-2001,
	title = {The {Crane} {Beach} {Conjecture}},
	booktitle = {{LICS}},
	author = {Barrington, David A Mix and Immerman, Neil and Lautemann, Clemens and Schweikardt, Nicole and Thérien, Denis},
	year = {2001},
	pages = {187--196}
}

@article{barrington-first-order-2005-2,
	title = {First-order expressibility of languages with neutral letters or: {The} {Crane} {Beach} conjecture},
	volume = {70},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix and Immerman, Neil and Lautemann, Clemens and Schweikardt, Nicole and Thérien, Denis},
	year = {2005},
	pages = {101--127}
}

@article{barrington-bounded-width-1989-1,
	title = {Bounded-{Width} {Polynomial}-{Size} {Branching} {Programs} {Recognize} {Exactly} {Those} {Languages} in {{NC}{\textasciicircum}1}},
	volume = {38},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Barrington, David A Mix},
	year = {1989},
	pages = {150--164}
}

@article{barrington-uniformity-1990-2,
	title = {On uniformity within {NC} 1},
	url = {http://www.sciencedirect.com/science/article/pii/002200009090022D},
	journal = {Journal of Computer and …},
	author = {Barrington, DAM and Immerman, N and Straubing, H},
	year = {1990},
	pages = {1--34},
	file = {Barrington, Immerman, Straubing - 1990 - On uniformity within NC 1:/home/user/Zotero/storage/GNRPDZ3X/Barrington, Immerman, Straubing - 1990 - On uniformity within NC 1.pdf:application/pdf}
}

@inproceedings{barrington-bounded-width-1986,
	title = {Bounded-width polynomial-size branching programs recognize exactly those languages in {NC} 1},
	isbn = {0-89791-193-8},
	url = {http://portal.acm.org/citation.cfm?doid=12130.12131},
	doi = {10.1145/12130.12131},
	booktitle = {Proceedings of the eighteenth annual {ACM} symposium on {Theory} of computing - {STOC} '86},
	publisher = {ACM Press},
	author = {Barrington, D A},
	year = {1986},
	pages = {1--5},
	file = {Barrington - 1986 - Bounded-width polynomial-size branching programs recognize exactly those languages in NC 1:/home/user/Zotero/storage/PMEDAHTA/Barrington - 1986 - Bounded-width polynomial-size branching programs recognize exactly those languages in NC 1.pdf:application/pdf}
}

@inproceedings{barany-regularity-2006-1,
	title = {Regularity {Problems} for {Visibly} {Pushdown} {Languages}},
	url = {http://dx.doi.org/10.1007/11672142\_34},
	doi = {10.1007/11672142\_34},
	booktitle = {{{STACS}} 2006, 23rd {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Marseille}, {France}, {February} 23-25, 2006, {Proceedings}},
	author = {Bárány, Vince and Löding, Christof and Serre, Olivier},
	year = {2006},
	pages = {420--431}
}

@article{barany-finite-2012,
	title = {Finite satisfiability for guarded fixpoint logic},
	volume = {112},
	doi = {10.1016/j.ipl.2012.02.005},
	abstract = {The finite satisfiability problem for guarded fixpoint logic is decidable and complete for 2ExpTime (resp. ExpTime for formulas of bounded width). © 2012 Elsevier B.V. © 2012 Elsevier B.V. All rights reserved.},
	journal = {Information Processing Letters},
	author = {Bárány, Vince and Bojańczyk, Mikołaj},
	year = {2012},
	keywords = {Finite satisfiability, Formal methods, Guarded fixpoint logic, Guarded fragment},
	pages = {371--375},
	file = {journalsiplBaranyB12:/home/user/Zotero/storage/R3U9TTXR/journalsiplBaranyB12.pdf:application/pdf}
}

@article{bao-quantum-2015,
	title = {Quantum voting and violation of {Arrow}’s {Impossibility} {Theorem}},
	author = {Bao, Ning and Halpern, Nicole Yunger},
	year = {2015},
	pages = {1--11},
	file = {1501.00458v1:/home/user/Zotero/storage/QEKNNF83/1501.00458v1.pdf:application/pdf}
}

@article{banaschewski-t-1983,
	title = {T h e {Birkllofl} {T} h e o r e m for varieties of finite algebras},
	volume = {17},
	issn = {0303604395},
	author = {Banaschewski, B},
	year = {1983},
	pages = {360--368},
	file = {Banaschewski - 1983 - T h e Birkllofl T h e o r e m for varieties of finite algebras:/home/user/Zotero/storage/ET4U2EP6/Banaschewski - 1983 - T h e Birkllofl T h e o r e m for varieties of finite algebras.pdf:application/pdf}
}

@article{baker-relativizations-1975,
	title = {Relativizations of the {{P}} =? {{NP}} {Question}},
	volume = {4},
	url = {http://dx.doi.org/10.1137/0204037},
	doi = {10.1137/0204037},
	number = {4},
	journal = {{SIAM} J. Comput.},
	author = {Baker, Theodore P and Gill, John and Solovay, Robert},
	year = {1975},
	pages = {431--442}
}

@article{baker-tree-1978,
	title = {Tree {Transducers} and {Tree} {Languages}},
	volume = {37},
	url = {http://dx.doi.org/10.1016/S0019-9958(78)90538-7},
	doi = {10.1016/S0019-9958(78)90538-7},
	number = {3},
	journal = {Information and Control},
	author = {Baker, Brenda S},
	year = {1978},
	pages = {241--266}
}

@inproceedings{baeten-theoretical-2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Theoretical {Computer} {Science} - 7th {{IFIP}} {{TC}} 1/{WG} 2.2 {International} {Conference}, {{TCS}} 2012, {Amsterdam}, {The} {Netherlands}, {September} 26-28, 2012. {Proceedings}},
	volume = {7604},
	isbn = {978-3-642-33474-0},
	url = {http://dx.doi.org/10.1007/978-3-642-33475-7},
	doi = {10.1007/978-3-642-33475-7},
	publisher = {Springer},
	editor = {Baeten, Jos C M and Ball, Thomas and de Boer, Frank S},
	year = {2012}
}

@article{badban-semi-linear-2010,
	title = {Semi-linear {Parikh} images of regular expressions via reduction},
	volume = {6281 LNCS},
	issn = {364215154X},
	doi = {10.1007/978-3-642-15155-2\_57},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Badban, Bahareh and Torabi Dashti, Mohammad},
	year = {2010},
	pages = {653--664},
	file = {par-ext:/home/user/Zotero/storage/ZP7HN8BN/par-ext.pdf:application/pdf}
}

@article{audibert-combining-2007,
	title = {Combining {PAC}-{Bayesian} and {Generic} {Chaining} {Bounds}},
	volume = {8},
	author = {Audibert, Jean-yves},
	year = {2007},
	keywords = {generalization error bounds, pac-bayes theorems, statistical learning theory},
	pages = {863--889},
	file = {Audibert - 2007 - Combining PAC-Bayesian and Generic Chaining Bounds:/home/user/Zotero/storage/XCR7UA9Z/Audibert - 2007 - Combining PAC-Bayesian and Generic Chaining Bounds.pdf:application/pdf}
}

@inproceedings{babai-proceedings-2004,
	title = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {Chicago}, {IL}, {USA}, {June} 13-16, 2004},
	isbn = {1-58113-852-0},
	booktitle = {{STOC}},
	publisher = {ACM},
	editor = {Babai, László},
	year = {2004}
}

@article{awodey-category-nodate,
	title = {category theory},
	author = {{Awodey}},
	file = {[Steve_Awodey]_Category_Theory_(Oxford_Logic_Guide(BookZZ.org):/home/user/Zotero/storage/AC4JWKP3/[Steve_Awodey]_Category_Theory_(Oxford_Logic_Guide(BookZZ.org).pdf:application/pdf}
}

@book{arora-computational-2009,
	title = {Computational {Complexity} -- {A} {Modern} {Approach}},
	isbn = {978-0-521-42426-4},
	publisher = {Cambridge University Press},
	author = {Arora, Sanjeev and Barak, Boaz},
	year = {2009}
}

@article{areces-characterization-2014,
	title = {Characterization, definability and separation via saturated models},
	volume = {537},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S030439751400173X},
	doi = {10.1016/j.tcs.2014.02.047},
	journal = {Theoretical Computer Science},
	author = {Areces, Carlos and Carreiro, Facundo and Figueira, Santiago},
	year = {2014},
	pages = {72--86},
	file = {1-s2.0-S030439751400173X-main:/home/user/Zotero/storage/X3ZGBMZ4/1-s2.0-S030439751400173X-main.pdf:application/pdf}
}

@article{apr-model-2015,
	title = {The model checking fingerprints of {CTL} operators},
	author = {Apr, L O},
	year = {2015},
	pages = {25--27},
	file = {Apr - 2015 - The model checking fingerprints of CTL operators:/home/user/Zotero/storage/4S4HJID7/Apr - 2015 - The model checking fingerprints of CTL operators.pdf:application/pdf}
}

@article{apr-testable-nodate,
	title = {testable and unambiguous languages},
	author = {Apr, F L},
	file = {1304.6734v1:/home/user/Zotero/storage/98JTA5M2/1304.6734v1.pdf:application/pdf}
}

@article{ambainis-automata-2015,
	title = {Automata and {Quantum} {Computing}},
	url = {http://arxiv.org/abs/1507.01988},
	abstract = {Quantum computing is a new model of computation, based on quantum physics. Quantum computers can be exponentially faster than conventional computers for problems such as factoring. Besides full-scale quantum computers, more restricted models such as quantum versions of finite automata have been studied. In this paper, we survey various models of quantum finite automata and their properties. We also provide some open questions and new directions for researchers.},
	author = {Ambainis, Andris and Yakaryilmaz, Abuzer},
	year = {2015},
	keywords = {Computational complexity, bounded error, decidability and undecidability, nondeterminism, probabilistic finite automata, quantum finite automata, succinctness, unbounded error},
	pages = {1--32},
	file = {1507.01988v1:/home/user/Zotero/storage/FNA9VDKS/1507.01988v1.pdf:application/pdf}
}

@inproceedings{alur-visibly-2004,
	title = {Visibly pushdown languages},
	isbn = {1-58113-852-0},
	booktitle = {{STOC}},
	publisher = {ACM},
	author = {Alur, Rajeev and Madhusudan, P},
	editor = {Babai, László},
	year = {2004},
	pages = {202--211}
}

@inproceedings{alur-congruences-2005,
	title = {Congruences for {Visibly} {Pushdown} {Languages}},
	booktitle = {Automata, {Languages} and {Programming}, 32nd {International} {Colloquium}, {ICALP} 2005, {Lisbon}, {Portugal}, {July} 11-15, 2005, {Proceedings}},
	publisher = {Springer},
	author = {Alur, Rajeev and Kumar, Viraj and Madhusudan, P and Viswanathan, Mahesh},
	year = {2005},
	pages = {1102--1114}
}

@inproceedings{alur-congruences-2005-1,
	title = {Congruences for {{V}}isibly {{P}}ushdown {{L}}anguages},
	booktitle = {Proc. of {ICALP} (2005)},
	publisher = {Springer},
	author = {Alur, R and Kumar, V and Madhusudan, P and Viswanathan, M},
	year = {2005},
	pages = {1102--1114}
}

@article{almeida--nodate,
	title = {{ÐÐ} ¬ Ò Ø},
	author = {{Almeida} and {Escada}},
	file = {10.1.1.32.7915:/home/user/Zotero/storage/5WQERA2G/10.1.1.32.7915.pdf:application/pdf}
}

@article{almeida-free-1995,
	title = {Free profinite semigroups over semidirect products},
	volume = {39},
	journal = {Russian Mathem.},
	author = {Almeida, Jorge and Weil, Pascal},
	year = {1995},
	pages = {1--28}
}

@article{almeida-decidability-2000,
	title = {On the decidability of iterated semidirect products with applications to complexity},
	volume = {80},
	journal = {Proc. London Math. Soc. (3)},
	author = {Almeida, Jorge and Steinberg, Benjamin},
	year = {2000},
	pages = {50--74},
	file = {10.1.1.46.4951:/home/user/Zotero/storage/FBTDFB2B/10.1.1.46.4951.pdf:application/pdf}
}

@article{almeida-profinite-1998,
	title = {Profinite categories and semidirect products},
	volume = {123},
	doi = {10.1016/S0022-4049(96)00083-7},
	abstract = {After developing a theory of implicit operations and proving an analogue of Reiterman's theorem for categories, this paper addresses two complementary questions for semidirect products and two-sided semidirect products of pseudovarieties of semigroups: to determine when a pseudoidentity is valid in it, and to find a basis of pseudoidentities. The first question involves looking into the structure of relatively free profinite objects whereas, for the second question, a general approach is presented which is sufficiently powerful to allow the calculation of many semidirect products. A systematic translation of bases of pseudoidentities of pseudovarieties of categories into characterizations of semidirect products of pseudovarieties of semigroups is given. The latter characterizations, of a syntactic nature, are not effective but may in many cases be reduced to effective characterizations. Several known results are derived as examples â€” including a syntactic proof of a generalization of the Delay Theorem â€” and further new applications are obtained using these techniques},
	journal = {Journal of Pure and Applied Algebra},
	author = {Almeida, J. and Weil, Pascal},
	year = {1998},
	pages = {1--50},
	file = {Almeida, Weil - 1998 - Profinite categories and semidirect products:/home/user/Zotero/storage/7VA56GJS/Almeida, Weil - 1998 - Profinite categories and semidirect products.pdf:application/pdf}
}

@article{almeida-globals-2004,
	title = {the {Globals} of {Some} {Subpseudovarieties} of},
	volume = {14},
	doi = {10.1142/S0218196704001967},
	number = {OCTOBER},
	journal = {International Journal of Algebra and Computation},
	author = {Almeida, J. and Escada, a.},
	year = {2004},
	pages = {525--549},
	file = {00463523715c92b0c5000000(1):/home/user/Zotero/storage/CIBW77IE/00463523715c92b0c5000000(1).pdf:application/pdf}
}

@article{alur-visibly-2005,
	title = {Visibly {Pushdown} {Languages}},
	author = {Alur, Rajeev and Madhusudan, P.},
	year = {2005},
	pages = {202--211},
	file = {Alur, Madhusudan - 2005 - Visibly Pushdown Languages:/home/user/Zotero/storage/HZ8JGXMI/Alur, Madhusudan - 2005 - Visibly Pushdown Languages.pdf:application/pdf}
}

@inproceedings{alur-visibly-2004-1,
	title = {Visibly pushdown languages},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {Chicago}, {IL}, {USA}, {June} 13-16, 2004},
	publisher = {ACM},
	author = {Alur, Rajeev and Madhusudan, P},
	year = {2004},
	pages = {202--211}
}

@article{alur-marrying-nodate,
	title = {Marrying {Words} and {Trees}},
	issn = {9781595936851},
	author = {Alur, Rajeev},
	keywords = {xml, nested words, pushdown automata, tree automata},
	file = {Alur - Unknown - Marrying Words and Trees:/home/user/Zotero/storage/9KCR7MS7/Alur - Unknown - Marrying Words and Trees.pdf:application/pdf}
}

@inproceedings{alur-congruences-2005-2,
	title = {Congruences for {{V}}isibqly {{P}}ushdown {{L}}anguages},
	booktitle = {Proc. of {ICALP} (2005)},
	publisher = {Springer},
	author = {Alur, R and Kumar, V and Madhusudan, P and Viswanathan, M},
	year = {2005},
	pages = {1102--1114}
}

@article{almeida-globals-2007,
	title = {globals of pseudovarieties ordered},
	doi = {10.1051/ita},
	number = {July 2015},
	journal = {Information Security},
	author = {{Almeida} and {Escada}},
	year = {2007},
	pages = {315--336},
	file = {download(3):/home/user/Zotero/storage/B74WUSTA/download(3).pdf:application/pdf}
}

@article{almeida-automata-theoretic-2007,
	title = {An automata-theoretic approach to the word problem for ?? -terms over {R}},
	volume = {370},
	doi = {10.1016/j.tcs.2006.10.019},
	abstract = {This paper studies the pseudovariety R of all finite R-trivial semigroups. We give a representation of pseudowords over R by infinite trees, called R-trees. Then we show that a pseudoword is an ??-term if and only if its associated tree is regular (i.e. it can be folded into a finite graph), or equivalently, if the ??-term has a finite number of tails. We give a linear algorithm to compute a compact representation of the R-tree for ??-terms, which yields a linear solution of the word problem for ??-terms over R. We finally exhibit a basis for the ??-variety generated by R and we show that there is no finite basis. Several results can be compared to recent work of Bloom and Choffrut on long words. ?? 2006 Elsevier Ltd. All rights reserved.},
	journal = {Theoretical Computer Science},
	author = {Almeida, Jorge and Zeitoun, Marc},
	year = {2007},
	keywords = {Automata minimization, Identity basis, Omega-term, Pseudovariety, Pseudoword, Semigroup, Word problem},
	pages = {131--169},
	file = {1-s2.0-S0304397506007869-main:/home/user/Zotero/storage/EHB9KA3P/1-s2.0-S0304397506007869-main.pdf:application/pdf}
}

@article{almeida-profinite-1998-1,
	title = {Profinite {Categories} and {Semidirect} {Products}},
	volume = {123},
	journal = {J. Pure and Applied Algebra},
	author = {Almeida, Jorge and Weil, Pascal},
	year = {1998},
	pages = {1--50}
}

@book{almeida-finite-1995,
	title = {Finite {Semigroups} and {Universal} {Algebra}},
	publisher = {World Scientific, Singapore},
	author = {Almeida, Jorge},
	year = {1995}
}

@article{almeida-globals-2004-1,
	title = {the {Globals} of {Some} {Subpseudovarieties} of},
	volume = {14},
	doi = {10.1142/S0218196704001967},
	number = {OCTOBER},
	journal = {International Journal of Algebra and Computation},
	author = {Almeida, J. and Escada, a.},
	year = {2004},
	pages = {525--549},
	file = {00463523715c92b0c5000000:/home/user/Zotero/storage/UA288TQI/00463523715c92b0c5000000.pdf:application/pdf}
}

@article{almeida-pro-nodate,
	title = {Pro nite categories and semidirect products},
	author = {Almeida, J},
	keywords = {and phrases, derived category, implicit operation, pro nite category, pro nite semigroup, pseudoidentity, relational morphism, semidirect product, sided semidirect product, two-},
	file = {Almeida - Unknown - Pro nite categories and semidirect products:/home/user/Zotero/storage/5N9QSF62/Almeida - Unknown - Pro nite categories and semidirect products.pdf:application/pdf}
}

@article{almeida-pseudovariety-1997,
	title = {The pseudovariety {J} is hyperdecidable},
	author = {Almeida, J},
	year = {1997},
	file = {HyperJ2:/home/user/Zotero/storage/CKV6MZ8U/HyperJ2.pdf:application/pdf}
}

@inproceedings{allender-amplifying-2008,
	title = {Amplifying {Lower} {Bounds} by {Means} of {Self}-{Reducibility}},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Allender, Eric and Koucký, Michal},
	year = {2008},
	pages = {31--40}
}

@book{almeida-finite-nodate,
	title = {Finite semigroups and universal algebra},
	author = {{Almeida}},
	file = {[Jorge_Almeida]_Finite_Semigroups_and_Universal_Al(BookZZ.org):/home/user/Zotero/storage/5QJS5QZD/[Jorge_Almeida]_Finite_Semigroups_and_Universal_Al(BookZZ.org).pdf:application/pdf}
}

@article{allender-notitle-2014,
	volume = {10},
	url = {http://www.theoryofcomputing.org/articles/v010a008},
	doi = {10.4086/toc.2014.v010a008},
	number = {8},
	journal = {Theory of Computing},
	author = {Allender, Eric and Lange, Klaus-Joern},
	year = {2014},
	keywords = {and phrases, nondeterminism, circuit complexity, complexity classes, complexity theory, reversibility, symmetry},
	pages = {199--215},
	file = {v010a008:/home/user/Zotero/storage/4K6KRE7P/v010a008.pdf:application/pdf}
}

@article{allender-p-uniform-1989,
	title = {P-uniform circuit complexity},
	volume = {36},
	number = {4},
	journal = {J. ACM},
	author = {Allender, Eric},
	year = {1989},
	pages = {912--928}
}

@article{albin-math-2012,
	title = {Math 524 – {Linear} {Analysis} on {Manifolds}},
	author = {Albin, Pierre},
	year = {2012},
	file = {Albin - 2012 - Math 524 – Linear Analysis on Manifolds:/home/user/Zotero/storage/QSD4XFBP/Albin - 2012 - Math 524 – Linear Analysis on Manifolds.pdf:application/pdf}
}

@inproceedings{albers-automata-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automata, {Languages} and {Programming}, 36th {Internatilonal} {Collogquium}, {{ICALP}} 2009, {Rhodes}, greece, {July} 5-12, 2009, {Proceedings}, {Part} {{II}}},
	volume = {5556},
	isbn = {978-3-642-02929-5},
	url = {http://dx.doi.org/10.1007/978-3-642-02930-1},
	doi = {10.1007/978-3-642-02930-1},
	publisher = {Springer},
	editor = {Albers, Susanne and Marchetti-Spaccamela, Alberto and Matias, Yossi and Nikoletseas, Sotiris E and Thomas, Wolfgang},
	year = {2009}
}

@article{ajtai-^1-1-formulae-1983,
	title = {{Σ{\textasciicircum}1\_1}-formulae on finite structures},
	volume = {24},
	journal = {Ann. Pure Appl. Logic},
	author = {Ajtai, Miklós},
	year = {1983},
	pages = {1--48}
}

@article{aho-nested-1969,
	title = {Nested {Stack} {Automata}},
	volume = {16},
	number = {3},
	journal = {J. ACM},
	author = {Aho, Alfred V},
	year = {1969},
	pages = {383--406}
}

@article{aho-indexed-1968,
	title = {Indexed {Grammars} - {An} {Extension} of {Context}-{Free} {Grammars}},
	volume = {15},
	number = {4},
	journal = {J. ACM},
	author = {Aho, Alfred V},
	year = {1968},
	pages = {647--671}
}

@article{adamek-syntactic-nodate,
	title = {Syntactic {Monoids} in a {Category}},
	author = {Adámek, Jiří and Milius, Stefan and Urbat, Henning},
	keywords = {algebra, algebraic automata theory, and phrases syntactic monoid, coalgebra, commutative variety, duality, symmetric monoidal closed category, transition monoid},
	pages = {1--26},
	file = {1504.02694v2:/home/user/Zotero/storage/7NZP6IZM/1504.02694v2.pdf:application/pdf}
}

@inproceedings{aceto-automata-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automata, {Languages} and {Programming}, 35th {International} {Colloquium}, {{ICALP}} 2008, {Reykjavik}, {Iceland}, {July} 7-11, 2008, {Proceedings}, {Part} {{II}} - {Track} {{B}:} {Logic}, {Semantics}, and {Theory} of {Programming} {\&} {Track} {{C}:} {Security} and {Cryptography} {Foundations}},
	volume = {5126},
	isbn = {978-3-540-70582-6},
	publisher = {Springer},
	editor = {Aceto, Luca and Damgård, Ivan and Goldberg, Leslie Ann and Halldórsson, Magnús M and Ingólfsdóttir, Anna and Walukiewicz, Igor},
	year = {2008}
}

@article{academy-explosive-2014,
	title = {{EXPLOSIVE} {POPULATION} {GROWTH} {IN} {TROPICAL} {AFRICA} : {CRUCIAL} {OMISSION} {IN} {DEVELOPMENT} {FORECASTS} — {EMERGING} {RISKS} {AND} {WAY} {OUT}},
	doi = {10.1080/02604027.2014.894868},
	author = {Academy, Russian Presidential},
	year = {2014},
	keywords = {development fertility, education, malthusian scenarios, population pressure, secondary, tanzania, tropical africa},
	pages = {120--139},
	file = {Academy - 2014 - EXPLOSIVE POPULATION GROWTH IN TROPICAL AFRICA CRUCIAL OMISSION IN DEVELOPMENT FORECASTS — EMERGING RISKS AND WAY OU:/home/user/Zotero/storage/V2DBIWW4/Academy - 2014 - EXPLOSIVE POPULATION GROWTH IN TROPICAL AFRICA CRUCIAL OMISSION IN DEVELOPMENT FORECASTS — EMERGING RISKS AND WAY OU.pdf:application/pdf}
}

@article{aaronson-algebrization:-2009,
	title = {Algebrization: {{A}} {New} {Barrier} in {Complexity} {Theory}},
	volume = {1},
	url = {http://doi.acm.org/10.1145/1490270.1490272},
	doi = {10.1145/1490270.1490272},
	number = {1},
	journal = {TOCT},
	author = {Aaronson, Scott and Wigderson, Avi},
	year = {2009}
}

@article{aaronson-et-al.-complexity-nodate,
	title = {Complexity {Zoo}},
	author = {Aaronson et al., Scott}
}

@article{allender-minimum-2015,
	title = {The {Minimum} {Oracle} {Circuit} {Size} {Problem}},
	number = {Stacs},
	author = {Allender, Eric and Holden, Dhiraj and Kabanets, Valentine},
	year = {2015},
	keywords = {and phrases kolmogorov complexity, minimum circuit size problem, np-, pspace},
	pages = {21--33},
	file = {Allender, Holden, Kabanets - 2015 - The Minimum Oracle Circuit Size Problem:/home/user/Zotero/storage/WEVSRZKS/Allender, Holden, Kabanets - 2015 - The Minimum Oracle Circuit Size Problem.pdf:application/pdf}
}

@article{allender-uniform-1994,
	title = {A {Uniform} {Circuit} {Lower} {Bound} for the {Permanent}},
	volume = {23},
	number = {5},
	journal = {SIAM J. Comput.},
	author = {Allender, Eric and Gore, Vivek},
	year = {1994},
	pages = {1026--1049}
}

@article{alfy-overview-2001,
	title = {Overview},
	author = {Alfy, P P},
	year = {2001},
	file = {Alfy - 2001 - Overview:/home/user/Zotero/storage/N94JEKKM/Alfy - 2001 - Overview.pdf:application/pdf}
}

@article{ajtai-first-order-1989,
	title = {First-{Order} {Definability} on {Finite} {Structures}},
	volume = {45},
	number = {3},
	journal = {Ann. Pure Appl. Logic},
	author = {Ajtai, Miklós},
	year = {1989},
	pages = {211--225}
}

@article{agrawal-reductions-1998,
	title = {Reductions in {Circuit} {Complexity}: {An} {Isomorphism} {Theorem} and a {Gap} {Theorem}},
	volume = {57},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022000098915835},
	doi = {10.1006/jcss.1998.1583},
	number = {2},
	journal = {Journal of Computer and System Sciences},
	author = {Agrawal, Manindra and Allender, Eric and Rudich, Steven},
	month = oct,
	year = {1998},
	pages = {127--143},
	file = {Agrawal, Allender, Rudich - 1998 - Reductions in Circuit Complexity An Isomorphism Theorem and a Gap Theorem:/home/user/Zotero/storage/KXT4Q92M/Agrawal, Allender, Rudich - 1998 - Reductions in Circuit Complexity An Isomorphism Theorem and a Gap Theorem.pdf:application/pdf}
}

@article{adamek-syntactic-nodate-1,
	title = {Syntactic {Monoids} in a {Category}},
	author = {Adámek, Jiri and Milius, Stefan and Urbat, Henning},
	keywords = {algebra, algebraic automata theory, and phrases syntactic monoid, coalgebra, duality, symmetric monoidal closed category, transition monoid, entropic variety},
	pages = {1--26},
	file = {Adámek, Milius, Urbat - Unknown - Syntactic Monoids in a Category:/home/user/Zotero/storage/PNJ37ATJ/Adámek, Milius, Urbat - Unknown - Syntactic Monoids in a Category.pdf:application/pdf}
}

@article{aaronson-notitle-2013,
	volume = {9},
	url = {http://www.theoryofcomputing.org/articles/v009a009},
	doi = {10.4086/toc.2013.v009a009},
	number = {9},
	journal = {Theory of Computing},
	author = {Aaronson, Scott and Christiano, Paul},
	year = {2013},
	keywords = {and phrases, electronic cash, electronic cash, multivariate polynomials, quantum, multivariate polynomials, quantum, quantum cryptography},
	pages = {349--401},
	file = {v009a009:/home/user/Zotero/storage/DJC6IG6X/v009a009.pdf:application/pdf}
}

@article{aaronson-forrelation-2013,
	title = {Forrelation : {A} {Problem} that {Optimally} {Separates} {Quantum} from {Classical} {Computing}},
	volume = {600700},
	author = {Aaronson, Scott},
	year = {2013},
	pages = {1--60},
	file = {for:/home/user/Zotero/storage/HMF3GJUJ/for.pdf:application/pdf}
}

@article{aaronson-et-al.-complexity-nodate-1,
	title = {Complexity {Zoo}},
	author = {Aaronson et al., Scott}
}

@article{noauthor-abpr.pdf-nodate,
	title = {{ABPR}.pdf},
	file = {Unknown - Unknown - ABPR.pdf:/home/user/Zotero/storage/JIKKDIE8/Unknown - Unknown - ABPR.pdf.pdf:application/pdf}
}

@article{noauthor-aps-doi10.11032fphysrev.123.1511.pdf-nodate,
	title = {aps\_doi{\textasciitilde}10.1103\%2FPhysRev.123.1511.pdf},
	file = {Attachment:/home/user/Zotero/storage/VHFJSCER/Unknown - Unknown - aps_doi~10.1103pdfFPhysRev.123.1511.pdf.%3:application/pdf}
}

@article{noauthor-yh150.pdf-nodate,
	title = {yh150.pdf},
	file = {Unknown - Unknown - yh150.pdf:/home/user/Zotero/storage/X6U2JGPA/Unknown - Unknown - yh150.pdf.pdf:application/pdf}
}

@article{xiao-berry-2009,
	title = {Berry {Phase} {Effects} on {Electronic} {Properties}},
	url = {http://arxiv.org/abs/0907.2021},
	doi = {10.1103/RevModPhys.82.1959},
	abstract = {Ever since its discovery, the Berry phase has permeated through all branches of physics. Over the last three decades, it was gradually realized that the Berry phase of the electronic wave function can have a profound effect on material properties and is responsible for a spectrum of phenomena, such as ferroelectricity, orbital magnetism, various (quantum/anomalous/spin) Hall effects, and quantum charge pumping. This progress is summarized in a pedagogical manner in this review. We start with a brief summary of necessary background, followed by a detailed discussion of the Berry phase effect in a variety of solid state applications. A common thread of the review is the semiclassical formulation of electron dynamics, which is a versatile tool in the study of electron dynamics in the presence of electromagnetic fields and more general perturbations. Finally, we demonstrate a re-quantization method that converts a semiclassical theory to an effective quantum theory. It is clear that the Berry phase should be added as a basic ingredient to our understanding of basic material properties.},
	author = {Xiao, Di and Chang, Ming-Che and Niu, Qian},
	month = jul,
	year = {2009},
	pages = {48--48},
	file = {Xiao, Chang, Niu - 2009 - Berry Phase Effects on Electronic Properties:/home/user/Zotero/storage/UHNGPJ39/Xiao, Chang, Niu - 2009 - Berry Phase Effects on Electronic Properties.2021v1:application/pdf}
}

@article{wood-new-2014,
	title = {A {New} {Approach} to {Probabilistic} {Programming} {Inference}},
	volume = {33},
	journal = {Aistats},
	author = {Wood, Frank and Meent, Jan Willem Van De},
	year = {2014},
	pages = {1024--1032},
	file = {1507.00996v1:/home/user/Zotero/storage/BD84SJ6I/1507.00996v1.pdf:application/pdf;1507.00996v2:/home/user/Zotero/storage/ZC9CAAKM/1507.00996v2.pdf:application/pdf}
}

@article{xiao-berry-2010,
	title = {Berry phase effects on electronic properties},
	volume = {82},
	doi = {10.1103/RevModPhys.82.1959},
	number = {3},
	journal = {Rev. Mod. Phys.},
	author = {Xiao, D and Chang, M.-C. and Niu, Q},
	year = {2010},
	keywords = {Dielectric piezoelectric ferroelectric and antife, General formulation of transport theory, Quantum Hall effects, Spin-orbit coupling Zeeman and Stark splitting Jah},
	pages = {1959--2007}
}

@article{wocjan-computational-2003,
	title = {Computational {Power} of {Hamiltonians} in {Quantum} {Computing}},
	author = {Wocjan, Pawel},
	year = {2003},
	file = {dissertation:/home/user/Zotero/storage/87KN34WT/dissertation.pdf:application/pdf;Wocjan - 2003 - Computational Power of Hamiltonians in Quantum Computing:/home/user/Zotero/storage/HVXCKMMS/Wocjan - 2003 - Computational Power of Hamiltonians in Quantum Computing.pdf:application/pdf}
}

@article{wikipedia-contributors-renormalization-2012,
	title = {Renormalization group},
	url = {http://en.wikipedia.org/w/index.php?title=Renormalization\_group&oldid=490299330},
	abstract = {In theoretical physics, the renormalization group (RG) refers to a mathematical apparatus that allows systematic investigation of the changes of a physical system as viewed at different distance scales. In particle physics, it reflects the changes in the underlying force laws (codified in a quantum field theory) as the energy scale at which physical processes occur varies, energy/momentum and resolution distance scales being effectively conjugate under the uncertainty principle (cf. Compton wavelength).},
	author = {{Wikipedia contributors}},
	year = {2012},
	file = {Wikipedia contributors - 2012 - Renormalization group:/home/user/Zotero/storage/AHK4PSE5/Wikipedia contributors - 2012 - Renormalization group.pdf:application/pdf}
}

@article{vanderbilt-no-1993,
	title = {No {Title}},
	volume = {47},
	number = {3},
	author = {Vanderbilt, David},
	year = {1993},
	pages = {1651--1654},
	file = {Vanderbilt - 1993 - No Title:/home/user/Zotero/storage/PB2WDFBF/Vanderbilt - 1993 - No Title.pdf:application/pdf}
}

@article{thouless-topological-nodate,
	title = {topological},
	author = {{Thouless}},
	file = {Thouless - Unknown - topological:/home/user/Zotero/storage/BW84NAU5/Thouless - Unknown - topological.pdf:application/pdf}
}

@article{vouga-design-2012,
	title = {Design of self-supporting surfaces},
	url = {http://dl.acm.org/citation.cfm?id=2185583},
	journal = {ACM Transactions on …},
	author = {Vouga, Etienne and Höbinger, M},
	year = {2012},
	keywords = {architectural geome-, discrete differential geometry, discrete laplacians, grams, isotropic geometry, mean curvature, reciprocal force dia-, self - supporting masonry, thrust networks, try},
	file = {selfsupporting:/home/user/Zotero/storage/456GQJWE/selfsupporting.pdf:application/pdf}
}

@article{thouless-quantization-1983,
	title = {quantization particle transport},
	volume = {27},
	number = {10},
	author = {{Thouless}},
	year = {1983},
	pages = {6083--6087},
	file = {Thouless - 1983 - quantization particle transport:/home/user/Zotero/storage/VF82SW57/Thouless - 1983 - quantization particle transport.pdf:application/pdf}
}

@book{teufel-adiabatic-2003,
	title = {Adiabatic {Perturbation} {Theory} in {Quantum} {Dynamics}},
	publisher = {Springer},
	author = {Teufel, Stefan},
	year = {2003}
}

@article{sundaram-wave-packet-1999,
	title = {Wave-packet dynamics in slowly perturbed crystals: {Gradient} corrections and {Berry}-phase effects},
	volume = {59},
	url = {http://arxiv.org/abs/cond-mat/9908003v1},
	number = {23},
	journal = {Phys. Rev. B},
	author = {Sundaram, Ganesh and Niu, Qian},
	month = jul,
	year = {1999},
	pages = {14915--14925},
	file = {Sundaram, Niu - 1999 - Wave-packet dynamics in slowly perturbed crystals Gradient corrections and Berry-phase effects:/home/user/Zotero/storage/EGDNQ9FZ/Sundaram, Niu - 1999 - Wave-packet dynamics in slowly perturbed crystals Gradient corrections and Berry-phase effects.pdf:application/pdf}
}

@article{stiepan-semiclassical-2013,
	title = {Semiclassical approximations for {Hamiltonians} with operator-valued symbols},
	volume = {320},
	journal = {Commun. Math. Phys.},
	author = {Stiepan, H and Teufel, S},
	year = {2013},
	pages = {821--849}
}

@article{srednicki-quantum-2006,
	title = {Quantum {Field} {Theory}},
	author = {Srednicki, Mark},
	year = {2006},
	file = {Srednicki - 2006 - Quantum Field Theory:/home/user/Zotero/storage/ZZ9GF694/Srednicki - 2006 - Quantum Field Theory.pdf:application/pdf}
}

@article{stone-physics-2009,
	title = {Physics, {Topology}, {Logic} and {Computation}},
	journal = {Topology},
	author = {Stone, Ar and Baez, Jc},
	year = {2009},
	pages = {1--73},
	file = {Stone, Baez - 2009 - Physics, Topology, Logic and Computation:/home/user/Zotero/storage/QXQ6AUC8/Stone, Baez - 2009 - Physics, Topology, Logic and Computation.pdf:application/pdf}
}

@article{simon-no-1983,
	title = {No {Title}},
	volume = {51},
	number = {24},
	author = {Simon, Barry},
	year = {1983},
	pages = {2167--2170},
	file = {Simon - 1983 - No Title:/home/user/Zotero/storage/V7WGNFTJ/Simon - 1983 - No Title.pdf:application/pdf}
}

@article{simon-holonomy-1983,
	title = {Holonomy, the {Quantum} {Adiabatic} {Theorem}, and {Berry}'s {Phase}},
	volume = {51},
	number = {24},
	journal = {Physical Review Letters},
	author = {Simon, Barry},
	year = {1983},
	pages = {2167--2170}
}

@article{schulz-baldes-orbital-2013,
	title = {Orbital {Polarization} and {Magnetization} for {Independent} {Particles} in {Disordered} {Media}},
	volume = {319},
	url = {http://dx.doi.org/10.1007/s00220-012-1639-0},
	doi = {10.1007/s00220-012-1639-0},
	number = {3},
	journal = {Communications in Mathematical Physics},
	author = {Schulz-Baldes, Hermann and Teufel, Stefan},
	year = {2013},
	pages = {649--681}
}

@article{rotenberg-topological-2011,
	title = {Topological insulators: {The} dirt on topology},
	volume = {7},
	issn = {1745-2473\r1745-2481},
	doi = {10.1038/nphys1869},
	abstract = {Topological insulators have a conducting surface on which spin currents are not easily scattered, although the addition of magnetic impurities does affect electronic behaviour. But is this situation unique? Graphene comes to mind.},
	journal = {Nature Physics},
	author = {Rotenberg, Eli},
	year = {2011},
	pages = {8--10},
	file = {Rotenberg - 2011 - Topological insulators The dirt on topology:/home/user/Zotero/storage/WIJEFKG3/Rotenberg - 2011 - Topological insulators The dirt on topology.pdf:application/pdf}
}

@book{reed-methods-nodate,
	title = {Methods of {Modern} {Mathematical} {Physics}},
	publisher = {Academic Press},
	author = {Reed, Michael and Simon, Barry}
}

@article{pankov-introduction-nodate,
	title = {Introduction to {Spectral} {Theory} of {Schr} ¨ odinger {Operators}},
	author = {Pankov, A},
	file = {Pankov - Unknown - Introduction to Spectral Theory of Schr ¨ odinger Operators:/home/user/Zotero/storage/34NHHXRH/Pankov - Unknown - Introduction to Spectral Theory of Schr ¨ odinger Operators.pdf:application/pdf}
}

@article{panati-geometric-2009,
	title = {Geometric currents in piezoelectricity},
	volume = {91},
	journal = {Arch. Rat. Mech. Anal.},
	author = {Panati, G and Sparber, C and Teufel, S},
	year = {2009},
	pages = {387--422}
}

@article{ryu-topological-2010,
	title = {Topological {Insulators} and {Superconductors} from {D}-branes},
	url = {http://arxiv.org/abs/1001.0763},
	abstract = {Realization of topological insulators (TIs) and superconductors (TSCs), such as the quantum spin Hall effect and the Z\_2 topological insulator, in terms of D-branes in string theory is proposed. We establish a one-to-one correspondence between the K-theory classification of TIs/TSCs and D-brane charges. The string theory realization of TIs and TSCs comes naturally with gauge interactions, and the Wess-Zumino term of the D-branes gives rise to a gauge field theory of topological nature. This sheds light on TIs and TSCs beyond non-interacting systems, and the underlying topological field theory description thereof. In particular, our string theory realization includes the honeycomb lattice Kitaev model in two spatial dimensions, and its higher-dimensional extensions.},
	author = {Ryu, Shinsei and Takayanagi, Tadashi},
	year = {2010},
	pages = {5--5},
	file = {Ryu, Takayanagi - 2010 - Topological Insulators and Superconductors from D-branes:/home/user/Zotero/storage/MIDXGFFV/Ryu, Takayanagi - 2010 - Topological Insulators and Superconductors from D-branes.pdf:application/pdf}
}

@article{rohde--ramp-2015,
	title = {The on-ramp to the all-optical quantum information processing highway},
	volume = {349},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac7250},
	doi = {10.1126/science.aac7250},
	number = {6249},
	journal = {Science},
	author = {Rohde, P. P. and Dowling, J. P.},
	year = {2015},
	pages = {696--696},
	file = {Science-2015-Rohde-696:/home/user/Zotero/storage/2XKQB5RR/Science-2015-Rohde-696.pdf:application/pdf}
}

@article{peshkin-against-2010,
	title = {Against a proposed alternative explanation of the {Aharonov}-{Bohm} effect},
	url = {http://arxiv.org/abs/1009.1613},
	doi = {10.1088/1751-8113/43/35/354031},
	abstract = {The Aharonov-Bohm effect is understood to demonstrate that the Maxwell fields can act nonlocally in some situations. However it has been suggested from time to time that the AB effect is somehow a consequence of a local classical electromagnetic field phenomenon involving energy that is temporarily stored in the overlap between the external field and the field of which the beam particle is the source. That idea was shown in the past not to work for some models of the source of the external field. Here a more general proof is presented for the magnetic AB effect to show that the overlap energy is always compensated by another contribution to the energy of the magnetic field in such a way that the sum of the two is independent of the external flux. Therefore no such mechanism can underlie the Aharonov-Bohm effect.},
	author = {Peshkin, Murray},
	year = {2010},
	pages = {7--7},
	file = {Peshkin - 2010 - Against a proposed alternative explanation of the Aharonov-Bohm effect:/home/user/Zotero/storage/5DIT2WMG/Peshkin - 2010 - Against a proposed alternative explanation of the Aharonov-Bohm effect.pdf:application/pdf}
}

@article{panati-triviality-2007,
	title = {Triviality of {Bloch} and {Bloch}-{Dirac} {Bundles}},
	volume = {8},
	url = {http://arxiv.org/abs/math-ph/0601034v1},
	journal = {Ann. Henri Poincar{é}},
	author = {Panati, Gianluca},
	month = jan,
	year = {2007},
	pages = {995--1011},
	file = {Panati - 2006 - Triviality of Bloch and Bloch-Dirac bundles:/home/user/Zotero/storage/NAKWXVMM/Panati - 2006 - Triviality of Bloch and Bloch-Dirac bundles.pdf:application/pdf}
}

@article{littlejohn-0-1991,
	title = {=0 =0 2,},
	number = {22},
	author = {Littlejohn, Robert G and Flynn, William G},
	year = {1991},
	pages = {2839--2842},
	file = {Littlejohn, Flynn - 1991 - =0 =0 2,:/home/user/Zotero/storage/WRMDGEKX/Littlejohn, Flynn - 1991 - =0 =0 2,.pdf:application/pdf}
}

@article{niu-quantized-1985,
	title = {Quantized {Hall} conductance as a topological invariant},
	volume = {31},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.31.3372},
	doi = {10.1103/PhysRevB.31.3372},
	number = {6},
	journal = {Physical Review B},
	author = {Niu, Qian and Thouless, D. and Wu, Yong-Shi},
	month = mar,
	year = {1985},
	keywords = {Quantum Hall effect},
	pages = {3372--3377},
	file = {Niu, Thouless, Wu - 1985 - Quantized Hall conductance as a topological invariant:/home/user/Zotero/storage/M3MMT7GB/Niu, Thouless, Wu - 1985 - Quantized Hall conductance as a topological invariant.pdf:application/pdf}
}

@article{kumano-go-feynman-2004,
	title = {Feynman path integrals as analysis on path space by time slicing approximation},
	volume = {128},
	issn = {9781612091303},
	doi = {10.1016/j.bulsci.2004.01.002},
	abstract = {Using the time slicing approximation, we give a mathematically rigorous definition of Feynman path integrals for a general class of functionals on the path space. As an application, we prove the interchange with Riemann-Stieltjes integrals, the interchange with a limit, the perturbation expansion formula, the semiclassical approximation, and the fundamental theorem of calculus in Feynman path integral. © 2004 Elsevier SAS. All rights reserved.},
	journal = {Bulletin des Sciences Mathematiques},
	author = {Kumano-go, Naoto},
	year = {2004},
	keywords = {Fourier integral operator, Path integrals, Stochastic analysis},
	pages = {197--251},
	file = {Kumano-go - 2004 - Feynman path integrals as analysis on path space by time slicing approximation:/home/user/Zotero/storage/MS235AA7/Kumano-go - 2004 - Feynman path integrals as analysis on path space by time slicing approximation.pdf:application/pdf}
}

@article{kreimer-combinatorics-2000,
	title = {Combinatorics of (perturbative) {Quantum} {Field} {Theory}∗ {D}. {KREIMER},2000,paper(p47).pdf},
	author = {Kreimer, D},
	year = {2000},
	pages = {1--47},
	file = {Kreimer - 2000 - Combinatorics of (perturbative) Quantum Field Theory∗ D. KREIMER,2000,paper(p47).pdf:/home/user/Zotero/storage/75C48G8X/Kreimer - 2000 - Combinatorics of (perturbative) Quantum Field Theory∗ D. KREIMER,2000,paper(p47).pdf.pdf:application/pdf}
}

@article{king-smith-theory-1993,
	title = {Theory of {Polarization} of crystalline solids},
	volume = {B 47},
	journal = {Phys. Rev.},
	author = {King-Smith, R D and Vanderbilt, D},
	year = {1993},
	pages = {1651--1654}
}

@article{kato-adiabatic-1950,
	title = {On the {Adiabatic} {Theorem} of {Quantum} {Mechanics}},
	volume = {5},
	number = {6},
	journal = {Journal of the Physical Society of Japan},
	author = {Kato, Tosio},
	year = {1950},
	pages = {435--439}
}

@article{janzing-complexity-2000,
	title = {A {Complexity} {Measure} for {Continuous} {Time} {Quantum} {Algorithms}},
	url = {http://arxiv.org/abs/quant-ph/0009094},
	doi = {10.1103/PhysRevA.64.022301},
	abstract = {We consider unitary dynamical evolutions on n qubits caused by time dependent pair-interaction Hamiltonians and show that the running time of a parallelized two-qubit gate network simulating the evolution is given by the time integral over the chromatic index of the interaction graph. This defines a complexity measure of continuous and discrete quantum algorithms which are in exact one-to-one correspondence. Furthermore we prove a lower bound on the growth of large-scale entanglement depending on the chromatic index.},
	author = {Janzing, Dominik and Beth, Thomas},
	year = {2000},
	pages = {6--6},
	file = {0009094v1:/home/user/Zotero/storage/2Z3WX24I/0009094v1.pdf:application/pdf;Janzing, Beth - 2000 - A Complexity Measure for Continuous Time Quantum Algorithms:/home/user/Zotero/storage/8VVS2EQB/Janzing, Beth - 2000 - A Complexity Measure for Continuous Time Quantum Algorithms.pdf:application/pdf}
}

@book{hermann-vector-1970,
	address = {New York},
	title = {Vector {Bundles} in {Mathematical} {Physics}},
	publisher = {W. A. Benjamin},
	author = {Hermann, Robert},
	year = {1970}
}

@article{kreimer-quantization-2013,
	title = {Quantization of gauge fields, graph polynomials and graph homology},
	volume = {336},
	doi = {10.1016/j.aop.2013.04.019},
	abstract = {We review quantization of gauge fields using algebraic properties of 3-regular graphs. We derive the Feynman integrand at n loops for a non-abelian gauge theory quantized in a covariant gauge from scalar integrands for connected 3-regular graphs, obtained from the two Symanzik polynomials.The transition to the full gauge theory amplitude is obtained by the use of a third, new, graph polynomial, the corolla polynomial.This implies effectively a covariant quantization without ghosts, where all the relevant signs of the ghost sector are incorporated in a double complex furnished by the corolla polynomial-we call it cycle homology-and by graph homology. © 2013 Elsevier Inc.},
	journal = {Annals of Physics},
	author = {Kreimer, Dirk and Sars, Matthias and van Suijlekom, Walter D.},
	year = {2013},
	keywords = {Covariant quantization, Gauge theory, Graph homology},
	pages = {180--222},
	file = {Kreimer, Sars, van Suijlekom - 2013 - Quantization of gauge fields, graph polynomials and graph homology:/home/user/Zotero/storage/NBBUZJW4/Kreimer, Sars, van Suijlekom - 2013 - Quantization of gauge fields, graph polynomials and graph homology.pdf:application/pdf}
}

@article{hawking-large-nodate,
	title = {large scale structure spacetime},
	author = {{Hawking} and {Ellis}},
	file = {[Stephen_W._Hawking,_G._F._R._Ellis,_P._V._Landsho(BookZZ.org):/home/user/Zotero/storage/BVA2EACK/[Stephen_W._Hawking,_G._F._R._Ellis,_P._V._Landsho(BookZZ.org).djvu:image/vnd.djvu}
}

@article{hawking-particle-1975,
	title = {On particle creation by black holes},
	volume = {45},
	doi = {10.1007/BF01609863},
	abstract = {In the classical theory black holes can only absorb and not emit particles. However it is shown that quantum mechanical effects cause black holes to create and emit particles as if they were hot bodies with temperature hκ2πk≈10−6(M⊙M)∘K\frac{{h\kappa }}{{2\pi k}} \approx 10{\textasciicircum}{ - 6} \left( {\frac{{M\_ \odot }}{M}} \right){}{\textasciicircum} \circ K where κ is the surface gravity of the black hole. This thermal emission leads to a slow decrease in the mass of the black hole and to its eventual disappearance: any primordial black hole of mass less than about 1015 g would have evaporated by now. Although these quantum effects violate the classical law that the area of the event horizon of a black hole cannot decrease, there remains a Generalized Second Law:S+1/4A never decreases whereS is the entropy of matter outside black holes andA is the sum of the surface areas of the event horizons. This shows that gravitational collapse converts the baryons and leptons in the collapsing body into entropy. It is tempting to speculate that this might be the reason why the Universe contains so much entropy per baryon.},
	journal = {Communications in Mathematical Physics},
	author = {{Hawking}},
	year = {1975},
	pages = {9--34},
	file = {Hawking - 1975 - On particle creation by black holes:/home/user/Zotero/storage/DSSJFE6R/Hawking - 1975 - On particle creation by black holes.pdf:application/pdf}
}

@article{hanson-geometry-2012,
	title = {Geometry of {Discrete} {Quantum} {Computing}},
	volume = {1},
	url = {http://arxiv.org/abs/1206.5823},
	doi = {10.1088/1751-8113/46/18/185301},
	abstract = {Conventional quantum computing entails a geometry based on the description of an n-qubit state using 2{\textasciicircum}{n} infinite precision complex numbers denoting a vector in a Hilbert space. Such numbers are in general uncomputable using any real-world resources, and, if we have the idea of physical law as some kind of computational algorithm of the universe, we would be compelled to alter our descriptions of physics to be consistent with computable numbers. Our purpose here is to examine the implications of using finite fields F\_p and finite complexified fields F\_p{\textasciicircum}2 (based on primes p congruent to 3 (mod 4)) as the basis for computations in a theory of discrete quantum computing, which would therefore become a computable theory. Because the states of a discrete n-qubit system are in principle enumerable, we are able to determine the proportions of entangled and unentangled states. In particular, we extend the Hopf fibration that defines the irreducible state space of conventional continuous n-qubit theories (which is the complex projective space CP{\textasciicircum}{2{\textasciicircum}{n}-1}) to an analogous discrete geometry in which the Hopf circle for any n is found to be a discrete set of p+1 points. The tally of unit-length n-qubit states is given, and reduced via the generalized Hopf fibration to DCP{\textasciicircum}{2{\textasciicircum}{n}-1}, the discrete analogue of the complex projective space which has p{\textasciicircum}{2{\textasciicircum}{n}-1}(p{\textasciicircum}{2{\textasciicircum}{n}}-1)/(p+1) irreducible states. Using a measure of entanglement, the Purity, we explore the entanglement features of discrete quantum states and find that the n-qubit states based on the complexified field F\_p{\textasciicircum}2 have p{\textasciicircum}{n} (p-1){\textasciicircum}{n} unentangled states (the product of the tally for a single qubit) with purity one, and they have p{\textasciicircum}{n+1}(p-1)(p+1){\textasciicircum}{n-1} maximally entangled states with purity zero.},
	journal = {arXiv:1206.5823},
	author = {Hanson, Andrew J and Ortiz, Gerardo and Sabry, Amr},
	year = {2012},
	pages = {24--24},
	file = {geometry-quantum:/home/user/Zotero/storage/MEZSIGX5/geometry-quantum.pdf:application/pdf;Hanson, Ortiz, Sabry - 2012 - Geometry of Discrete Quantum Computing:/home/user/Zotero/storage/EZMX4G66/Hanson, Ortiz, Sabry - 2012 - Geometry of Discrete Quantum Computing.pdf:application/pdf}
}

@article{frenkel-dependence-2015,
	title = {Dependence of the time-reading process of the {Salecker}–{Wigner} quantum clock on the size of the clock},
	author = {Frenkel, Andor},
	year = {2015},
	pages = {1--14},
	file = {1501.00840v1:/home/user/Zotero/storage/RTSX2ABU/1501.00840v1.pdf:application/pdf}
}

@article{brown-angles-nodate,
	title = {Angles, scales and parametric renormalization},
	author = {Brown, Francis and Kreimer, Dirk},
	file = {Brown, Kreimer - Unknown - Angles, scales and parametric renormalization:/home/user/Zotero/storage/AQNDZC4J/Brown, Kreimer - Unknown - Angles, scales and parametric renormalization.pdf:application/pdf}
}

@article{childs-quantum-2004,
	title = {Quantum information processing in continuous time},
	url = {http://dl.acm.org/citation.cfm?id=1037515},
	journal = {Unpublished},
	author = {Childs, Andrew M.},
	year = {2004},
	file = {Childs - 2004 - Quantum information processing in continuous time:/home/user/Zotero/storage/9QXKPGFA/Childs - 2004 - Quantum information processing in continuous time.pdf:application/pdf;thesis(1):/home/user/Zotero/storage/KVGB6FGQ/thesis(1).pdf:application/pdf}
}

@article{cheng-learnability-nodate,
	title = {The {Learnability} of {Unknown} {Quantum} {Measurements}},
	number = {2},
	author = {Cheng, Hao-chung and Hsieh, Min-hsiu and Yeh, Ping-cheng},
	pages = {1--29},
	file = {1501.00559v1:/home/user/Zotero/storage/BW4Q4WRC/1501.00559v1.pdf:application/pdf}
}

@article{carolan-universal-2015,
	title = {Universal linear optics},
	volume = {349},
	number = {6249},
	author = {Carolan, Jacques and Harrold, Christopher and Sparrow, Chris and Martín-lópez, Enrique and Russell, Nicholas J and Silverstone, Joshua W and Shadbolt, Peter J and Matsuda, Nobuyuki and Oguma, Manabu and Itoh, Mikitaka and Marshall, Graham D and Thompson, Mark G and Matthews, Jonathan C F and Hashimoto, Toshikazu and Brien, Jeremy L O and Laing, Anthony},
	year = {2015},
	pages = {11--16},
	file = {711.full:/home/user/Zotero/storage/R3UG6N79/711.full.pdf:application/pdf}
}

@article{brown-decomposing-2012,
	title = {Decomposing {Feynman} rules},
	number = {257638},
	author = {Brown, Francis},
	year = {2012},
	file = {Brown - 2012 - Decomposing Feynman rules:/home/user/Zotero/storage/BNSKITUU/Brown - 2012 - Decomposing Feynman rules.pdf:application/pdf}
}

@article{bonesteel-topological-2009,
	title = {Topological {Quantum} {Computing}},
	volume = {2},
	author = {Bonesteel, Nick},
	year = {2009},
	pages = {1--73},
	file = {Bonesteel - 2009 - Topological Quantum Computing:/home/user/Zotero/storage/4JGW9359/Bonesteel - 2009 - Topological Quantum Computing.pdf:application/pdf}
}

@article{arkhipov-bosonic-2011,
	title = {The bosonic birthday paradox},
	url = {http://arxiv.org/abs/1106.0849},
	abstract = {We motivate and prove a version of the birthday paradox for \$k\$ identical bosons in \$n\$ possible modes. If the bosons are in the uniform mixed state, also called the maximally mixed quantum state, then we need \$k \sim \sqrt{n}\$ bosons to expect two in the same state, which is smaller by a factor of \$\sqrt{2}\$ than in the case of distinguishable objects (boltzmannons). While the core result is elementary, we generalize the hypothesis and strengthen the conclusion in several ways. One side result is that boltzmannons with a randomly chosen multinomial distribution have the same birthday statistics as bosons. This last result is interesting as a quantum proof of a classical probability theorem; we also give a classical proof.},
	author = {Arkhipov, Alex and Kuperberg, Greg},
	month = jun,
	year = {2011},
	pages = {3--3},
	file = {Arkhipov, Kuperberg - 2011 - The bosonic birthday paradox:/home/user/Zotero/storage/RDE8I52M/Arkhipov, Kuperberg - 2011 - The bosonic birthday paradox.pdf:application/pdf}
}

@article{braun-how-2015,
	title = {How precisely can the speed of light and the metric of space-time be determined in principle ?},
	author = {Braun, Daniel and Fischer, Uwe R},
	year = {2015},
	pages = {1--23},
	file = {Braun, Fischer - 2015 - How precisely can the speed of light and the metric of space-time be determined in principle:/home/user/Zotero/storage/VWWQWITK/Braun, Fischer - 2015 - How precisely can the speed of light and the metric of space-time be determined in principle.pdf:application/pdf}
}

@article{berry-quantal-1984,
	title = {Quantal {Phase} {Factors} {Accompanying} {Adiabatic} {Changes}},
	volume = {392},
	number = {1802},
	journal = {Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences},
	author = {Berry, M V},
	year = {1984},
	pages = {45--57},
	file = {Berry Berry Phase 1984:/home/user/Zotero/storage/7E4HSZD4/Berry Berry Phase 1984.pdf:application/pdf}
}

@article{avron-adiabatic-1998,
	title = {Adiabatic {Theorem} without a {Gap} {Condition}},
	url = {http://arxiv.org/abs/math-ph/9805022v4},
	author = {Avron, J. E. and Elgart, a.},
	month = may,
	year = {1998},
	file = {Avron, Elgart - 1998 - Adiabatic Theorem without a Gap Condition:/home/user/Zotero/storage/C4V7NA4Z/Avron, Elgart - 1998 - Adiabatic Theorem without a Gap Condition.pdf:application/pdf}
}

@book{ashcroft-solid-1976,
	address = {Philadelphia},
	title = {Solid {State} {Physics}},
	publisher = {Saunders College},
	author = {Ashcroft, Neil W and Mermin, N David},
	year = {1976}
}

@article{andrei-no-1982,
	title = {No {Title}},
	volume = {49},
	number = {6},
	author = {Andrei, N and Stone, M and Kohmoto, M},
	year = {1982},
	pages = {405--408},
	file = {Andrei, Stone, Kohmoto - 1982 - No Title:/home/user/Zotero/storage/HD8QJECN/Andrei, Stone, Kohmoto - 1982 - No Title.pdf:application/pdf}
}

@article{andrei-no-1982-1,
	title = {No {Title}},
	volume = {49},
	number = {6},
	author = {Andrei, N and Stone, M and Kohmoto, M},
	year = {1982},
	pages = {405--408},
	file = {Andrei, Stone, Kohmoto - 1982 - No Title:/home/user/Zotero/storage/JEU5HZX5/Andrei, Stone, Kohmoto - 1982 - No Title.pdf:application/pdf}
}

@article{altland-primer-nodate,
	title = {Primer on topological insulators},
	author = {Altland, Alexander and Fritz, Lars},
	file = {Altland, Fritz - Unknown - Primer on topological insulators:/home/user/Zotero/storage/WFHMV9R8/Altland, Fritz - Unknown - Primer on topological insulators.pdf:application/pdf}
}

@article{aharonov-time-1961,
	title = {Time in the quantum theory and the uncertainty relation for time and energy},
	volume = {122},
	doi = {10.1103/PhysRev.122.1649},
	abstract = {Because time does not appear in Schrödinger's equation as an operator but only as a parameter, the time-energy uncertainty relation must be formulated in a special way. This problem has in fact been studied by many authors and we give a summary of their treatments. We then criticize the main conclusion of these treatments; viz., that in a measurement of energy carried out in a time interval, Δt, there must be a minimum uncertainty in the transfer of energy to the observed system, given by Δ(E′-E){\textgreater}{\textasciitilde}h/Δt. We show that this conclusion is erroneous in two respects. First, it is not consistent with the general principles of the quantum theory, which require that all uncertainty relations be expressible in terms of the mathematical formalism, i.e., by means of operators, wave functions, etc. Secondly, the examples of measurement processes that were used to derive the above uncertainty relation are not general enough. We then develop a systematic presentation of our own point of view, with regard to the role of time in the quantum theory, and give a concrete example of a measurement process not satisfying the above uncertainty relation.},
	journal = {Physical Review},
	author = {Aharonov, Y. and Bohm, D.},
	year = {1961},
	pages = {1649--1658},
	file = {Aharonov, Bohm - 1961 - Time in the quantum theory and the uncertainty relation for time and energy:/home/user/Zotero/storage/9IGFFWAE/Aharonov, Bohm - 1961 - Time in the quantum theory and the uncertainty relation for time and energy.pdf:application/pdf}
}

@article{aharonov-significance-1959,
	title = {Significance of electromagnetic potentials in quantum theory},
	volume = {115},
	journal = {Physical Review},
	author = {Aharonov, Y and Bohm, D},
	year = {1959},
	pages = {485--491},
	file = {Aharonov, Bohm - Unknown - aharabohm.pdf:/home/user/Zotero/storage/WUVKRM5D/Aharonov, Bohm - Unknown - aharabohm.pdf.pdf:application/pdf}
}

@article{aaronson-closed-2008,
	title = {Closed {Timelike} {Curves} {Make} {Quantum} and {Classical} {Computing} {Equivalent}},
	url = {http://arxiv.org/abs/0808.2669},
	doi = {10.1098/rspa.2008.0350},
	abstract = {While closed timelike curves (CTCs) are not known to exist, studying their consequences has led to nontrivial insights in general relativity, quantum information, and other areas. In this paper we show that if CTCs existed, then quantum computers would be no more powerful than classical computers: both would have the (extremely large) power of the complexity class PSPACE, consisting of all problems solvable by a conventional computer using a polynomial amount of memory. This solves an open problem proposed by one of us in 2005, and gives an essentially complete understanding of computational complexity in the presence of CTCs. Following the work of Deutsch, we treat a CTC as simply a region of spacetime where a "causal consistency" condition is imposed, meaning that Nature has to produce a (probabilistic or quantum) fixed-point of some evolution operator. Our conclusion is then a consequence of the following theorem: given any quantum circuit (not necessarily unitary), a fixed-point of the circuit can be (implicitly) computed in polynomial space. This theorem might have independent applications in quantum information.},
	author = {Aaronson, Scott and Watrous, John},
	year = {2008},
	pages = {15--15},
	file = {ctc:/home/user/Zotero/storage/XJ3VS9BT/ctc.pdf:application/pdf}
}

@article{aaronson-is-2004,
	title = {Is {Quantum} {Mechanics} {An} {Island} {In} {Theoryspace}?},
	url = {http://arxiv.org/abs/quant-ph/0401062},
	abstract = {This recreational paper investigates what happens if we change quantum mechanics in several ways. The main results are as follows. First, if we replace the 2-norm by some other p-norm, then there are no nontrivial norm-preserving linear maps. Second, if we relax the demand that norm be preserved, we end up with a theory that allows rapid solution of PP-complete problems (as well as superluminal signalling). And third, if we restrict amplitudes to be real, we run into a difficulty much simpler than the usual one based on parameter-counting of mixed states.},
	author = {Aaronson, Scott},
	year = {2004},
	pages = {9--9},
	file = {0401062:/home/user/Zotero/storage/6JUWZDI7/0401062.pdf:application/pdf}
}

@article{aerts-spins-nodate,
	title = {Do {Spins} {Have} {Directions} ?},
	author = {Aerts, Diederik and Bianchi, Massimiliano Sassoli De},
	keywords = {tion, extended bloch sphere, hidden-measurement interpreta-, multiplex realism, non-spatiality, spin eigenstate, spin measurement},
	file = {1501.00693v1:/home/user/Zotero/storage/P32JSMA8/1501.00693v1.pdf:application/pdf}
}

@article{aaronson-quantum-2005,
	title = {Quantum computing and hidden variables},
	volume = {71},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.71.032325},
	abstract = {This paper initiates the study of hidden variables from a quantum computing perspective. For us, a hidden-variable theory is simply a way to convert a unitary matrix that maps one quantum state to another into a stochastic matrix that maps the initial probability distribution to the final one in some fixed basis. We list five axioms that we might want such a theory to satisfy and then investigate which of the axioms can be satisfied simultaneously. Toward this end, we propose a new hidden-variable theory based on network flows. In a second part of the paper, we show that if we could examine the entire history of a hidden variable, then we could efficiently solve problems that are believed to be intractable even for quantum computers. In particular, under any hidden-variable theory satisfying a reasonable axiom, we could solve the graph isomorphism problem in polynomial time, and could search an N-item database using O(N1∕3) queries, as opposed to O(N1∕2) queries with Grover’s search algorithm. On the other hand, the N1∕3 bound is optimal, meaning that we could probably not solve NP-complete problems in polynomial time. We thus obtain the first good example of a model of computation that appears slightly more powerful than the quantum computing model.},
	journal = {Physical Review A},
	author = {Aaronson, Scott},
	year = {2005},
	file = {qchvpra:/home/user/Zotero/storage/9QFK8BR3/qchvpra.pdf:application/pdf}
}

@article{xiang-before-nodate,
	title = {before and after},
	issn = {7738340924},
	author = {Xiang, Ming and Hanink, Emily and Vegh, Genna},
	pages = {1--15},
	file = {Xiangetal:/home/user/Zotero/storage/U4AB3BVZ/Xiangetal.pdf:application/pdf}
}

@book{noauthor-constructing-nodate,
	title = {Constructing covert dependencies—the case of wh-in-situ processing {Ming} {Xiang}},
	isbn = {7-7383-4092-4},
	file = {Feb2014-whinsitu-final:/home/user/Zotero/storage/H3JVIDR4/Feb2014-whinsitu-final.pdf:application/pdf}
}

@article{xiang-processing-2011,
	title = {Processing morphological ambiguity: {An} experimental investigation of {Russian} numerical phrases},
	volume = {121},
	url = {http://dx.doi.org/10.1016/j.lingua.2010.10.016},
	doi = {10.1016/j.lingua.2010.10.016},
	abstract = {Russian nouns in nominative and accusative numerical expressions appear in three different forms, depending on the numeral: nominative singular with the numeral 1, genitive singular with the paucal numerals 2-4, and genitive plural with all other numerals. Results from an acceptability judgment task and a self-paced reading task on Russian case/number marking provide support for a theory stating that the suffix used with paucal nouns is morphologically ambiguous. The ambiguity resolution process involving this suffix leads to extra processing cost, compared to the unambiguous suffixes in other numeral contexts (the number 1, and the numbers 5+). This would account for the additional processing time observed with the paucal nouns. The status of the form occurring with the paucal numerals has long been a challenging issue in Russian linguistics, and the new results add to the growing body of literature which makes use of experimental methods to address issues of linguistic theory and analysis. © 2010 Elsevier B.V.},
	number = {3},
	journal = {Lingua},
	author = {Xiang, Ming and Harizanov, Boris and Polinsky, Maria and Kravtchenko, Ekaterina},
	year = {2011},
	keywords = {sentence processing, Russian, Inflectional morphology, Numerical expressions, Slavic, Surface and underlying representations},
	pages = {548--560},
	file = {Lingua:/home/user/Zotero/storage/T46CMIT3/Lingua.pdf:application/pdf}
}

@article{xiang-ellipsis-2014,
	title = {Ellipsis sites induce structural priming effects},
	url = {http://lucian.uchicago.edu/blogs/lpl/files/2011/09/ellipsis.primes.withnames.pdf},
	number = {July},
	author = {Xiang, Ming and Grove, Julian and Merchant, Jason},
	year = {2014},
	pages = {1--23},
	file = {ellipsis.primes.withnames:/home/user/Zotero/storage/5ZAU9HSQ/ellipsis.primes.withnames.pdf:application/pdf}
}

@article{xiang-dependency-dependent-2013,
	title = {Dependency-dependent interference: {NPI} interference, agreement attraction, and global pragmatic inferences.},
	volume = {4},
	issn = {7738340924},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3791380&tool=pmcentrez&rendertype=abstract},
	doi = {10.3389/fpsyg.2013.00708},
	abstract = {Previous psycholinguistics studies have shown that when forming a long distance dependency in online processing, the parser sometimes accepts a sentence even though the required grammatical constraints are only partially met. A mechanistic account of how such errors arise sheds light on both the underlying linguistic representations involved and the processing mechanisms that put such representations together. In the current study, we contrast the negative polarity items (NPI) interference effect, as shown by the acceptance of an ungrammatical sentence like "The bills that democratic senators have voted for will ever become law," with the well-known phenomenon of agreement attraction ("The key to the cabinets are … "). On the surface, these two types of errors look alike and thereby can be explained as being driven by the same source: similarity based memory interference. However, we argue that the linguistic representations involved in NPI licensing are substantially different from those of subject-verb agreement, and therefore the interference effects in each domain potentially arise from distinct sources. In particular, we show that NPI interference at least partially arises from pragmatic inferences. In a self-paced reading study with an acceptability judgment task, we showed NPI interference was modulated by participants' general pragmatic communicative skills, as quantified by the Autism-Spectrum Quotient (AQ, Baron-Cohen et al., 2001), especially in offline tasks. Participants with more autistic traits were actually less prone to the NPI interference effect than those with fewer autistic traits. This result contrasted with agreement attraction conditions, which were not influenced by individual pragmatic skill differences. We also show that different NPI licensors seem to have distinct interference profiles. We discuss two kinds of interference effects for NPI licensing: memory-retrieval based and pragmatically triggered.},
	number = {October},
	journal = {Frontiers in psychology},
	author = {Xiang, Ming and Grove, Julian and Giannakidou, Anastasia},
	year = {2013},
	keywords = {autistic traits, individual differences, memory interference, memory interference, pragmatic inference, individu, npi licensing, pragmatic inference},
	pages = {708--708},
	file = {fpsyg-04-00708:/home/user/Zotero/storage/CGV3ZIS2/fpsyg-04-00708.pdf:application/pdf}
}

@article{xiang-processing-2013,
	title = {Processing covert dependencies: an {SAT} study on {Mandarin} wh-in-situ questions},
	volume = {23},
	url = {http://link.springer.com/10.1007/s10831-013-9115-1},
	doi = {10.1007/s10831-013-9115-1},
	journal = {Journal of East Asian Linguistics},
	author = {Xiang, Ming and Dillon, Brian and Wagers, Matt and Liu, Fengqin and Guo, Taomei},
	year = {2013},
	keywords = {processing complexity, chinese, covert dependencies, long distance dependencies, memory mechanisms, wh -in-situ},
	pages = {207--232},
	file = {xiangetal_2013:/home/user/Zotero/storage/5UVSWNWB/xiangetal_2013.pdf:application/pdf}
}

@article{xiang-illusory-2009,
	title = {Illusory licensing effects across dependency types: {ERP} evidence},
	volume = {108},
	issn = {0093-934X},
	url = {http://dx.doi.org/10.1016/j.bandl.2008.10.002},
	doi = {10.1016/j.bandl.2008.10.002},
	abstract = {A number of recent studies have argued that grammatical illusions can arise in the process of completing linguistic dependencies, such that unlicensed material is temporarily treated as licensed due to the presence of a potential licensor that is semantically appropriate but in a syntactically inappropriate position. A frequently studied case involves illusory licensing of negative polarity items (NPIs) like ever and any, which must appear in the scope (i.e., c-command domain) of a negative element. Speakers often show intrusive licensing effects in sentences where an NPI is preceded but not c-commanded by a negative element, as in *The restaurants that no newspapers have recommended in their reviews have ever gone out of business. Existing accounts of intrusive licensing have focused on the role of general memory retrieval processes. In contrast, we propose that intrusive licensing of NPIs reflects semantic/pragmatic processes that are more specific to NPI licensing. As a test of this claim, we present results from an ERP study that presents a structurally matched comparison of intrusive licensing in two types of linguistic dependencies, namely NPI licensing and the binding of reflexive anaphors like himself, and herself. In the absence of a potential licensor, both NPIs and reflexives elicit a P600 response, but whereas there is an immediate ERP analog of the intrusion effect for NPI licensing, no such effect is found for reflexive binding. This suggests that the NPI intrusion effect does not reflect general-purpose retrieval mechanisms. ?? 2008 Elsevier Inc. All rights reserved.},
	number = {1},
	journal = {Brain and Language},
	author = {Xiang, Ming and Dillon, Brian and Phillips, Colin},
	year = {2009},
	keywords = {sentence processing, erp, binding, anaphora, Negative polarity items (NPI), Semantic/pragmatic processing},
	pages = {40--55},
	file = {xiangetal_2009:/home/user/Zotero/storage/FDQRG7KZ/xiangetal_2009.pdf:application/pdf}
}

@article{xiang-processing-2009,
	title = {Processing {Covert} {Dependencies}: {An} {SAT} {Study} on {Mandarin} {Wh} in-situ {Questions}},
	author = {Xiang, Ming},
	year = {2009},
	pages = {1--40},
	file = {JEAL-submission-Oct2011:/home/user/Zotero/storage/6HJEZFIW/JEAL-submission-Oct2011.pdf:application/pdf}
}

@article{xiang-plurality-2008,
	title = {Plurality, maximality and scalar inferences: {A} case study of {Mandarin} {Dou}},
	volume = {17},
	issn = {1083100890},
	doi = {10.1007/s10831-008-9025-9},
	abstract = {The Mandarin functional morpheme dou appears to have been interpreted, among other things, as a distributor, focus marker even, or already. This paper aims at providing a unified semantic account for these different uses. I argue that the semantic core of these different usages is the same: dou is simply a maximality operator. It gives rise to different meanings by applying maximality to a contextually determined plural set. This could be a set of covers, a set of focus-induced alternatives, or a set of degrees ordered on a scale. This analysis also connects dou in these contexts with dou in environments that license polarity items, as discussed in Giannakidou and Cheng.},
	number = {July},
	journal = {Journal of East Asian Linguistics},
	author = {Xiang, Ming},
	year = {2008},
	keywords = {Degree constructions, Distributivity, Dou, Focus, Maximality, Plurality, Scales},
	pages = {227--245},
	file = {00b49532cb09f936b5000000:/home/user/Zotero/storage/99SEVNXI/00b49532cb09f936b5000000.pdf:application/pdf}
}

@article{wang-time-2012,
	title = {The time course of semantic and syntactic processing in reading {Chinese}: {Evidence} from {ERPs}},
	doi = {10.1080/01690965.2012.660169},
	abstract = {The time course of semantic and syntactic processing in reading Chinese\nwas examined by recording event-related brain potentials (ERPs) as\nnative Chinese speakers read individually presented sentences for\ncomprehension and performed semantic plausibility judgments. The\ntransitivity of the verbs in Chinese ba/bei constructions was\nmanipulated to form three types of stimuli: Congruent sentences (CON),\nsentences with semantic violation (SEM), and sentences with combined\nsemantic and syntactic violation (SEM+SYN). Compared with the critical\nwords in CON, those in SEM and SEM+SYN elicited an N400-P600 biphasic\npattern. The N400 effects in both violation conditions were of similar\nsize and distribution, but the P600 in SEM+SYN was bigger than that in\nSEM. Overall, the lack of a difference between SEM and SEM+SYN in the\nearlier time window (i.e., N400 window) suggested that syntactic\nprocessing in Chinese does not necessarily occur earlier than semantic\nprocessing.},
	number = {October},
	journal = {Language and Cognitive Processes},
	author = {Wang, Suiping and Mo, Deyuan and Xiang, Ming and Xu, Ruiping and Chen, Hsuan-Chih},
	year = {2012},
	pages = {1--20},
	file = {LCP20121:/home/user/Zotero/storage/UARH85JG/LCP20121.pdf:application/pdf}
}

@article{van-berkum-electrophysiology-2012,
	title = {The electrophysiology of discourse and conversation},
	volume = {2005},
	issn = {9780521677929},
	url = {http://www.cogsci.ucsd.edu/~coulson/CNL/vanberkum.pdf},
	abstract = {1. Introduction What’s happening in the brains of two people having a conversation? One reasonable guess is that in the fMRI scanner we’d see most of their brains light up. Another is that their EEG will be a total mess, reflecting dozens of interacting neuronal systems. Conversation recruits all of the basic language systems reviewed in this book. It also heavily taxes cognitive systems more likely to be found in handbooks of memory, attention and control, or social cognition (Brownell \& Friedman, 2001). With most conversations going beyond the single utterance, for instance, they place a heavy load on episodic memory, as well as on the systems that allow us to reallocate cognitive resources to meet the demands of a dynamically changing situation. Furthermore, conversation is a deeply social and collaborative enterprise (Clark, 1996; this volume), in which interlocutors have to keep track of each others state of mind and coordinate on such things as taking turns, establishing common ground, and the goals of the conversation.},
	number = {911},
	journal = {The Cambridge handbook of psycholinguistics},
	author = {van Berkum, Jja and Berkum, Jos J A Van},
	year = {2012},
	pages = {1--20},
	file = {vanberkum:/home/user/Zotero/storage/DTX828EH/vanberkum.pdf:application/pdf;vanberkum-handbookofpsycholinguistics-revision:/home/user/Zotero/storage/Z6TG2ZVD/vanberkum-handbookofpsycholinguistics-revision.pdf:application/pdf}
}

@article{kutas-erp-1994,
	title = {{ERP} {Psycholinguistics} electrified},
	url = {http://kutaslab.ucsd.edu/people/kutas/pdfs/1994.HP.83.pdf},
	abstract = {... Database: PsycINFO. Chapter. Psycholinguistics electrified : Event - related brain potential investigations ... methods and data in the domain of the electrophysiology of psycholinguistics aimed at ... who may wish to use ERPs to address certain psycholinguistic questions concerned ...},
	journal = {Handbook of psycholinguistics},
	author = {Kutas, M and Van Petten, C},
	year = {1994},
	pages = {83--143},
	file = {1994.HP.83:/home/user/Zotero/storage/IF63X3VI/1994.HP.83.pdf:application/pdf}
}

@article{katsos-are-2011,
	title = {Are children with {Specific} {Language} {Impairment} competent with the pragmatics and logic of quantification?},
	volume = {119},
	issn = {1873-7838 (Electronic) 0010-0277 (Linking)},
	url = {http://dx.doi.org/10.1016/j.cognition.2010.12.004},
	doi = {10.1016/j.cognition.2010.12.004},
	abstract = {Specific Language Impairment (SLI) is understood to be a disorder that predominantly affects phonology, morphosyntax and/or lexical semantics. There is little conclusive evidence on whether children with SLI are challenged with regard to Gricean pragmatic maxims and on whether children with SLI are competent with the logical meaning of quantifying expressions. We use the comprehension of statements quantified with 'all', 'none', 'some', 'some...not', 'most' and 'not all' as a paradigm to study whether Spanish-speaking children with SLI are competent with the pragmatic maxim of informativeness, as well as with the logical meaning of these expressions. Children with SLI performed more poorly than a group of age-matched typically-developing peers, and both groups performed more poorly with pragmatics than with logical meaning. Moreover, children with SLI were disproportionately challenged by pragmatic meaning compared to their age-matched peers. However, the performance of children with SLI was comparable to that of a group of younger language-matched typically-developing children. The findings document that children with SLI do face difficulties with employing the maxim of informativeness, as well as with understanding the logical meaning of quantifiers, but also that these difficulties are in keeping with their overall language difficulties rather than exceeding them. The implications of these findings for SLI, linguistic theory, and clinical practice are discussed. © 2010 Elsevier B.V.},
	journal = {Cognition},
	author = {Katsos, Napoleon and Roqueta, Clara Andrés and Estevan, Rosa Ana Clemente and Cummins, Chris},
	year = {2011},
	keywords = {scalar implicature, Quantification, quantifiers, Pragmatics, Logic, Informativeness, Specific Language Impairment},
	pages = {43--57},
	file = {KAREC_quantification_in_press:/home/user/Zotero/storage/G4CJR4X9/KAREC_quantification_in_press.pdf:application/pdf}
}

@article{jager-retrieval-2015,
	title = {Retrieval interference in reflexive processing: experimental evidence from {Mandarin}, and computational modeling},
	volume = {6},
	url = {http://www.frontiersin.org/Language\_Sciences/10.3389/fpsyg.2015.00617/abstract},
	doi = {10.3389/fpsyg.2015.00617},
	number = {May},
	journal = {Frontiers in Psychology},
	author = {JÃ¤ger, Lena a. and Engelmann, Felix and Vasishth, Shravan},
	year = {2015},
	keywords = {eye-tracking, Computational modeling, act-r, chinese reflexives, Chinese reflexives, ACT-R, eye-tracking, interfere, content-addressable memory, cue-based retrieval, interference, ziji},
	pages = {1--24},
	file = {fpsyg-06-00617:/home/user/Zotero/storage/IKJIN2GP/fpsyg-06-00617.pdf:application/pdf}
}

@article{ferguson-listeners-2012,
	title = {Listeners' eyes reveal spontaneous sensitivity to others' perspectives},
	volume = {48},
	url = {http://dx.doi.org/10.1016/j.jesp.2011.08.007},
	doi = {10.1016/j.jesp.2011.08.007},
	abstract = {During everyday social interactions, we typically anticipate (or explain) others' behaviour according to their current mental states (e.g. their knowledge, beliefs and intentions). To date, very little is known about the time-course with which such perspective information influences communication. We report a novel interactive 'visual world' study examining these processes. Here, two communicators watched videos depicting transfer events and subsequently described these events to each other. Critically, on half the trials a screen blocked the speakers' (but not the listeners') view part-way through the video, establishing a discrepancy in the knowledge held by the two communicators. Eye-tracking analyses showed that listeners were rapidly sensitive to their partner's perspective, as evidenced by a significantly reduced reality-bias when speakers held out-of-date knowledge about a privileged transfer event. However, we also found that under these conditions, listeners suffered ongoing interference from their own knowledge of reality, which inhibited successful anticipation of the speaker's intended referents. ?? 2011 Elsevier Inc.},
	journal = {Journal of Experimental Social Psychology},
	author = {Ferguson, Heather J. and Breheny, Richard},
	year = {2012},
	keywords = {Theory of Mind, Eye tracking, Social communication},
	pages = {257--263},
	file = {false belief eye-tracking:/home/user/Zotero/storage/36URE4AE/false belief eye-tracking.pdf:application/pdf}
}

@article{sag-flexible-nodate,
	title = {Flexible {Processing} and the {Design} of {Grammar} ∗},
	issn = {1093601493},
	doi = {10.1007/s10936-014-9332-4},
	author = {Sag, Ivan a and Wasow, Thomas and Borsley, Bob and Carpenter, Bob and Clark, Herb and Copestake, Ann and Eckert, Penny and Flickinger, Dan and Hobbs, Jerry and Kaplan, Ron and Kay, Martin and Kay, Paul and Lascarides, Alex and Liberman, Mark and Pereira, Fernando and Rumelhart, Dave and Shieber, Stuart and Tanenhaus, Mike},
	pages = {1--19},
	file = {SagCUNY:/home/user/Zotero/storage/8UBFWHF2/SagCUNY.pdf:application/pdf}
}

@article{hartshorne-why-2006,
	title = {Why girls say 'holded' more than boys},
	volume = {9},
	issn = {1363-755X\n1467-7687},
	doi = {10.1111/j.1467-7687.2005.00459.x},
	abstract = {Women are better than men at verbal memory tasks, such as remembering word lists. These tasks depend on declarative memory. The declarative/procedural model of language, which posits that the lexicon of stored words is part of declarative memory, while grammatical composition of complex forms depends on procedural memory, predicts a female superiority in aspects of lexical memory. Other neurocognitive models of language have not made this prediction. Here we examine the prediction in past-tense over-regularizations (e.g. holded) produced by children. We expected that girls would remember irregular past-tense forms (held) better than boys, and thus would over-regularize less. To our surprise, girls over-regularized far more than boys. We investigated potential explanations for this sex difference. Analyses showed that in girls but not boys, over-regularization rates correlated with measures of the number of similar-sounding regulars (folded, molded). This sex difference in phonological neighborhood effects is taken to suggest that girls tend to produce over-regularizations in associative lexical memory, generalizing over stored neighboring regulars, while boys are more likely to depend upon rule-governed affixation (hold+-ed). The finding is consistent with the hypothesis that, likely due to their superior lexical abilities, females tend to retrieve from memory complex forms (walked) that men generally compose with the grammatical system (walk+-ed). The results suggest that sex may be an important factor in the acquisition and computation of language.},
	journal = {Developmental Science},
	author = {Hartshorne, Joshua K. and Ullman, Michael T.},
	year = {2006},
	pages = {21--32},
	file = {Hartshorne_DevSci_06:/home/user/Zotero/storage/4BUNPJJP/Hartshorne_DevSci_06.pdf:application/pdf}
}

@article{cummins-experimental-2012,
	title = {Experimental {Investigations} of the {Typology} of {Presupposition} {Triggers}},
	volume = {23},
	url = {http://www.humanamente.eu/PDF/Issue\_23\_Papers\_Cummins\_Amaral\_Katsos.pdf},
	journal = {Humana.Mente Journal of Philosophical Studies},
	author = {Cummins, Chris and Amaral, P and Katsos, Napoleon},
	year = {2012},
	keywords = {experimental pragmatics, information structure, accommodation, presuppositions, qud},
	pages = {1--15},
	file = {Issue_23_Papers_Cummins_Amaral_Katsos:/home/user/Zotero/storage/D8N9G4FR/Issue_23_Papers_Cummins_Amaral_Katsos.pdf:application/pdf}
}

@article{cummins-modeling-nodate,
	title = {modeling implicatures},
	author = {{Cummins}},
	keywords = {scalar implicature, quantifiers, Context, numerals, priming},
	pages = {1--35},
	file = {Cummins_Modelling_implicatures_draft:/home/user/Zotero/storage/HF82PXB5/Cummins_Modelling_implicatures_draft.pdf:application/pdf}
}

@article{breheny-taking-nodate,
	title = {Taking the {Epistemic} {Step}},
	volume = {44},
	number = {0},
	author = {Breheny, Richard and Ferguson, Heather J and Katsos, Napoleon},
	pages = {1--53},
	file = {epistemic step final sub:/home/user/Zotero/storage/6PUIM2B8/epistemic step final sub.pdf:application/pdf}
}

@article{brouwer-getting-2012,
	title = {Getting real about {Semantic} {Illusions}: {Rethinking} the functional role of the {P}600 in language comprehension},
	volume = {1446},
	url = {http://dx.doi.org/10.1016/j.brainres.2012.01.055},
	doi = {10.1016/j.brainres.2012.01.055},
	abstract = {In traditional theories of language comprehension, syntactic and semantic processing are inextricably linked. This assumption has been challenged by the 'Semantic Illusion Effect' found in studies using Event Related brain Potentials. Semantically anomalous sentences did not produce the expected increase in N400 amplitude but rather one in P600 amplitude. To explain these findings, complex models have been devised in which an independent semantic processing stream can arrive at a sentence interpretation that may differ from the interpretation prescribed by the syntactic structure of the sentence. We review five such multi-stream models and argue that they do not account for the full range of relevant results because they assume that the amplitude of the N400 indexes some form of semantic integration. Based on recent evidence we argue that N400 amplitude might reflect the retrieval of lexical information from memory. On this view, the absence of an N400-effect in Semantic Illusion sentences can be explained in terms of priming. Furthermore, we suggest that semantic integration, which has previously been linked to the N400 component, might be reflected in the P600 instead. When combined, these functional interpretations result in a single-stream account of language processing that can explain all of the Semantic Illusion data. ?? 2012 Elsevier B.V.},
	journal = {Brain Research},
	author = {Brouwer, Harm and Fitz, Hartmut and Hoeks, John},
	year = {2012},
	keywords = {sentence comprehension, Memory retrieval, N400, Semantic Illusion, Semantic P600},
	pages = {127--143},
	file = {Brouwer2012GettingReal:/home/user/Zotero/storage/7QFVF9TX/Brouwer2012GettingReal.pdf:application/pdf}
}

@article{breheny-dynamic-2003,
	title = {On the {Dynamic} {Turn} in the {Study} of {Meaning} and {Interpretation}},
	issn = {0080441874},
	url = {http://discovery.ucl.ac.uk/136020/},
	author = {Breheny, R},
	year = {2003},
	keywords = {dynamic semantics},
	file = {dynamic turn:/home/user/Zotero/storage/5ADRVCHU/dynamic turn.pdf:application/pdf}
}

@article{frank-adding-2013-3,
	title = {Adding sentence types to a model of syntactic category acquisition},
	volume = {5},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12030/full},
	number = {3},
	urldate = {2015-10-15},
	journal = {Topics in cognitive science},
	author = {Frank, Stella and Goldwater, Sharon and Keller, Frank},
	year = {2013},
	pages = {495--521},
	file = {topics13b.pdf:/home/user/Zotero/storage/I768A4Q4/topics13b.pdf:application/pdf}
}

@article{levon-sexuality-2007,
	title = {Sexuality in context: {Variation} and the sociolinguistic perception of identity},
	volume = {36},
	shorttitle = {Sexuality in context},
	url = {http://journals.cambridge.org/abstract\_S0047404507070431},
	number = {04},
	urldate = {2015-10-15},
	journal = {Language in Society},
	author = {Levon, Erez},
	year = {2007},
	pages = {533--554},
	file = {LinSLevon2007.pdf:/home/user/Zotero/storage/EZJEKET6/LinSLevon2007.pdf:application/pdf}
}

@article{levon-gender-2012,
	title = {Gender, prescriptivism, and language change: {Morphological} variation in {Hebrew} animate reference},
	volume = {24},
	issn = {0954-3945, 1469-8021},
	shorttitle = {Gender, prescriptivism, and language change},
	url = {http://www.journals.cambridge.org/abstract\_S095439451200004X},
	doi = {10.1017/S095439451200004X},
	language = {en},
	number = {01},
	urldate = {2015-10-15},
	journal = {Language Variation and Change},
	author = {Levon, Erez},
	month = mar,
	year = {2012},
	pages = {33--58},
	file = {LevonLVC2012.pdf:/home/user/Zotero/storage/633SMCPW/LevonLVC2012.pdf:application/pdf}
}

@article{levon-voice-2012,
	title = {The voice of others: {Identity}, alterity and gender normativity among gay men in {Israel}},
	volume = {41},
	issn = {0047-4045, 1469-8013},
	shorttitle = {The voice of others},
	url = {http://www.journals.cambridge.org/abstract\_S0047404512000048},
	doi = {10.1017/S0047404512000048},
	language = {en},
	number = {02},
	urldate = {2015-10-15},
	journal = {Language in Society},
	author = {Levon, Erez},
	month = apr,
	year = {2012},
	pages = {187--211},
	file = {LevonLinS2012.pdf:/home/user/Zotero/storage/PJ55IU6U/LevonLinS2012.pdf:application/pdf}
}

@article{levon-east-2013,
	title = {East {End} {Boys} and {West} {End} {Girls}:/s/-{Fronting} in {Southeast} {England}},
	volume = {19},
	shorttitle = {East {End} {Boys} and {West} {End} {Girls}},
	url = {http://repository.upenn.edu/pwpl/vol19/iss2/13/},
	number = {2},
	urldate = {2015-10-15},
	journal = {University of Pennsylvania Working Papers in Linguistics},
	author = {Levon, Erez and Holmes-Elliott, Sophie},
	year = {2013},
	pages = {13},
	file = {LevonHolmes-ElliottPWPL.pdf:/home/user/Zotero/storage/VP7Q87ZP/LevonHolmes-ElliottPWPL.pdf:application/pdf}
}

@article{levon-dimensions-2009,
	title = {Dimensions of style: {Context}, politics and motivation in gay {Israeli} speech1},
	volume = {13},
	shorttitle = {Dimensions of style},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9841.2008.00396.x/full},
	number = {1},
	urldate = {2015-10-15},
	journal = {Journal of Sociolinguistics},
	author = {Levon, Erez},
	year = {2009},
	pages = {29--58},
	file = {JofSLevon2009.pdf:/home/user/Zotero/storage/ZR3H6Z4W/JofSLevon2009.pdf:application/pdf}
}

@article{levon-mosaic-2006,
	title = {Mosaic identity and style: {Phonological} variation among {Reform} {American} {Jews}1},
	volume = {10},
	shorttitle = {Mosaic identity and style},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1360-6441.2006.00324.x/full},
	number = {2},
	urldate = {2015-10-15},
	journal = {Journal of Sociolinguistics},
	author = {Levon, Erez},
	year = {2006},
	pages = {181--204},
	file = {JofSLevon2006.pdf:/home/user/Zotero/storage/A2ZXPMX3/JofSLevon2006.pdf:application/pdf}
}

@article{levon-language-2015,
	title = {Language and the politics of gender},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9781118896877.wbiehs259/full},
	urldate = {2015-10-15},
	journal = {The International Encyclopedia of Human Sexuality},
	author = {Levon, Erez},
	year = {2015},
	file = {IEHS_Levon_2015.pdf:/home/user/Zotero/storage/QBXVQH9V/IEHS_Levon_2015.pdf:application/pdf}
}

@article{levon-teasing-2011,
	title = {{TEASING} {APART} {TO} {BRING} {TOGETHER}: {GENDER} {AND} {SEXUALITY} {IN} {VARIATIONIST} {RESEARCH}},
	volume = {86},
	issn = {0003-1283, 1527-2133},
	shorttitle = {{TEASING} {APART} {TO} {BRING} {TOGETHER}},
	url = {http://americanspeech.dukejournals.org/cgi/doi/10.1215/00031283-1277519},
	doi = {10.1215/00031283-1277519},
	language = {en},
	number = {1},
	urldate = {2015-10-15},
	journal = {American Speech},
	author = {Levon, E.},
	month = mar,
	year = {2011},
	pages = {69--84},
	file = {AmSpLevon2011.pdf:/home/user/Zotero/storage/B2W99J6H/AmSpLevon2011.pdf:application/pdf}
}

@article{levon-hearing-2006,
	title = {{HEARING} "{GAY}": {PROSODY}, {INTERPRETATION}, {AND} {THE} {AFFECTIVE} {JUDGMENTS} {OF} {MEN}'{S} {SPEECH}},
	volume = {81},
	issn = {0003-1283, 1527-2133},
	shorttitle = {{HEARING} "{GAY}"},
	url = {http://americanspeech.dukejournals.org/cgi/doi/10.1215/00031283-2006-003},
	doi = {10.1215/00031283-2006-003},
	language = {en},
	number = {1},
	urldate = {2015-10-15},
	journal = {American Speech},
	author = {Levon, E.},
	month = mar,
	year = {2006},
	pages = {56--78},
	file = {AmSpLevon2006.pdf:/home/user/Zotero/storage/2EW35WCK/AmSpLevon2006.pdf:application/pdf}
}

@book{noauthor-language-nodate,
	title = {Language and the {Politics} of {Sexuality} - {Erez} {Levon}},
	url = {http://www.palgrave.com%2Fpage%2Fdetail%2F%3Fsf1%3Did\_product%26st1%3D354840},
	abstract = {How can we treat sexuality in sociolinguistic research? The question is a deceptively simple one that gets at the heart of a tension in sociolinguistics about how to reconcile standard quantitative models with the knowledge that identity factors like sexuality are not static phenomena. Using the example of lesbians and gay men in Israel, this book develops a new method for studying language and sexuality, one that complicates traditional categories of analysis and asks us to rethink how language functions in its social context. Based on an ethnographically situated examination of language use among activist groups, this book examines how gay and lesbian Israelis work to reconcile their performances of sexuality with Israeli national ideologies of gender and belonging. Combining historical, linguistic and sociological perspectives, the book details the interconnections between sexuality and national politics and describes the effect they both have on language use.},
	urldate = {2015-10-15},
	file = {Snapshot:/home/user/Zotero/storage/JU2M8V2V/detail.html:text/html}
}

@misc{noauthor-table-nodate,
	title = {Table of {Contents}},
	url = {http://sociolinguisticdatacollection.com/table-of-contents/},
	abstract = {- Methodologies for data collection are central to the study of social variation in language. With a focus on real language, a primary aim of sociolinguistics is to create and refine methods for th...},
	urldate = {2015-10-15},
	journal = {Data Collection in Sociolinguistics: Methods and Applications},
	file = {Snapshot:/home/user/Zotero/storage/ZHDDUHMR/table-of-contents.html:text/html}
}

@misc{noauthor-research-nodate,
	title = {Research methods in linguistics},
	url = {http://www.cambridge.org/gb/academic/subjects/languages-linguistics/research-methods-linguistics/},
	abstract = {Academic publishing from Cambridge University Press, including: research monographs, reference books, textbooks, books for professionals, and paperbacks aimed at graduate students.},
	urldate = {2015-10-15},
	journal = {Cambridge University Press},
	file = {Snapshot:/home/user/Zotero/storage/9DK7DK5R/research-methods-linguistics.html:text/html}
}

@article{levon-social-2014,
	title = {Social {Salience} and the {Sociolinguistic} {Monitor} {A} {Case} {Study} of {ING} and {TH}-fronting in {Britain}},
	volume = {42},
	issn = {0075-4242, 1552-5457},
	url = {http://eng.sagepub.com/content/42/3/185},
	doi = {10.1177/0075424214531487},
	abstract = {This article examines the role of social salience, or the relative ability of a linguistic variable to evoke social meaning, in structuring listeners’ perceptions of quantitative sociolinguistic distributions. Building on the foundational work of Labov et al. (2006, 2011) on the “sociolinguistic monitor” (a proposed cognitive mechanism responsible for sociolinguistic perception), we examine whether listeners’ evaluative judgments of speech change as a function of the type of variable presented. We consider two variables in British English, ING and TH-fronting, which we argue differ in their relative social salience. Replicating the design of Labov et al.’s studies, we test 149 British listeners’ reactions to different quantitative distributions of these variables. Our experiments elicit a very different pattern of perceptual responses than those reported previously. In particular, our results suggest that a variable’s social salience determines both whether and how it is perceptually evaluated. We argue that this finding is crucial for understanding how sociolinguistic information is cognitively processed.},
	language = {en},
	number = {3},
	urldate = {2015-10-15},
	journal = {Journal of English Linguistics},
	author = {Levon, Erez and Fox, Sue},
	month = sep,
	year = {2014},
	keywords = {British English, ING, social salience, sociolinguistic monitor, TH-fronting},
	pages = {185--217},
	file = {Snapshot:/home/user/Zotero/storage/7SEQ7MF8/185.html:text/html}
}

@book{zimman-queer-2014,
	title = {Queer {Excursions}: {Retheorizing} {Binaries} in {Language}, {Gender}, and {Sexuality}},
	isbn = {978-0-19-993731-8},
	shorttitle = {Queer {Excursions}},
	abstract = {Across scholarship on gender and sexuality, binaries like female versus male and gay versus straight have been problematized as a symbol of the stigmatization and erasure of non-normative subjects and practices. The chapters in offer a series of distinct perspectives on these binaries, as well as on a number of other, less immediately apparent dichotomies that nevertheless permeate the gendered and sexual lives of speakers. Several chapters focus on the limiting or misleading qualities of binaristic analyses, while others suggest that binaries are a crucial component of social meaning within particular communities of study. Rather than simply accepting binary structures as inevitable, or discarding them from our analyses entirely based on their oppressive or reductionary qualities, this volume advocates for a re-theorization of the binary that affords more complex and contextually-grounded engagement with speakers' own orientations to dichotomous systems. It is from this perspective that contributors identify a number of diverging conceptualizations of binaries, including those that are non-mutually exclusive, those that liberate in the same moment that they constrain, those that are imposed implicitly by researchers, and those that re-contextualize familiar divisions with innovative meanings. Each chapter offers a unique perspective on locally salient linguistic practices that help constitute gender and sexuality in marginalized communities. As a collection, argues that researchers must be careful to avoid the assumption that our own preconceptions about binary social structures will be shared by the communities we study.},
	publisher = {OUP USA Studies in Language and Gender},
	editor = {Zimman, Lal and Davis, Jenny and Raclaw, Joshua},
	month = aug,
	year = {2014},
	keywords = {Linguistics, Academic, Professional, \& General, Language \& Linguistics, Sociolinguistics}
}

@article{levon-categories-2014,
	title = {Categories, stereotypes, and the linguistic perception of sexuality},
	volume = {43},
	issn = {1469-8013},
	url = {http://journals.cambridge.org/article\_S0047404514000554},
	doi = {10.1017/S0047404514000554},
	abstract = {This article examines how social stereotypes influence listeners' perceptions of indexical language. Building on recent developments in linguistics and social psychology, I investigate the extent to which stereotypical attitudes and beliefs about categories of speakers serve to enable the association of linguistic features with particular social meanings while simultaneously blocking others. My arguments are based on an analysis of listener perceptions of the intersecting categories of gender, sexuality, and social class among men in the UK. Using a modified matched-guise paradigm to test three category-relevant variables (mean pitch, spectral characteristics of /s/, and TH-fronting), I demonstrate how the perception of social meaning is governed by a combination of both attitudinal and cognitive factors. This finding is important because it illustrates the listener-dependent nature of sociolinguistic perception. Moreover, it also provides further empirical support for an understanding of social meaning as an emergent property of language-in-use. (speech perception, attitudes and stereotypes, sexuality, phonetic variation)*},
	number = {05},
	urldate = {2015-10-15},
	journal = {Language in Society},
	author = {Levon, Erez},
	month = nov,
	year = {2014},
	pages = {539--566},
	file = {Cambridge Journals Snapshot:/home/user/Zotero/storage/96JTA3PN/displayAbstract.html:text/html;Levon_2014_Categories, stereotypes, and the linguistic perception of sexuality.pdf:/home/user/Zotero/storage/4SKN2HZ9/Levon_2014_Categories, stereotypes, and the linguistic perception of sexuality.pdf:application/pdf}
}

@misc{noauthor-language-nodate-1,
	type = {Text},
	title = {Language and {Masculinities}: {Performances}, {Intersections}, {Dislocations} ({Hardback}) - {Routledge}},
	shorttitle = {Language and {Masculinities}},
	url = {https://www.routledge.com/products/9781138791961},
	abstract = {This volume showcases cutting-edge research in the linguistic and discursive study of masculinities, comprising the first significant edited collection on language and masculinities since Johnson and Meinhof’s 1997 volume. Overall, the chapters are…},
	urldate = {2015-10-15},
	file = {Snapshot:/home/user/Zotero/storage/VDMUVSZK/9781138791961.html:text/html}
}

@article{levon-perception-2015,
	title = {Perception, cognition, and linguistic structure: {The} effect of linguistic modularity and cognitive style on sociolinguistic processing},
	volume = {27},
	issn = {1469-8021},
	shorttitle = {Perception, cognition, and linguistic structure},
	url = {http://journals.cambridge.org/article\_S0954394515000149},
	doi = {10.1017/S0954394515000149},
	abstract = {The Interface Principle posits that morphosyntactic variation does not elicit the same kinds of perceptual reactions as phonetic variables because “members of the speech community evaluate the surface form of language but not more abstract structural features” (Labov, 1993:4). This article examines the effect of linguistic modularity on listeners' social evaluations. Our point of departure is the sociolinguistic monitor, a hypothesized cognitive mechanism that governs frequency-linked perceptual awareness (Labov, Ash, Ravindranath, Weldon, \& Nagy, 2011). Results indicate that “higher level” structural variables are available to the sociolinguistic monitor. Moreover, listeners' reactions are conditioned by independent effects of region of provenance and individual cognitive style. Overall, our findings support the claim that sociolinguistic processing is influenced by a range of social and psychological constraints (Campbell-Kibler, 2011; Preston, 2010; Wagner \& Hesson, 2014) while also demonstrating the need for models of sociolinguistic cognition to include patterns of grammatical variation (Meyerhoff \& Walker, 2013; Walker, 2010).},
	number = {03},
	urldate = {2015-10-15},
	journal = {Language Variation and Change},
	author = {Levon, Erez and Buchstaller, Isabelle},
	month = oct,
	year = {2015},
	pages = {319--348},
	file = {Cambridge Journals Snapshot:/home/user/Zotero/storage/SADMN2SR/displayAbstract.html:text/html;Levon_Buchstaller_2015_Perception, cognition, and linguistic structure.pdf:/home/user/Zotero/storage/VEK579JT/Levon_Buchstaller_2015_Perception, cognition, and linguistic structure.pdf:application/pdf}
}

@article{vanmarcke-influence-2015,
	title = {The influence of age and gender on ultra-rapid categorization},
	volume = {0},
	issn = {1350-6285},
	url = {http://dx.doi.org/10.1080/13506285.2015.1091801},
	doi = {10.1080/13506285.2015.1091801},
	abstract = {In ultra-rapid categorization studies, population-level reaction time differences in performance are consistently reported. In a previous study, we replicated these findings and also observed consistent gender differences in young adults (18–24 years old). We now tested a group of adolescents (11–16 years old) on the same ultra-rapid categorization tasks. Results indicated that age had a significant impact on categorization performance. Although women outperformed men during adulthood, this effect reversed in adolescence (boys faster than girls). This gender x age interaction for categorizing meaningful (non-)social visual scenes could be caused by gender-specific development processes underlying emotion regulation strategies and/or context sensitivity.},
	number = {0},
	urldate = {2015-10-14},
	journal = {Visual Cognition},
	author = {Vanmarcke, Steven and Wagemans, Johan},
	month = oct,
	year = {2015},
	pages = {1--23},
	file = {Snapshot:/home/user/Zotero/storage/PHDX4MPQ/13506285.2015.html:text/html;Vanmarcke_Wagemans_2015_The influence of age and gender on ultra-rapid categorization.pdf:/home/user/Zotero/storage/MIGN5DUX/Vanmarcke_Wagemans_2015_The influence of age and gender on ultra-rapid categorization.pdf:application/pdf}
}

@article{hamilton-impaired-2015,
	title = {Impaired integration in psychopathy: {A} unified theory of psychopathic dysfunction},
	volume = {122},
	issn = {0033-295X},
	shorttitle = {Impaired integration in psychopathy},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d2015-45017-004%26site%3dehost-live},
	doi = {10.1037/a0039703},
	abstract = {This article introduces a novel theoretical framework for psychopathy that bridges dominant affective and cognitive models. According to the proposed impaired integration (II) framework of psychopathic dysfunction, topographical irregularities and abnormalities in neural connectivity in psychopathy hinder the complex process of information integration. Central to the II theory is the notion that psychopathic individuals are “‘wired up’ differently” (Hare, Williamson, \& Harpur, 1988, p. 87). Specific theoretical assumptions include decreased functioning of the Salience and Default Mode Networks, normal functioning in executive control networks, and less coordination and flexible switching between networks. Following a review of dominant models of psychopathy, we introduce our II theory as a parsimonious account of behavioral and brain irregularities in psychopathy. The II theory provides a unified theoretical framework for understanding psychopathic dysfunction and integrates principle tenets of affective and cognitive perspectives. Moreover, it accommodates evidence regarding connectivity abnormalities in psychopathy through its network theoretical perspective. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
	number = {4},
	urldate = {2015-10-14},
	journal = {Psychological Review},
	author = {Hamilton, Rachel K. B. and Hiatt Racer, Kristina and Newman, Joseph P.},
	month = oct,
	year = {2015},
	keywords = {Emotions, *Psychopathy, *Theories, Attention, Cognitive Processes, emotion, integration, Models, network theory, psychopathy, Theories},
	pages = {770--791},
	file = {Hamilton et al_2015_Impaired integration in psychopathy.pdf:/home/user/Zotero/storage/XT7FEQ37/Hamilton et al_2015_Impaired integration in psychopathy.pdf:application/pdf}
}

@article{froyen-bayesian-2015,
	title = {Bayesian hierarchical grouping: {Perceptual} grouping as mixture estimation},
	volume = {122},
	issn = {0033-295X},
	shorttitle = {Bayesian hierarchical grouping},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d2015-39655-001%26site%3dehost-live},
	doi = {10.1037/a0039540},
	abstract = {We propose a novel framework for perceptual grouping based on the idea of mixture models, called Bayesian hierarchical grouping (BHG). In BHG, we assume that the configuration of image elements is generated by a mixture of distinct objects, each of which generates image elements according to some generative assumptions. Grouping, in this framework, means estimating the number and the parameters of the mixture components that generated the image, including estimating which image elements are “owned” by which objects. We present a tractable implementation of the framework, based on the hierarchical clustering approach of Heller and Ghahramani (2005). We illustrate it with examples drawn from a number of classical perceptual grouping problems, including dot clustering, contour integration, and part decomposition. Our approach yields an intuitive hierarchical representation of image elements, giving an explicit decomposition of the image into mixture components, along with estimates of the probability of various candidate decompositions. We show that BHG accounts well for a diverse range of empirical data drawn from the literature. Because BHG provides a principled quantification of the plausibility of grouping interpretations over a wide range of grouping problems, we argue that it provides an appealing unifying account of the elusive Gestalt notion of Prägnanz. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
	number = {4},
	urldate = {2015-10-14},
	journal = {Psychological Review},
	author = {Froyen, Vicky and Feldman, Jacob and Singh, Manish},
	month = oct,
	year = {2015},
	keywords = {Computational Modeling, *Cluster Analysis, *Computational Modeling, *Estimation, *Statistical Probability, *Visual Perception, Bayesian inference, Cluster Analysis, computational model, Estimation, hierarchical representation, Inference, perceptual grouping, Statistical Probability, visual perception},
	pages = {575--597},
	file = {Froyen et al_2015_Bayesian hierarchical grouping.pdf:/home/user/Zotero/storage/5I8PHMFF/Froyen et al_2015_Bayesian hierarchical grouping.pdf:application/pdf}
}

@article{lucas-improved-2015,
	title = {An improved probabilistic account of counterfactual reasoning},
	volume = {122},
	issn = {0033-295X},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d2015-45017-003%26site%3dehost-live},
	doi = {10.1037/a0039655},
	abstract = {When people want to identify the causes of an event, assign credit or blame, or learn from their mistakes, they often reflect on how things could have gone differently. In this kind of reasoning, one considers a counterfactual world in which some events are different from their real-world counterparts and considers what else would have changed. Researchers have recently proposed several probabilistic models that aim to capture how people do (or should) reason about counterfactuals. We present a new model and show that it accounts better for human inferences than several alternative models. Our model builds on the work of Pearl (2000), and extends his approach in a way that accommodates backtracking inferences and that acknowledges the difference between counterfactual interventions and counterfactual observations. We present 6 new experiments and analyze data from 4 experiments carried out by Rips (2010), and the results suggest that the new model provides an accurate account of both mean human judgments and the judgments of individuals. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
	number = {4},
	urldate = {2015-10-14},
	journal = {Psychological Review},
	author = {Lucas, Christopher G. and Kemp, Charles},
	month = oct,
	year = {2015},
	keywords = {Judgment, Reasoning, conditionals, *Causality, *Reasoning, causal reasoning, Causality, counterfactuals},
	pages = {700--734},
	file = {Lucas_Kemp_2015_An improved probabilistic account of counterfactual reasoning.pdf:/home/user/Zotero/storage/GJ7PP9E2/Lucas_Kemp_2015_An improved probabilistic account of counterfactual reasoning.pdf:application/pdf}
}

@article{rasanen-joint-2015,
	title = {A joint model of word segmentation and meaning acquisition through cross-situational learning},
	volume = {122},
	issn = {0033-295X},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d2015-45017-005%26site%3dehost-live},
	doi = {10.1037/a0039702},
	abstract = {Human infants learn meanings for spoken words in complex interactions with other people, but the exact learning mechanisms are unknown. Among researchers, a widely studied learning mechanism is called cross-situational learning (XSL). In XSL, word meanings are learned when learners accumulate statistical information between spoken words and co-occurring objects or events, allowing the learner to overcome referential uncertainty after having sufficient experience with individually ambiguous scenarios. Existing models in this area have mainly assumed that the learner is capable of segmenting words from speech before grounding them to their referential meaning, while segmentation itself has been treated relatively independently of the meaning acquisition. In this article, we argue that XSL is not just a mechanism for word-to-meaning mapping, but that it provides strong cues for proto-lexical word segmentation. If a learner directly solves the correspondence problem between continuous speech input and the contextual referents being talked about, segmentation of the input into word-like units emerges as a by-product of the learning. We present a theoretical model for joint acquisition of proto-lexical segments and their meanings without assuming a priori knowledge of the language. We also investigate the behavior of the model using a computational implementation, making use of transition probability-based statistical learning. Results from simulations show that the model is not only capable of replicating behavioral data on word learning in artificial languages, but also shows effective learning of word segments and their meanings from continuous speech. Moreover, when augmented with a simple familiarity preference during learning, the model shows a good fit to human behavioral data in XSL tasks. These results support the idea of simultaneous segmentation and meaning acquisition and show that comprehensive models of early word segmentation should take referential word meanings into account. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
	number = {4},
	urldate = {2015-10-14},
	journal = {Psychological Review},
	author = {Räsänen, Okko and Rasilo, Heikki},
	month = oct,
	year = {2015},
	keywords = {Language Development, language acquisition, statistical learning, Learning, word learning, Models, *Infant Development, *Language Development, *Learning, *Word Meaning, Infant Development, synergies in word learning, Word Meaning, word segmentation, Words (Phonetic Units)},
	pages = {792--829},
	file = {Räsänen_Rasilo_2015_A joint model of word segmentation and meaning acquisition through.pdf:/home/user/Zotero/storage/4JGI3ARW/Räsänen_Rasilo_2015_A joint model of word segmentation and meaning acquisition through.pdf:application/pdf}
}

@article{rupprecht-retrieval-induced-nodate,
	title = {Retrieval-induced forgetting in item recognition: {Retrieval} specificity revisited},
	issn = {0749-596X},
	shorttitle = {Retrieval-induced forgetting in item recognition},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X1500114X},
	doi = {10.1016/j.jml.2015.09.003},
	abstract = {Retrieval-induced forgetting (RIF) refers to the finding that retrieval practice on a subset of studied items can induce later forgetting of related unpracticed items. Although previous studies indicated that RIF is retrieval specific – i.e., it arises after retrieval practice but not after reexposure cycles -, the results of more recent work suggest otherwise, indicating that some reexposure formats can induce RIF very similar to how retrieval practice does. Whereas this prior work employed recall at test, here we revisited retrieval specificity of RIF employing item recognition. The results of three experiments are reported, which examined the effects of retrieval practice and some of the recently suggested reexposure formats on unpracticed items’ recognition. In each of these experiments, we showed RIF after retrieval practice but did not find any evidence for RIF-like forgetting after reexposure. These findings demonstrate retrieval specificity of RIF in item recognition, challenging strength-based accounts of RIF and indicating a critical role of inhibition in RIF. Together with the results from the recent recall studies, which we replicated in three further experiments, the present findings are consistent with a two-factor account of RIF, which assigns a role for both inhibition and strength-based blocking in RIF. While both inhibition and blocking may contribute to RIF in certain recall formats, only inhibition may induce RIF in item recognition.},
	urldate = {2015-10-14},
	journal = {Journal of Memory and Language},
	author = {Rupprecht, Julia and Bäuml, Karl-Heinz T.},
	keywords = {Blocking, Episodic memory, Inhibition, Recognition, Retrieval-induced forgetting},
	file = {Rupprecht_Bäuml_Retrieval-induced forgetting in item recognition.pdf:/home/user/Zotero/storage/EI5MGQ7R/Rupprecht_Bäuml_Retrieval-induced forgetting in item recognition.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/KX6UZKFS/S0749596X1500114X.html:text/html}
}

@article{westbury-telling-nodate,
	title = {Telling the world’s least funny jokes: {On} the quantification of humor as entropy},
	issn = {0749-596X},
	shorttitle = {Telling the world’s least funny jokes},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001023},
	doi = {10.1016/j.jml.2015.09.001},
	abstract = {In assessing aphasics or conducting experiments using a lexical decision task, we have observed informally that some non-words (NWs) reliably make people laugh. In this paper, we describe a set of studies aimed at illuminating what underlies this effect, performing the first quantitative test of a 200 year old theory of humor proposed by Schopenhauer (1818). We begin with a brief overview of the history of humor theories. Schopenhauer’s theory is formulated in terms of detection/violation of patterns of co-occurrence and thereby suggests a method to quantify NW humor using Shannon entropy. A survey study demonstrates that there is much more consistency than could be expected by chance in human judgments of which NWs are funny. Analysis of that survey data and two experiments all demonstrate that Shannon entropy does indeed correctly predict human judgments of NW funniness, demonstrating as well that the perceived humor is a quantifiable function of how far the NWs are from being words.},
	urldate = {2015-10-14},
	journal = {Journal of Memory and Language},
	author = {Westbury, Chris and Shaoul, Cyrus and Moroschan, Gail and Ramscar, Michael},
	keywords = {Information theory, Humor, Lexical access, Non-words, Schopenhauer, Shannon},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/RQARH6BQ/S0749596X15001023.html:text/html;Westbury et al_Telling the world’s least funny jokes.pdf:/home/user/Zotero/storage/H6HUDNG7/Westbury et al_Telling the world’s least funny jokes.pdf:application/pdf}
}

@article{kittredge-learning-nodate,
	title = {Learning to speak by listening: {Transfer} of phonotactics from perception to production},
	issn = {0749-596X},
	shorttitle = {Learning to speak by listening},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15000972},
	doi = {10.1016/j.jml.2015.08.001},
	abstract = {The language production and perception systems rapidly learn novel phonotactic constraints. In production, for example, producing syllables in which /f/ is restricted to onset position (e.g. as /h/ is in English) causes one’s speech errors to mirror that restriction. We asked whether or not perceptual experience of a novel phonotactic distribution transfers to production. In three experiments, participants alternated hearing and producing strings of syllables. In the same condition, the production and perception trials followed identical phonotactics (e.g. /f/ is onset). In the opposite condition, they followed reverse constraints (e.g. /f/ is onset for production, but /f/ is coda for perception). The tendency for speech errors to follow the production constraint was diluted when the opposite pattern was present on perception trials, thus demonstrating transfer of learning from perception to production. Transfer only occurred for perceptual tasks that may involve internal production, including an error monitoring task, which we argue engages production via prediction.},
	urldate = {2015-10-14},
	journal = {Journal of Memory and Language},
	author = {Kittredge, Audrey K. and Dell, Gary S.},
	keywords = {Learning, Language perception, Language production, Phonotactics, Speech errors, Transfer},
	file = {Kittredge_Dell_Learning to speak by listening.pdf:/home/user/Zotero/storage/BZISDQJS/Kittredge_Dell_Learning to speak by listening.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/3MTRU5HW/S0749596X15000972.html:text/html}
}

@article{van-den-hurk-search-nodate,
	title = {The {Search} for the {Face} of the {Visual} {Homunculus}},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002314},
	doi = {10.1016/j.tics.2015.09.007},
	abstract = {The functional organization within face-sensitive regions in the brain is largely unknown. A new fMRI study provided evidence that a face-selective region contains neighboring patches of cortex that encode physically neighboring face features. We suggest that multiple mechanisms should be considered for a full understanding of the functional maps in face-selective cortex.},
	urldate = {2015-10-14},
	journal = {Trends in Cognitive Sciences},
	author = {van den Hurk, Job and Pegado, Felipe and Martens, Farah and Op de Beeck, Hans P.},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/7SP8ZDHA/S1364661315002314.html:text/html;van den Hurk et al_The Search for the Face of the Visual Homunculus.pdf:/home/user/Zotero/storage/D7KP7M6T/van den Hurk et al_The Search for the Face of the Visual Homunculus.pdf:application/pdf}
}

@article{creel-apples-nodate,
	title = {Apples and {Oranges}: {Developmental} {Discontinuities} in {Spoken}-{Language} {Processing}?},
	issn = {1364-6613},
	shorttitle = {Apples and {Oranges}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002302},
	doi = {10.1016/j.tics.2015.09.006},
	abstract = {Much research focuses on speech processing in infancy, sometimes generating the impression that speech-sound categories do not develop further. Yet other studies suggest substantial plasticity throughout mid-childhood. Differences between infant versus child and adult experimental methods currently obscure how language processing changes across childhood, calling for approaches that span development.},
	urldate = {2015-10-14},
	journal = {Trends in Cognitive Sciences},
	author = {Creel, Sarah C. and Quam, Carolyn},
	keywords = {speech perception, word learning, developmental methodologies, phonological development},
	file = {Creel_Quam_Apples and Oranges.pdf:/home/user/Zotero/storage/Z52F8TT7/Creel_Quam_Apples and Oranges.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/G7BXA8HN/S1364661315002302.html:text/html}
}

@article{noauthor-bilingual-2010,
	title = {bilingual processing; language non-selective activation; lexical decision; eye movements; {Japanese}-{English} bilinguals},
	volume = {1},
	number = {780},
	year = {2010},
	file = {MiwaDijkstraBolgerBaayen2013BLC:/home/user/Zotero/storage/8TD79ATD/MiwaDijkstraBolgerBaayen2013BLC.pdf:application/pdf}
}

@article{noauthor-unacceptable-nodate,
	title = {Unacceptable but comprehensible: the facilitation effect of resumptive pronouns},
	number = {McCloskey 2006},
	pages = {1--46},
	file = {RPdraft-final-Apr2015:/home/user/Zotero/storage/UIZD74JI/RPdraft-final-Apr2015.pdf:application/pdf}
}

@article{noauthor-evolutionary-2002,
	title = {Evolutionary {Game} {Theory} for linguists . {A} primer {Gerhard} {J} ¨ {Stanford} {University} and {University} of {Potsdam}},
	year = {2002},
	pages = {1--43},
	file = {egtPrimer:/home/user/Zotero/storage/FS5WV488/egtPrimer.pdf:application/pdf}
}

@article{noauthor-definiteness-nodate,
	title = {Definiteness as {Maximal} {Informativeness}},
	pages = {1--2},
	file = {glow-abstract-2011:/home/user/Zotero/storage/U6AP6WHG/glow-abstract-2011.pdf:application/pdf}
}

@article{noauthor-no-2010,
	title = {No {Title}},
	volume = {2009},
	number = {7},
	year = {2010},
	pages = {1--9},
	file = {names:/home/user/Zotero/storage/TPEH2U3B/names.pdf:application/pdf;Unknown - 2010 - No Title:/home/user/Zotero/storage/5BP6D9EK/Unknown - 2010 - No Title.pdf:application/pdf}
}

@inproceedings{cadilhac-algebraic-2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Algebraic} {Theory} of {Parikh} {Automata}},
	isbn = {978-3-642-40662-1},
	url = {http://dblp.uni-trier.de/db/conf/cai/cai2013.html#CadilhacKM13},
	doi = {10.1007/978-3-642-40663-8},
	booktitle = {Algebraic {Informatics} - 5th {International} {Conference}, {{CAI}} 2013, {Porquerolles}, {France}, {September} 3-6, 2013. {Proceedings}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cadilhac, Michaël and Krebs, Andreas and McKenzie, Pierre},
	editor = {Muntean, Traian and Poulakis, Dimitrios and Rolland, Robert},
	year = {2013},
	pages = {60--73}
}

@article{cadilhac-universite-2012,
	title = {Université de {Montréal} {Automates} à contraintes semilinéaires {Automata} with a semilinear constraint par {Michaël} {Cadilhac} {Département} d ’ informatique et de recherche opérationnelle {Faculté} des arts et des sciences},
	author = {{Cadilhac}},
	year = {2012},
	file = {Cadilhac - 2012 - Université de Montréal Automates à contraintes semilinéaires Automata with a semilinear constraint par Micha:/home/user/Zotero/storage/WVFEB3CF/Cadilhac - 2012 - Université de Montréal Automates à contraintes semilinéaires Automata with a semilinear constraint par Micha.pdf:application/pdf}
}

@article{buss-optimal-1992,
	title = {An {Optimal} {Parallel} {Algorithm} for {Formula} {Evaluation}},
	volume = {21},
	number = {4},
	journal = {SIAM J. Comput.},
	author = {Buss, Samuel R and Cook, S and Gupta, A and Ramachandran, V},
	year = {1992},
	pages = {755--780}
}

@article{burris-course-1981,
	series = {Graduate {Texts} in {Mathematics}},
	title = {A {Course} in {Universal} {Algebra}},
	volume = {78},
	issn = {978-1-4613-8132-7},
	url = {http://www.springerlink.com/index/10.1007/978-1-4613-8130-3},
	doi = {10.1007/978-1-4613-8130-3},
	author = {Burris, Stanley and Sankappanavar, H. P.},
	year = {1981},
	file = {Burris, Sankappanavar - 1981 - A Course in Universal Algebra:/home/user/Zotero/storage/5MXPBVVG/Burris, Sankappanavar - 1981 - A Course in Universal Algebra.pdf:application/pdf}
}

@article{bulatov-dichotomy-2002,
	title = {A dichotomy theorem for constraints on a three-element set},
	issn = {0-7695-1822-2},
	doi = {10.1109/SFCS.2002.1181990},
	abstract = {The Constraint Satisfaction Problem (CSP) provides a common framework for many combinatorial problems. The general CSP is known to be NP-complete; however, certain restrictions on the possible form of constraints may affect the complexity, and lead to tractable problem classes. There is, therefore, a fundamental research direction, aiming to separate those subclasses of the CSP which are tractable, from those which remain NP-complete. In 1978 Schaefer gave an exhaustive solution of this problem for the CSP on a 2-element domain. In this paper we generalise this result to a classification of the complexity of CSPs on a 3-element domain. The main result states that every subclass of the CSP defined by a set of allowed constraints is either tractable or NP-complete, and the criterion separating them is that conjectured by Bulatov et al. (2001). We also exhibit a polynomial time algorithm which, for a given set of allowed constraints, determines whether if this set gives rise to a tractable problem class. To obtain the main result and the algorithm we extensively use the algebraic technique for the CSP developed by Jeavons (1998) and Bulatov et al.},
	journal = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
	author = {Bulatov, a.a.},
	year = {2002},
	file = {Bulatov - 2002 - A dichotomy theorem for constraints on a three-element set:/home/user/Zotero/storage/6VDRKHRC/Bulatov - 2002 - A dichotomy theorem for constraints on a three-element set.pdf:application/pdf}
}

@inproceedings{buss-boolean-1987,
	title = {The {Boolean} {Formula} {Value} {Problem} {Is} in {ALOGTIME}},
	booktitle = {{STOC}},
	publisher = {ACM},
	author = {Buss, Samuel R},
	year = {1987},
	pages = {123--131}
}

@article{buss-optimal-1992-1,
	title = {An optimal parallel algorithm for formula evaluation},
	url = {http://epubs.siam.org/doi/abs/10.1137/0221046},
	journal = {SIAM Journal on Computing},
	author = {Buss, S and Cook, S and Gupta, A and Ramachandran, V},
	year = {1992},
	pages = {1--42},
	file = {Buss et al. - 1992 - An optimal parallel algorithm for formula evaluation:/home/user/Zotero/storage/Q5HETN7R/Buss et al. - 1992 - An optimal parallel algorithm for formula evaluation.pdf:application/pdf}
}

@article{buchi-weak-1960,
	title = {Weak second-order arithmetic and finite automata},
	volume = {6},
	journal = {Z. Math. Logik Gundlag. Math. 6},
	author = {Büchi, J R},
	year = {1960},
	pages = {66--92}
}

@article{brodsky-impossibility-2005,
	title = {An impossibility gap between width-4 and width-5 permutation branching programs},
	volume = {94},
	doi = {10.1016/j.ipl.2005.01.012},
	number = {January},
	journal = {Information Processing Letters},
	author = {Brodsky, Alex},
	year = {2005},
	keywords = {Computational complexity, Branching programs, Formal languages},
	pages = {159--164},
	file = {Brodsky - 2005 - An impossibility gap between width-4 and width-5 permutation branching programs:/home/user/Zotero/storage/NFPIENE4/Brodsky - 2005 - An impossibility gap between width-4 and width-5 permutation branching programs.pdf:application/pdf}
}

@article{branco-equations-2009,
	title = {Equations defining the polynomial closure of a lattice of regular languages},
	volume = {5556 LNCS},
	issn = {3642029299},
	doi = {10.1007/978-3-642-02930-1\_10},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Branco, M. J J and Pin, J. É},
	year = {2009},
	pages = {115--126},
	file = {Branco, Pin - 2009 - Equations defining the polynomial closure of a lattice of regular languages:/home/user/Zotero/storage/8WP9PBN2/Branco, Pin - 2009 - Equations defining the polynomial closure of a lattice of regular languages.pdf:application/pdf}
}

@article{brainerd-minimalization-1968,
	title = {The {Minimalization} of {Tree} {Automata}},
	volume = {13},
	url = {http://dx.doi.org/10.1016/S0019-9958(68)90917-0},
	doi = {10.1016/S0019-9958(68)90917-0},
	number = {5},
	journal = {Information and Control},
	author = {Brainerd, Walter S},
	year = {1968},
	pages = {484--491}
}

@article{bott-hermitian-1965,
	title = {Hermitian vector bundles and the equidistribution of the zeroes of their holomorphic sections},
	volume = {114},
	url = {http://link.springer.com/10.1007/BF02391818},
	doi = {10.1007/BF02391818},
	number = {1},
	journal = {Acta Mathematica},
	author = {Bott, Raoul and Chern, S. S.},
	month = dec,
	year = {1965},
	pages = {71--112},
	file = {Bott, Chern - 1965 - Hermitian vector bundles and the equidistribution of the zeroes of their holomorphic sections:/home/user/Zotero/storage/UNUQGEMK/Bott, Chern - 1965 - Hermitian vector bundles and the equidistribution of the zeroes of their holomorphic sections.pdf:application/pdf}
}

@article{borodin-relating-1977,
	title = {On {Relating} {Time} and {Space} to {Size} and {Depth}},
	volume = {6},
	number = {4},
	journal = {SIAM J. Comput.},
	author = {Borodin, Allan},
	year = {1977},
	pages = {733--744}
}

@inproceedings{borger-computer-1991,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Science} {Logic}, 4th {Workshop}, {CSL} '90, {Heidelberg}, {Germany}, {October} 1-5, 1990, {Proceedings}},
	volume = {533},
	isbn = {3-540-54487-9},
	booktitle = {{CSL}},
	publisher = {Springer},
	editor = {Börger, Egon and Büning, Hans Kleine and Richter, Michael M and Schönfeld, Wolfgang},
	year = {1991}
}

@inproceedings{bonfante-programming-2006,
	title = {Some {Programming} {Languages} for {Logspace} and {Ptime}},
	booktitle = {{AMAST}},
	author = {Bonfante, Guillaume},
	year = {2006},
	pages = {66--80}
}

@inproceedings{bojanczyk-forest-2008,
	series = {Texts in {Logic} and {Games}},
	title = {Forest algebras},
	volume = {2},
	isbn = {978-90-5356-576-6},
	url = {http://books.google.com/books?hl=en&lr=&id=dhVQPAqRpL0C&oi=fnd&pg=PA107&dq=Forest+Algebras&ots=q8AzStP1rq&sig=ghiBiAaFXxLmPVHL-0V514ammug},
	booktitle = {Logic and {Automata}: {History} and {Perspectives}},
	publisher = {Amsterdam University Press},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	editor = {Flum, Jörg and Grädel, Erich and Wilke, Thomas},
	year = {2008},
	pages = {107--132},
	file = {Bojanczyk, Walukiewicz - 2007 - Forest algebras:/home/user/Zotero/storage/4D34HHUJ/Bojanczyk, Walukiewicz - 2007 - Forest algebras.pdf:application/pdf}
}

@article{bojanczyk-characterizing-2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Characterizing {{EF}} and {{EX}} tree logics},
	volume = {358},
	issn = {3-540-22940-X},
	url = {http://dx.doi.org/10.1007/978-3-540-28644-8\_9},
	doi = {10.1007/978-3-540-28644-8\_9},
	number = {2-3},
	journal = {Theor. Comput. Sci.},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	editor = {Gardner, Philippa and Yoshida, Nobuko},
	year = {2006},
	pages = {255--272},
	file = {Bojanczyk, Walukiewicz - 2006 - Characterizing EF and EX tree logics:/home/user/Zotero/storage/EM58GH2Q/Bojanczyk, Walukiewicz - 2006 - Characterizing EF and EX tree logics.pdf:application/pdf;Bojanczyk, Walukiewicz - 2006 - Characterizing EF and EX tree logics(2):/home/user/Zotero/storage/HCUCD7I9/Bojanczyk, Walukiewicz - 2006 - Characterizing EF and EX tree logics(2).pdf:application/pdf}
}

@article{bojanczyk-forest-2007,
	title = {Forest algebras},
	url = {http://books.google.com/books?hl=en&lr=&id=dhVQPAqRpL0C&oi=fnd&pg=PA107&dq=Forest+Algebras&ots=q8AzStP1rq&sig=ghiBiAaFXxLmPVHL-0V514ammug},
	journal = {Automata and logic: history and …},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	year = {2007},
	pages = {1--24},
	file = {Bojanczyk, Walukiewicz - 2007 - Forest algebras:/home/user/Zotero/storage/QB4PC5PR/Bojanczyk, Walukiewicz - 2007 - Forest algebras.pdf:application/pdf}
}

@article{bojanczyk-characterizing-2006-1,
	title = {Characterizing {{EF}} and {{EX}} tree logics},
	volume = {358},
	url = {http://dx.doi.org/10.1016/j.tcs.2006.01.018},
	doi = {10.1016/j.tcs.2006.01.018},
	number = {2-3},
	journal = {Theor. Comput. Sci.},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	year = {2006},
	pages = {255--272}
}

@article{bojanczyk-wreath-2012,
	title = {Wreath {Products} of {Forest} {Algebras}, with {Applications} to {Tree} {Logics}},
	volume = {8},
	url = {http://dx.doi.org/10.2168/LMCS-8(3:19)2012},
	doi = {10.2168/LMCS-8(3:19)2012},
	number = {3},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj and {straubing} and Walukiewicz, Igor},
	year = {2012},
	file = {Unknown - Unknown - WREATH PRODUCTS OF FOREST ALGEBRAS, WITH APPLICATIONS TO TREE LOGICS:/home/user/Zotero/storage/CM7DRTRC/Unknown - Unknown - WREATH PRODUCTS OF FOREST ALGEBRAS, WITH APPLICATIONS TO TREE LOGICS.pdf:application/pdf}
}

@inproceedings{bojanczyk-piecewise-2008,
	title = {Piecewise {Testable} {Tree} {Languages}},
	isbn = {978-0-7695-3183-0},
	url = {http://dx.doi.org/10.1109/LICS.2008.46},
	doi = {10.1109/LICS.2008.46},
	booktitle = {Proceedings of the {Twenty}-{Third} {Annual} {{IEEE}} {Symposium} on {Logic} in {Computer} {Science}, {{LICS}} 2008, 24-27 {June} 2008, {Pittsburgh}, {PA}, {{USA}}},
	publisher = {{IEEE} Computer Society},
	author = {Bojanczyk, Mikolaj and Segoufin, Luc and Straubing, Howard},
	year = {2008},
	pages = {442--451}
}

@article{bojanczyk-piecewise-2012,
	title = {Piecewise testable tree languages},
	volume = {8},
	issn = {978-0-7695-3183-0},
	url = {http://dx.doi.org/10.2168/LMCS-8(3:26)2012},
	doi = {10.1109/LICS.2008.46},
	number = {3},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj and Segoufin, Luc and Straubing, Howard},
	year = {2012},
	keywords = {and phrases, algebra, first-order logic on trees},
	pages = {442--451},
	file = {Nczyk, Segoufin, Straubing - 2012 - Piecewise testable tree languages:/home/user/Zotero/storage/TDPN537S/Nczyk, Segoufin, Straubing - 2012 - Piecewise testable tree languages.pdf:application/pdf}
}

@article{bojanczyk-wreath-2012-1,
	title = {Wreath {Products} of {Forest} {Algebras}, with {Applications} to {Tree} {Logics}},
	volume = {8},
	url = {http://dblp.uni-trier.de/db/journals/lmcs/lmcs8.html#abs-1208-6172},
	doi = {10.2168/LMCS-8(3:19)2012},
	number = {3},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor and Straubing, Howard},
	editor = {Thomas, Wolfgang},
	month = sep,
	year = {2012}
}

@inproceedings{bojanczyk-forest-2008-1,
	series = {Texts in {Logic} and {Games}},
	title = {Forest algebras},
	volume = {2},
	isbn = {978-90-5356-576-6},
	booktitle = {Logic and {Automata}: {History} and {Perspectives} [in {Honor} of {Wolfgang} {Thomas}].},
	publisher = {Amsterdam University Press},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	editor = {Flum, Jörg and Grädel, Erich and Wilke, Thomas},
	year = {2008},
	pages = {107--132}
}

@inproceedings{bojanczyk-characterizing-2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Characterizing {{EF}} and {{EX}} {Tree} {Logics}},
	volume = {3170},
	isbn = {3-540-22940-X},
	url = {http://dx.doi.org/10.1007/978-3-540-28644-8\_9},
	doi = {10.1007/978-3-540-28644-8\_9},
	booktitle = {{{CONCUR}} 2004 - {Concurrency} {Theory}, 15th {International} {Conference}, {London}, {UK}, {August} 31 - {September} 3, 2004, {Proceedings}},
	publisher = {Springer},
	author = {Bojanczyk, Mikolaj and Walukiewicz, Igor},
	editor = {Gardner, Philippa and Yoshida, Nobuko},
	year = {2004},
	pages = {131--145}
}

@article{bojanczyk-weak-nodate,
	title = {Weak {MSO} + {U} over infinite trees ∗},
	author = {Bojańczyk, Mikołaj and Toruńczyk, Szymon},
	keywords = {4230, digital object identifier 10, lipics, stacs, 2012, 648, and phrases infinite trees, distance automata, mso, profinite words, u},
	pages = {648--660},
	file = {confstacsBojanczykT12:/home/user/Zotero/storage/S8RWXECC/confstacsBojanczykT12.pdf:application/pdf}
}

@article{bojanczyk-tree-2010,
	title = {Tree {Languages} {Defined} in {First}-{Order} {Logic} with {One} {Quantifier} {Alternation}},
	volume = {6},
	url = {http://dx.doi.org/10.2168/LMCS-6(4:1)2010},
	doi = {10.2168/LMCS-6(4:1)2010},
	number = {4},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj and Segoufin, Luc},
	year = {2010}
}

@article{bojanczyk-regular-2012,
	title = {Regular languages of infinite trees that are boolean combinations of open sets},
	volume = {7392 LNCS},
	issn = {9783642315848},
	doi = {10.1007/978-3-642-31585-5-13},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bojańczyk, Mikołaj and Place, Thomas},
	year = {2012},
	pages = {104--115},
	file = {bool-open:/home/user/Zotero/storage/B385CZ7W/bool-open.pdf:application/pdf}
}

@article{bojanczyk-tree-2010-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tree {Languages} {Defined} in {First}-{Order} {Logic} with {One} {Quantifier} {Alternation}},
	volume = {6},
	issn = {978-3-540-70582-6},
	url = {http://dx.doi.org/10.2168/LMCS-6(4:1)2010},
	doi = {10.2168/LMCS-6},
	number = {4},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj and Segoufin, Luc},
	editor = {Aceto, Luca and Damgård, Ivan and Goldberg, Leslie Ann and Halldórsson, Magnús M and Ingólfsdóttir, Anna and Walukiewicz, Igor},
	year = {2010},
	pages = {1--26},
	file = {Bojanczyk, Segoufin - 2010 - Tree languages defined in first-order logic with one quantifier alternation:/home/user/Zotero/storage/IHGS2HNU/Bojanczyk, Segoufin - 2010 - Tree languages defined in first-order logic with one quantifier alternation.pdf:application/pdf;Segoufin - Unknown - Tree languages defined in first-order logic with one quantifier alternation:/home/user/Zotero/storage/MKBKNPE2/Segoufin - Unknown - Tree languages defined in first-order logic with one quantifier alternation.pdf:application/pdf}
}

@inproceedings{bojanczyk-tree-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tree {Languages} {Defined} in {First}-{Order} {Logic} with {One} {Quantifier} {Alternation}},
	volume = {5126},
	isbn = {978-3-540-70582-6},
	url = {http://dx.doi.org/10.1007/978-3-540-70583-3\_20},
	doi = {10.1007/978-3-540-70583-3\_20},
	booktitle = {Automata, {Languages} and {Programming}, 35th {International} {Colloquium}, {{ICALP}} 2008, {Reykjavik}, {Iceland}, {July} 7-11, 2008, {Proceedings}, {Part} {{II}} - {Track} {{B}:} {Logic}, {Semantics}, and {Theory} of {Programming} {\&} {Track} {{C}:} {Security} and {Cryptography} {Foundations}},
	publisher = {Springer},
	author = {Bojanczyk, Mikolaj and Segoufin, Luc},
	editor = {Aceto, Luca and Damgård, Ivan and Goldberg, Leslie Ann and Halldórsson, Magnús M and Ingólfsdóttir, Anna and Walukiewicz, Igor},
	year = {2008},
	pages = {233--245}
}

@article{bojanczyk-regular-2013,
	title = {Regular languages of thin trees},
	issn = {9783939897507},
	doi = {10.4230/LIPIcs.STACS.2013.562},
	number = {239850},
	journal = {Stacs},
	author = {Bojańczyk, Mikołaj and Idziaszek, Tomasz and Skrzypczak, Michał},
	year = {2013},
	keywords = {regular languages, and phrases infinite trees, effective characterizations, topological},
	pages = {562--573},
	file = {confstacsBojanczykIS13:/home/user/Zotero/storage/NAX37JTI/confstacsBojanczykIS13.pdf:application/pdf}
}

@article{bojanczyk-bounds-2006,
	title = {Bounds in \$\omega\$-regularity},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Bounds+in+?-regularity#0},
	journal = {Lics'06},
	author = {Bojańczyk, Mikołaj and Colcombet, Thomas and Bojanczyk, M. and Colcombet, T.},
	year = {2006},
	pages = {285--296},
	file = {conflicsBojanczykC06:/home/user/Zotero/storage/N5KBQS5B/conflicsBojanczykC06.pdf:application/pdf}
}

@article{bojanczyk-factorization-2009,
	title = {Factorization forests},
	volume = {5583 LNCS},
	issn = {3642027369},
	doi = {10.1007/978-3-642-02737-6\_1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bojańczyk, Mikołaj},
	year = {2009},
	pages = {1--17},
	file = {forests-dlt:/home/user/Zotero/storage/IGV2REWF/forests-dlt.pdf:application/pdf}
}

@article{bojanczyk-transducers-nodate,
	title = {Transducers with origin information},
	author = {Bojanczyk, Mikolaj},
	pages = {1--21},
	file = {conficalpBojanczyk14:/home/user/Zotero/storage/8Q47FPRS/conficalpBojanczyk14.pdf:application/pdf}
}

@phdthesis{bojanczyk-decidable-nodate,
	title = {Decidable {Properties} of {Tree} {Languages}},
	author = {Bojanczyk, Mikolaj}
}

@inproceedings{bojanczyk-factorization-2009-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Factorization {Forests}},
	volume = {5583},
	isbn = {978-3-642-02736-9},
	url = {http://dx.doi.org/10.1007/978-3-642-02737-6\_1},
	doi = {10.1007/978-3-642-02737-6\_1},
	booktitle = {Developments in {Language} {Theory}, 13th {International} {Conference}, {{DLT}} 2009, {Stuttgart}, {Germany}, {June} 30 - {July} 3, 2009. {Proceedings}},
	publisher = {Springer},
	author = {Bojanczyk, Mikolaj},
	editor = {Diekert, Volker and Nowotka, Dirk},
	year = {2009},
	pages = {1--17}
}

@article{bojanczyk-two-way-2009,
	title = {Two-{Way} {Unary} {Temporal} {Logic} over {Trees}},
	volume = {5},
	url = {http://arxiv.org/abs/0904.4119},
	number = {3},
	journal = {Logical Methods in Computer Science},
	author = {Bojanczyk, Mikolaj},
	year = {2009}
}

@article{bojaaczyk-two-way-2007,
	title = {Two-way unary temporal logic over trees},
	issn = {0769529089},
	doi = {10.1109/LICS.2007.51},
	abstract = {We consider a temporal logic EF + F-1 for unranked, unordered finite trees. The logic has two operators: EFphi , which says "in some proper descendant phi holds", and F-1phi , which says "in some proper ancestor phi holds". We present an algorithm for deciding if a regular language of unranked finite trees can be expressed in EF + F-1. The algorithm uses a characterization expressed in terms of forest algebras.},
	journal = {Proceedings - Symposium on Logic in Computer Science},
	author = {Bojaáczyk, Mikołaj},
	year = {2007},
	pages = {121--130},
	file = {unarytemporal:/home/user/Zotero/storage/HCJ5FVK5/unarytemporal.pdf:application/pdf}
}

@article{blumensath-syntactic-nodate,
	title = {A syntactic congruence for languages of birooted trees},
	author = {Blumensath, Achim and Janin, David},
	pages = {1--25},
	file = {Blumensath, Janin - Unknown - A syntactic congruence for languages of birooted trees:/home/user/Zotero/storage/FWN5A94T/Blumensath, Janin - Unknown - A syntactic congruence for languages of birooted trees.pdf:application/pdf}
}

@inproceedings{bojanczyk-two-way-2002,
	title = {Two-way alternating automata and finite models},
	volume = {2380},
	isbn = {3-540-43864-5},
	url = {http://www.springerlink.com/index/rh6qlkctrjby8fha.pdf},
	booktitle = {{ICALP}},
	author = {Bojańczyk, Mikołaj},
	year = {2002},
	pages = {833--844},
	file = {conficalpBojanczyk02:/home/user/Zotero/storage/JB8ZH889/conficalpBojanczyk02.pdf:application/pdf}
}

@inproceedings{bojanczyk-effective-2008,
	title = {Effective characterizations of tree logics},
	isbn = {978-1-60558-108-8},
	url = {http://doi.acm.org/10.1145/1376916.1376925},
	doi = {10.1145/1376916.1376925},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {{ACM}} {{SIGMOD}-{SIGACT}-{SIGART}} {Symposium} on {Principles} of {Database} {Systems}, {{PODS}} 2008, {June} 9-11, 2008, {Vancouver}, {BC}, {Canada}},
	publisher = {ACM},
	author = {Bojanczyk, Mikolaj},
	editor = {Lenzerini, Maurizio and Lembo, Domenico},
	year = {2008},
	pages = {53--66},
	file = {Bojanczyk - 2008 - Effective characterizations of tree logics:/home/user/Zotero/storage/8D27QJGD/Bojanczyk - 2008 - Effective characterizations of tree logics.pdf:application/pdf}
}

@inproceedings{bojanczyk-forest-2007-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Forest {Expressions}},
	volume = {4646},
	isbn = {978-3-540-74914-1},
	url = {http://dx.doi.org/10.1007/978-3-540-74915-8\_14},
	doi = {10.1007/978-3-540-74915-8\_14},
	booktitle = {Computer {Science} {Logic}, 21st {International} {Workshop}, {{CSL}} 2007, 16th {Annual} {Conference} of the {EACSL}, {Lausanne}, {Switzerland}, {September} 11-15, 2007, {Proceedings}},
	publisher = {Springer},
	author = {Bojanczyk, Mikolaj},
	editor = {Duparc, Jacques and Henzinger, Thomas A},
	year = {2007},
	pages = {146--160}
}

@phdthesis{bojanczyk-decidable-nodate-1,
	title = {Decidable {Properties} of {Tree} {Languages}},
	author = {Bojanczyk, Mikolaj},
	file = {Bojanczyk - Unknown - Decidable Properties of Tree Languages:/home/user/Zotero/storage/8DAW3X4F/Bojanczyk - Unknown - Decidable Properties of Tree Languages.pdf:application/pdf;Øý, Ù - Unknown - Ð èöóô öø × ó ìö ä ò ù ×:/home/user/Zotero/storage/KVZNKPBK/Øý, Ù - Unknown - Ð èöóô öø × ó ìö ä ò ù ×.pdf:application/pdf}
}

@inproceedings{bojanczyk-algebra-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Algebra for {Tree} {Languages}},
	volume = {5771},
	isbn = {978-3-642-04026-9},
	url = {http://dx.doi.org/10.1007/978-3-642-04027-6\_1},
	doi = {10.1007/978-3-642-04027-6\_1},
	booktitle = {Computer {Science} {Logic}, 23rd international {Workshop}, {{CSL}} 2009, 18th {Annual} {Conference} of the {EACSL}, {Coimbra}, {Portugal}, {September} 7-11, 2009. {Proceedings}},
	publisher = {Springer},
	author = {Bojanczyk, Mikolaj},
	editor = {Grädel, Erich and Kahle, Reinhard},
	year = {2009},
	pages = {1--1}
}

@inproceedings{bojanczyk-effective-2008-1,
	title = {Effective characterizations of tree logics},
	isbn = {978-1-60558-108-8},
	url = {http://doi.acm.org/10.1145/1376916.1376925},
	doi = {10.1145/1376916.1376925},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {{ACM}} {{SIGMOD}-{SIGACT}-{SIGART}} {Symposium} on {Principles} of {Database} {Systems}, {{PODS}} 2008, {June} 9-11, 2008, {Vancouver}, {BC}, {Canada}},
	publisher = {ACM},
	author = {Bojanczyk, Mikolaj},
	editor = {Lenzerini, Maurizio and Lembo, Domenico},
	year = {2008},
	pages = {53--66}
}

@article{bojanczyk-recognisable-2015,
	title = {Recognisable {Languages} over {Monads}},
	author = {Bojanczyk, Mikolaj},
	year = {2015},
	pages = {1--92},
	file = {Bojanczyk - 2015 - Recognisable Languages over Monads:/home/user/Zotero/storage/PWAGPTDP/Bojanczyk - 2015 - Recognisable Languages over Monads.pdf:application/pdf}
}

@article{blumensath-two-way-2014,
	title = {Two-{Way} {Cost} {Automata} and {Cost} {Logics} over {Infinite} {Trees}},
	issn = {9781450328869},
	doi = {10.1145/2603088.2603104},
	author = {Blumensath, Achim and Colcombet, Thomas and Kuperberg, Denis and Parys, Paweł and Boom, Michael Vanden},
	year = {2014},
	pages = {1--31},
	file = {Blumensath et al. - 2014 - Two-Way Cost Automata and Cost Logics over Infinite Trees:/home/user/Zotero/storage/SD6CKKXB/Blumensath et al. - 2014 - Two-Way Cost Automata and Cost Logics over Infinite Trees.pdf:application/pdf}
}

@article{blumensath-asymptotic-nodate,
	title = {Asymptotic {Monadic} {Second}-{Order} {Logic}},
	number = {259454},
	author = {Blumensath, Achim and Carton, Olivier and Colcombet, Thomas},
	file = {Blumensath, Carton, Colcombet - Unknown - Asymptotic Monadic Second-Order Logic:/home/user/Zotero/storage/7XA76GSR/Blumensath, Carton, Colcombet - Unknown - Asymptotic Monadic Second-Order Logic.pdf:application/pdf}
}

@article{blumensath--nodate,
	title = {A  {A}  {P}   {R}  ’  {T}  {T} },
	author = {Blumensath, Achim},
	keywords = {infinite trees, monadic second-order logic, recognisabil-},
	file = {Blumensath - Unknown - A  A  P   R  ’  T  T :/home/user/Zotero/storage/2WZX2VJN/Blumensath - Unknown - A  A  P   R  ’  T  T .pdf:application/pdf}
}

@phdthesis{blumensath-automatic-1999,
	title = {Automatic {Structures}},
	author = {Blumensath, Achim},
	year = {1999},
	file = {Blumensath - 1999 - Automatic Structures:/home/user/Zotero/storage/8Q4AMHQS/Blumensath - 1999 - Automatic Structures.pdf:application/pdf}
}

@article{blumensath--nodate-1,
	title = {½ áòøöó ù ø óò},
	author = {Blumensath, Achim},
	file = {Blumensath - Unknown - ½ áòøöó ù ø óò:/home/user/Zotero/storage/66VKNAAR/Blumensath - Unknown - ½ áòøöó ù ø óò.pdf:application/pdf}
}

@article{blumensath-s-nodate,
	title = {S  {M}  {T}   {P}  {W} },
	author = {Blumensath, Achim},
	file = {Blumensath - Unknown - S  M  T   P  W :/home/user/Zotero/storage/TRBN87PJ/Blumensath - Unknown - S  M  T   P  W .pdf:application/pdf}
}

@article{blumensath--nodate-2,
	title = {  },
	author = {Blumensath, Achim},
	keywords = {tree automata, infinite trees, monadic, recognisability},
	file = {Blumensath - Unknown -   :/home/user/Zotero/storage/NTKN9TXE/Blumensath - Unknown -   .pdf:application/pdf}
}

@article{blumensath-s-nodate-1,
	title = {S  {M}  {T}   {I} },
	author = {Blumensath, Achim},
	file = {Blumensath - Unknown - S  M  T   I :/home/user/Zotero/storage/WXX8GKRA/Blumensath - Unknown - S  M  T   I .pdf:application/pdf}
}

@article{blumensath-linear-2014,
	title = {Linear temporal logic for regular cost functions},
	volume = {10},
	doi = {10.2168/LMCS-10)},
	author = {{Blumensath}},
	year = {2014},
	pages = {1--37},
	file = {Blumensath - 2014 - Linear temporal logic for regular cost functions:/home/user/Zotero/storage/H498AQCJ/Blumensath - 2014 - Linear temporal logic for regular cost functions.pdf:application/pdf}
}

@article{blumensath--nodate-3,
	title = {¾ £ ¸ ¾ ¸},
	author = {{Blumensath}},
	file = {Blumensath - Unknown - ¾ £ ¸ ¾ ¸:/home/user/Zotero/storage/9CKQJQHA/Blumensath - Unknown - ¾ £ ¸ ¾ ¸.pdf:application/pdf}
}

@article{blumensath-syntactic-nodate-1,
	title = {A {Syntactic} {Congruence} for {Infinite} {Trees}},
	author = {Blumensath, Achim},
	pages = {1--27},
	file = {Blumensath - Unknown - A Syntactic Congruence for Infinite Trees:/home/user/Zotero/storage/SAAEQGPK/Blumensath - Unknown - A Syntactic Congruence for Infinite Trees.pdf:application/pdf}
}

@article{blumensath-axiomatising-nodate,
	title = {Axiomatising {Tree}-interpretable {Structures}},
	author = {Blumensath, Achim},
	file = {Blumensath - Unknown - Axiomatising Tree-interpretable Structures:/home/user/Zotero/storage/2A2XZM4D/Blumensath - Unknown - Axiomatising Tree-interpretable Structures.pdf:application/pdf}
}

@article{blumensath-s-nodate-2,
	title = {S = {S} - a ’ + {\textasciitilde} -lg / {\textasciitilde}},
	author = {{Blumensath}},
	pages = {47--47},
	file = {Blumensath - Unknown - S = S - a ’ ~ -lg ~:/home/user/Zotero/storage/UTQCJEKB/Blumensath - Unknown - S = S - a ’ ~ -lg ~.pdf:application/pdf}
}

@article{bloom-equational-2005,
	title = {The equational theory of regular words},
	volume = {197},
	doi = {10.1016/j.ic.2005.01.004},
	abstract = {Courcelle introduced the study of regular words, i.e., words isomorphic to frontiers of regular trees. Heilbrunner showed that a nonempty word is regular iff it can be generated from the singletons by the operations of concatenation, omega power, omega-op power, and the infinite family of shuffle operations. We prove that the algebra of nonempty regular words on the set A, equipped with these operations, is freely generated by A in a variety which is axiomatizable by an infinite collection of some natural equations. We also show that this variety has no finite equational basis and that its equational theory is decidable in polynomial time. © 2005 Elsevier Inc. All rights reserved.},
	journal = {Information and Computation},
	author = {Bloom, Stephen L. and Ésik, Zoltán},
	year = {2005},
	keywords = {word, Arrangement, Equational theory, Linear order, Regular},
	pages = {55--89},
	file = {1-s2.0-S0890540105000192-main(1):/home/user/Zotero/storage/ZIMMDMI7/1-s2.0-S0890540105000192-main(1).pdf:application/pdf}
}

@article{bloom-equational-2005-1,
	title = {The equational theory of regular words},
	volume = {197},
	doi = {10.1016/j.ic.2005.01.004},
	abstract = {Courcelle introduced the study of regular words, i.e., words isomorphic to frontiers of regular trees. Heilbrunner showed that a nonempty word is regular iff it can be generated from the singletons by the operations of concatenation, omega power, omega-op power, and the infinite family of shuffle operations. We prove that the algebra of nonempty regular words on the set A, equipped with these operations, is freely generated by A in a variety which is axiomatizable by an infinite collection of some natural equations. We also show that this variety has no finite equational basis and that its equational theory is decidable in polynomial time. © 2005 Elsevier Inc. All rights reserved.},
	journal = {Information and Computation},
	author = {Bloom, Stephen L. and Ésik, Zoltán},
	year = {2005},
	keywords = {word, Arrangement, Equational theory, Linear order, Regular},
	pages = {55--89},
	file = {1-s2.0-S0890540105000192-main(1):/home/user/Zotero/storage/EP68WT33/1-s2.0-S0890540105000192-main(1).pdf:application/pdf}
}

@article{bloom-long-2001,
	title = {Long words: {The} theory of concatenation and ??-power},
	volume = {259},
	doi = {10.1016/S0304-3975(00)00040-2},
	abstract = {It is shown that for any set A, the algebra of ordinal words on the alphabet A equipped with the operations of concatenation and ??-power is axiomatized by the equations x??(y??z)=(x??y)??z, (x??y)??=x??(y??x)??, (xn)??=x??,n???1. Indeed, the algebra freely generated by A in the variety determined by these equations is the algebra of tail-finite ordinal words of length},
	journal = {Theoretical Computer Science},
	author = {Bloom, Stephen L. and Choffrut, Christian},
	year = {2001},
	keywords = {??-power, Axiomatization, Choueka automata, Series product},
	pages = {533--548},
	file = {1-s2.0-S0304397500000402-main:/home/user/Zotero/storage/7MBSUZ5P/1-s2.0-S0304397500000402-main.pdf:application/pdf}
}

@article{blondin-complexity-2012,
	title = {The {Complexity} of {Intersecting} {Finite} {Automata} {Having} {Few} {Final} {States}},
	volume = {19},
	journal = {Electronic Colloquium on Computational Complexity (ECCC)},
	author = {Blondin, Michael and Krebs, Andreas and McKenzie, Pierre},
	year = {2012},
	pages = {90--90}
}

@article{birkhoff-heterogeneous-1970,
	title = {Heterogeneous algebras},
	volume = {8},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S002198007080014X},
	doi = {10.1016/S0021-9800(70)80014-X},
	number = {1},
	journal = {Journal of Combinatorial Theory},
	author = {Birkhoff, Garrett and Lipson, John D.},
	month = jan,
	year = {1970},
	pages = {115--133},
	file = {Birkhoff, Lipson - 1970 - Heterogeneous algebras:/home/user/Zotero/storage/4J8GMSBG/Birkhoff, Lipson - 1970 - Heterogeneous algebras.pdf:application/pdf}
}

@inproceedings{beyersdorff-verifying-2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Verifying {Proofs} in {Constant} {Depth}},
	volume = {6907},
	isbn = {978-3-642-22992-3},
	booktitle = {{MFCS}},
	publisher = {Springer},
	author = {Beyersdorff, Olaf and Datta, Samir and Mahajan, Meena and Scharfenberger-Fabian, Gido and Sreenivasaiah, Karteek and Thomas, Michael and Vollmer, Heribert},
	editor = {Murlak, Filip and Sankowski, Piotr},
	year = {2011},
	pages = {84--95}
}

@article{blanchet-sadri-product-1998,
	title = {On a {Product} of {Finite} {Monoids}},
	volume = {57},
	url = {http://link.springer.com/10.1007/PL00005969},
	doi = {10.1007/PL00005969},
	number = {1},
	journal = {Semigroup Forum},
	author = {Blanchet-Sadri, F. and Gaddis, F. Dale},
	month = jul,
	year = {1998},
	pages = {75--91},
	file = {Blanchet-Sadri, Gaddis - 1998 - On a Product of Finite Monoids:/home/user/Zotero/storage/RX3V57WM/Blanchet-Sadri, Gaddis - 1998 - On a Product of Finite Monoids.pdf:application/pdf}
}

@article{birkhoff-structure-1935,
	title = {On the structure of abstract algebras},
	volume = {31},
	journal = {Proceedings of the Cambridge Philosophical Society},
	author = {Birkhoff, Garrett},
	year = {1935},
	pages = {433--454}
}

@inproceedings{bezem-computer-2011,
	series = {{LIPIcs}},
	title = {Computer {Science} {Logic}, 25th {International} {Workshop} / 20th {Annual} {Conference} of the {EACSL}, {CSL} 2011, {September} 12-15, 2011, {Bergen}, {Norway}, {Proceedings}},
	volume = {12},
	isbn = {978-3-939897-32-3},
	booktitle = {{CSL}},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
	editor = {Bezem, Marc},
	year = {2011}
}

@article{beyersdorff-verifying-2013,
	title = {Verifying {Proofs} in {Constant} {Depth}},
	journal = {Transactions on Computation Theory (accepted)},
	author = {Beyersdorff, Olaf and Datta, Samir and Krebs, Andreas and Mahajan, Meena and Scharfenberger-Fabian, Gido and Sreenivasaiah, Karteek and Thomas, Michael and Vollmer, Heribert},
	year = {2013}
}

@article{beyersdorff-proof-2015,
	title = {Proof {Complexity} of {Resolution}-based {QBF} {Calculi}},
	number = {Stacs},
	author = {Beyersdorff, Olaf and Chew, Leroy and Janota, Mikoláš},
	year = {2015},
	keywords = {4230, digital object identifier 10, lipics, stacs, 2015, 76, and phrases proof complexity, lower bound techniques, qbf, separations},
	pages = {76--89},
	file = {Beyersdorff, Chew, Janota - 2015 - Proof Complexity of Resolution-based QBF Calculi:/home/user/Zotero/storage/EPTG2DA6/Beyersdorff, Chew, Janota - 2015 - Proof Complexity of Resolution-based QBF Calculi.pdf:application/pdf}
}

@article{berstel-towards-1996,
	title = {Towards an algebraic theory of context-free languages},
	volume = {25},
	url = {http://www-igm.univ-mlv.fr/~berstel/Articles/1996AlgebraicTheory.pdf},
	journal = {Fundamenta Informaticae},
	author = {Berstel, Jean and Boasson, Luc},
	year = {1996},
	pages = {217--239},
	file = {Berstel, Boasson - 1996 - Towards an algebraic theory of context-free languages:/home/user/Zotero/storage/T2JAFTAS/Berstel, Boasson - 1996 - Towards an algebraic theory of context-free languages.pdf:application/pdf}
}

@article{berkholz-uber-2010,
	title = {Über die {Schaltkreiskomplexität} parametrisierter {Probleme} {Diplomarbeit}},
	author = {Berkholz, Christoph},
	year = {2010},
	file = {Berkholz - 2010 - Über die Schaltkreiskomplexität parametrisierter Probleme Diplomarbeit:/home/user/Zotero/storage/IA7Q9SBW/Berkholz - 2010 - Über die Schaltkreiskomplexität parametrisierter Probleme Diplomarbeit.pdf:application/pdf}
}

@inproceedings{bensch-input-driven-2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Input-{Driven} {Stack} {Automata}},
	volume = {7604},
	isbn = {978-3-642-33474-0},
	url = {http://dx.doi.org/10.1007/978-3-642-33475-7\_3},
	doi = {10.1007/978-3-642-33475-7\_3},
	booktitle = {Theoretical {Computer} {Science} - 7th {{IFIP}} {{TC}} 1/{WG} 2.2 {International} {Conference}, {{TCS}} 2012, {Amsterdam}, {The} {Netherlands}, {September} 26-28, 2012. {Proceedings}},
	publisher = {Springer},
	author = {Bensch, Suna and Holzer, Markus and Kutrib, Martin and Malcher, Andreas},
	editor = {Baeten, Jos C M and Ball, Thomas and de Boer, Frank S},
	year = {2012},
	pages = {28--42}
}

@inproceedings{benedikt-regular-2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Regular {Tree} {Languages} {Definable} in {{FO}}},
	volume = {3404},
	isbn = {3-540-24998-2},
	url = {http://dx.doi.org/10.1007/978-3-540-31856-9\_27},
	doi = {10.1007/978-3-540-31856-9\_27},
	booktitle = {{{STACS}} 2005, 22nd {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Stuttgart}, {Germany}, {February} 24-26, 2005, {Proceedings}},
	publisher = {Springer},
	author = {Benedikt, Michael and Segoufin, Luc},
	editor = {Diekert, Volker and Durand, Bruno},
	year = {2005},
	pages = {327--339}
}

@article{benedikt-relational-2000,
	title = {Relational queries over interpreted structures},
	volume = {47},
	number = {4},
	journal = {J. ACM},
	author = {Benedikt, Michael and Libkin, Leonid},
	year = {2000},
	pages = {644--680}
}

@article{benedikt-regular-nodate,
	title = {Regular tree languages definable in {FO}},
	author = {Benedikt, Michael},
	keywords = {Logic, tree automata},
	pages = {1--36},
	file = {Benedikt - Unknown - Regular tree languages definable in FO:/home/user/Zotero/storage/NGAT389E/Benedikt - Unknown - Regular tree languages definable in FO.pdf:application/pdf;Unknown - Unknown - journal:/home/user/Zotero/storage/CRKWTUVT/Unknown - Unknown - journal.ps:application/postscript}
}

@article{benedikt-regular-nodate-1,
	title = {Regular tree languages definable in {FO}},
	author = {Benedikt, Michael},
	keywords = {Logic, tree automata},
	pages = {1--36},
	file = {Benedikt - Unknown - Regular tree languages definable in FO:/home/user/Zotero/storage/HUIHT8XC/Benedikt - Unknown - Regular tree languages definable in FO.pdf:application/pdf}
}

@inproceedings{behle-fo[<]-uniformity-2006,
	title = {{{FO}[{\textless}]}-{{U}}niformity},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@inproceedings{behle-fo[<]-uniformity-2006-1,
	title = {{FO}[{\textless}]-{Uniformity}},
	booktitle = {21st {Annual} {IEEE} {Conference} on {Computational} {Complexity} ({CCC} 2006), 16-20 {July} 2006, {Prague}, {Czech} {Republic}},
	publisher = {IEEE Computer Society},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@inproceedings{behle-fo[<]-uniformity-2006-2,
	title = {{{FO}[{\textless}]}-{{U}}niformity},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@inproceedings{behle-typed-2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Typed {{M}}onoids - {An} {{E}}ilenberg-{Like} {{T}}heorem for {Non} {Regular} {Languages}},
	volume = {6742},
	isbn = {978-3-642-21492-9},
	booktitle = {{CAI}},
	publisher = {Springer},
	author = {Behle, Christoph and Krebs, Andreas and Reifferscheid, Stephanie},
	editor = {Winkler, Franz},
	year = {2011},
	pages = {97--114}
}

@article{behle-approach-2009,
	title = {An {Approach} to characterize the {Regular} {Languages} in {TC}0 with {Linear} {Wires}},
	volume = {16},
	number = {085},
	journal = {Electronic Colloquium on Computational Complexity (ECCC)},
	author = {Behle, Christoph and Krebs, Andreas and Reifferscheid, Stephanie},
	year = {2009}
}

@inproceedings{behle-non-solvable-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Non-solvable {Groups} {Are} {Not} in {FO}+{MOD}+{M}{Â}{J}2[{REG}]},
	volume = {5457},
	isbn = {978-3-642-00981-5},
	booktitle = {{LATA}},
	publisher = {Springer},
	author = {Behle, Christoph and Krebs, Andreas and Reifferscheid, Stephanie},
	editor = {Dediu, Adrian Horia and Ionescu, Armand-Mihai and Martín-Vide, Carlos},
	year = {2009},
	pages = {129--140}
}

@article{berman-learning-nodate,
	title = {learning one-counter languages in polynomial time},
	author = {Berman, Piotr and Roos, Robert},
	file = {Berman, Roos - Unknown - learning one-counter languages in polynomial time:/home/user/Zotero/storage/VUPZB85M/Berman, Roos - Unknown - learning one-counter languages in polynomial time.pdf:application/pdf}
}

@article{berkholz-bounds-nodate,
	title = {Bounds for the quantifier depth in finite-variable logics : {Alternation} hierarchy},
	volume = {i},
	author = {Berkholz, Christoph and Krebs, Andreas and Verbitsky, Oleg},
	keywords = {p, 4230, digital object identifier 10, lipics, and phrases alternation hierarchy, finite-variable logic, xxx, yyy},
	pages = {1--20},
	file = {Berkholz, Krebs, Verbitsky - Unknown - Bounds for the quantifier depth in finite-variable logics Alternation hierarchy:/home/user/Zotero/storage/56XHRK7J/Berkholz, Krebs, Verbitsky - Unknown - Bounds for the quantifier depth in finite-variable logics Alternation hierarchy.pdf:application/pdf}
}

@article{berkholz-bounds-nodate-1,
	title = {Bounds for the quantifier depth in finite-variable logics : {Alternation} hierarchy {arXiv} : 1212 . 2747v4 [ cs . {LO} ] 8 {Aug} 2013},
	author = {Berkholz, Christoph and Krebs, Andreas},
	pages = {1--28},
	file = {Berkholz, Krebs - Unknown - Bounds for the quantifier depth in finite-variable logics Alternation hierarchy arXiv 1212 . 2747v4 cs .:/home/user/Zotero/storage/5QMFF58D/Berkholz, Krebs - Unknown - Bounds for the quantifier depth in finite-variable logics Alternation hierarchy arXiv 1212 . 2747v4 cs ..pdf:application/pdf}
}

@article{benedikt-regular-2009,
	title = {Regular tree languages definable in {{FO}} and in {{FO}\_{mod}}},
	volume = {11},
	url = {http://doi.acm.org/10.1145/1614431.1614435},
	doi = {10.1145/1614431.1614435},
	number = {1},
	journal = {{ACM} Trans. Comput. Log.},
	author = {Benedikt, Michael and Segoufin, Luc},
	year = {2009}
}

@inproceedings{benedikt-regular-2005-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Regular {Tree} {Languages} {Definable} in {{FO}}},
	volume = {3404},
	isbn = {3-540-24998-2},
	url = {http://dx.doi.org/10.1007/978-3-540-31856-9\_27},
	doi = {10.1007/978-3-540-31856-9\_27},
	booktitle = {{{STACS}} 2005, 22nd {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Stuttgart}, {Germany}, {February} 24-26, 2005, {Proceedings}},
	publisher = {Springer},
	author = {Benedikt, Michael and Segoufin, Luc},
	editor = {Diekert, Volker and Durand, Bruno},
	year = {2005},
	pages = {327--339},
	file = {Benedikt, Segoufin - 2009 - Regular tree languages definable in FO and in FO mod:/home/user/Zotero/storage/2PHR9BJC/Benedikt, Segoufin - 2009 - Regular tree languages definable in FO and in FO mod.pdf:application/pdf}
}

@article{benedikt-regular-2009-1,
	title = {Regular tree languages definable in {{FO}} and in {FO}\({}\_{\mbox{\emph{mod}}}\)},
	volume = {11},
	url = {http://dblp.uni-trier.de/db/journals/tocl/tocl11.html#BenediktS09},
	doi = {10.1145/1614431.1614435},
	number = {1},
	journal = {{ACM} Trans. Comput. Log.},
	author = {Benedikt, Michael and Segoufin, Luc},
	month = oct,
	year = {2009},
	pages = {1--32},
	file = {Benedikt, Segoufin - 2009 - Regular tree languages definable in FO and in FO mod:/home/user/Zotero/storage/AXAR4MGW/Benedikt, Segoufin - 2009 - Regular tree languages definable in FO and in FO mod.pdf:application/pdf}
}

@inproceedings{behle-fo[<]-uniformity-2006-3,
	title = {{{FO}[{\textless}]}-{{U}}niformity},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@inproceedings{behle-fo[textless]-uniformity-2006,
	title = {{FO}[{\textless}]-{Uniformity}},
	isbn = {0-7695-2596-2},
	url = {http://doi.ieeecomputersociety.org/10.1109/CCC.2006.20},
	doi = {10.1109/CCC.2006.20},
	booktitle = {21st {Annual} {{IEEE}} {Conference} on {Computational} {Complexity} {({CCC}} 2006), 16-20 {July} 2006, {Prague}, {Czech} {Republic}},
	publisher = {{IEEE} Computer Society},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@inproceedings{behle-fo[<]-uniformity-2006-4,
	title = {{FO}[{\textless}]-{Uniformity}},
	booktitle = {21st {Annual} {IEEE} {Conference} on {Computational} {Complexity} ({CCC} 2006), 16-20 {July} 2006, {Prague}, {Czech} {Republic}},
	publisher = {IEEE Computer Society},
	author = {Behle, Christoph and Lange, Klaus-Jörn},
	year = {2006},
	pages = {183--189}
}

@article{behle-boolean-nodate,
	title = {The {{B}}oolean {{F}}ormula {{V}}alue {{P}}roblem as a {{F}}ormal {{L}}anguage},
	author = {Behle, Christoph and Krebs, Andres and Lange, Klaus-Jörn}
}

@inproceedings{behle-regular-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Regular {Languages} {Definable} by {Majority} {Quantifiers} with {Two} {Variables}},
	volume = {5583},
	isbn = {978-3-642-02736-9},
	booktitle = {Developments in {Language} {Theory}},
	publisher = {Springer},
	author = {Behle, Christoph and Krebs, Andreas and Reifferscheid, Stephanie},
	editor = {Diekert, Volker and Nowotka, Dirk},
	year = {2009},
	pages = {91--102}
}

@article{behle-linear-2013,
	title = {Linear circuits, two-variable logic and weakly blocked monoids},
	volume = {501},
	url = {http://dx.doi.org/10.1016/j.tcs.2013.07.005},
	doi = {10.1016/j.tcs.2013.07.005},
	journal = {Theor. Comput. Sci.},
	author = {Behle, Christoph and Krebs, Andreas and Mercer, Mark},
	year = {2013},
	pages = {20--33}
}

@article{behle-low-2011,
	title = {Low uniform versions of {NC}1},
	volume = {18},
	journal = {Electronic Colloquium on Computational Complexity (ECCC)},
	author = {Behle, Christoph and Krebs, Andreas and Lange, Klaus-Jörn and McKenzie, Pierre},
	year = {2011},
	pages = {95--95}
}

@inproceedings{behle-typed-2011-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Typed {Monoids} - {An} {Eilenberg}-{Like} {Theorem} for {Non} {Regular} {Languages}},
	volume = {6742},
	isbn = {978-3-642-21492-9},
	url = {http://dx.doi.org/10.1007/978-3-642-21493-6\_6},
	doi = {10.1007/978-3-642-21493-6\_6},
	booktitle = {Algebraic {Informatics} - 4th {International} {Conference}, {{CAI}} 2011, {Linz}, {Austria}, {June} 21-24, 2011. {Proceedings}},
	publisher = {Springer},
	author = {Behle, Christoph and Krebs, Andreas and Reifferscheid, Stephanie},
	editor = {Winkler, Franz},
	year = {2011},
	pages = {97--114}
}

@inproceedings{behle-linear-2007,
	title = {Linear {Circuits}, {Two}-{Variable} {Logic} and {Weakly} {Blocked} {Monoids}},
	booktitle = {{MFCS}},
	author = {Behle, Christoph and Krebs, Andreas and Mercer, Mark},
	year = {2007},
	pages = {147--158}
}

@article{bedard-extensions-1993-1,
	title = {Extensions to {Barrington}'s {M}-program model},
	volume = {107},
	url = {http://linkinghub.elsevier.com/retrieve/pii/030439759390253P},
	doi = {10.1016/0304-3975(93)90253-P},
	number = {1},
	journal = {Theoretical Computer Science},
	author = {Bédard, François and Lemieux, François and McKenzie, Pierre},
	month = jan,
	year = {1993},
	pages = {31--61},
	file = {Bédard, Lemieux, McKenzie - 1993 - Extensions to Barrington's M-program model:/home/user/Zotero/storage/KI6FDQ57/Bédard, Lemieux, McKenzie - 1993 - Extensions to Barrington's M-program model.pdf:application/pdf}
}

@article{behle-typed-2011-2,
	title = {Typed {Monoids} - {An} {Eilenberg}-like {Theorem} for non regular {Languages}},
	volume = {35},
	number = {35},
	author = {Behle, Christoph and Krebs, Andreas},
	year = {2011},
	pages = {1--19},
	file = {Behle, Krebs - 2011 - Typed Monoids – An Eilenberg-like Theorem for non regular Languages:/home/user/Zotero/storage/D2M69UEN/Behle, Krebs - 2011 - Typed Monoids – An Eilenberg-like Theorem for non regular Languages.pdf:application/pdf}
}

@article{esik-algebraic-2007,
	title = {Algebraic characterization of logically defined tree languages},
	url = {http://arxiv.org/abs/0709.2962},
	abstract = {We give an algebraic characterization of the tree languages that are defined by logical formulas using certain Lindstr\"om quantifiers. An important instance of our result concerns first-order definable tree languages. Our characterization relies on the usage of preclones, an algebraic structure introduced by the authors in a previous paper, and of the block product operation on preclones. Our results generalize analogous results on finite word languages, but it must be noted that, as they stand, they do not yield an algorithm to decide whether a given regular tree language is first-order definable.},
	author = {Esik, Zoltan and Weil, Pascal},
	month = sep,
	year = {2007},
	pages = {46--46},
	file = {Esik, Weil - 2007 - Algebraic characterization of logically defined tree languages:/home/user/Zotero/storage/HR8SG98Z/Esik, Weil - 2007 - Algebraic characterization of logically defined tree languages.pdf:application/pdf}
}

@article{esik-characterizing-2006,
	title = {Characterizing {CTL}-like logics on finite trees},
	volume = {356},
	url = {http://dx.doi.org/10.1016/j.tcs.2006.01.034},
	doi = {10.1016/j.tcs.2006.01.034},
	number = {1-2},
	journal = {Theor. Comput. Sci.},
	author = {Ésik, Zoltán},
	year = {2006},
	pages = {136--152}
}

@article{esik-axiomatizing-2010,
	title = {Axiomatizing the equational theory of regular tree languages},
	volume = {79},
	url = {http://dx.doi.org/10.1016/j.jlap.2009.10.001},
	doi = {10.1016/j.jlap.2009.10.001},
	number = {2},
	journal = {J. Log. Algebr. Program.},
	author = {Ésik, Zoltán},
	year = {2010},
	pages = {189--213}
}

@inproceedings{esik-fundamentals-1993,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fundamentals of {Computation} {Theory}, 9th {International} {Symposium}, {{FCT}} '93, {Szeged}, {Hungary}, {August} 23-27, 1993, {Proceedings}},
	volume = {710},
	isbn = {3-540-57163-9},
	publisher = {Springer},
	editor = {Ésik, Zoltán},
	year = {1993}
}

@article{esik-extended-2002,
	title = {Extended {Temporal} {Logic} on {Finite} {Words}},
	number = {December},
	author = {Esik, Zoltan},
	year = {2002},
	file = {BRICS-RS-02-47:/home/user/Zotero/storage/VI66Z76K/BRICS-RS-02-47.pdf:application/pdf}
}

@inproceedings{etessami-first-order-1997,
	title = {First-{Order} {Logic} with {Two} {Variables} and {Unary} {Temporal} {Logic}},
	booktitle = {{LICS}},
	author = {Etessami, Kousha and Vardi, Moshe Y and Wilke, Thomas},
	year = {1997},
	pages = {228--235}
}

@inproceedings{esparza-automata-2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automata, {Languages}, and {Programming} - 41st {International} {Colloquium}, {{ICALP}} 2014, {Copenhagen}, {Denmark}, {July} 8-11, 2014, {Proceedings}, {Part} {{II}}},
	volume = {8573},
	isbn = {978-3-662-43950-0},
	url = {http://dx.doi.org/10.1007/978-3-662-43951-7},
	doi = {10.1007/978-3-662-43951-7},
	publisher = {Springer},
	editor = {Esparza, Javier and Fraigniaud, Pierre and Husfeldt, Thore and Koutsoupias, Elias},
	year = {2014}
}

@article{esik-catgeory-2011,
	title = {catgeory simulations},
	author = {{Esik} and Maletti, Andreas},
	year = {2011},
	file = {esimal11:/home/user/Zotero/storage/7GK2VG43/esimal11.pdf:application/pdf}
}

@article{esik-algebraic-2010,
	title = {Algebraic {Characterization} of {Logically} {Defined} {Tree} {Languages}},
	volume = {20},
	url = {http://dx.doi.org/10.1142/S0218196710005595},
	doi = {10.1142/S0218196710005595},
	number = {2},
	journal = {IJAC},
	author = {Ésik, Zoltán and Weil, Pascal},
	year = {2010},
	pages = {195--239}
}

@article{esik-regular-2003,
	title = {Regular languages definable by {{L}}indstr{ö}m quantifiers},
	volume = {37},
	number = {3},
	journal = {ITA},
	author = {Ésik, Zoltán and Larsen, Kim Guldstrand},
	year = {2003},
	pages = {179--241}
}

@article{esik-characterizing-2006-1,
	title = {Characterizing {CTL}-like logics on finite trees},
	volume = {356},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397506001101},
	doi = {10.1016/j.tcs.2006.01.034},
	number = {1-2},
	journal = {Theoretical Computer Science},
	author = {Ésik, Zoltán},
	month = may,
	year = {2006},
	keywords = {tree automata, Pseudovariety, cascade product, temporal logic},
	pages = {136--152},
	file = {Ésik - 2006 - Characterizing CTL-like logics on finite trees:/home/user/Zotero/storage/ENC8PXI3/Ésik - 2006 - Characterizing CTL-like logics on finite trees.pdf:application/pdf}
}

@article{esik-axiomatizing-2010-1,
	title = {Axiomatizing the equational theory of regular tree languages},
	volume = {79},
	url = {http://dx.doi.org/10.1016/j.jlap.2009.10.001},
	doi = {10.1016/j.jlap.2009.10.001},
	number = {2},
	journal = {J. Log. Algebr. Program.},
	author = {Ésik, Zoltán},
	year = {2010},
	pages = {189--213},
	file = {Ésik - 2010 - Axiomatizing the equational theory of regular tree languages:/home/user/Zotero/storage/TPN6UQ35/Ésik - 2010 - Axiomatizing the equational theory of regular tree languages.pdf:application/pdf}
}

@inproceedings{esik-computer-2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Science} {Logic}, 20th {International} {Workshop}, {CSL} 2006, 15th {Annual} {Conference} of the {EACSL}, {Szeged}, {Hungary}, {September} 25-29, 2006, {Proceedings}},
	volume = {4207},
	isbn = {3-540-45458-6},
	booktitle = {{CSL}},
	publisher = {Springer},
	editor = {Ésik, Zoltán},
	year = {2006}
}

@article{eric-results-2004,
	title = {Some results on {C} -varieties},
	number = {August},
	author = {Eric, Jean- pin},
	year = {2004},
	pages = {1--23},
	file = {Eric - 2004 - Some results on C -varieties:/home/user/Zotero/storage/RP4JFDZG/Eric - 2004 - Some results on C -varieties.pdf:application/pdf}
}

@article{eric-kernel-2011,
	title = {The kernel of a monoid morphism},
	number = {November},
	author = {Eric, Jean-},
	year = {2011},
	file = {Kernels:/home/user/Zotero/storage/3MTCKGZT/Kernels.pdf:application/pdf}
}

@article{engel-one-parameter-2001,
	title = {One-parameter semigroups for linear evolution equations},
	volume = {63},
	url = {http://link.springer.com/10.1007/s002330010042},
	doi = {10.1007/s002330010042},
	number = {2},
	journal = {Semigroup Forum},
	author = {Engel, Klaus-Jochen and Nagel, Rainer},
	month = jun,
	year = {2001},
	pages = {278--280},
	file = {Engel, Nagel - 2001 - One-parameter semigroups for linear evolution equations:/home/user/Zotero/storage/VHDGZB99/Engel, Nagel - 2001 - One-parameter semigroups for linear evolution equations.pdf:application/pdf}
}

@article{elsevier-modulo-counting-1994,
	title = {Modulo-counting finite trees *},
	volume = {126},
	author = {Elsevier, Theoretical and Science, Computer and Mathematik, Germany and Kiel, Christian-albrechts-universittit},
	year = {1994},
	pages = {97--112},
	file = {Elsevier et al. - 1994 - Modulo-counting finite trees:/home/user/Zotero/storage/9MSRCBKJ/Elsevier et al. - 1994 - Modulo-counting finite trees.pdf:application/pdf}
}

@inproceedings{elberfeld-algorithmic-2012,
	series = {{LIPIcs}},
	title = {Algorithmic {Meta} {Theorems} for {Circuit} {Classes} of {Constant} and {Logarithmic} {Depth}},
	volume = {14},
	isbn = {978-3-939897-35-4},
	url = {http://dx.doi.org/10.4230/LIPIcs.STACS.2012.66},
	doi = {10.4230/LIPIcs.STACS.2012.66},
	booktitle = {29th {International} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {{STACS}} 2012, {February} 29th - {March} 3rd, 2012, {Paris}, {France}},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
	author = {Elberfeld, Michael and Jakoby, Andreas and Tantau, Till},
	editor = {Dürr, Christoph and Wilke, Thomas},
	year = {2012},
	pages = {66--77},
	file = {Elberfeld, Jakoby, Tantau - 2012 - Algorithmic meta theorems for circuit classes of constant and logarithmic depth:/home/user/Zotero/storage/ZU9CXCMZ/Elberfeld, Jakoby, Tantau - 2012 - Algorithmic meta theorems for circuit classes of constant and logarithmic depth.pdf:application/pdf}
}

@article{er-demystifying-1992,
	title = {Demystifying reachability in vector addition systems ´},
	author = {Er, J and Schmitz, Sylvain},
	year = {1992},
	keywords = {fast-growing complexity, ideal, reachability, vector addition system, well quasi order},
	file = {Er, Schmitz - 1992 - Demystifying reachability in vector addition systems ´:/home/user/Zotero/storage/5NBX9UI2/Er, Schmitz - 1992 - Demystifying reachability in vector addition systems ´.pdf:application/pdf}
}

@article{eoynooo-quantified-2011,
	title = {Quantified constraint satisfaction and the polynomially generated powers property},
	volume = {65},
	issn = {3540705821},
	doi = {10.1007/s00012-011-0125-4},
	journal = {Algebra Universalis},
	author = {Èóðýñóöô, Ñ and Chen, Hubie},
	year = {2011},
	keywords = {Computational complexity, dichotomy theorem, quantified constraint satisfaction},
	pages = {213--241},
	file = {Chen - 2011 - Quantified constraint satisfaction and the polynomially generated powers property:/home/user/Zotero/storage/GXV834K7/Chen - 2011 - Quantified constraint satisfaction and the polynomially generated powers property.pdf:application/pdf;Èóðýñóöô - Unknown - ÓÒ × ØÖ ÒØ Ä Ò Ù ×¸ Ò ËÝÑÑ ØÖ:/home/user/Zotero/storage/MQ2HNENB/Èóðýñóöô - Unknown - ÓÒ × ØÖ ÒØ Ä Ò Ù ×¸ Ò ËÝÑÑ ØÖ.pdf:application/pdf}
}

@article{einsiedler-functional-2015,
	title = {Functional {Analysis} , {Spectral} {Theory} , and {Applications}},
	number = {May},
	author = {Einsiedler, Manfred},
	year = {2015},
	file = {FAnotes:/home/user/Zotero/storage/T8PC96BK/FAnotes.pdf:application/pdf}
}

@book{eilenberg-automata-1976,
	address = {New York},
	title = {Automata, languages, and machines, {Vol}. {B}},
	publisher = {Academic Press},
	author = {Eilenberg, Samuel},
	year = {1976}
}

@article{eilenberg-pseudovarieties-1976,
	title = {On {Pseudovarieties}},
	author = {Eilenberg, Samuel and Sch{\"u}tzenberger, M. P.},
	year = {1976},
	file = {Eilenberg, Sch u tzenberger - 1976 - On Pseudovarieties:/home/user/Zotero/storage/Q6V8G8DG/Eilenberg, Sch u tzenberger - 1976 - On Pseudovarieties.pdf:application/pdf}
}

@book{eilenberg-automata-1976-1,
	address = {New York},
	title = {Automata, languages, and machines, {Vol}. {A}},
	author = {Eilenberg, Samuel},
	year = {1976}
}

@article{dymond-input-driven-1988,
	title = {Input-{Driven} {Languages} are in log n {Depth}},
	volume = {26},
	url = {http://dx.doi.org/10.1016/0020-0190(88)90148-2},
	doi = {10.1016/0020-0190(88)90148-2},
	number = {5},
	journal = {Inf. Process. Lett.},
	author = {Dymond, Patrick W},
	year = {1988},
	pages = {247--250}
}

@inproceedings{duparc-computer-2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Science} {Logic}, 21st {International} {Workshop}, {{CSL}} 2007, 16th {Annual} {Conference} of the {EACSL}, {Lausanne}, {Switzerland}, {September} 11-15, 2007, {Proceedings}},
	volume = {4646},
	isbn = {978-3-540-74914-1},
	publisher = {Springer},
	editor = {Duparc, Jacques and Henzinger, Thomas A},
	year = {2007}
}

@article{droste-kleene-2005,
	title = {A {Kleene} theorem for weighted tree automata},
	volume = {38},
	doi = {10.1007/s00224-004-1096-z},
	journal = {Theory of Computing Systems},
	author = {Droste, Manfred and Pech, Christian and Vogler, Heiko},
	year = {2005},
	pages = {1--38},
	file = {fulmalvog07:/home/user/Zotero/storage/CMK9WJN6/fulmalvog07.pdf:application/pdf}
}

@article{doctor-philosophiae-nodate,
	title = {Philosophiae {Doctor} 1 ({Nathanaël} {Fijalkow})},
	volume = {1},
	author = {Doctor, Philosophiae},
	file = {Doctor - Unknown - Philosophiae Doctor 1 (Nathanaël Fijalkow):/home/user/Zotero/storage/GUJJFVCD/Doctor - Unknown - Philosophiae Doctor 1 (Nathanaël Fijalkow).pdf:application/pdf}
}

@article{dzamonja-chain-2011,
	title = {Chain {Models}, {Trees} of {Singular} {Cardinality} and {Dynamic} {Ef}-{Games}},
	volume = {11},
	doi = {10.1142/S0219061311001006},
	journal = {Journal of Mathematical Logic},
	author = {Džamonja, Mirna and Väänänen, Jouko},
	year = {2011},
	keywords = {03e04, 54e99, msc 2000 classification, rank, singular cardinal, trees},
	pages = {61--85},
	file = {latest7:/home/user/Zotero/storage/ZR4RP66J/latest7.pdf:application/pdf}
}

@inproceedings{durr-29th-2012,
	series = {{LIPIcs}},
	title = {29th {International} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {{STACS}} 2012, {February} 29th - {March} 3rd, 2012, {Paris}, {France}},
	volume = {14},
	isbn = {978-3-939897-35-4},
	url = {http://drops.dagstuhl.de/portals/extern/index.php?semnr=12001},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
	editor = {Dürr, Christoph and Wilke, Thomas},
	year = {2012}
}

@article{droste-infinite-nodate,
	title = {Infinite {Nested} {Words}},
	volume = {1763},
	author = {Droste, Manfred and Stefan, D},
	keywords = {nested words, quanti-, tative automata, valuation monoids, weighted automata, weighted logics},
	pages = {1--12},
	file = {1506.07031v1(1):/home/user/Zotero/storage/VZBK4455/1506.07031v1(1).pdf:application/pdf}
}

@article{droste-infinite-nodate-1,
	title = {Infinite {Nested} {Words}},
	volume = {1763},
	author = {Droste, Manfred and Stefan, D},
	keywords = {nested words, quanti-, tative automata, valuation monoids, weighted automata, weighted logics},
	pages = {1--12},
	file = {1506.07031v1(1):/home/user/Zotero/storage/XTFSKXQ7/1506.07031v1(1).pdf:application/pdf}
}

@article{diekert-star-free-2011,
	title = {Star-{Free} {Languages} are {Church}-{Rosser} {Congruential}},
	url = {http://arxiv.org/abs/1111.4300},
	doi = {10.1016/j.tcs.2012.01.028},
	abstract = {The class of Church-Rosser congruential languages has been introduced by McNaughton, Narendran, and Otto in 1988. A language L is Church-Rosser congruential (belongs to CRCL), if there is a finite, confluent, and length-reducing semi-Thue system S such that L is a finite union of congruence classes modulo S. To date, it is still open whether every regular language is in CRCL. In this paper, we show that every star-free language is in CRCL. In fact, we prove a stronger statement: For every star-free language L there exists a finite, confluent, and subword-reducing semi-Thue system S such that the total number of congruence classes modulo S is finite and such that L is a union of congruence classes modulo S. The construction turns out to be effective.},
	author = {Diekert, Volker and Kufleitner, Manfred and Weil, Pascal},
	month = nov,
	year = {2011},
	keywords = {aperiodic monoid, church-rosser system, guage, local divisor, star-free lan-, string rewriting},
	pages = {1--13},
	file = {Diekert, Kufleitner, Weil - 2011 - Star-Free Languages are Church-Rosser Congruential:/home/user/Zotero/storage/WUCN3HSW/Diekert, Kufleitner, Weil - 2011 - Star-Free Languages are Church-Rosser Congruential.pdf:application/pdf}
}

@article{diekert-krohn-rhodes-2011,
	title = {The {Krohn}-{Rhodes} {Theorem} and {Local} {Divisors}},
	url = {http://arxiv.org/abs/1111.1585},
	abstract = {We give a new proof of the Krohn-Rhodes Theorem using local divisors. The proof provides nearly as good a decomposition in terms of size as the holonomy decomposition of Eilenberg, avoids induction on the size of the state set, and works exclusively with monoids with the base case of the induction being that of a group.},
	author = {Diekert, Volker and Kufleitner, Manfred and Steinberg, Benjamin},
	month = nov,
	year = {2011},
	keywords = {automaton, monoid, transformation monoid, wreath product},
	pages = {1--14},
	file = {Diekert, Kufleitner, Steinberg - 2011 - The Krohn-Rhodes Theorem and Local Divisors:/home/user/Zotero/storage/P3QHEQ3A/Diekert, Kufleitner, Steinberg - 2011 - The Krohn-Rhodes Theorem and Local Divisors.pdf:application/pdf}
}

@article{diekert-regular-2012,
	title = {Regular languages are church-rosser congruential},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31585-5\_19},
	number = {September 2011},
	journal = {Automata, Languages, and …},
	author = {Diekert, Volker and Kufleitner, Manfred and Reinhardt, Klaus and Walter, Tobias},
	year = {2012},
	keywords = {church-rosser system, guage, local divisor, string rewriting, finite monoid, finite semigroup, regular lan-},
	pages = {1--15},
	file = {Diekert et al. - 2012 - Regular languages are church-rosser congruential:/home/user/Zotero/storage/26ZHW6KU/Diekert et al. - 2012 - Regular languages are church-rosser congruential.pdf:application/pdf}
}

@inproceedings{diekert-stacs-2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{{STACS}} 2005, 22nd {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Stuttgart}, {Germany}, {February} 24-26, 2005, {Proceedings}},
	volume = {3404},
	isbn = {3-540-24998-2},
	publisher = {Springer},
	editor = {Diekert, Volker and Durand, Bruno},
	year = {2005}
}

@article{delgado-type-nodate,
	title = {Type {II} theorem and hyperdecidability of pseudovarieties of groups},
	author = {Delgado, Manuel},
	pages = {1--11},
	file = {Proceedings-St-Andrews-corrected-version:/home/user/Zotero/storage/TNNXQJI9/Proceedings-St-Andrews-corrected-version.pdf:application/pdf}
}

@article{deitmar-k-theorie-nodate,
	title = {K-{Theorie} {Inhaltsverzeichnis} {Vektorbuendel} {Definitionen} {Definition} 1 . 1 . 1 . {Sei} {K} = {R} , {C} . {Ein} {K} -{Vektorbuendel} ueber einem topologischen {Raum} {X} ist eine stetige {Abbildung} p : {E} → {X} , wobei {E} ein topologischer {Raum} ist , so dass gilt : ( b ) {Zu} jedem},
	author = {Deitmar, Anton},
	pages = {1--63},
	file = {K-Theorie:/home/user/Zotero/storage/IHV2X2WU/K-Theorie.pdf:application/pdf}
}

@article{diekert-context-free-2013,
	title = {Context-{Free} {Groups} and {Bass}-{Serre} {Theory}},
	url = {http://arxiv.org/abs/1307.8297},
	abstract = {The word problem of a finitely generated group is the formal language of words over the generators which are equal to the identity in the group. If this language happens to be context-free, then the group is called context-free. Finitely generated virtually free groups are context-free. In a seminal paper Muller and Schupp showed the converse: A context-free group is virtually free. Over the past decades a wide range of other characterizations of context-free groups have been found. The present notes survey most of these characterizations. Our aim is to show how the different characterizations of context-free groups are interconnected. Moreover, we present a self-contained access to the Muller-Schupp theorem without using Stallings' structure theorem or a separate accessibility result. We also give an introduction to some classical results linking groups with formal language theory.},
	author = {Diekert, Volker and Weiß, Armin},
	month = jul,
	year = {2013},
	file = {Diekert, Weiß - 2013 - Context-Free Groups and Bass-Serre Theory:/home/user/Zotero/storage/DD6UHMCP/Diekert, Weiß - 2013 - Context-Free Groups and Bass-Serre Theory.pdf:application/pdf}
}

@inproceedings{diekert-developments-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Developments in {Language} {Theory}, 13th {International} {Conference}, {{DLT}} 2009, {Stuttgart}, {Germany}, {June} 30 - {July} 3, 2009. {Proceedings}},
	volume = {5583},
	isbn = {978-3-642-02736-9},
	url = {http://dx.doi.org/10.1007/978-3-642-02737-6},
	doi = {10.1007/978-3-642-02737-6},
	publisher = {Springer},
	editor = {Diekert, Volker and Nowotka, Dirk},
	year = {2009}
}

@article{diekert-uchi-nodate,
	title = {uchi automata},
	author = {Diekert, Volker and Muscholl, Anca and Walukiewicz, Igor},
	pages = {1--19},
	file = {1507.01020:/home/user/Zotero/storage/QV4JD6N4/1507.01020.pdf:application/pdf}
}

@article{diekert-krohn-rhodes-2012,
	title = {The {Krohn}-{Rhodes} {Theorem} and {Local} {Divisors}},
	volume = {116},
	url = {http://dx.doi.org/10.3233/FI-2012-669},
	doi = {10.3233/FI-2012-669},
	number = {1-4},
	journal = {Fundam. Inform.},
	author = {Diekert, Volker and Kufleitner, Manfred and Steinberg, Benjamin},
	year = {2012},
	pages = {65--77}
}

@article{diekert-fragments-2009,
	title = {Fragments of first-order logic over infinite words},
	url = {http://arxiv.org/abs/0906.2995},
	abstract = {We give topological and algebraic characterizations as well as language theoretic descriptions of the following subclasses of first-order logic FO[{\textless}] for omega-languages: Sigma\_2, FO{\textasciicircum}2, the intersection of FO{\textasciicircum}2 and Sigma\_2, and Delta\_2 (and by duality Pi\_2 and the intersection of FO{\textasciicircum}2 and Pi\_2). These descriptions extend the respective results for finite words. In particular, we relate the above fragments to language classes of certain (unambiguous) polynomials. An immediate consequence is the decidability of the membership problem of these classes, but this was shown before by Wilke and Bojanczyk and is therefore not our main focus. The paper is about the interplay of algebraic, topological, and language theoretic properties.},
	author = {Diekert, Volker and Kufleitner, Manfred},
	month = jun,
	year = {2009},
	pages = {1--26},
	file = {Diekert, Kufleitner - 2009 - Fragments of first-order logic over infinite words:/home/user/Zotero/storage/FXN849FX/Diekert, Kufleitner - 2009 - Fragments of first-order logic over infinite words.pdf:application/pdf}
}

@article{diekert-survey-nodate,
	title = {A {Survey} on the {Local} {Divisor} {Technique}},
	author = {Diekert, Volker and Kufleitner, Manfred},
	keywords = {bounded synchronization delay, factorization forests, kamp, linear temporal logic, local divisors, s theorem},
	pages = {1--16},
	file = {1410.6026v2:/home/user/Zotero/storage/3724UWCI/1410.6026v2.pdf:application/pdf}
}

@article{delgado-solving-nodate,
	title = {{SOLVING} {SYSTEMS} {OF} {EQUATIONS} {MODULO}},
	author = {Delgado, Manuel and Masuda, Ariane and Steinberg, Benjamin},
	pages = {1--9},
	file = {Delgado, Masuda, Steinberg - Unknown - SOLVING SYSTEMS OF EQUATIONS MODULO:/home/user/Zotero/storage/JVTSAR4T/Delgado, Masuda, Steinberg - Unknown - SOLVING SYSTEMS OF EQUATIONS MODULO.pdf:application/pdf}
}

@article{delgado-hyperdecidability-2001,
	title = {on the {Hyperdecidability} of {Pseudovarieties} of {Groups}},
	volume = {11},
	doi = {10.1142/S0218196701000802},
	journal = {International Journal of Algebra and Computation},
	author = {Delgado, Manuel},
	year = {2001},
	pages = {753--771},
	file = {10.1.1.46.6891:/home/user/Zotero/storage/T4PVXFDP/10.1.1.46.6891.pdf:application/pdf}
}

@article{deitmar-zahlentheorie-2013,
	title = {Zahlentheorie ueber {Funktionenkoerpern} {Inhaltsverzeichnis}},
	author = {Deitmar, Anton},
	year = {2013},
	file = {ZTFunkt:/home/user/Zotero/storage/MKV7FMRW/ZTFunkt.pdf:application/pdf}
}

@inproceedings{dediu-language-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Language and {Automata} {Theory} and {Applications}, {Third} {International} {Conference}, {LATA} 2009, {Tarragona}, {Spain}, {April} 2-8, 2009. {Proceedings}},
	volume = {5457},
	isbn = {978-3-642-00981-5},
	booktitle = {{LATA}},
	publisher = {Springer},
	editor = {Dediu, Adrian Horia and Ionescu, Armand-Mihai and Martín-Vide, Carlos},
	year = {2009}
}

@article{decelle-computational-2014,
	title = {Computational {Complexity}, {Phase} {Transitions}, and {Message}-{Passing} for {Community} {Detection}},
	url = {http://arxiv.org/abs/1409.2290},
	abstract = {We take a whirlwind tour of problems and techniques at the boundary of computer science and statistical physics. We start with a brief description of P, NP, and NP-completeness. We then discuss random graphs, including the emergence of the giant component and the k-core, using techniques from branching processes and differential equations. Using these tools as well as the second moment method, we give upper and lower bounds on the critical clause density for random k-SAT. We end with community detection in networks, variational methods, the Bethe free energy, belief propagation, the detectability transition, and the non-backtracking matrix.},
	author = {Decelle, Aurélien and Hüttel, Janina and Saade, Alaa and Moore, Cristopher},
	month = sep,
	year = {2014},
	pages = {1--29},
	file = {Decelle et al. - 2014 - Computational Complexity, Phase Transitions, and Message-Passing for Community Detection:/home/user/Zotero/storage/BSVTDK26/Decelle et al. - 2014 - Computational Complexity, Phase Transitions, and Message-Passing for Community Detection.pdf:application/pdf}
}

@article{de-crescenzo-visibly-2014,
	title = {Visibly {Pushdown} {Modular} {Games}},
	volume = {161},
	url = {http://arxiv.org/abs/1408.5969v1},
	doi = {10.4204/EPTCS.161.22},
	number = {GandALF},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {De Crescenzo, Ilaria and La Torre, Salvatore and Velner, Yaron},
	month = aug,
	year = {2014},
	pages = {260--274},
	file = {De Crescenzo, La Torre, Velner - 2014 - Visibly Pushdown Modular Games:/home/user/Zotero/storage/UEBDI4K6/De Crescenzo, La Torre, Velner - 2014 - Visibly Pushdown Modular Games.pdf:application/pdf}
}

@phdthesis{daviaud-comportements-2014,
	title = {Comportements {Asymptotiques} des {Automates} {Max}-plus et {Min}-plus},
	author = {{Daviaud}},
	year = {2014},
	file = {these-daviaud:/home/user/Zotero/storage/PWR5D4PA/these-daviaud.pdf:application/pdf}
}

@article{damm-expressing-1997,
	title = {Expressing {Uniformity} via {Oracles}},
	volume = {30},
	number = {4},
	journal = {Theory Comput. Syst.},
	author = {Damm, Carsten and Holzer, Markus and Rossmanith, Peter},
	year = {1997},
	pages = {355--366}
}

@article{crespi-reghizzi-noncounting-1978,
	title = {Noncounting {Context}-{Free} {Languages}},
	volume = {25},
	number = {4},
	journal = {Journal of the Association for Computing Machinery},
	author = {Crespi-Reghizzi, S. and Guida, G. and Mandrioli, M.},
	year = {1978},
	pages = {571--580}
}

@article{czarnetzki-thesis-2004,
	title = {thesis},
	issn = {9780511541285},
	doi = {10.1093/jicru/ndh007},
	abstract = {Understanding the formation and evolution of the first stars and galaxies represents one of the most exciting frontiers in astronomy. Since the universe was filled with neutral hydrogen at early times, the most promising method for observing the epoch of the first stars is using the prominent 21-cm spectral line of the hydrogen atom. Current observational efforts are focused on the reionization era (cosmic age t {\textasciitilde} 500 Myr), with earlier times considered much more challenging. However, the next frontier of even earlier galaxy formation (t {\textasciitilde} 200 Myr) is emerging as a promising observational target. This is made possible by a recently noticed effect of a significant relative velocity between the baryons and dark matter at early times. The velocity difference suppresses star formation, causing a unique form of early luminosity bias. The spatial variation of this suppression enhances large-scale clustering and produces a prominent cosmic web on 100 comoving Mpc scales in the 21-cm intensity distribution. This structure makes it much more feasible for radio astronomers to detect these early stars, and should drive a new focus on this era, which is rich with little-explored astrophysics.},
	journal = {Sciences-New York},
	author = {Czarnetzki, Silke},
	year = {2004},
	pages = {1--31},
	file = {Czarnetzki - 2004 - thesis:/home/user/Zotero/storage/BK75VBKU/Czarnetzki - 2004 - thesis.pdf:application/pdf}
}

@article{courcelle-monadic-1996,
	title = {The {Monadic} {Second}-{Order} {Logic} of {Graphs} {{X}:} {Linear} {Orderings}},
	volume = {160},
	url = {http://dx.doi.org/10.1016/0304-3975(95)00083-6},
	doi = {10.1016/0304-3975(95)00083-6},
	number = {1{\&}2},
	journal = {Theor. Comput. Sci.},
	author = {Courcelle, Bruno},
	year = {1996},
	pages = {87--143}
}

@article{costa-operators-2013,
	title = {Some {Operators} {That} {Preserve} the {Locality} of a {Pseudovariety} of {Semigroups}},
	volume = {23},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218196713500112},
	doi = {10.1142/S0218196713500112},
	number = {DECEMBER 2011},
	journal = {International Journal of Algebra and Computation},
	author = {Costa, Alfredo and Escada, Ana},
	year = {2013},
	pages = {583--610},
	file = {1112.6120:/home/user/Zotero/storage/DTK47EXT/1112.6120.pdf:application/pdf}
}

@article{cornelissen-zeta-2008,
	title = {Zeta functions that hear the shape of a {Riemann} surface},
	volume = {58},
	doi = {10.1016/j.geomphys.2007.12.011},
	abstract = {To a compact hyperbolic Riemann surface, we associate a finitely summable spectral triple whose underlying topological space is the limit set of a corresponding Schottky group, and whose "Riemannian" aspect (Hilbert space and Dirac operator) encode the boundary action through its Patterson-Sullivan measure. We prove that the ergodic rigidity theorem for this boundary action implies that the zeta functions of the spectral triple suffice to characterize the (anti-)complex isomorphism class of the corresponding Riemann surface. Thus, you can hear the complex analytic shape of a Riemann surface, by listening to a suitable spectral triple. ?? 2008 Elsevier Ltd. All rights reserved.},
	number = {5},
	journal = {Journal of Geometry and Physics},
	author = {Cornelissen, Gunther and Marcolli, Matilde},
	year = {2008},
	pages = {619--632},
	file = {SchottkyS3d:/home/user/Zotero/storage/2P6HP5RC/SchottkyS3d.pdf:application/pdf}
}

@article{coquand-proof-1996,
	title = {Proof {Theory} in {Type} {Theory}},
	number = {September},
	author = {Coquand, Thierry},
	year = {1996},
	pages = {1--6},
	file = {Coquand - 1996 - Proof Theory in Type Theory:/home/user/Zotero/storage/HSJ3A4XG/Coquand - 1996 - Proof Theory in Type Theory.pdf:application/pdf}
}

@article{cook-pebbles-2012,
	title = {Pebbles and {Branching} {Programs} for {Tree} {Evaluation}},
	volume = {3},
	number = {2},
	journal = {TOCT},
	author = {Cook, Stephen A and McKenzie, Pierre and Wehr, Dustin and Braverman, Mark and Santhanam, Rahul},
	year = {2012},
	pages = {4--4}
}

@article{cook-problems-1987,
	title = {Problems {Complete} for {Deterministic} {Logarithmic} {Space}},
	volume = {8},
	number = {3},
	journal = {J. Algorithms},
	author = {Cook, Stephen A and McKenzie, Pierre},
	year = {1987},
	pages = {385--394}
}

@article{cook-characterizations-1971,
	title = {Characterizations of {Pushdown} {Machines} in {Terms} of {Time}-{Bounded} {Computers}},
	volume = {18},
	number = {1},
	journal = {J. ACM},
	author = {Cook, Stephen A},
	year = {1971},
	pages = {4--18}
}

@article{computation-synthesis-1993,
	title = {Synthesis of {Real} {Time} {Acceptors}},
	author = {Computation, J Symbolic},
	year = {1993},
	pages = {807--842},
	file = {Computation - 1993 - Synthesis of Real Time Acceptors:/home/user/Zotero/storage/H5SVRGTS/Computation - 1993 - Synthesis of Real Time Acceptors.pdf:application/pdf}
}

@article{colcombet-notes-2010,
	title = {Notes de cours sur la théorie des fonctions régulières de coût},
	url = {http://www.liafa.jussieu.fr/~colcombe/Files/notes\_cours\_mpri09\_fonctions\_de\_cout.pdf},
	journal = {Liafa.Jussieu.Fr},
	author = {Colcombet, Thomas},
	year = {2010},
	file = {Colcombet - 2010 - Notes de cours sur la théorie des fonctions régulières de coût:/home/user/Zotero/storage/CAHGV4CH/Colcombet - 2010 - Notes de cours sur la théorie des fonctions régulières de coût.pdf:application/pdf}
}

@article{cohen-expressive-1993,
	title = {On the {Expressive} {Power} of {Temporal} {Logic}},
	volume = {46},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Cohen, Joëlle and Perrin, Dominique and Pin, Jean-Eric},
	year = {1993},
	pages = {271--294}
}

@article{cobham-base-dependence-1969,
	title = {On the {Base}-{Dependence} of {Sets} of {Numbers} {Recognizable} by {Finite} {Automata}},
	volume = {3},
	number = {2},
	journal = {Mathematical Systems Theory},
	author = {Cobham, Alan},
	year = {1969},
	pages = {186--192}
}

@book{clifford-algebraic-1961,
	address = {Providence},
	title = {The {Algebraic} {Theory} of {Semigroups}},
	publisher = {American Mathematical Society},
	author = {Clifford, A. H. and Preston, G. B.},
	year = {1961}
}

@article{clemente-reachability-2015,
	title = {Reachability analysis of first-order definable pushdown systems},
	url = {http://arxiv.org/abs/1504.02651},
	abstract = {We study pushdown systems where control states, stack alphabet, and transition relation, instead of being finite, are first-order definable in a fixed countably-infinite structure. We show that the reachability analysis can be addressed with the well-known saturation technique for the wide class of oligomorphic structures. Moreover, for the more restrictive homogeneous structures, we are able to give concrete complexity upper bounds. We show ample applicability of our technique by presenting several concrete examples of homogeneous structures, subsuming, with optimal complexity, known results from the literature. We show that infinitely many such examples of homogeneous structures can be obtained with the classical wreath product construction.},
	author = {Clemente, Lorenzo and Lasota, Sławomir},
	year = {2015},
	keywords = {and phrases automata theory, pushdown systems, saturation tech-, sets with atoms},
	file = {1504.02651v2:/home/user/Zotero/storage/6DW8V2JC/1504.02651v2.pdf:application/pdf;Clemente, Lasota - 2015 - Reachability analysis of first-order definable pushdown systems:/home/user/Zotero/storage/G9MAKGHC/Clemente, Lasota - 2015 - Reachability analysis of first-order definable pushdown systems.pdf:application/pdf}
}

@incollection{chomsky-algebraic-1963,
	address = {Amsterdam},
	title = {The {Algebraic} {Theory} of {Context}-{Free} {Languages}},
	booktitle = {Computer {Programming} and {Formal} {Systems}},
	publisher = {North-Holland},
	author = {Chomsky, Noam and Sch{\"u}tzenberger, Marcel},
	year = {1963}
}

@inproceedings{cook-complexity-1971,
	title = {The {Complexity} of {Theorem}-{Proving} {Procedures}},
	booktitle = {Proceedings of the 3rd {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {May} 3-5, 1971, {Shaker} {Heights}, {Ohio}, {USA}},
	publisher = {ACM},
	author = {Cook, Stephen A},
	year = {1971},
	pages = {151--158}
}

@inproceedings{cook-deterministic-1979,
	title = {Deterministic {{CFL}'s} {Are} {Accepted} {Simultaneously} in {Polynomial} {Time} and {Log} {Squared} {Space}},
	booktitle = {{STOC}},
	publisher = {ACM},
	author = {Cook, Stephen A},
	year = {1979},
	pages = {338--345}
}

@article{cook-observation-1974,
	title = {An {Observation} on {Time}-{Storage} {Trade} {Off}},
	volume = {9},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Cook, Stephen A},
	year = {1974},
	pages = {308--316}
}

@article{cohen-learning-2014,
	title = {Learning the {Irreducible} {Representations} of {Commutative} {Lie} {Groups}},
	volume = {32},
	issn = {9781634393973},
	abstract = {We present a new probabilistic model of commutative Lie groups, that produces invariant-equivariant and disentangled representations of data. We borrow a fundamental principle from physics, used to define the elementary particles of a physical system, and use it to give a mathematically precise definition of the popular but heretofore rather vague notion of a "disentangled representation". Our model is based on a newfound Bayesian conjugacy relation that enables us to perform fully tractable probabilistic inference over so-called Toroidal Lie groups -- a class that includes practically relevant groups such as rotations and translations of images. We train the model on pairs of transformed image patches, and show that it produces a completely invariant representation which is highly effective for classification.},
	journal = {Proceedings of the 31st International Conference on Machine Learning},
	author = {Cohen, Taco and Welling, Max},
	year = {2014},
	pages = {1755--1763},
	file = {cohen14:/home/user/Zotero/storage/X7MCJGTS/cohen14.pdf:application/pdf}
}

@article{ci-ursa:-2012,
	title = {Ursa: a system for uniform reduction to sat},
	volume = {8},
	issn = {2111201200},
	doi = {10.2168/LMCS-8)},
	author = {Ci, Predrag Jani},
	year = {2012},
	pages = {1--39},
	file = {1506.03843v1:/home/user/Zotero/storage/CI3FCA6H/1506.03843v1.pdf:application/pdf}
}

@article{choffrut-first-order-2012,
	title = {First-order logics: {Some} characterizations and closure properties},
	volume = {49},
	doi = {10.1007/s00236-012-0157-z},
	journal = {Acta Informatica},
	author = {Choffrut, Christian and Malcher, Andreas and Mereghetti, Carlo and Palano, Beatrice},
	year = {2012},
	keywords = {first-order logic, concatenation, counting, quantifier, semilinear sets, shuffle, word bounded languages},
	pages = {225--248},
	file = {ChoffMalchMerePala:/home/user/Zotero/storage/GP7BEBBU/ChoffMalchMerePala.pdf:application/pdf}
}

@article{choffrut-for-2008,
	title = {for finite words on infinite alphabets {Introduction}},
	author = {Choffrut, Christian and Grigorieff, Serge},
	year = {2008},
	pages = {1--21},
	file = {AlphabetsInfinis2:/home/user/Zotero/storage/BGIS9CBB/AlphabetsInfinis2.pdf:application/pdf}
}

@article{chern-characteristic-1946,
	title = {Characteristic {Classes} of {Hermitian} {Manifolds}},
	volume = {47},
	number = {1},
	journal = {Annals of Mathematics, Second Series},
	author = {Chern, Shiing-Shen},
	year = {1946},
	pages = {85--121}
}

@article{chern-annals-2014,
	title = {Annals of {Mathematics} {Characteristic} {Classes} of {Hermitian} {Manifolds}},
	volume = {47},
	number = {1},
	author = {Chern, Author Shiing-shen},
	year = {2014},
	pages = {85--121},
	file = {Chern - 2014 - Annals of Mathematics Characteristic Classes of Hermitian Manifolds:/home/user/Zotero/storage/HQNQ97EW/Chern - 2014 - Annals of Mathematics Characteristic Classes of Hermitian Manifolds.pdf:application/pdf}
}

@article{chen-fibrational-nodate,
	title = {A {Fibrational} {Approach} to {Automata} {Theory}},
	author = {Chen, Liang-ting and Urbat, Henning},
	keywords = {coalgebra, duality, and phrases eilenberg, grothendieck fibration, s variety theorem},
	pages = {1--20},
	file = {Chen, Urbat - Unknown - A Fibrational Approach to Automata Theory:/home/user/Zotero/storage/FVZ9GTPW/Chen, Urbat - Unknown - A Fibrational Approach to Automata Theory.pdf:application/pdf}
}

@inproceedings{chaubard-first-2006,
	title = {First {Order} {Formulas} with {Modular} {Ppredicates}},
	booktitle = {{LICS}},
	author = {Chaubard, Laura and Pin, Jean-Eric and Straubing, Howard},
	year = {2006},
	pages = {211--220}
}

@inproceedings{chattopadhyay-languages-2007,
	title = {Languages with {Bounded} {Multiparty} {Communication} {Complexity}},
	booktitle = {{STACS} 2007, 24th {Annual} {Symposium} on {Theoretical} {Aspects} of {Computer} {Science}, {Aachen}, {Germany}, {February} 22-24, 2007, {Proceedings}},
	publisher = {Springer},
	author = {Chattopadhyay, Arkadev and Krebs, Andreas and Koucký, Michal and Szegedy, Mario and Tesson, Pascal and Thérien, Denis},
	year = {2007},
	pages = {500--511}
}

@article{chandra-unbounded-1985,
	title = {Unbounded {Fan}-{In} {Circuits} and {Associative} {Functions}},
	volume = {30},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Chandra, Ashok K and Fortune, Steven and Lipton, Richard J},
	year = {1985},
	pages = {222--234}
}

@article{ceccherini-silberstein-multipass-2015,
	title = {Multipass automata and group word problems},
	author = {Ceccherini-silberstein, Tullio and Coornaert, Michel and Fiorenzi, Francesca and Schupp, Paul E and Touikan, Nicholas W M},
	year = {2015},
	pages = {1--23},
	file = {1404.7442v4:/home/user/Zotero/storage/GQSIZJXF/1404.7442v4.pdf:application/pdf}
}

@article{carton-wreath-2000,
	title = {Wreath product and infinite words},
	volume = {153},
	doi = {10.1016/S0022-4049(99)00093-6},
	journal = {Journal of Pure and Applied Algebra},
	author = {Carton, Olivier},
	year = {2000},
	pages = {129--150},
	file = {1-s2.0-S0022404999000936-main:/home/user/Zotero/storage/ZM8DKEX3/1-s2.0-S0022404999000936-main.pdf:application/pdf}
}

@article{carreiro-coalgebraic-2013,
	title = {Coalgebraic announcement logics},
	volume = {7966 LNCS},
	issn = {9783642392115},
	doi = {10.1007/978-3-642-39212-2\_12},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Carreiro, Facundo and Gorín, Daniel and Schröder, Lutz},
	year = {2013},
	pages = {101--112},
	file = {coalg-announcements:/home/user/Zotero/storage/R749EZIW/coalg-announcements.pdf:application/pdf}
}

@article{carreiro-characterization-2015,
	title = {Characterization theorems for {PDL} and {FO}({TC})},
	author = {Carreiro, Facundo},
	year = {2015},
	keywords = {acterization theorem, bisimulation-invariant fragment, chain logic, char-, complete additivity, fixed points, janin, parity automata, propositional dynamic logic, transitive-closure logic, van benthem theorem, walukiewicz theorem},
	file = {1501.02607v2:/home/user/Zotero/storage/93H97ZFJ/1501.02607v2.pdf:application/pdf}
}

@article{carreiro-and-2011,
	title = {and ω -{Saturated} {Models}},
	author = {Carreiro, Facundo},
	year = {2011},
	pages = {62--76},
	file = {omegasat-ictac:/home/user/Zotero/storage/HKSVCDJD/omegasat-ictac.pdf:application/pdf}
}

@article{caralp-trimming-nodate,
	title = {Trimming {Visibly} {Pushdown} {Automata}},
	author = {Caralp, Mathieu and Reynier, Pierre-alain and Talbot, Jean-marc},
	pages = {1--12},
	file = {Caralp, Reynier, Talbot - Unknown - Trimming Visibly Pushdown Automata:/home/user/Zotero/storage/A42XJ4RG/Caralp, Reynier, Talbot - Unknown - Trimming Visibly Pushdown Automata.pdf:application/pdf}
}

@article{caralp-visibly-2012,
	title = {Visibly pushdown automata with multiplicities: {Finiteness} and {K}-boundedness},
	volume = {7410 LNCS},
	issn = {9783642316524},
	doi = {10.1007/978-3-642-31653-1\_21},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Caralp, Mathieu and Reynier, Pierre Alain and Talbot, Jean Marc},
	year = {2012},
	pages = {226--238},
	file = {Caralp, Reynier, Talbot - 2012 - Visibly pushdown automata with multiplicities Finiteness and K-boundedness:/home/user/Zotero/storage/WH5C3TUG/Caralp, Reynier, Talbot - 2012 - Visibly pushdown automata with multiplicities Finiteness and K-boundedness.pdf:application/pdf}
}

@article{caprace-decomposing-2008,
	title = {Decomposing locally compact groups into simple pieces},
	url = {http://arxiv.org/abs/0811.4101},
	doi = {10.1017/S0305004110000368},
	abstract = {We present a contribution to the structure theory of locally compact groups. The emphasis is on compactly generated locally compact groups which admit no infinite discrete quotient. It is shown that such a group possesses a characteristic cocompact subgroup which is either connected or admits a non-compact non-discrete topologically simple quotient. We also provide a description of characteristically simple groups and of groups all of whose proper quotients are compact. We show that Noetherian locally compact groups without infinite discrete quotient admit a subnormal series with all subquotients compact, compactly generated Abelian, or compactly generated topologically simple. Two appendices introduce results and examples around the concept of quasi-product.},
	author = {Caprace, Pierre-Emmanuel and Monod, Nicolas},
	year = {2008},
	file = {0811.4101:/home/user/Zotero/storage/DAZGHFI2/0811.4101.pdf:application/pdf}
}

@article{cai-probability-1989,
	title = {With {Probability} {One}, a {Random} {Oracle} {Separates} {PSPACE} from the {Polynomial}-{Time} {Hierarchy}},
	volume = {38},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Cai, Jin-yi},
	year = {1989},
	pages = {68--85}
}

@article{characterization-pdl-nodate,
	title = {{PDL} {Inside} the µ -calculus : {A} {Syntactic} and an},
	author = {Characterization, Automata-theoretic and Carreiro, Facundo and Venema, Yde},
	keywords = {propositional dynamic logic, automata theory, modal µ -calculus},
	file = {Carreiro-Venema:/home/user/Zotero/storage/R43EKG9A/Carreiro-Venema.pdf:application/pdf}
}

@article{chandra-constant-1984,
	title = {constant depth reducibility},
	author = {{Chandra} and {Stockmeyer} and {Vishkin}},
	year = {1984},
	file = {Chandra, Stockmeyer, Vishkin - 1984 - constant depth reducibility:/home/user/Zotero/storage/IGPTU3S6/Chandra, Stockmeyer, Vishkin - 1984 - constant depth reducibility.pdf:application/pdf}
}

@article{champarnaud-bottom-nodate,
	title = {Bottom {Up} {Quotients} and {Residuals} for {Tree} {Languages}},
	author = {Champarnaud, Jean-marc and Mignot, Ludovic and Ouali-sebti, Nadia and Ziadi, Djelloul},
	file = {1506.02863v1:/home/user/Zotero/storage/PMTUK6ZA/1506.02863v1.pdf:application/pdf}
}

@article{carreiro-weak-2014,
	title = {Weak {MSO}: {Automata} and {Expressiveness} {Modulo} {Bisimilarity}},
	issn = {9781450328869},
	url = {http://arxiv.org/abs/1401.4374},
	doi = {10.1145/2603088.2603101},
	abstract = {We prove that the bisimulation-invariant fragment of weak monadic second-order logic (WMSO) is equivalent to the fragment of the modal \$\mu\$-calculus where the application of the least fixpoint operator \$\mu p.\varphi\$ is restricted to formulas \$\varphi\$ that are continuous in \$p\$. Our proof is automata-theoretic in nature; in particular, we introduce a class of automata characterizing the expressive power of WMSO over tree models of arbitrary branching degree. The transition map of these automata is defined in terms of a logic \$\mathrm{FOE}\_1{\textasciicircum}\infty\$ that is the extension of first-order logic with a generalized quantifier \$\exists{\textasciicircum}\infty\$, where \$\exists{\textasciicircum}\infty x. \phi\$ means that there are infinitely many objects satisfying \$\phi\$. An important part of our work consists of a model-theoretic analysis of \$\mathrm{FOE}\_1{\textasciicircum}\infty\$.},
	journal = {arXiv:1401.4374 [cs]},
	author = {Carreiro, Facundo and Facchini, Alessandro and Venema, Yde and Zanasi, Fabio},
	year = {2014},
	file = {1401.4374v2:/home/user/Zotero/storage/AUDS5F92/1401.4374v2.pdf:application/pdf}
}

@article{cain-conjecture-nodate,
	title = {A conjecture relating growth of languages and monoids},
	author = {Cain, Alan J},
	file = {Cain - Unknown - A conjecture relating growth of languages and monoids:/home/user/Zotero/storage/SW8IQG3J/Cain - Unknown - A conjecture relating growth of languages and monoids.pdf:application/pdf}
}

@article{cadilhac-algebraic-nodate,
	title = {The {Algebraic} {Theory} of {Parikh} {Automata}},
	journal = {draft},
	author = {Cadilhac, Michaël and Krebs, Andreas and McKenzie, Pierre}
}

@article{cadilhac-circuit-2015,
	title = {The {Circuit} {Complexity} of {Transductions}},
	author = {Cadilhac, M and Krebs, A and Ludwig, M and Paperman, C},
	year = {2015},
	file = {stamped:/home/user/Zotero/storage/GNJTUANH/stamped.pdf:application/pdf}
}

@article{burgisser-explicit-2012,
	title = {Explicit {Lower} {Bounds} via {Geometric} {Complexity} {Theory}},
	issn = {9781450320290},
	url = {http://arxiv.org/abs/1210.8368},
	abstract = {We prove the lower bound R(M\_m) \geq 3/2 m{\textasciicircum}2 - 2 on the border rank of m x m matrix multiplication by exhibiting explicit representation theoretic (occurence) obstructions in the sense of the geometric complexity theory (GCT) program. While this bound is weaker than the one recently obtained by Landsberg and Ottaviani, these are the first significant lower bounds obtained within the GCT program. Behind the proof is the new combinatorial concept of obstruction designs, which encode highest weight vectors in Sym{\textasciicircum}d\otimes{\textasciicircum}3(C{\textasciicircum}n){\textasciicircum}* and provide new insights into Kronecker coefficients.},
	author = {Bürgisser, Peter and Ikenmeyer, Christian},
	month = oct,
	year = {2012},
	keywords = {geometric complexity theory, cation, kronecker coefficients, matrix multipli-, permanent versus determi-, tensor rank},
	pages = {10--10},
	file = {Bürgisser, Ikenmeyer - 2012 - Explicit Lower Bounds via Geometric Complexity Theory:/home/user/Zotero/storage/562SVW53/Bürgisser, Ikenmeyer - 2012 - Explicit Lower Bounds via Geometric Complexity Theory.pdf:application/pdf}
}

@article{brumm-titel-nodate,
	title = {Titel},
	author = {Brumm, Bernd},
	file = {Brumm - Unknown - Titel:/home/user/Zotero/storage/P3AX68H3/Brumm - Unknown - Titel.pdf:application/pdf}
}

@article{brandeho-optimal-2014,
	title = {An optimal adiabatic quantum query algorithm},
	url = {http://arxiv.org/abs/1409.3558},
	abstract = {Quantum query complexity is known to be characterized by the so-called quantum adversary bound. While this result has been proved in the standard discrete-time model of quantum computation, it also holds for continuous-time (or Hamiltonian-based) quantum computation, due to a known equivalence between these two query complexity models. In this work, we revisit this result by providing a direct proof in the continuous-time model. One originality of our proof is that it draws new connections between the adversary bound, a modern theoretical computer science technique, and early theorems of quantum mechanics. Indeed, the proof of the lower bound is based on Ehrenfest's theorem, while the upper bound relies on the Adiabatic theorem, as we construct an optimal adiabatic quantum query algorithm.},
	author = {Brandeho, Mathieu and Roland, Jérémie},
	month = sep,
	year = {2014},
	pages = {1--17},
	file = {Brandeho, Roland - 2014 - An optimal adiabatic quantum query algorithm:/home/user/Zotero/storage/2CGN7ND4/Brandeho, Roland - 2014 - An optimal adiabatic quantum query algorithm.pdf:application/pdf}
}

@inproceedings{krebs-counting-2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Counting {Paths} in {VPA} {Is} {Complete} for \#{NC}{\textasciicircum}{\mbox{1}}},
	volume = {6196},
	isbn = {978-3-642-14030-3},
	booktitle = {{COCOON}},
	publisher = {Springer},
	author = {Krebs, Andreas and Limaye, Nutan and Mahajan, Meena},
	editor = {Thai, My T and Sahni, Sartaj},
	year = {2010},
	pages = {44--53}
}

@article{krebs-characterizing-2007-1,
	title = {Characterizing {TC}0 in {Terms} of {Infinite} {Groups}},
	volume = {40},
	url = {http://dblp.uni-trier.de/db/journals/mst/mst40.html#KrebsLR07},
	doi = {10.1007/s00224-006-1310-2},
	number = {4},
	journal = {Theory of Computing Systems},
	author = {Krebs, Andreas and Lange, Klaus-Jorn and Reifferscheid, Stephanie},
	month = jun,
	year = {2007},
	pages = {303--325}
}

@inproceedings{krebs-visibly-2015-1,
	title = {Visibly {Counter} {Languages} and {Constant} {Depth} {Circuits}},
	booktitle = {{({STACS}} 2015)},
	author = {Krebs, Andreas and Lange, Klaus-Jörn and Ludwig, Michael},
	year = {2015}
}

@article{krebs-visibly-2014,
	title = {Visibly {Counter} {Languages} and {Constant} {Depth} {Circuits}},
	volume = {21},
	url = {http://eccc.hpi-web.de/report/2014/177},
	journal = {Electronic Colloquium on Computational Complexity {(ECCC)}},
	author = {Krebs, Andreas and Lange, Klaus-Jörn and Ludwig, Michael},
	year = {2014},
	pages = {177--177}
}

@inproceedings{koucky-circuit-2006-2,
	title = {Circuit {Lower} {Bounds} via {{E}}hrenfeucht-{{F}}raisse {Games}},
	booktitle = {{IEEE} {Conference} on {Computational} {Complexity}},
	author = {Koucký, Michal and Lautemann, Clemens and Poloczek, Sebastian and Thérien, Denis},
	year = {2006},
	pages = {190--201}
}

@article{kolman-courcelles-2015,
	title = {Courcelle's {Theorem}: {An} {Extension} {Complexity} {Analogue}},
	url = {http://arxiv.org/abs/1507.04907},
	abstract = {Courcelle's theorem states that given an MSO formula \$\varphi\$ and a graph \$G\$ with \$n\$ vertices and treewidth \$\tau\$, checking whether \$G\$ satisfies \$\varphi\$ or not can be done in time \$f(\tau,{\textbar}\varphi{\textbar})\cdot n\$ where \$f\$ is some computable function. We show an analogous result for extension complexity. In particular, we consider the polytope \$P\_{\varphi}(G)\$ of all satisfying assignments of a given MSO formula \$\varphi\$ on a given graph \$G\$ and show that \$P\_{\varphi}(G)\$ can be described by a linear program with \$f({\textbar}\varphi{\textbar}, \tau)\cdot n\$ inequalities where \$f\$ is some computable function, \$n\$ is the number of vertices in \$G\$ and \$\tau\$ is the treewidth of \$G\$. In other words, we prove that the extension complexity of \$P\_{\varphi}(G)\$ is linear in the size of the graph \$G\$. This provides a first meta theorem about the extension complexity of polytopes related to a wide class of problems and graphs. Furthermore, even though linear time {\em optimization} versions of Courcelle's theorem are known, our result provides a linear size LP for these problems out of the box. We also introduce a simple tool for polyhedral manipulation, called the \emph{glued product} of polytopes which is a slight generalization of the usual product of polytopes. We use it to build our extended formulation by identifying a case for \$0/1\$ polytopes when the glued product does not increase the extension complexity too much. The glued product may be of independent interest.},
	author = {Kolman, Petr and Koutecký, Martin and Tiwary, Hans Raj},
	year = {2015},
	file = {1507.04907v1:/home/user/Zotero/storage/KVB7GCHJ/1507.04907v1.pdf:application/pdf}
}

@article{klaus-joern-just-nodate,
	title = {Just another algebraic approach},
	author = {Klaus-joern, Lange},
	file = {Klaus-joern - Unknown - Just another algebraic approach:/home/user/Zotero/storage/K4T3HSNJ/Klaus-joern - Unknown - Just another algebraic approach.pdf:application/pdf}
}

@article{kiss-introduction-nodate,
	title = {{AN} {INTRODUCTION} {TO} {TAME} {CONGRUENCE} {THEORY}},
	author = {Kiss, Emil W},
	file = {Kiss - Unknown - AN INTRODUCTION TO TAME CONGRUENCE THEORY:/home/user/Zotero/storage/VSW9RS9U/Kiss - Unknown - AN INTRODUCTION TO TAME CONGRUENCE THEORY.pdf:application/pdf}
}

@article{kearnes-modularity-1999,
	title = {Modularity prevents tails},
	volume = {127},
	number = {1},
	author = {Kearnes, Keith A and Kiss, Emil W},
	year = {1999},
	pages = {11--19},
	file = {Kearnes, Kiss - 1999 - Modularity prevents tails:/home/user/Zotero/storage/5NRSHPQX/Kearnes, Kiss - 1999 - Modularity prevents tails.pdf:application/pdf}
}

@article{kearnes-minimal-nodate,
	title = {{MINIMAL} {CLONES} {WITH} {ABELIAN}},
	author = {Kearnes, Keith A},
	pages = {1--17},
	file = {Kearnes - Unknown - MINIMAL CLONES WITH ABELIAN:/home/user/Zotero/storage/XUZ28B8T/Kearnes - Unknown - MINIMAL CLONES WITH ABELIAN.pdf:application/pdf}
}

@inproceedings{karpinski-fundamentals-1983,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fundamentals of {Computation} {Theory}, {Proceedings} of the 1983 {International} {FCT}-{Conference}, {Borgholm}, {Sweden}, {August} 21-27, 1983},
	volume = {158},
	isbn = {3-540-12689-9},
	publisher = {Springer},
	editor = {Karpinski, Marek},
	year = {1983}
}

@inproceedings{karp-reducibility-1972,
	title = {Reducibility {Among} {Combinatorial} {Problems}},
	booktitle = {Proceedings of a symposium on the {Complexity} of {Computer} {Computations}, held {March} 20-22, 1972, at the {IBM} {Thomas} {J}. {Watson} {Research} {Center}, {Yorktown} {Heights}, {New} {York}},
	publisher = {Plenum Press, New York},
	author = {Karp, Richard M},
	year = {1972},
	pages = {85--103}
}

@article{kamp-tense-1968,
	title = {Tense {Logic} and the {Theory} of {Linear} {Order}},
	journal = {Ph.D. thesis, University of California, Berkeley},
	author = {Kamp, Johan Anthony Willem},
	year = {1968}
}

@inproceedings{kaminski-computer-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Science} {Logic}, 22nd {International} {Workshop}, {{CSL}} 2008, 17th {Annual} {Conference} of the {EACSL}, {Bertinoro}, {Italy}, {September} 16-19, 2008. {Proceedings}},
	volume = {5213},
	isbn = {978-3-540-87530-7},
	publisher = {Springer},
	editor = {Kaminski, Michael and Martini, Simone},
	year = {2008}
}

@article{kiss-easy-1997,
	title = {An {Easy} {Way} to {Minimal} {Algebras}},
	volume = {07},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S021819679700006X},
	doi = {10.1142/S021819679700006X},
	number = {01},
	journal = {International Journal of Algebra and Computation},
	author = {Kiss, Emil W.},
	month = feb,
	year = {1997},
	keywords = {and phrases, 1903, and 16432, and by the nserc, grant nos, hungarian national foundation for, minimal algebra, of canada, operating grant a-9128 of, professor ivo rosenberg, research supported by the, scientific research, tame congruence theory, twin polynomials},
	pages = {55--75},
	file = {Kiss - 1997 - An Easy Way to Minimal Algebras:/home/user/Zotero/storage/9F5Z64P5/Kiss - 1997 - An Easy Way to Minimal Algebras.pdf:application/pdf}
}

@book{kiss-shape-nodate,
	title = {The {Shape} of {Congruence} {Lattices} {Keith} {A} . {Kearnes} {Emil} {W} . {Kiss}},
	author = {Kiss, Emil W},
	file = {Kiss - Unknown - The Shape of Congruence Lattices Keith A . Kearnes Emil W . Kiss:/home/user/Zotero/storage/CZK4GH77/Kiss - Unknown - The Shape of Congruence Lattices Keith A . Kearnes Emil W . Kiss.pdf:application/pdf}
}

@article{kingdom-absorption-2015,
	title = {absorption semigroups},
	volume = {11},
	doi = {10.2168/LMCS-11)},
	author = {Kingdom, United and Team, Software Development and Kingdom, United},
	year = {2015},
	pages = {1--50},
	file = {1504.02899v2:/home/user/Zotero/storage/R939F967/1504.02899v2.pdf:application/pdf}
}

@inproceedings{karp-connections-1980,
	title = {Some {Connections} between {Nonuniform} and {Uniform} {Complexity} {Classes}},
	url = {http://doi.acm.org/10.1145/800141.804678},
	doi = {10.1145/800141.804678},
	booktitle = {Proceedings of the 12th {Annual} {{ACM}} {Symposium} on {Theory} of {Computing}, {April} 28-30, 1980, {Los} {Angeles}, {California}, {{USA}}},
	publisher = {ACM},
	author = {Karp, Richard M and Lipton, Richard J},
	editor = {Miller, Raymond E and Ginsburg, Seymour and Burkhard, Walter A and Lipton, Richard J},
	year = {1980},
	pages = {302--309}
}

@article{kallas-first-order-nodate,
	title = {First-order {Fragments} with {Successor} over {Infinite}},
	author = {Kallas, Jakub and Kufleitner, Manfred and Lauser, Alexander},
	keywords = {first-order logic, regular languages, automata theory, and phrases infinite words, semi-},
	pages = {356--367},
	file = {Kallas, Kufleitner, Lauser - Unknown - First-order Fragments with Successor over Infinite:/home/user/Zotero/storage/XSDSPCQI/Kallas, Kufleitner, Lauser - Unknown - First-order Fragments with Successor over Infinite.pdf:application/pdf}
}

@inproceedings{jurgensen-descriptional-2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Descriptional {Complexity} of {Formal} {Systems} - 16th {International} {Workshop}, {{DCFS}} 2014, {Turku}, {Finland}, {August} 5-8, 2014. {Proceedings}},
	volume = {8614},
	isbn = {978-3-319-09703-9},
	url = {http://dx.doi.org/10.1007/978-3-319-09704-6},
	doi = {10.1007/978-3-319-09704-6},
	publisher = {Springer},
	editor = {Jürgensen, Helmut and Karhumäki, Juhani and Okhotin, Alexander},
	year = {2014}
}

@article{jenner-completeness-2003,
	title = {Completeness results for graph isomorphism},
	volume = {66},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Jenner, Birgit and Köbler, Johannes and McKenzie, Pierre and Torán, Jacobo},
	year = {2003},
	pages = {549--566}
}

@article{jagadeesan-programming-2008,
	title = {Programming with {Role}-{Based} {Access} {Control}},
	volume = {4},
	doi = {10.2168/LMCS-4},
	journal = {Logical Methods in Computer Science},
	author = {Jagadeesan, Radha and Jagadeesan, Radha and Jeffrey, Alan and Jeffrey, Alan and Pitcher, Corin and Pitcher, Corin and Riely, James and Riely, James},
	year = {2008},
	pages = {1--24},
	file = {Jagadeesan et al. - 2008 - Programming with Role-Based Access Control:/home/user/Zotero/storage/45J2V9HZ/Jagadeesan et al. - 2008 - Programming with Role-Based Access Control.pdf:application/pdf}
}

@article{ivan-varieties-nodate,
	title = {Some varieties of finite tree automata related to restricted temporal logics},
	author = {Iván, Szabolcs},
	file = {Iván - Unknown - Some varieties of finite tree automata related to restricted temporal logics:/home/user/Zotero/storage/GNKZDQGC/Iván - Unknown - Some varieties of finite tree automata related to restricted temporal logics.pdf:application/pdf}
}

@article{ishii-monitoring-2015,
	title = {Monitoring {Bounded} {LTL} {Properties} {Using} {Interval} {Analysis}},
	url = {http://arxiv.org/abs/1506.01762},
	abstract = {Verification of temporal logic properties plays a crucial role in proving the desired behaviors of hybrid systems. In this paper, we propose an interval method for verifying the properties described by a bounded linear temporal logic. We relax the problem to allow outputting an inconclusive result when verification process cannot succeed with a prescribed precision, and present an efficient and rigorous monitoring algorithm that demonstrates that the problem is decidable. This algorithm performs a forward simulation of a hybrid automaton, detects a set of time intervals in which the atomic propositions hold, and validates the property by propagating the time intervals. A continuous state at a certain time computed in each step is enclosed by an interval vector that is proven to contain a unique solution. In the experiments, we show that the proposed method provides a useful tool for formal analysis of nonlinear and complex hybrid systems.},
	author = {Ishii, Daisuke and Yonezaki, Naoki and Goldsztejn, Alexandre},
	year = {2015},
	keywords = {linear temporal logic, bounded model checking, hybrid systems, interval analysis},
	file = {1506.01762v2:/home/user/Zotero/storage/EXRUBRZX/1506.01762v2.pdf:application/pdf}
}

@article{informatik-hyper-minimization-2013,
	title = {Hyper-{Minimization} for {Deterministic} {Tree} {Automata} ∗ {Institute} for {Natural} {Language} {Processing} , {Universität} {Stuttgart}},
	author = {Informatik, Max-planck-institut and Campus, E},
	year = {2013},
	pages = {1--16},
	file = {jezmal12b:/home/user/Zotero/storage/8BV245UT/jezmal12b.pdf:application/pdf}
}

@article{impagliazzo-size-depth-1997,
	title = {Size-{Depth} {Tradeoffs} for {Threshold} {Circuits}},
	volume = {26},
	number = {3},
	journal = {SIAM J. Comput.},
	author = {Impagliazzo, Russell and Paturi, Ramamohan and Saks, Michael E},
	year = {1997},
	pages = {693--707}
}

@article{immerman-languages-1987,
	title = {Languages that {Capture} {Complexity} {Classes}},
	volume = {16},
	number = {4},
	journal = {SIAM J. Comput.},
	author = {Immerman, Neil},
	year = {1987},
	pages = {760--778}
}

@article{idziaszek-regular-2015,
	title = {Regular {Languages} of {Thin} {Trees}},
	url = {http://link.springer.com/10.1007/s00224-014-9595-z},
	doi = {10.1007/s00224-014-9595-z},
	journal = {Theory of Computing Systems},
	author = {Idziaszek, Tomasz and Skrzypczak, Michał and Bojańczyk, Mikołaj},
	year = {2015},
	keywords = {regular languages, effective characterizations, infinite trees, 239850, all authors were supported, boja, by erc starting grant, idziaszek, m, no, skrzypczak, sosna, t, topological complexity},
	file = {Idziaszek, Skrzypczak, Bojańczyk - 2015 - Regular Languages of Thin Trees:/home/user/Zotero/storage/TN7HS43U/Idziaszek, Skrzypczak, Bojańczyk - 2015 - Regular Languages of Thin Trees.pdf:application/pdf}
}

@book{jukna-boolean-2012,
	series = {Algorithms and combinatorics},
	title = {Boolean {Function} {Complexity} -- {Advances} and {Frontiers}},
	volume = {27},
	isbn = {978-3-642-24507-7},
	publisher = {Springer},
	author = {Jukna, Stasys},
	year = {2012}
}

@incollection{johnson-catalog-1990,
	title = {A {Catalog} of {Complexity} {Classes}},
	booktitle = {Handbook of {Theoretical} {Computer} {Science}, {Volume} {A}: {Algorithms} and {Complexity} ({A})},
	publisher = {The MIT Press},
	author = {Johnson, David S},
	year = {1990},
	pages = {67--161}
}

@article{ito-wiener-1960,
	title = {The {Wiener} {Integral}},
	issn = {9781614442097},
	url = {http://projecteuclid.org/euclid.bsmsp/1200512604},
	doi = {10.7135/UPO9781614442097.010},
	number = {x},
	journal = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Contributions to Probability Theory},
	author = {Itô, Kiyosi},
	year = {1960},
	keywords = {Physics, Quantum mechanics},
	pages = {227--238},
	file = {Itô - 1960 - The Wiener Integral:/home/user/Zotero/storage/KH5N6X86/Itô - 1960 - The Wiener Integral.pdf:application/pdf}
}

@article{ir-origins-1999,
	title = {{ORIGINS} {OF} {ISLAM} : {POLITICAL}-{ANTHROPOLOGICAL} {AND} {ENVIRONMENTAL} {CONTEXT}},
	volume = {52},
	author = {Ir, L A D I M and Ko, L I M E N},
	year = {1999},
	pages = {243--276},
	file = {Ir, Ko - 1999 - ORIGINS OF ISLAM POLITICAL-ANTHROPOLOGICAL AND ENVIRONMENTAL CONTEXT:/home/user/Zotero/storage/TB555GUC/Ir, Ko - 1999 - ORIGINS OF ISLAM POLITICAL-ANTHROPOLOGICAL AND ENVIRONMENTAL CONTEXT.pdf:application/pdf}
}

@inproceedings{impagliazzo-size-depth-1993,
	title = {Size-depth trade-offs for threshold circuits},
	booktitle = {{STOC}},
	author = {Impagliazzo, Russell and Paturi, Ramamohan and Saks, Michael E},
	year = {1993},
	pages = {541--550}
}

@book{immerman-descriptive-1999,
	series = {Graduate texts in computer science},
	title = {Descriptive complexity},
	isbn = {978-0-387-98600-5},
	publisher = {Springer},
	author = {Immerman, Neil},
	year = {1999}
}

@book{immerman-descriptive-1999-1,
	title = {Descriptive {Complexity}},
	publisher = {Springer, New York},
	author = {Immerman, Neil},
	year = {1999}
}

@article{ideas-great-2007,
	title = {Great {Theoretical} {Ideas} in {Computer} {Science}},
	abstract = {CS 15-251},
	author = {Ideas, Great Theoretical},
	year = {2007},
	pages = {1--8},
	file = {Ideas - 2007 - Great Theoretical Ideas in Computer Science:/home/user/Zotero/storage/PARW9JF6/Ideas - 2007 - Great Theoretical Ideas in Computer Science.pdf:application/pdf}
}

@article{ibarra-two-way-1973,
	title = {On {Two}-way {Multihead} {Automata}},
	volume = {7},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Ibarra, Oscar H},
	year = {1973},
	pages = {28--36}
}

@article{humphreys-lie-nodate,
	title = {lie algebras},
	author = {{Humphreys}},
	file = {[J.E._Humphreys]_Introduction_to_Lie_algebras_and_(BookZZ.org):/home/user/Zotero/storage/UUIEI9AM/[J.E._Humphreys]_Introduction_to_Lie_algebras_and_(BookZZ.org).djvu:image/vnd.djvu}
}

@book{hobby-structure-1988,
	address = {Providence},
	title = {The {Structure} of {Finite} {Algebras}},
	publisher = {American Mathematical Society},
	author = {Hobby, David and McKenzie, Ralph},
	year = {1988}
}

@book{howie-fundamentals-1995,
	title = {Fundamentals of {Semigroup} {Theory}},
	publisher = {Clarendon Press, Oxford},
	author = {Howie, John M},
	year = {1995}
}

@book{hinman-fundamentals-2005,
	address = {Wellesley},
	title = {Fundamentals of {{M}}athematical {{L}}ogic},
	publisher = {A K Peters},
	author = {Hinman, Peter},
	year = {2005}
}

@book{hermann-vector-1970-1,
	address = {New York},
	title = {Vector {Bundles} in {Mathematical} {Physics}},
	publisher = {W. A. Benjamin},
	author = {Hermann, Robert},
	year = {1970}
}

@article{henkin-completeness-1950,
	title = {Completeness in the {{T}}heory of {{T}}ypes},
	volume = {15},
	number = {2},
	journal = {The Journal of Symbolic Logic},
	author = {Henkin, Leon},
	year = {1950},
	pages = {81--91}
}

@article{henckell-aperiodic-2007,
	title = {Aperiodic {Pointlikes} and {Beyond}},
	url = {http://arxiv.org/abs/0706.0248},
	doi = {10.1142/S0218196710005662},
	abstract = {We prove that if \$\pi\$ is a recursive set of primes, then pointlike sets are decidable for the pseudovariety of semigroups whose subgroups are \$\pi\$-groups. In particular, when \$\pi\$ is the empty set, we obtain Henckell's decidability of aperiodic pointlikes. Our proof, restricted to the case of aperiodic semigroups, is simpler than the original proof.},
	author = {Henckell, Karsten and Rhodes, John and Steinberg, Benjamin},
	year = {2007},
	pages = {1--17},
	file = {10.1.1.250.6857:/home/user/Zotero/storage/4I8WK6SV/10.1.1.250.6857.pdf:application/pdf}
}

@article{henckell-aperiodic-2007-1,
	title = {Aperiodic {Pointlikes} and {Beyond}},
	url = {http://arxiv.org/abs/0706.0248},
	doi = {10.1142/S0218196710005662},
	abstract = {We prove that if \$\pi\$ is a recursive set of primes, then pointlike sets are decidable for the pseudovariety of semigroups whose subgroups are \$\pi\$-groups. In particular, when \$\pi\$ is the empty set, we obtain Henckell's decidability of aperiodic pointlikes. Our proof, restricted to the case of aperiodic semigroups, is simpler than the original proof.},
	author = {Henckell, Karsten and Rhodes, John and Steinberg, Benjamin},
	year = {2007},
	pages = {1--17},
	file = {0706.0248v2:/home/user/Zotero/storage/K85GKS59/0706.0248v2.pdf:application/pdf}
}

@article{henckell-ordered-1999,
	title = {Ordered monoids and {J} -trivial monoids},
	author = {Henckell, Karsten and Pin, Jean-eric},
	year = {1999},
	pages = {1--20},
	file = {OrderedJtrivial:/home/user/Zotero/storage/H4UCX5TJ/OrderedJtrivial.pdf:application/pdf}
}

@article{henckell-gh-nodate,
	title = {{gH} 1 {H} 2 {H}},
	author = {Henckell, Karsten and Margolis, Stuart W and Pin, Jean-eric and Rhodes, John},
	pages = {1--24},
	file = {10.1.1.6.5654:/home/user/Zotero/storage/RFK9CZBQ/10.1.1.6.5654.pdf:application/pdf}
}

@inproceedings{hastad-almost-1986,
	title = {Almost {Optimal} {Lower} {Bounds} for {Small} {Depth} {Circuits}},
	booktitle = {{STOC}},
	publisher = {ACM},
	author = {Håstad, Johan},
	year = {1986},
	pages = {6--20}
}

@article{hastad-computational-1987,
	title = {Computational {Limitations} for {Small} {Depth} {Circuits}},
	journal = {MIT Press, Cambridge, MA},
	author = {Håstad, Johan},
	year = {1987}
}

@article{hansen-exposition-2012,
	title = {An {Exposition} of the erien {Classification}},
	author = {Hansen, Kristoffer Arnsfelt},
	year = {2012},
	pages = {1--17},
	file = {Hansen - 2012 - An Exposition of the erien Classification:/home/user/Zotero/storage/Q5ISNGGR/Hansen - 2012 - An Exposition of the erien Classification.pdf:application/pdf}
}

@book{hobby-structure-1988-1,
	address = {Providence},
	title = {The {Structure} of {Finite} {Algebras}},
	publisher = {American Mathematical Society},
	author = {Hobby, David and McKenzie, Ralph},
	year = {1988},
	file = {Hobby, McKenzie - 1988 - The Structure of Finite Algebras:/home/user/Zotero/storage/GNM4ZN63/Hobby, McKenzie - 1988 - The Structure of Finite Algebras.pdf:application/pdf}
}

@article{henckell-pointlike-1988,
	title = {Pointlike sets: the finest aperiodic cover of a finite semigroup},
	volume = {55},
	doi = {10.1016/0022-4049(88)90042-4},
	number = {July},
	journal = {Journal of Pure and Applied Algebra},
	author = {Henckell, Karsten},
	year = {1988},
	pages = {85--126},
	file = {0f317537c92eaadd3e000000:/home/user/Zotero/storage/PQVVJXXJ/0f317537c92eaadd3e000000.pdf:application/pdf}
}

@article{hajnal-threshold-1993,
	title = {Threshold {Circuits} of {Bounded} {Depth}},
	volume = {46},
	number = {2},
	journal = {J. Comput. Syst. Sci.},
	author = {Hajnal, András and Maass, Wolfgang and Pudlák, Pavel and Szegedy, Mario and Turán, György},
	year = {1993},
	pages = {129--154}
}

@inproceedings{hajnal-threshold-1987,
	title = {Threshold circuits of bounded depth},
	booktitle = {{FOCS}},
	author = {Hajnal, András and Maass, Wolfgang and Pudlák, Pavel and Szegedy, Mario and Turán, György},
	year = {1987},
	pages = {99--110}
}

@inproceedings{hahn-visibly-2015,
	title = {Visibly {Counter} {Languages} and the {Structure} of {NC}1},
	booktitle = {Proceedings of {Mathematical} {Foundations} of {Computer} {Science} ({MFCS}) 2015},
	author = {Hahn, Michael and Krebs, Andreas and Lange, Klaus-J{\"o}rn and Ludwig, Michael},
	year = {2015},
	file = {paper:/home/user/Zotero/storage/IBM328R7/paper.pdf:application/pdf}
}

@book{haase-ergodic-2011,
	title = {Ergodic {Theory}—{An} {Operator}-theoretic {Approach}},
	url = {https://www.fa.uni-tuebingen.de/lehre/wintersemester-2011-12/ergodentheorie/scripts/ET-27-Aug-2011-small.pdf},
	author = {Haase, M and Eisner, T and Farkas, B and Nagel, R},
	year = {2011},
	file = {Haase et al. - 2011 - Ergodic Theory—An Operator-theoretic Approach:/home/user/Zotero/storage/RE8GV3M7/Haase et al. - 2011 - Ergodic Theory—An Operator-theoretic Approach.pdf:application/pdf}
}

@article{gurevich-logic-1984,
	title = {A {Logic} for {Constant}-{Depth} {Circuits}},
	volume = {61},
	number = {1},
	journal = {Information and Control},
	author = {Gurevich, Yuri and Lewis, Harry R},
	year = {1984},
	pages = {65--74}
}

@article{griffiths-theorem-1962,
	title = {On a theorem of {Chern}},
	volume = {6},
	number = {3},
	journal = {Illinois Journal of Mathematics},
	author = {Griffiths, Phillip A},
	year = {1962},
	pages = {468--479},
	file = {Theorem, Chern - 1962 - ON A THEOREM OF CHERN:/home/user/Zotero/storage/CN5ZF6VG/Theorem, Chern - 1962 - ON A THEOREM OF CHERN.pdf:application/pdf}
}

@article{green-not-2011,
	title = {On (not) computing the {Mobius} function using bounded depth circuits},
	url = {http://arxiv.org/abs/1103.4991},
	abstract = {Any function F : {0,...,N-1} -{\textgreater} {-1,1} such that F(x) can be computed from the binary digits of x using a bounded depth circuit is orthogonal to the Mobius function mu in the sense that E\_{0 {\textless}= x {\textless}= N-1} mu(x)F(x) = o(1). The proof combines a result of Linial, Mansour and Nisan with techniques of Katai and Harman-Katai, used in their work on finding primes with specified digits.},
	author = {Green, Ben},
	month = mar,
	year = {2011},
	pages = {10--10},
	file = {Green - 2011 - On (not) computing the Mobius function using bounded depth circuits:/home/user/Zotero/storage/RCHPJM4U/Green - 2011 - On (not) computing the Mobius function using bounded depth circuits.pdf:application/pdf}
}

@article{gray-infinite-2013,
	title = {Infinite monoids as geometric objects {Groups} , monoids , and geometry},
	number = {June},
	author = {Gray, Robert},
	year = {2013},
	file = {Gray - 2013 - Infinite monoids as geometric objects Groups , monoids , and geometry:/home/user/Zotero/storage/EZPNN7W6/Gray - 2013 - Infinite monoids as geometric objects Groups , monoids , and geometry.pdf:application/pdf}
}

@article{gorazd-complexity-nodate,
	title = {Complexity of term equation problem},
	author = {Gorazd, Tomasz A},
	pages = {7--9},
	file = {Gorazd - Unknown - Complexity of term equation problem:/home/user/Zotero/storage/CX9M5RGX/Gorazd - Unknown - Complexity of term equation problem.pdf:application/pdf}
}

@article{goodman-overtwisted-2004,
	title = {Overtwisted open books from sobering arcs},
	volume = {5},
	url = {http://arxiv.org/abs/math/0407420},
	doi = {10.2140/agt.2005.5.1173},
	abstract = {We study open books on three manifolds which are compatible with an overtwisted contact structure. We show that the existence of certain arcs, called sobering arcs, is a sufficient condition for an open book to be overtwisted, and is necessary up to stabilization by positive Hopf-bands. Using these techniques we prove that some open books arising as the boundary of symplectic configurations are overtwisted, answering a question of Gay in Algebr. Geom. Topol. 3 (2003) 569--586.},
	number = {September},
	author = {Goodman, Noah},
	year = {2004},
	keywords = {contact structure, open book, overtwisted, plectic configuration graph, sobering arc, sym-},
	pages = {1173--1195},
	file = {0407420v2:/home/user/Zotero/storage/BRM8AI2Q/0407420v2.pdf:application/pdf}
}

@inproceedings{goldschlager-unified-1978,
	title = {A {Unified} {Approach} to {Models} of {Synchronous} {Parallel} {Machines}},
	booktitle = {{STOC}},
	author = {Goldschlager, Leslie M},
	year = {1978},
	pages = {89--94}
}

@book{goldreich-computational-2008,
	title = {Computational complexity - a conceptual perspective},
	isbn = {978-0-521-88473-0},
	publisher = {Cambridge University Press},
	author = {Goldreich, Oded},
	year = {2008}
}

@article{gentzen-untersuchungen-1935,
	title = {Untersuchungen über das logische {{S}}chlie{ß}en {{I}}{{I}}},
	volume = {39},
	journal = {Mathematische Zeitschrift},
	author = {Gentzen, Gerhard},
	year = {1935},
	pages = {405--431}
}

@inproceedings{gehrke-ultrafilters-2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {From {Ultrafilters} on {Words} to the {Expressive} {Power} of a {Fragment} of {Logic}},
	volume = {8614},
	isbn = {978-3-319-09703-9},
	url = {http://dx.doi.org/10.1007/978-3-319-09704-6\_13},
	doi = {10.1007/978-3-319-09704-6\_13},
	booktitle = {Descriptional {Complexity} of {Formal} {Systems} - 16th {International} {Workshop}, {{DCFS}} 2014, {Turku}, {Finland}, {August} 5-8, 2014. {Proceedings}},
	publisher = {Springer},
	author = {Gehrke, Mai and Krebs, Andreas and Pin, Jean-Éric},
	editor = {Jürgensen, Helmut and Karhumäki, Juhani and Okhotin, Alexander},
	year = {2014},
	pages = {138--149}
}

@inproceedings{gehrke-topological-2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Topological} {Approach} to {Recognition}},
	volume = {6199},
	isbn = {978-3-642-14161-4},
	url = {http://dx.doi.org/10.1007/978-3-642-14162-1\_13},
	doi = {10.1007/978-3-642-14162-1\_13},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gehrke, Mai and Grigorieff, Serge and Pin, Jean-Éric},
	editor = {Abramsky, Samson and Gavoille, Cyril and Kirchner, Claude and auf der Heide, Friedhelm and Spirakis, PaulG.},
	year = {2010},
	pages = {151--162}
}

@inproceedings{gehrke-duality-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Duality and {Equational} {Theory} of {Regular} {Languages}},
	volume = {5126},
	isbn = {978-3-540-70582-6},
	booktitle = {{ICALP} (2)},
	publisher = {Springer},
	author = {Gehrke, Mai and Grigorieff, Serge and Pin, Jean-Eric},
	editor = {Aceto, Luca and Damgård, Ivan and Goldberg, Leslie Ann and Halldórsson, Magnús M and Ingólfsdóttir, Anna and Walukiewicz, Igor},
	year = {2008},
	pages = {246--257}
}

@article{gavalda-learning-2006,
	title = {Learning expressions and programs over monoids},
	volume = {204},
	doi = {10.1016/j.ic.2005.09.003},
	abstract = {We study the problem of learning an unknown function represented as an expression or a program over a known finite monoid.As inother areas of computational complexity where programs over algebras have been used, the goal is to relate the computational complexity of the learning problem with the algebraic complexity of the finite monoid. Indeed, our results indicate a close connection between both kinds of complexity. We focus on monoids which are either groups or aperiodic, and on the learning model of exact learning from queries. For a group G, we prove that expressions over G are efficiently learnable if G is nilpotent, and impossible to learn efficiently (under cryptographic assumptions) if G is nonsolvable. We present some results for restricted classes of solvable groups, and point out a connection between their efficient learnability and the existence of lower bounds on their computational power in the program model. For aperiodic monoids, our results seem to indicate that the monoid class known as DA captures exactly learnability of expressions by polynomially many Evaluation queries. When using programs instead of expressions, we show that our results for groups remain true, while the situation is quite different for aperiodic monoids. © 2005 Elsevier Inc. All rights reserved.},
	journal = {Information and Computation},
	author = {Gavaldà, Ricard and Tesson, Pascal and Thérien, Denis},
	year = {2006},
	pages = {177--209},
	file = {Gavaldà, Tesson, Thérien - 2006 - Learning expressions and programs over monoids:/home/user/Zotero/storage/S2V9ZSDA/Gavaldà, Tesson, Thérien - 2006 - Learning expressions and programs over monoids.pdf:application/pdf}
}

@inproceedings{gradel-computer-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Science} {Logic}, 23rd international {Workshop}, {{CSL}} 2009, 18th {Annual} {Conference} of the {EACSL}, {Coimbra}, {Portugal}, {September} 7-11, 2009. {Proceedings}},
	volume = {5771},
	isbn = {978-3-642-04026-9},
	url = {http://dx.doi.org/10.1007/978-3-642-04027-6},
	doi = {10.1007/978-3-642-04027-6},
	publisher = {Springer},
	editor = {Grädel, Erich and Kahle, Reinhard},
	year = {2009}
}

@book{godel-consistency-1940,
	title = {The {Consistency} of the {Axiom} of {Choice} and of the {Generalized} {Continuum}-{Hypothesis} {With} the {Axioms} of {Set} {Theory}},
	publisher = {Princeton University Press},
	author = {Gödel, Kurt},
	year = {1940}
}

@book{ginzburg-algebraic-1968,
	title = {Algebraic {Theory} of {Automata}},
	publisher = {Academic Press, New York},
	author = {Ginzburg, Abraham},
	year = {1968}
}

@article{ginsburg-semigroups-1966,
	title = {Semigroups, {Presburger} {Formulas}, and {Languages}},
	volume = {16},
	number = {2},
	journal = {Pacific journal of Mathematics},
	author = {Ginsburg, Seymour and Spanier, Edwin H},
	year = {1966},
	pages = {285--296}
}

@article{gentzen-neue-1938,
	title = {{{N}}eue {{F}}assung des {{W}}iderspruchsfreiheitsbeweises fur die reine {{Z}}ahlentheorie},
	volume = {4},
	journal = {Forschungen zur Logik und zur Grundlegung der exakten Wissenschaften},
	author = {Gentzen, Gerhard},
	year = {1938},
	keywords = {Logic},
	pages = {17--44}
}

@article{gentzen-widerspruchsfreiheit-1936,
	title = {Die {Widerspruchsfreiheit} der reinen {Zahlentheorie}},
	volume = {112},
	url = {http://dx.doi.org/10.1007/BF01565428},
	number = {1},
	journal = {Mathematische Annalen},
	author = {Gentzen, Gerhard},
	year = {1936},
	keywords = {Logic},
	pages = {493--565}
}

@phdthesis{gelderie-classifying-2010,
	title = {Classifying {Regular} {Languages} via {Cascade} {Products} of {Automata} {Diploma} {Thesis}},
	author = {Gelderie, Marcus},
	year = {2010},
	file = {Gelderie - Unknown - Classifying Regular Languages via Cascade Products of Automata:/home/user/Zotero/storage/HQGIJI3R/Gelderie - Unknown - Classifying Regular Languages via Cascade Products of Automata.pdf:application/pdf;Unknown - 2010 - Classifying Regular Languages via Cascade Products of Automata Diploma Thesis:/home/user/Zotero/storage/IGA5WDQQ/Unknown - 2010 - Classifying Regular Languages via Cascade Products of Automata Diploma Thesis.pdf:application/pdf}
}

@article{gehrke-dual-nodate,
	title = {Dual {Spaces}},
	journal = {draft},
	author = {Gehrke, Mai and Krebs, Andreas and Pin, Jean-Eric}
}

@inproceedings{gehrke-ultrafilters-2014-1,
	title = {From ultrafilters on words to the expressive power of a fragment of logic ⋆},
	volume = {1},
	author = {Gehrke, Mai and Krebs, Andreas and Eric, Jean-},
	year = {2014},
	pages = {1--12},
	file = {Gehrke, Krebs, Eric - 2014 - From ultrafilters on words to the expressive power of a fragment of logic ⋆:/home/user/Zotero/storage/M2G5WMAC/Gehrke, Krebs, Eric - 2014 - From ultrafilters on words to the expressive power of a fragment of logic ⋆.pdf:application/pdf}
}

@article{gehrke-ultrafilters-2014-2,
	title = {Ultrafilters on words for a fragment of logic},
	number = {1},
	author = {Gehrke, Mai and Krebs, Andreas and Eric, Jean-},
	year = {2014},
	pages = {1--29},
	file = {Gehrke, Krebs, Eric - 2014 - Ultrafilters on words for a fragment of logic:/home/user/Zotero/storage/WUVH3BZP/Gehrke, Krebs, Eric - 2014 - Ultrafilters on words for a fragment of logic.pdf:application/pdf}
}

@article{gehrke-topological-nodate,
	title = {A topological approach to recognition},
	author = {Gehrke, Mai and Grigorieff, Serge and Pin, Jean-Eric},
	pages = {1--12},
	file = {Gehrke, Grigorieff, Pin - Unknown - A topological approach to recognition:/home/user/Zotero/storage/V3XRB8AN/Gehrke, Grigorieff, Pin - Unknown - A topological approach to recognition.pdf:application/pdf}
}

@article{gaspard-block-1997,
	title = {The block product of categories and {Tilson} ' s division},
	volume = {30},
	author = {Gaspard, Institut and Universite, Monge and Marne, De and Rey, Jean-francois},
	year = {1997},
	file = {Gaspard et al. - 1997 - The block product of categories and Tilson ' s division:/home/user/Zotero/storage/RR9BJSJZ/Gaspard et al. - 1997 - The block product of categories and Tilson ' s division.pdf:application/pdf}
}

@inproceedings{garg-garbled-2015,
	title = {Garbled {RAM} {From} {One}-{Way} {Functions}},
	isbn = {978-1-4503-3536-2},
	booktitle = {{STOC}},
	author = {Garg, Sanjam and Lu, Steve and Scafuro, Alessandra},
	year = {2015},
	pages = {449--458},
	file = {p449-garg:/home/user/Zotero/storage/QP554QTB/p449-garg.pdf:application/pdf}
}

@inproceedings{galil-two-1974,
	title = {Two {Way} {Deterministic} {Pushdown} {Automaton} {Languages} and {Some} {Open} {Problems} in the {Theory} of {Computation}},
	booktitle = {{FOCS}},
	publisher = {IEEE},
	author = {Galil, Zvi},
	year = {1974},
	pages = {170--177}
}

@article{gal-incremental-2008,
	title = {Incremental {Branching} {Programs}},
	volume = {43},
	number = {2},
	journal = {Theory Comput. Syst.},
	author = {Gál, Anna and Koucký, Michal and McKenzie, Pierre},
	year = {2008},
	pages = {159--184}
}

@inproceedings{flum-logic-2008,
	series = {Texts in {Logic} and {Games}},
	title = {Logic and {Automata}: {History} and {Perspectives} [in {Honor} of {Wolfgang} {Thomas}]},
	volume = {2},
	isbn = {978-90-5356-576-6},
	publisher = {Amsterdam University Press},
	editor = {Flum, Jörg and Grädel, Erich and Wilke, Thomas},
	year = {2008}
}

@article{fakult-logical-2006,
	title = {Logical {Fragments} for {Mazurkiewicz} {Traces} : {Expressive} {Power} and {Algebraic} {Acknowledgments} {First} and foremost , {I} would like to thank my supervisor {Volker} {Diekert} for},
	author = {Fakult, Von Der and Kufleitner, Manfred},
	year = {2006},
	file = {Fakult, Kufleitner - 2006 - Logical Fragments for Mazurkiewicz Traces Expressive Power and Algebraic Acknowledgments First and foremost:/home/user/Zotero/storage/VG2MTFSI/Fakult, Kufleitner - 2006 - Logical Fragments for Mazurkiewicz Traces Expressive Power and Algebraic Acknowledgments First and foremost.pdf:application/pdf}
}

@inproceedings{gardner-concur-2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{{CONCUR}} 2004 - {Concurrency} {Theory}, 15th {International} {Conference}, {London}, {UK}, {August} 31 - {September} 3, 2004, {Proceedings}},
	volume = {3170},
	isbn = {3-540-22940-X},
	publisher = {Springer},
	editor = {Gardner, Philippa and Yoshida, Nobuko},
	year = {2004}
}

@inproceedings{furst-parity-1981,
	title = {Parity, {Circuits}, and the {Polynomial}-{Time} {Hierarchy}},
	booktitle = {22nd {Annual} {Symposium} on {Foundations} of {Computer} {Science}, {Nashville}, {Tennessee}, {USA}, 28-30 {October} 1981},
	publisher = {IEEE Computer Society},
	author = {Furst, Merrick L and Saxe, James B and Sipser, Michael},
	year = {1981},
	pages = {260--270}
}

@inproceedings{furst-parity-1981-1,
	title = {Parity, {Circuits}, and the {Polynomial}-{Time} {Hierarchy}},
	booktitle = {{FOCS}},
	author = {Furst, Merrick L and Saxe, James B and Sipser, Michael},
	year = {1981},
	pages = {260--270}
}

@article{frougny-synchronized-1993,
	title = {Synchronized rational relations of finite and infinite words},
	volume = {108},
	url = {http://www.sciencedirect.com/science/article/pii/030439759390230Q},
	journal = {Theoretical Computer Science},
	author = {Frougny, C and Sakarovitch, J},
	year = {1993},
	pages = {45--82},
	file = {Frougny, Sakarovitch - 1993 - Synchronized rational relations of finite and infinite words:/home/user/Zotero/storage/7BV6C76N/Frougny, Sakarovitch - 1993 - Synchronized rational relations of finite and infinite words.pdf:application/pdf}
}

@article{friedmann-ramsey-2013,
	title = {Ramsey goes visibly pushdown},
	volume = {7966 LNCS},
	issn = {9783642392115},
	doi = {10.1007/978-3-642-39212-2\_22},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Friedmann, Oliver and Klaedtke, Felix and Lange, Martin},
	year = {2013},
	pages = {224--237},
	file = {Friedmann, Klaedtke, Lange - 2013 - Ramsey goes visibly pushdown:/home/user/Zotero/storage/TIXTVXCQ/Friedmann, Klaedtke, Lange - 2013 - Ramsey goes visibly pushdown.pdf:application/pdf}
}

@article{franek-quasi-decidability-2012,
	title = {Quasi-decidability of a {Fragment} of the {Analytic} {First}-order {Theory} of {Real} {Numbers} ∗ {Institute} of {Computer} {Science} , {Academy} of {Sciences} of the {Czech} {Republic}},
	author = {Franek, Peter and Ratschan, Stefan and Zgliczynski, Piotr},
	year = {2012},
	pages = {1--28},
	file = {1309.6280v4:/home/user/Zotero/storage/NZC5ZTEQ/1309.6280v4.pdf:application/pdf}
}

@article{fleischer-block-2014,
	title = {Block {Products} and {Nesting} {Negations} in {FO} 2 {Main} result},
	author = {Fleischer, Lukas},
	year = {2014},
	file = {Fleischer - 2014 - Block Products and Nesting Negations in FO 2 Main result:/home/user/Zotero/storage/KBXVJMZX/Fleischer - 2014 - Block Products and Nesting Negations in FO 2 Main result.pdf:application/pdf}
}

@article{et-al.-ufo-nodate,
	title = {{UFO}},
	journal = {.},
	author = {et al., Pierre}
}

@article{et-al.-ufo-nodate-1,
	title = {{UFO}},
	journal = {.},
	author = {et al., Pierre}
}

@article{esik-algebraic-2006,
	title = {Algebraic recognizability of regular tree languages},
	volume = {abs/cs/060},
	url = {http://arxiv.org/abs/cs/0609113},
	journal = {CoRR},
	author = {Ésik, Zoltán and Weil, Pascal},
	year = {2006}
}

@article{esik-algebraic-2006-1,
	title = {Algebraic recognizability of regular tree languages},
	url = {http://arxiv.org/abs/cs/0609113},
	abstract = {We propose a new algebraic framework to discuss and classify recognizable tree languages, and to characterize interesting classes of such languages. Our algebraic tool, called preclones, encompasses the classical notion of syntactic Sigma-algebra or minimal tree automaton, but adds new expressivity to it. The main result in this paper is a variety theorem \`{a} la Eilenberg, but we also discuss important examples of logically defined classes of recognizable tree languages, whose characterization and decidability was established in recent papers (by Benedikt and S\'{e}goufin, and by Bojanczyk and Walukiewicz) and can be naturally formulated in terms of pseudovarieties of preclones. Finally, this paper constitutes the foundation for another paper by the same authors, where first-order definable tree languages receive an algebraic characterization.},
	number = {September},
	author = {Esik, Zoltan and Weil, Pascal},
	month = sep,
	year = {2006},
	file = {Esik, Weil - 2006 - Algebraic recognizability of regular tree languages:/home/user/Zotero/storage/QPIB926V/Esik, Weil - 2006 - Algebraic recognizability of regular tree languages.pdf:application/pdf}
}

@article{fahmy-efficient-nodate,
	title = {Efficient {Learning} of {Real} {Time} {One}-{Counter} {Automata}},
	author = {Fahmy, Amr F and Roos, Robert S},
	pages = {1--16},
	file = {Fahmy, Roos - Unknown - Efficient Learning of Real Time One-Counter Automata:/home/user/Zotero/storage/EBDJGX4B/Fahmy, Roos - Unknown - Efficient Learning of Real Time One-Counter Automata.pdf:application/pdf}
}

@article{fackler-structure-2010,
	title = {On the structure of semigroups on l},
	author = {Fackler, Stephan},
	year = {2010},
	keywords = {and phrases, -calculus, bounded h, by a scholarship of, landesgraduiertenförderung baden-, p -operator spaces, positive semigroups, the, the author was supported},
	pages = {1--13},
	file = {Fackler - 2010 - On the structure of semigroups on l:/home/user/Zotero/storage/IVE3FFJW/Fackler - 2010 - On the structure of semigroups on l.pdf:application/pdf}
}

@article{et-al.-ufo-nodate-2,
	title = {{UFO}},
	journal = {.},
	author = {et al., Pierre}
}

@article{shneerson-polynomial-2008,
	title = {Polynomial growth in semigroup varieties},
	volume = {320},
	journal = {J. Algebra},
	author = {Shneerson, L.M.},
	year = {2008},
	pages = {2218--2279}
}

@article{scott-are-1997,
	title = {Are {All} {Groups} {Finite}?},
	url = {http://books.google.com/books?hl=en&lr=&id=o97cBMDoYCgC&oi=fnd&pg=PA133&dq=Are+All+Groups+Finite+?&ots=Kn09LVszvc&sig=JmBJfkjxOcN7AJ5ugoWTuT\_t6W4},
	number = {C},
	journal = {Representation theory of finite groups (Columbus, OH, …},
	author = {Scott, L},
	year = {1997},
	file = {Scott - 1997 - Are All Groups Finite:/home/user/Zotero/storage/ZZ5ACSGJ/Scott - 1997 - Are All Groups Finite.pdf:application/pdf}
}

@article{science-enis-1989,
	title = {enis {IEN}},
	volume = {64},
	number = {86},
	author = {Science, Theoretical Computer and November, Perrin Received},
	year = {1989},
	pages = {271--280},
	file = {Science, November - 1989 - enis IEN:/home/user/Zotero/storage/DFGKEPKZ/Science, November - 1989 - enis IEN.pdf:application/pdf}
}

@article{schutzenberger-finite-1965,
	title = {On {Finite} {Monoids} {Having} {Only} {Trivial} {Subgroups}},
	volume = {8},
	number = {2},
	journal = {Information and Control},
	author = {Schützenberger, Marcel Paul},
	year = {1965},
	pages = {190--194}
}

@inproceedings{schweikardt-addition-invariant-2010,
	title = {Addition-{Invariant} {FO} and {Regularity}},
	booktitle = {Proceedings of the 25th {Annual} {IEEE} {Symposium} on {Logic} in {Computer} {Science}, {LICS} 2010, 11-14 {July} 2010, {Edinburgh}, {United} {Kingdom}},
	publisher = {IEEE Computer Society},
	author = {Schweikardt, Nicole and Segoufin, Luc},
	year = {2010},
	pages = {273--282}
}

@article{schweikardt-expressive-2001,
	title = {On the {Expressive} {Power} of {First}-{Order} {Logic} with {Built}-{In} {Predicates}},
	journal = {Dissertation, Universit{ä}t Mainz},
	author = {Schweikardt, Nicole},
	year = {2001}
}

@article{schoner-x-2010,
	title = {X 1+ {X} = 1 {X}},
	number = {2},
	author = {Schoner, Sebastian},
	year = {2010},
	pages = {15--18},
	file = {Schoner - 2010 - X 1 X = 1 X:/home/user/Zotero/storage/6U4MAW2W/Schoner - 2010 - X 1 X = 1 X.pdf:application/pdf}
}

@article{schnoebelen-fonctions-2013,
	title = {Fonctions régulières de coût {Thomas} {Colcombet}},
	author = {Schnoebelen, D R Philippe and Walukiewicz, D R Igor},
	year = {2013},
	file = {habilitation-colcombet_v1.1:/home/user/Zotero/storage/HZUE7F6N/habilitation-colcombet_v1.1.pdf:application/pdf}
}

@article{schaetzle-analysis-2009,
	title = {Analysis {III}},
	issn = {9783764374792},
	doi = {10.1007/978-3-7643-7480-8},
	author = {{Schaetzle}},
	year = {2009},
	pages = {468--468},
	file = {Schaetzle - 2009 - Analysis III:/home/user/Zotero/storage/F3K8ACXH/Schaetzle - 2009 - Analysis III.pdf:application/pdf}
}

@article{schaefer-complexity-1978,
	title = {The complexity of satisfiability problems},
	url = {http://portal.acm.org/ft\_gateway.cfm?id=804350&type=pdf&coll=DL&dl=GUIDE&CFID=6223213&CFTOKEN=74251195\npapers2://publication/uuid/E60322F0-DEDB-477C-B5C3-30BB9C8556F5},
	doi = {10.1145/800133.804350},
	abstract = {The problem of deciding whether a given propositional formula in conjunctive normal form is satisfiable has been widely studied. I t is known that, when restricted to formulas having only two literals per clause, this problem has an efficient (polynomial-time) solution. But the same problem on formulas having three literals per clause is NP-complete, and hence probably does not have any efficient solution. In this paper, we consider an infinite class of satisfiability problems which contains these two particular problems as special cases, and show that every member of this class is either polynomial-time decidable or NP-complete. The infinite collection of new NP-complete problems so obtained may prove very useful in finding other new NP-complete problems. The classification of the polynomial-time decidable cases yields new problems that are complete in polynomial time and in nondeterministic log space. We also consider an analogous class of problems, involving quantified formulas, which has the property that every member is either polynomial time decidable or complete in polynomial space.},
	journal = {STOC '78: Proceedings of the tenth annual ACM symposium on Theory of computing},
	author = {Schaefer, Thomas},
	year = {1978},
	pages = {216--226},
	file = {Schaefer - 1978 - The complexity of satisfiability problems:/home/user/Zotero/storage/3EBC5F8C/Schaefer - 1978 - The complexity of satisfiability problems.pdf:application/pdf}
}

@article{salehi-varieties-2007,
	title = {Varieties of many-sorted recognizable sets},
	volume = {18},
	number = {3},
	author = {Salehi, Saeed and Steinby, Magnus},
	year = {2007},
	pages = {319--343},
	file = {Salehi, Steinby - 2007 - Varieties of many-sorted recognizable sets:/home/user/Zotero/storage/7KTKBEGV/Salehi, Steinby - 2007 - Varieties of many-sorted recognizable sets.pdf:application/pdf}
}

@inproceedings{sakarovitch-algebraic-1976,
	title = {An {Algebraic} {Framework} for the {Study} of the {Syntactic} {Monoids} {Application} to the {Group} {Languages}},
	booktitle = {{MFCS}},
	author = {Sakarovitch, Jacques},
	year = {1976},
	pages = {510--516}
}

@article{ruzzo-uniform-1981,
	title = {On uniform circuit complexity},
	volume = {22},
	url = {http://www.sciencedirect.com/science/article/pii/0022000081900386},
	doi = {10.1016/0022-0000(81)90038-6},
	abstract = {We argue that uniform circuit complexity introduced by Borodin is a reasonable model of parallel complexity. Three main results are presented. First, we show that alternating Turing machines are also a surprisingly good model of parallel complexity, by showing that simultaneous size/depth of uniform circuits is the same as space/time of alternating Turing machines, with depth and time within a constant factor and likewise log(size) and space. Second, we apply this to characterize NC, the class of polynomial size and polynomial-in-log depth circuits, in terms of tree-size bounded alternating TMs and other models. In particular, this enables us to show that context-free language recognition is in NC. Third, we investigate various definitions of uniform circuit complexity, showing that it is fairly insensitive to the choice of definition.},
	number = {3},
	journal = {Journal of Computer and System Sciences},
	author = {Ruzzo, Walter L.},
	month = jun,
	year = {1981},
	pages = {365--383},
	file = {Ruzzo - 1981 - On uniform circuit complexity:/home/user/Zotero/storage/MUR9MM3F/Ruzzo - 1981 - On uniform circuit complexity.pdf:application/pdf}
}

@article{ruzzo-uniform-1981-1,
	title = {On {Uniform} {Circuit} {Complexity}},
	volume = {22},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Ruzzo, Walter L},
	year = {1981},
	pages = {365--383},
	file = {Ruzzo - 1981 - On uniform circuit complexity:/home/user/Zotero/storage/CZZV7IXD/Ruzzo - 1981 - On uniform circuit complexity.pdf:application/pdf}
}

@article{ruzzo-uniform-1981-2,
	title = {On {Uniform} {Circuit} {Complexity}},
	volume = {22},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Ruzzo, Walter L},
	year = {1981},
	pages = {365--383}
}

@article{ruzzo-uniform-1981-3,
	title = {On {Uniform} {Circuit} {Complexity}},
	volume = {22},
	number = {3},
	journal = {J. Comput. Syst. Sci.},
	author = {Ruzzo, Walter L},
	year = {1981},
	pages = {365--383},
	file = {Ruzzo - 1981 - On uniform circuit complexity:/home/user/Zotero/storage/ZVEFZC5U/Ruzzo - 1981 - On uniform circuit complexity.pdf:application/pdf}
}

@inproceedings{ruhl-counting-1999,
	title = {Counting and {Addition} {Cannot} {Express} {Deterministic} {Transitive} {Closure}},
	booktitle = {{LICS}},
	author = {Ruhl, Matthias},
	year = {1999},
	pages = {326--334}
}

@article{roy-definability-2007,
	title = {Definability of {Languages} by {Generalized} {First}-{Order} {Formulas} over {N}{\textasciicircum}{\mbox{+}}},
	volume = {37},
	number = {2},
	journal = {SIAM J. Comput.},
	author = {Roy, Amitabha and Straubing, Howard},
	year = {2007},
	pages = {502--521}
}

@article{schatzle-pde-2012,
	title = {pde},
	author = {Schätzle, Reiner},
	year = {2012},
	file = {Schätzle - 2012 - pde:/home/user/Zotero/storage/VIIES5UE/Schätzle - 2012 - pde.pdf:application/pdf}
}

@article{schoning-uniform-1982,
	title = {A {UNIform} {APPROACH} {TO} {OBTAIN} {DIAGONAL}},
	volume = {18},
	author = {{Sch{\"o}ning}},
	year = {1982},
	pages = {95--103},
	file = {Sch o ning - 1982 - A UNIform APPROACH TO OBTAIN DIAGONAL:/home/user/Zotero/storage/DE245WH2/Sch o ning - 1982 - A UNIform APPROACH TO OBTAIN DIAGONAL.pdf:application/pdf}
}

@article{salehi-tree-2007,
	title = {Tree algebras and varieties of tree languages},
	volume = {377},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397507000771},
	doi = {10.1016/j.tcs.2007.02.006},
	number = {1-3},
	journal = {Theoretical Computer Science},
	author = {Salehi, Saeed and Steinby, Magnus},
	month = may,
	year = {2007},
	keywords = {tree automata, binary trees, syntactic tree algebras, tree algebras, tree languages, varieties of tree languages},
	pages = {1--24},
	file = {Salehi, Steinby - 2007 - Tree algebras and varieties of tree languages:/home/user/Zotero/storage/9PR4PQAK/Salehi, Steinby - 2007 - Tree algebras and varieties of tree languages.pdf:application/pdf}
}

@phdthesis{salehi-varieties-2005,
	title = {Varieties of {Tree} {Languages}},
	author = {Salehi, Saeed},
	year = {2005},
	file = {Salehi - 2005 - Varieties of Tree Languages:/home/user/Zotero/storage/BK274JDP/Salehi - 2005 - Varieties of Tree Languages.pdf:application/pdf}
}

@inproceedings{ruhl-counting-1999-1,
	title = {Counting and {Addition} {Cannot} {Express} {Deterministic} {Transitive} {Closure}},
	booktitle = {14th {Annual} {IEEE} {Symposium} on {Logic} in {Computer} {Science}, {Trento}, {Italy}, {July} 2-5, 1999},
	publisher = {IEEE Computer Society},
	author = {Ruhl, Matthias},
	year = {1999},
	pages = {326--334}
}

@article{roy-definability-2007-1,
	title = {Definability of {Languages} by {Generalized} {First}-{Order} {Formulas} over {{N}}{\textasciicircum}{\mbox{+}}},
	volume = {37},
	number = {2},
	journal = {SIAM J. Comput.},
	author = {Roy, Amitabha and Straubing, Howard},
	year = {2007},
	pages = {502--521}
}

@inproceedings{roy-definability-2006,
	title = {Definability of {Languages} by {Generalized} {First}-{Order} {Formulas} over {({N},+)}},
	booktitle = {{STACS}},
	author = {Roy, Amitabha and Straubing, Howard},
	year = {2006},
	pages = {489--499}
}

@inproceedings{rovan-mathematical-2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mathematical {Foundations} of {Computer} {Science} 2012 - 37th {International} {Symposium}, {{MFCS}} 2012, {Bratislava}, {Slovakia}, {August} 27-31, 2012. {Proceedings}},
	volume = {7464},
	isbn = {978-3-642-32588-5},
	url = {http://dx.doi.org/10.1007/978-3-642-32589-2},
	doi = {10.1007/978-3-642-32589-2},
	publisher = {Springer},
	editor = {Rovan, Branislav and Sassone, Vladimiro and Widmayer, Peter},
	year = {2012}
}

@inproceedings{rounds-complexity-1973,
	title = {Complexity of {Recognition} in {Intermediate}-{Level} {Languages}},
	url = {http://dx.doi.org/10.1109/SWAT.1973.5},
	doi = {10.1109/SWAT.1973.5},
	booktitle = {14th {Annual} {Symposium} on {Switching} and {Automata} {Theory}, {Iowa} {City}, {Iowa}, {USA}, {October} 15-17, 1973},
	publisher = {{IEEE} Computer Society},
	author = {Rounds, William C},
	year = {1973},
	pages = {145--158}
}

@book{rhodes-q-theory-nodate,
	title = {q-theory},
	isbn = {978-0-387-87820-1},
	author = {{Rhodes} and {Steinberg}},
	file = {[John_Rhodes,_Benjamin_Steinberg]_The_q-theory_of_(BookZZ.org):/home/user/Zotero/storage/SGUWZJWJ/[John_Rhodes,_Benjamin_Steinberg]_The_q-theory_of_(BookZZ.org).pdf:application/pdf}
}

@article{rhodes-kernel-1989,
	title = {The kernel of monoid morphisms},
	volume = {62},
	url = {http://www.sciencedirect.com/science/article/pii/0022404989901370},
	doi = {10.1016/0022-4049(89)90137-0},
	number = {3},
	journal = {Journal of Pure and Applied Algebra},
	author = {Rhodes, John and Tilson, Bret},
	month = dec,
	year = {1989},
	pages = {227--268}
}

@article{rhodes-kernel-1989-1,
	title = {The {Kernel} of {Monoid} {Morphisms}},
	volume = {62},
	journal = {J. Pure Applied Alg.},
	author = {Rhodes, John L and Tilson, Bret},
	year = {1989},
	pages = {27--268}
}

@article{rhodes-decomposition-1989,
	title = {{DECOMPOSITION} {TECHNIQUES} {USING} {CATEGORIES} {I} {FOR} {FINITE} {SEMIGROUPS} , a ,: {ScT} ( or {S} {\textless} {T} ).},
	volume = {62},
	author = {Rhodes, John},
	year = {1989},
	pages = {269--284},
	file = {Rhodes - 1989 - DECOMPOSITION TECHNIQUES USING CATEGORIES I FOR FINITE SEMIGROUPS , a , ScT ( or S T ):/home/user/Zotero/storage/S4D7J684/Rhodes - 1989 - DECOMPOSITION TECHNIQUES USING CATEGORIES I FOR FINITE SEMIGROUPS , a , ScT ( or S T ).pdf:application/pdf}
}

@article{rey-block-nodate,
	title = {block product of categories and tilson's division},
	author = {Rey, Jean-francois},
	file = {Rey - Unknown - block product of categories and tilson's division:/home/user/Zotero/storage/IU6P5W4J/Rey - Unknown - block product of categories and tilson's division.pdf:application/pdf}
}

@article{rougemont-streaming-nodate,
	title = {Streaming {Property} {Testing} of {Visibly} {Pushdown} {Languages} ∗},
	author = {Rougemont, Michel De and Serre, Olivier},
	file = {Rougemont, Serre - Unknown - Streaming Property Testing of Visibly Pushdown Languages ∗:/home/user/Zotero/storage/2EFDWII3/Rougemont, Serre - Unknown - Streaming Property Testing of Visibly Pushdown Languages ∗.pdf:application/pdf}
}

@book{robinson-course-1996,
	title = {A {Course} in the {Theory} of {Groups}},
	publisher = {Springer, New York},
	author = {Robinson, Derek J S},
	year = {1996}
}

@article{rhodes-pointlieks-nodate,
	title = {pointlieks hyperdecidability},
	author = {{Rhodes} and Steinberg, benjamin},
	file = {und:/home/user/Zotero/storage/FNIVIX2H/und.ps:application/postscript}
}

@article{rhodes-kernel-1989-2,
	title = {The kernel of monoid morphisms},
	volume = {62},
	doi = {10.1016/0022-4049(89)90137-0},
	journal = {Journal of Pure and Applied Algebra},
	author = {Rhodes, John and Tilson, Bret},
	year = {1989},
	pages = {227--268},
	file = {Rhodes, Tilson - 1989 - The kernel of monoid morphisms:/home/user/Zotero/storage/IUPJBJQ7/Rhodes, Tilson - 1989 - The kernel of monoid morphisms.pdf:application/pdf}
}

@article{reiterman-birkhoff-1982,
	title = {The {Birkhoff} theorem for finite algebras},
	volume = {14},
	journal = {Algebra Universalis},
	author = {Reiterman, Jan},
	year = {1982},
	pages = {1--10},
	file = {Reiterman - 1982 - The Birkhoff theorem for finite algebras:/home/user/Zotero/storage/ENVXMETT/Reiterman - 1982 - The Birkhoff theorem for finite algebras.pdf:application/pdf}
}

@article{reghizzi-algebraic-2009,
	title = {Algebraic properties of structured context-free languages: old approaches and novel developments},
	url = {http://arxiv.org/abs/0907.2130},
	abstract = {The historical research line on the algebraic properties of structured CF languages initiated by McNaughton's Parenthesis Languages has recently attracted much renewed interest with the Balanced Languages, the Visibly Pushdown Automata languages (VPDA), the Synchronized Languages, and the Height-deterministic ones. Such families preserve to a varying degree the basic algebraic properties of Regular languages: boolean closure, closure under reversal, under concatenation, and Kleene star. We prove that the VPDA family is strictly contained within the Floyd Grammars (FG) family historically known as operator precedence. Languages over the same precedence matrix are known to be closed under boolean operations, and are recognized by a machine whose pop or push operations on the stack are purely determined by terminal letters. We characterize VPDA's as the subclass of FG having a peculiarly structured set of precedence relations, and balanced grammars as a further restricted case. The non-counting invariance property of FG has a direct implication for VPDA too.},
	author = {Reghizzi, Stefano Crespi and Mandrioli, Dino},
	month = jul,
	year = {2009},
	file = {Reghizzi, Mandrioli - 2009 - Algebraic properties of structured context-free languages old approaches and novel developments:/home/user/Zotero/storage/R6KKG27S/Reghizzi, Mandrioli - 2009 - Algebraic properties of structured context-free languages old approaches and novel developments.pdf:application/pdf}
}

@book{reed-methods-nodate-1,
	title = {Methods of {Modern} {Mathematical} {Physics}},
	publisher = {Academic Press},
	author = {Reed, Michael and Simon, Barry}
}

@article{razborov-natural-1997,
	title = {Natural {Proofs}},
	volume = {55},
	url = {http://dx.doi.org/10.1006/jcss.1997.1494},
	doi = {10.1006/jcss.1997.1494},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Razborov, Alexander A and Rudich, Steven},
	year = {1997},
	pages = {24--35}
}

@inproceedings{raoult-caap-1992,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{{CAAP}} '92, 17th {Colloquium} on {Trees} in {Algebra} and {Programming}, {Rennes}, {France}, {February} 26-28, 1992, {Proceedings}},
	volume = {581},
	isbn = {3-540-55251-0},
	publisher = {Springer},
	editor = {Raoult, Jean-Claude},
	year = {1992}
}

@article{ragde-linear-size-1991,
	title = {Linear-{Size} {Constant}-{Depth} {Polylog}-{Treshold} {Circuits}},
	volume = {39},
	number = {3},
	journal = {Inf. Process. Lett.},
	author = {Ragde, Prabhakar and Wigderson, Avi},
	year = {1991},
	pages = {143--146}
}

@incollection{potthoff-first-order-1995,
	title = {first-order logic on finite trees},
	booktitle = {{LNCS} 915},
	author = {Potthoff, Andreas},
	year = {1995},
	pages = {125--139},
	file = {Potthoff - 1995 - first-order logic on finite trees:/home/user/Zotero/storage/2KS3PD3W/Potthoff - 1995 - first-order logic on finite trees.pdf:application/pdf}
}

@article{potthoff-modulo-counting-1994,
	title = {Modulo-{Counting} {Quantifiers} {Over} {Finite} {Trees}},
	volume = {126},
	url = {http://dx.doi.org/10.1016/0304-3975(94)90270-4},
	doi = {10.1016/0304-3975(94)90270-4},
	number = {1},
	journal = {Theor. Comput. Sci.},
	author = {Potthoff, Andreas},
	year = {1994},
	pages = {97--112}
}

@article{potthoff-regular-nodate,
	title = {Regular {Tree} {Languages} {W} i t h o u t {U} n a r y {Symbols} are {Star}-{Free} *},
	number = {Asmics 2},
	author = {Potthoff, Andreas},
	file = {Potthoff - Unknown - Regular Tree Languages W i t h o u t U n a r y Symbols are Star-Free:/home/user/Zotero/storage/KHTZKDAJ/Potthoff - Unknown - Regular Tree Languages W i t h o u t U n a r y Symbols are Star-Free.pdf:application/pdf}
}

@inproceedings{potthoff-first-order-1995-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {First-{Order} {Logic} on {Finite} {Trees}},
	volume = {915},
	isbn = {3-540-59293-8},
	url = {http://dx.doi.org/10.1007/3-540-59293-8\_191},
	doi = {10.1007/3-540-59293-8\_191},
	booktitle = {{TAPSOFT}'95: {Theory} and {Practice} of {Software} {Development}, 6th {International} {Joint} {Conference} {CAAP}/{FASE}, {Aarhus}, {Denmark}, {May} 22-26, 1995, {Proceedings}},
	publisher = {Springer},
	author = {Potthoff, Andreas},
	editor = {Mosses, Peter D and Nielsen, Mogens and Schwartzbach, Michael I},
	year = {1995},
	pages = {125--139},
	file = {Potthoff - 1995 - first-order logic on finite trees:/home/user/Zotero/storage/BUS3DT96/Potthoff - 1995 - first-order logic on finite trees.pdf:application/pdf}
}

@article{regan-gap-languages-2000,
	title = {gap-languages and log-time complexity classes},
	author = {Regan, Kenneth W and {Vollmer}},
	year = {2000},
	pages = {1--16},
	file = {Regan, Vollmer - 2000 - gap-languages and log-time complexity classes:/home/user/Zotero/storage/ABR8J9M4/Regan, Vollmer - 2000 - gap-languages and log-time complexity classes.pdf:application/pdf}
}

@article{razborov-equivalence-1993,
	title = {An {Equivalence} between {Second} {Order} {Bounded} {Domain} {Bounded} {Arithmetic} and {First} {Order} {Bounded} {Arithmetic}},
	journal = {Arithmetic, Proof Theory and Computational Complexity},
	author = {Razborov, Alexander A},
	year = {1993},
	pages = {247--277}
}

@article{razborov-lower-1987,
	title = {Lower bounds for the size of circuits of bounded depth with basis {\vee,\oplus}},
	volume = {41},
	journal = {Math. notes of the Academy of Sciences of the USSR},
	author = {Razborov, A},
	year = {1987},
	pages = {333--338}
}

@phdthesis{prawitz-natural-1965,
	title = {Natural deduction: {A} proof-theoretical study},
	author = {Prawitz, Dag},
	year = {1965}
}

@article{prawitz-note-2012,
	title = {A note on {{G}}entzen's 2nd consistency proof and normalization in 1st order arithmetic},
	author = {Prawitz, Dag},
	year = {2012}
}

@inproceedings{potthoff-regular-1993,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Regular {Tree} {Languages} {Without} {Unary} {Symbols} are {Star}-{Free}},
	volume = {710},
	isbn = {3-540-57163-9},
	url = {http://dx.doi.org/10.1007/3-540-57163-9\_34},
	doi = {10.1007/3-540-57163-9\_34},
	booktitle = {Fundamentals of {Computation} {Theory}, 9th {International} {Symposium}, {{FCT}} '93, {Szeged}, {Hungary}, {August} 23-27, 1993, {Proceedings}},
	publisher = {Springer},
	author = {Potthoff, Andreas and Thomas, Wolfgang},
	editor = {Ésik, Zoltán},
	year = {1993},
	pages = {396--405}
}

@inproceedings{potthoff-modulo-1992,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Modulo {Counting} {Quantifiers} {Over} {Finite} {Trees}},
	volume = {581},
	isbn = {3-540-55251-0},
	url = {http://dx.doi.org/10.1007/3-540-55251-0\_15},
	doi = {10.1007/3-540-55251-0\_15},
	booktitle = {{{CAAP}} '92, 17th {Colloquium} on {Trees} in {Algebra} and {Programming}, {Rennes}, {France}, {February} 26-28, 1992, {Proceedings}},
	publisher = {Springer},
	author = {Potthoff, Andreas},
	editor = {Raoult, Jean-Claude},
	year = {1992},
	pages = {265--278}
}

@article{porst-hu-nodate,
	title = {Hu ’ s {Primal} {Algebra} {Theorem} {Revisited}},
	author = {Porst, Hans E},
	keywords = {s theorem, 4, 5, characterizes the varieties, equivalence between varieties, equivalent, generated by, hu, in birkhoff, lawvere theory, of boolean algebras as, post algebras, primal algebra, s sense, those varieties which are, to the variety bool},
	pages = {1--6},
	file = {Porst - Unknown - Hu ’ s Primal Algebra Theorem Revisited:/home/user/Zotero/storage/J4V96TM2/Porst - Unknown - Hu ’ s Primal Algebra Theorem Revisited.pdf:application/pdf}
}

@article{podolskaya-circuit-2014,
	title = {On {Circuit} {Complexity} of {Parity} and {Majority} {Functions} in {Antichain} {Basis}},
	url = {http://arxiv.org/abs/1410.2456},
	abstract = {We study the circuit complexity of boolean functions in a certain infinite basis. The basis consists of all functions that take value \$1\$ on antichains over the boolean cube. We prove that the circuit complexity of the parity function and the majority function of \$n\$ variables in this basis is \$\lfloor \frac{n+1}{2} \rfloor\$ and \$\left\lfloor \frac{n}{2} \right \rfloor +1\$ respectively. We show that the asymptotic of the maximum complexity of \$n\$-variable boolean functions in this basis equals \$n.\$},
	author = {Podolskaya, Olga},
	month = oct,
	year = {2014},
	pages = {11--11},
	file = {Podolskaya - 2014 - On Circuit Complexity of Parity and Majority Functions in Antichain Basis:/home/user/Zotero/storage/24AETNEP/Podolskaya - 2014 - On Circuit Complexity of Parity and Majority Functions in Antichain Basis.pdf:application/pdf}
}

@article{plotkin-decompositions-nodate,
	title = {{DECOMPOSITIONS} {AND} {COMPLEXITY} {OF} {LINEAR}},
	author = {Plotkin, Boris and Plotkin, Tatjana},
	keywords = {automaton, wreath product, 20m35, 68q70, algebraic model of an, cascade connection, krohn-, mathematics subject classification 2010, rhodes complexity, semigroup automaton, triangular product},
	pages = {1--17},
	file = {1506.06017v1:/home/user/Zotero/storage/7X46PI2J/1506.06017v1.pdf:application/pdf}
}

@article{plotkin-automata-nodate,
	title = {Automata and automata mappings of semigroups},
	author = {Plotkin, B and Plotkin, T},
	keywords = {automaton, 20m35, 68q70, algebraic model of an, mathematics subject classification 2010, semigroup automaton, cascade, connection, serial connection},
	pages = {1--6},
	file = {1506.06004v1:/home/user/Zotero/storage/HQEX3RFH/1506.06004v1.pdf:application/pdf}
}

@article{place-transfer-2010,
	title = {A {Transfer} {Theorem} for the {Separation} {Problem} ∗},
	author = {Place, Thomas and Zeitoun, Marc},
	year = {2010},
	keywords = {semidirect product, acterizations, and phrases separation problem, decidable char-, logics, regular word languages},
	file = {1501.00569v1:/home/user/Zotero/storage/EVEXJTIF/1501.00569v1.pdf:application/pdf}
}

@article{place-going-2014,
	title = {Going higher in the {First}-order {Quantifier} {Alternation} {Hierarchy} on {Words}},
	url = {http://arxiv.org/abs/1404.6832},
	abstract = {We investigate the quantifier alternation hierarchy in first-order logic on finite words. Levels in this hierarchy are defined by counting the number of quantifier alternations in formulas. We prove that one can decide membership of a regular language to the levels \$\mathcal{B}\Sigma\_2\$ (boolean combination of formulas having only 1 alternation) and \$\Sigma\_3\$ (formulas having only 2 alternations beginning with an existential block). Our proof works by considering a deeper problem, called separation, which, once solved for lower levels, allows us to solve membership for higher levels.},
	author = {Place, Thomas and Zeitoun, Marc},
	month = apr,
	year = {2014},
	file = {Place, Zeitoun - 2014 - Going higher in the First-order Quantifier Alternation Hierarchy on Words:/home/user/Zotero/storage/XFQJHPHW/Place, Zeitoun - 2014 - Going higher in the First-order Quantifier Alternation Hierarchy on Words.pdf:application/pdf}
}

@article{place-separating-2014,
	title = {Separating {Regular} {Languages} with {First}-{Order} {Logic}},
	issn = {9781450328869},
	url = {http://arxiv.org/abs/1402.3277},
	doi = {10.1145/2603088.2603098},
	abstract = {Given two languages, a separator is a third language that contains the first one and is disjoint from the second one. We investigate the following decision problem: given two regular input languages of finite words, decide whether there exists a first-order definable separator. We prove that in order to answer this question, sufficient information can be extracted from semigroups recognizing the input languages, using a fixpoint computation. This yields an EXPTIME algorithm for checking first-order separability. Moreover, the correctness proof of this algorithm yields a stronger result, namely a description of a possible separator. Finally, we generalize this technique to answer the same question for regular languages of infinite words.},
	journal = {arXiv:1402.3277 [cs]},
	author = {Place, Thomas and Zeitoun, Marc},
	year = {2014},
	pages = {1--23},
	file = {1402.3277v1:/home/user/Zotero/storage/BGNT3J9M/1402.3277v1.pdf:application/pdf}
}

@inproceedings{place-going-2014-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Going {Higher} in the {First}-{Order} {Quantifier} {Alternation} {Hierarchy} on {Words}},
	volume = {8573},
	isbn = {978-3-662-43950-0},
	url = {http://dx.doi.org/10.1007/978-3-662-43951-7\_29},
	doi = {10.1007/978-3-662-43951-7\_29},
	booktitle = {Automata, {Languages}, and {Programming} - 41st {International} {Colloquium}, {{ICALP}} 2014, {Copenhagen}, {Denmark}, {July} 8-11, 2014, {Proceedings}, {Part} {{II}}},
	publisher = {Springer},
	author = {Place, Thomas and Zeitoun, Marc},
	editor = {Esparza, Javier and Fraigniaud, Pierre and Husfeldt, Thore and Koutsoupias, Elias},
	year = {2014},
	pages = {342--353}
}

@article{place-decidable-2011,
	title = {A decidable characterization of locally testable tree languages},
	volume = {7},
	url = {http://dx.doi.org/10.2168/LMCS-7(4:3)2011},
	doi = {10.2168/LMCS-7(4:3)2011},
	number = {4},
	journal = {Logical Methods in Computer Science},
	author = {Place, Thomas and Segoufin, Luc},
	year = {2011}
}

@inproceedings{place-deciding-2010,
	title = {Deciding {Definability} in {FO}\_2({\textless}\_h, {\textless}\_v) on {Trees}},
	isbn = {978-1-4244-7588-9},
	url = {http://dblp.uni-trier.de/db/conf/lics/lics2010.html#PlaceS10},
	doi = {10.1109/LICS.2010.17},
	booktitle = {2010 25th {Annual} {IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	publisher = {IEEE},
	author = {Place, Thomas and Segoufin, Luc},
	month = jul,
	year = {2010},
	pages = {253--262}
}

@article{place-deciding-2015,
	title = {Deciding {Definability} in {Fo}2},
	url = {http://arxiv.org/abs/1012.1255},
	doi = {10.2168/LMCS-},
	author = {{Place} and {Segoufin}},
	year = {2015},
	keywords = {and phrases, tree languages, tree},
	file = {Place, Segoufin - 2015 - Deciding Definability in Fo2:/home/user/Zotero/storage/H2DA7RUE/Place, Segoufin - 2015 - Deciding Definability in Fo2.pdf:application/pdf}
}

@article{place-separation-2015,
	title = {Separation and the {Successor} {Relation} ∗},
	number = {Stacs},
	author = {Place, Thomas and Zeitoun, Marc},
	year = {2015},
	keywords = {4230, digital object identifier 10, lipics, stacs, 2015, semidirect product, and phrases separation problem, logics, regular word languages, 662, decidable character-, izations},
	pages = {662--675},
	file = {Place, Zeitoun - 2015 - Separation and the Successor Relation ∗:/home/user/Zotero/storage/5B8K63PW/Place, Zeitoun - 2015 - Separation and the Successor Relation ∗.pdf:application/pdf}
}

@article{place-decidable-2011-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A decidable characterization of locally testable tree languages},
	volume = {7},
	issn = {978-3-642-02929-5},
	url = {http://dx.doi.org/10.2168/LMCS-7(4:3)2011},
	doi = {10.2168/LMCS-7(4:3)2011},
	number = {4},
	journal = {Logical Methods in Computer Science},
	author = {Place, Thomas and Segoufin, Luc},
	editor = {Thomas, Wolfgang},
	month = nov,
	year = {2011},
	pages = {1--25},
	file = {Place, Segoufin - 2011 - A decidable characterization of locally testable tree languages:/home/user/Zotero/storage/JP459BJU/Place, Segoufin - 2011 - A decidable characterization of locally testable tree languages.pdf:application/pdf}
}

@inproceedings{place-deciding-2010-1,
	title = {Deciding {Definability} in {{FO}\_2[{\textless}]} (or {XPath}) on {Trees}},
	isbn = {978-0-7695-4114-3},
	url = {http://dx.doi.org/10.1109/LICS.2010.17},
	doi = {10.1109/LICS.2010.17},
	booktitle = {Proceedings of the 25th {Annual} {{IEEE}} {Symposium} on {Logic} in {Computer} {Science}, {{LICS}} 2010, 11-14 {July} 2010, {Edinburgh}, {United} {Kingdom}},
	publisher = {{IEEE} Computer Society},
	author = {Place, Thomas and Segoufin, Luc},
	year = {2010},
	pages = {253--262}
}

@inproceedings{place-decidable-2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Decidable} {Characterization} of {Locally} {Testable} {Tree} {Languages}},
	volume = {5556},
	isbn = {978-3-642-02929-5},
	url = {http://dx.doi.org/10.1007/978-3-642-02930-1\_24},
	doi = {10.1007/978-3-642-02930-1\_24},
	booktitle = {Automata, {Languages} and {Programming}, 36th {Internatilonal} {Collogquium}, {{ICALP}} 2009, {Rhodes}, greece, {July} 5-12, 2009, {Proceedings}, {Part} {{II}}},
	publisher = {Springer},
	author = {Place, Thomas and Segoufin, Luc},
	editor = {Albers, Susanne and Marchetti-Spaccamela, Alberto and Matias, Yossi and Nikoletseas, Sotiris E and Thomas, Wolfgang},
	year = {2009},
	pages = {285--296}
}

@inproceedings{place-deciding-2010-2,
	title = {Deciding {Definability} in {FO}\({}\_{\mbox{2}}\)({\textless}) (or {XPath}) on {Trees}},
	isbn = {978-0-7695-4114-3},
	url = {http://dx.doi.org/10.1109/LICS.2010.17},
	doi = {10.1109/LICS.2010.17},
	booktitle = {Proceedings of the 25th {Annual} {{IEEE}} {Symposium} on {Logic} in {Computer} {Science}, {{LICS}} 2010, 11-14 {July} 2010, {Edinburgh}, {United} {Kingdom}},
	publisher = {{IEEE} Computer Society},
	author = {Place, Thomas and Segoufin, Luc},
	year = {2010},
	pages = {253--262}
}

@article{place-decidable-nodate,
	title = {Decidable {Characterizations} for {Tree} {Logics}},
	author = {Place, Thomas and Segoufin, Jean-Eric},
	file = {Place, Segoufin - Unknown - Decidable Characterizations for Tree Logics:/home/user/Zotero/storage/8F4U4MJ2/Place, Segoufin - Unknown - Decidable Characterizations for Tree Logics.pdf:application/pdf}
}

@inproceedings{place-characterization-2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Characterization of {Logics} over {Ranked} {Tree} {Languages}},
	volume = {5213},
	isbn = {978-3-540-87530-7},
	url = {http://dx.doi.org/10.1007/978-3-540-87531-4\_29},
	doi = {10.1007/978-3-540-87531-4\_29},
	booktitle = {Computer {Science} {Logic}, 22nd {International} {Workshop}, {{CSL}} 2008, 17th {Annual} {Conference} of the {EACSL}, {Bertinoro}, {Italy}, {September} 16-19, 2008. {Proceedings}},
	publisher = {Springer},
	author = {Place, Thomas},
	editor = {Kaminski, Michael and Martini, Simone},
	year = {2008},
	pages = {401--415}
}

@inproceedings{place-characterization-2008-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Characterization of {Logics} over {Ranked} {Tree} {Languages}},
	volume = {5213},
	isbn = {978-3-540-87530-7},
	url = {http://dx.doi.org/10.1007/978-3-540-87531-4\_29},
	doi = {10.1007/978-3-540-87531-4\_29},
	booktitle = {Computer {Science} {Logic}, 22nd {International} {Workshop}, {{CSL}} 2008, 17th {Annual} {Conference} of the {EACSL}, {Bertinoro}, {Italy}, {September} 16-19, 2008. {Proceedings}},
	publisher = {Springer},
	author = {Place, Thomas},
	editor = {Kaminski, Michael and Martini, Simone},
	year = {2008},
	pages = {401--415},
	file = {Place - Unknown - Characterization of Logics Over Ranked Tree Languages:/home/user/Zotero/storage/2K4N2X4M/Place - Unknown - Characterization of Logics Over Ranked Tree Languages.pdf:application/pdf}
}

@article{pippenger-regular-1997,
	title = {Regular {Languages} and {Stone} {Duality}},
	volume = {30},
	url = {http://dx.doi.org/10.1007/s002240000045},
	doi = {10.1007/s002240000045},
	number = {2},
	journal = {Theory Comput. Syst.},
	author = {Pippenger, Nicholas},
	year = {1997},
	pages = {121--134},
	file = {Pippenger - 1997 - Theory of Computing Systems Regular Languages and Stone Duality:/home/user/Zotero/storage/SPK2A3BZ/Pippenger - 1997 - Theory of Computing Systems Regular Languages and Stone Duality.pdf:application/pdf}
}

@article{pippenger-theory-1997,
	title = {Theory of {Computing} {Systems} {Regular} {Languages} and {Stone} {Duality} *},
	volume = {134},
	author = {Pippenger, N},
	year = {1997},
	pages = {121--134},
	file = {Pippenger - 1997 - Theory of Computing Systems Regular Languages and Stone Duality:/home/user/Zotero/storage/HUFNUTD2/Pippenger - 1997 - Theory of Computing Systems Regular Languages and Stone Duality.pdf:application/pdf}
}

@article{pin-polynomial-nodate,
	title = {polynomial closure and unambiguous product},
	author = {{Pin} and Weil, Pascal},
	file = {Pin, Weil - Unknown - polynomial closure and unambiguous product:/home/user/Zotero/storage/Q73A5HEV/Pin, Weil - Unknown - polynomial closure and unambiguous product.pdf:application/pdf}
}

@article{pin-1.-nodate,
	title = {1. {Introduction}},
	author = {Pin, Jean-eric and Weil, Pascal},
	file = {Pin, Weil - Unknown - 1. Introduction:/home/user/Zotero/storage/D62IDDV9/Pin, Weil - Unknown - 1. Introduction.pdf:application/pdf}
}

@article{pin-ponynominal-1997,
	title = {Ponynominal {Closure} and {Unambiguous} {Product}},
	volume = {30},
	number = {4},
	journal = {Theory Comput. Syst.},
	author = {Pin, Jean-Eric and Weil, Pascal},
	year = {1997},
	pages = {383--422}
}

@article{pin-results-2005,
	title = {Some results on {{C}}-varieties},
	volume = {39},
	number = {1},
	journal = {ITA},
	author = {Pin, Jean-Eric and Straubing, Howard},
	year = {2005},
	pages = {239--262}
}

@article{pin-equational-2012,
	title = {Equational {Descriptions} of {Languages}},
	volume = {23},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0129054112400497},
	doi = {10.1142/S0129054112400497},
	journal = {International Journal of Foundations of Computer Science},
	author = {Pin, Jean-Éric},
	year = {2012},
	keywords = {regular languages, s variety theorem, a concise de-, a historical, eilenberg, equations have been used, for a long time, in mathematics to provide, lattices, objects, of languages, profinite equations, scription of various mathematical, stone duality, this article roughly follows, variety of finite semigroups},
	pages = {1227--1240},
	file = {Pin - 2012 - Equational Descriptions of Languages:/home/user/Zotero/storage/H78HDWPG/Pin - 2012 - Equational Descriptions of Languages.pdf:application/pdf}
}

@article{pin-results-2004,
	title = {Some results on {C} -varieties},
	number = {August},
	author = {Pin, Jean-Eric},
	year = {2004},
	pages = {1--23},
	file = {Eric - 2004 - Some results on C -varieties:/home/user/Zotero/storage/C5JD6UQS/Eric - 2004 - Some results on C -varieties.pdf:application/pdf}
}

@book{pin-varieties-1986,
	title = {Varieties {Of} {Formal} {Languages}},
	author = {Pin, Jean-{\´E}ric},
	year = {1986}
}

@phdthesis{piatkowski-n-head-1963,
	title = {N-head finite state machines},
	author = {Piatkowski, Thomas Frank},
	year = {1963}
}

@inproceedings{pin-positive-1998,
	title = {Positive {Varieties} and {Infinite} {Words}},
	booktitle = {{LATIN}},
	author = {Pin, Jean-Eric},
	year = {1998},
	pages = {76--87}
}

@article{pin-variety-1995,
	title = {A variety theorem without complementation},
	volume = {39},
	journal = {Izvestiya VUZ Matematika},
	author = {Pin, Jean-Eric},
	year = {1995},
	pages = {80--90}
}

@article{pin-mathematical-2010,
	title = {Mathematical foundations of automata theory},
	url = {http://www.liafa.univ-paris-diderot.fr/~jep/PDF/MPRI/MPRI.pdf},
	journal = {Lecture notes LIAFA, Université Paris},
	author = {Pin, Jé},
	year = {2010},
	file = {Pin - 2010 - Mathematical foundations of automata theory:/home/user/Zotero/storage/5CWSHCRZ/Pin - 2010 - Mathematical foundations of automata theory.pdf:application/pdf}
}

@article{petkovic-positive-2005,
	title = {Positive varieties of tree languages},
	volume = {347},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397505004627},
	doi = {10.1016/j.tcs.2005.07.026},
	number = {1-2},
	journal = {Theoretical Computer Science},
	author = {Petković, Tatjana and Salehi, Saeed},
	month = nov,
	year = {2005},
	keywords = {tree automata, tree languages, ordered algebras, ordered monoids, variety theorem},
	pages = {1--35},
	file = {Petković, Salehi - 2005 - Positive varieties of tree languages:/home/user/Zotero/storage/4G7SWZ7U/Petković, Salehi - 2005 - Positive varieties of tree languages.pdf:application/pdf}
}

@article{parchmann-closure-1980,
	title = {Closure {Properties} of {Deterministic} {Indexed} {Languages}},
	volume = {46},
	number = {3},
	journal = {Information and Control},
	author = {Parchmann, Rainer and Duske, Jürgen and Specht, Johann},
	year = {1980},
	pages = {128--200}
}

@article{paperman-these-2014,
	title = {Thèse},
	author = {Paperman, Charles},
	year = {2014},
	file = {Paperman - 2014 - Thèse:/home/user/Zotero/storage/X8CWZPPN/Paperman - 2014 - Thèse.pdf:application/pdf}
}

@article{osang-unavoidable-2012,
	title = {Unavoidable trees and forests in graphs},
	author = {Osang, Georg and Rutter, Ignaz},
	year = {2012},
	file = {Osang, Rutter - 2012 - Unavoidable trees and forests in graphs:/home/user/Zotero/storage/57XJKKXR/Osang, Rutter - 2012 - Unavoidable trees and forests in graphs.pdf:application/pdf}
}

@article{nikolov-finitely-2007,
	title = {On finitely generated profinite groups, {I}: {Strong} completeness and uniform bounds},
	volume = {165},
	doi = {10.4007/annals.2007.165.171},
	abstract = {We prove that in every finitely generated profinite group, every subgroup of finite index is open; this implies that the topology on such groups is determined by the algebraic structure. This is deduced from the main result about finite groups: let \$w\$ be a `locally finite' group word and \$d\in\mathbb{N}\$. Then there exists \$f=f(w,d)\$ such that in every \$d\$-generator finite group \$G\$, every element of the verbal subgroup \$w(G)\$ is equal to a product of \$f\$ \$w\$-values.   An analogous theorem is proved for commutators; this implies that in every finitely generated profinite group, each term of the lower central series is closed.   The proofs rely on some properties of the finite simple groups, to be established in Part II.},
	journal = {Annals of Mathematics},
	author = {Nikolov, Nikolay and Segal, Dan},
	year = {2007},
	pages = {171--238},
	file = {Nikolov, Segal - 2007 - On finitely generated profinite groups, I Strong completeness and uniform bounds:/home/user/Zotero/storage/PE2ZAHI2/Nikolov, Segal - 2007 - On finitely generated profinite groups, I Strong completeness and uniform bounds.pdf:application/pdf}
}

@article{nikolov-algebraic-2011,
	title = {Algebraic properties of profinite groups},
	url = {http://arxiv.org/abs/1108.5130},
	abstract = {Recently there has been a lot of research and progress in profinite groups. We survey some of the new results and discuss open problems. A central theme is decompositions of finite groups into bounded products of subsets of various kinds which give rise to algebraic properties of topological groups.},
	author = {Nikolov, Nikolay},
	year = {2011},
	pages = {23--23},
	file = {Nikolov - 2011 - Algebraic properties of profinite groups:/home/user/Zotero/storage/NRAWW6RA/Nikolov - 2011 - Algebraic properties of profinite groups.pdf:application/pdf}
}

@article{perrin-semigroups-1995,
	title = {Semigroups and automata on infinite words},
	doi = {10.1016/j.chiabu.2011.09.001},
	journal = {NATO Advanced Study Institute Semigroups, Formal Languages and Groups},
	author = {Perrin, Dominique and Pin, Jean-Eric},
	year = {1995},
	pages = {49--72},
	file = {YorkInfini:/home/user/Zotero/storage/2PWZMIMZ/YorkInfini.pdf:application/pdf}
}

@article{peladeau-formulas-1992,
	title = {Formulas, {Regular} {Languages} and {Boolean} {Circuits}},
	volume = {101},
	number = {1},
	journal = {Theor. Comput. Sci.},
	author = {Péladeau, Pierre},
	year = {1992},
	pages = {133--141}
}

@article{paperman-finite-degree-2015,
	title = {Finite-{Degree} {Predicates} and {Two}-{Variable} {First}-{Order} {Logic}},
	url = {http://arxiv.org/abs/1507.05175},
	abstract = {We consider two-variable first-order logic on finite words with a fixed number of quantifier alternations. We show that all languages with a neutral letter definable using the order and finite-degree predicates are also definable with the order predicate only. From this result we derive the separation of the alternation hierarchy of two-variable logic on this signature.},
	author = {Paperman, Charles},
	year = {2015},
	pages = {1--19},
	file = {1507.05175v1:/home/user/Zotero/storage/76BPVRBB/1507.05175v1.pdf:application/pdf}
}

@article{oti-tree-2010,
	title = {Tree on top of maltsev ´},
	author = {Oti, M A R},
	year = {2010},
	pages = {1--4},
	file = {Oti - 2010 - Tree on top of maltsev ´:/home/user/Zotero/storage/U32J8PS6/Oti - 2010 - Tree on top of maltsev ´.pdf:application/pdf}
}

@book{nicolaescu-lectures-2007,
	edition = {2},
	title = {Lectures on the {Geometry} of {Manifolds}},
	publisher = {World Scientific},
	author = {Nicolaescu, Liviu I},
	year = {2007},
	file = {Nicolaescu - 2014 - Lectures on the Geometry of Manifolds:/home/user/Zotero/storage/MUW2ENC7/Nicolaescu - 2014 - Lectures on the Geometry of Manifolds.pdf:application/pdf}
}

@article{nicolaescu-notes-2013,
	title = {Notes on the {Atiyah}-{Singer} {Index} {Theorem}},
	author = {Nicolaescu, Liviu I},
	year = {2013},
	file = {Nicolaescu - 2013 - Notes on the Atiyah-Singer Index Theorem:/home/user/Zotero/storage/4UBATDD8/Nicolaescu - 2013 - Notes on the Atiyah-Singer Index Theorem.pdf:application/pdf}
}

@article{nerode-linear-1958,
	title = {Linear automaton transformations},
	volume = {9},
	journal = {Proceedings of the American Mathematical Society},
	author = {Nerode, A.},
	year = {1958},
	pages = {541--544}
}

@article{nemeth-automata-2006,
	title = {Automata on infinite biposets},
	volume = {17},
	journal = {Acta Cybernetica},
	author = {Németh, Zoltán L.},
	year = {2006},
	keywords = {abstract, the, automata and the generalization, basic theory of automata, central part of the, definable by such au-, on infinite trees, properties of tree languages, standard model of nondeterministic, theory considers the closure, this article presents the, to alternating automata, we introduce the},
	pages = {765--797},
	file = {inf-tree-automata:/home/user/Zotero/storage/DZGVZZVV/inf-tree-automata.pdf:application/pdf}
}

@article{moore-circuits-2000,
	title = {Circuits and {Expressions} with {Nonassociative} {Gates}},
	journal = {J. Comput. Syst. Sci.},
	author = {Moore, Cristopher and Thérien, Denis and Lemieux, Francois François and Berman, Joshua and Drisko, Arthur},
	year = {2000},
	pages = {368--394},
	file = {Berman et al. - 1997 - Circuits and Expressions with Non-Associative Gates:/home/user/Zotero/storage/MHUKVABK/Berman et al. - 1997 - Circuits and Expressions with Non-Associative Gates.pdf:application/pdf}
}

@article{bojanczyk-decomposition-2014,
	title = {Decomposition {Theorems} and {Model}-{Checking} for the {Modal} \$\mu\$-{Calculus}},
	issn = {9781450328869},
	url = {http://arxiv.org/abs/1405.2234},
	doi = {10.1145/2603088.2603144},
	abstract = {We prove a general decomposition theorem for the modal \$\mu\$-calculus \$L\_\mu\$ in the spirit of Feferman and Vaught's theorem for disjoint unions. In particular, we show that if a structure (i.e., transition system) is composed of two substructures \$M\_1\$ and \$M\_2\$ plus edges from \$M\_1\$ to \$M\_2\$, then the formulas true at a node in \$M\$ only depend on the formulas true in the respective substructures in a sense made precise below. As a consequence we show that the model-checking problem for \$L\_\mu\$ is fixed-parameter tractable (fpt) on classes of structures of bounded Kelly-width or bounded DAG-width. As far as we are aware, these are the first fpt results for \$L\_\mu\$ which do not follow from embedding into monadic second-order logic.},
	journal = {arXiv:1405.2234 [cs, math]},
	author = {Bojanczyk, Mikolaj and Dittmann, Christoph and Kreutzer, Stephan},
	year = {2014},
	file = {Bojanczyk, Dittmann, Kreutzer - 2014 - Decomposition Theorems and Model-Checking for the Modal \$mu\$-Calculus:/home/user/Zotero/storage/JSQV6H67/Bojanczyk, Dittmann, Kreutzer - 2014 - Decomposition Theorems and Model-Checking for the Modal \$mu\$-Calculus.pdf:application/pdf}
}

@article{blunsom-note-2009,
	title = {A {Note} on the {Implementation} of {Hierarchical} {Dirichlet} {Processes}},
	issn = {9781617382581},
	url = {http://eprints.pascal-network.org/archive/00006746/},
	doi = {10.3115/1667583.1667688},
	abstract = {The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring con- siderable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach.},
	number = {August},
	author = {Blunsom, Phil and Cohn, Trevor and Goldwater, Sharon and Johnson, Mark},
	year = {2009},
	keywords = {Learning/Statistics \& Optimisation, Natural Language Processing},
	pages = {337--340},
	file = {P09-2085:/home/user/Zotero/storage/3B3JG25X/P09-2085.pdf:application/pdf}
}

@article{blundell-weight-2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	volume = {37},
	author = {Blundell, Charles and Com, Wierstra Google},
	year = {2015},
	file = {blundell15:/home/user/Zotero/storage/ZJDPAM7M/blundell15.pdf:application/pdf}
}

@article{bertsekas-dynamic-2010,
	title = {Dynamic {Programming} and {Optimal} {Control} 3rd {Edition} , {Volume} {II} by {Chapter} 6 {Approximate} {Dynamic} {Programming} {Approximate} {Dynamic} {Programming}},
	volume = {II},
	issn = {1886529302},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Dynamic+Programming+and+Optimal+Control+3rd+Edition#9},
	abstract = {This is an updated version of the research-oriented Chapter 6 on Approximate Dynamic Programming. It will be periodically updated as new research becomes available, and will replace the current Chapter 6 in the books next printing.},
	journal = {Control},
	author = {Bertsekas, Dimitri P},
	year = {2010},
	pages = {1--200},
	file = {dpchapter:/home/user/Zotero/storage/QGCT3WV7/dpchapter.pdf:application/pdf}
}

@article{bigham-what-2015,
	title = {What ’ s {Hot} in {Crowdsourcing} and {Human} {Computation}},
	author = {Bigham, Jeffrey P},
	year = {2015},
	keywords = {What's Hot Abstracts Track},
	pages = {4318--4319},
	file = {Bigham - 2015 - What ’ s Hot in Crowdsourcing and Human Computation:/home/user/Zotero/storage/P28J9SA6/Bigham - 2015 - What ’ s Hot in Crowdsourcing and Human Computation.pdf:application/pdf}
}

@article{berbeglia-bargaining-2015,
	title = {A {Bargaining} {Mechanism} for {One}-{Way} {Games}},
	number = {Ijcai},
	author = {Berbeglia, Gerardo and Hentenryck, Pascal Van},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {440--446},
	file = {IJCAI15-068:/home/user/Zotero/storage/WBJIDZCK/IJCAI15-068.pdf:application/pdf}
}

@article{berant-modeling-2014,
	title = {Modeling {Biological} {Processes} for {Reading} {Comprehension}},
	journal = {EMNLP (Best Paper Award)},
	author = {Berant, Jonathan and Clark, Peter},
	year = {2014},
	file = {Berant, Clark - 2014 - Modeling Biological Processes for Reading Comprehension:/home/user/Zotero/storage/7T92D7FX/Berant, Clark - 2014 - Modeling Biological Processes for Reading Comprehension.pdf:application/pdf}
}

@article{beaver-stochastic-2012,
	title = {Stochastic variational inference},
	volume = {14},
	url = {http://arxiv.org/abs/1206.7051},
	abstract = {Abstract: We develop stochastic variational inference , a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and ...},
	journal = {arXiv preprint arXiv:1206.7051},
	author = {{Beaver} and {Clark}},
	year = {2012},
	pages = {1303--1347},
	file = {HoffmanBleiWangPaisley2013:/home/user/Zotero/storage/XRG62T3K/HoffmanBleiWangPaisley2013.pdf:application/pdf}
}

@article{bartok-algorithms-2010,
	title = {Algorithms for {Reinforcement} {Learning} ({Errata})},
	issn = {9781608454921},
	doi = {10.2200/S00268ED1V01Y201005AIM009},
	author = {Bartok, Gabor},
	year = {2010},
	pages = {22--23},
	file = {Bartok - 2010 - Algorithms for Reinforcement Learning (Errata):/home/user/Zotero/storage/HFXZ45K9/Bartok - 2010 - Algorithms for Reinforcement Learning (Errata).pdf:application/pdf}
}

@article{bartlett-reproducing-2008,
	title = {Reproducing {Kernel} {Hilbert} {Spaces} {Reproducing} {Kernel} {Hilbert} {Spaces}},
	author = {Bartlett, Lecturer Peter},
	year = {2008},
	pages = {1--4},
	file = {Bartlett - 2008 - Reproducing Kernel Hilbert Spaces Reproducing Kernel Hilbert Spaces:/home/user/Zotero/storage/NRWBMUK6/Bartlett - 2008 - Reproducing Kernel Hilbert Spaces Reproducing Kernel Hilbert Spaces.pdf:application/pdf}
}

@inproceedings{bengio-unsupervised-2011,
	title = {Unsupervised models of images by spike-and-slab {RBMs}},
	volume = {10},
	isbn = {978-1-4503-0619-5},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Courville\_591.pdf},
	abstract = {... modelling paradigms for unsuper- vised feature learning is the Restricted Boltzmann Ma- chine ... in Proceedings of the 28th International Con- ference on Machine Learning, Bellevue ... velopment of more sophisticated models such as Deep Boltzmann Machines (Salakhutdinov \& ... \n},
	booktitle = {{ICML}},
	author = {Bengio, Yoshua},
	year = {2011},
	pages = {8--8},
	file = {591_icmlpaper:/home/user/Zotero/storage/8BAZT3KZ/591_icmlpaper.pdf:application/pdf}
}

@article{barcz-game-theoretic-2015,
	title = {The {Game}-{Theoretic} {Interaction} {Index} on {Social} {Networks} with {Applications} to {Link} {Prediction} and {Community} {Detection}},
	number = {Ijcai},
	author = {Barcz, Aleksy and Michalak, Tomasz P and Rahwan, Talal and Szczepa, Piotr L},
	year = {2015},
	pages = {638--644},
	file = {IJCAI15-096:/home/user/Zotero/storage/BX6ZNATX/IJCAI15-096.pdf:application/pdf}
}

@article{barber-learning-nodate,
	title = {Learning with {Hidden} {Variables} 1},
	author = {Barber, David},
	keywords = {deep learning, dynamics, statistical models},
	file = {Barber - Unknown - Learning with Hidden Variables 1:/home/user/Zotero/storage/NQWHM3WQ/Barber - Unknown - Learning with Hidden Variables 1.pdf:application/pdf}
}

@article{baral-knowledge-2014,
	title = {Knowledge {Representation} and {Reasoning} : {What} ’ s {Hot}},
	number = {Ovchinnikova},
	author = {Baral, Chitta and Giacomo, Giuseppe De},
	year = {2014},
	keywords = {What's Hot Abstracts Track},
	pages = {4316--4317},
	file = {Baral, Giacomo - 2014 - Knowledge Representation and Reasoning What ’ s Hot:/home/user/Zotero/storage/ZKTA4KHK/Baral, Giacomo - 2014 - Knowledge Representation and Reasoning What ’ s Hot.pdf:application/pdf}
}

@article{barber-bayesian-2011,
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	issn = {9780521518147},
	url = {http://eprints.pascal-network.org/archive/00007920/},
	doi = {10.1017/CBO9780511804779},
	abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
	author = {Barber, David},
	year = {2011},
	keywords = {Learning/Statistics \& Optimisation, Computational, Information-Theoretic Learning with, Theory \& Algorithms},
	file = {Barber - 2011 - Bayesian Reasoning and Machine Learning:/home/user/Zotero/storage/839INWKF/Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf:application/pdf}
}

@article{banerjee-multi-document-2015,
	title = {Multi-{Document} {Abstractive} {Summarization} {Using} {ILP} {Based} {Multi}-{Sentence} {Compression}},
	number = {Ijcai},
	author = {Banerjee, Siddhartha and Mitra, Prasenjit and Sugiyama, Kazunari},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1208--1214},
	file = {IJCAI15-174:/home/user/Zotero/storage/W42K6WKE/IJCAI15-174.pdf:application/pdf}
}

@inproceedings{balcan-learning-2015,
	title = {Learning {Cooperative} {Games}},
	booktitle = {{IJCAI}},
	author = {Balcan, Maria-florina and Procaccia, Ariel D and Zick, Yair},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {475--481},
	file = {IJCAI15-073:/home/user/Zotero/storage/GM8GEBE7/IJCAI15-073.pdf:application/pdf}
}

@article{balasubramanian-isomap-2002,
	title = {The isomap algorithm and topological stability.},
	volume = {295},
	doi = {10.1126/science.295.5552.7a},
	abstract = {10.1126/science.295.5552.7a},
	number = {2002},
	journal = {Science (New York, N.Y.)},
	author = {Balasubramanian, Mukund and Schwartz, Eric L},
	year = {2002},
	pages = {7--7},
	file = {10.1.1.211.5680:/home/user/Zotero/storage/UEHI8JMS/10.1.1.211.5680.pdf:application/pdf;Balasubramanian, Schwartz - 2002 - The isomap algorithm and topological stability:/home/user/Zotero/storage/SAT7AJBB/Balasubramanian, Schwartz - 2002 - The isomap algorithm and topological stability.pdf:application/pdf}
}

@article{bai-aligned-2015,
	title = {An {Aligned} {Subtree} {Kernel} for {Weighted} {Graphs}},
	volume = {37},
	author = {Bai, Lu and Rossi, Luca and Zhang, Zhihong and Hancock, Edwin R},
	year = {2015},
	file = {bai15:/home/user/Zotero/storage/KSZH3UK8/bai15.pdf:application/pdf}
}

@article{babbar-practical-2015,
	title = {Practical on {Kernel} {Methods}},
	author = {Babbar, Rohit and Lopez-paz, David and Muandet, Krikamol},
	year = {2015},
	file = {kernel-practical-mlss2015:/home/user/Zotero/storage/DESFURHU/kernel-practical-mlss2015.pdf:application/pdf}
}

@article{babagholami-mohamadabadi-d-mfvi-nodate,
	title = {D-{MFVI} : {Distributed} {Mean} {Field} {Variational} {Inference} using {Bregman} {ADMM}},
	author = {Babagholami-mohamadabadi, Behnam and Pavlovic, Vladimir},
	pages = {1--19},
	file = {1507.00824v1:/home/user/Zotero/storage/D5HBBTAZ/1507.00824v1.pdf:application/pdf}
}

@inproceedings{aziz-adjusted-2015,
	title = {The {Adjusted} {Winner} {Procedure} : {Characterizations} and {Equilibria}},
	booktitle = {{IJCAI}},
	author = {Aziz, Haris and Filos-ratsikas, Aris},
	year = {2015},
	pages = {454--460},
	file = {IJCAI15-070:/home/user/Zotero/storage/WCPRK4CG/IJCAI15-070.pdf:application/pdf}
}

@article{ayels-generic-nodate,
	title = {{GENERIC} {OBSERVABILITY} {OF} {DIFFERENTIABLE} {SYSTEMS}.pdf},
	author = {{Ayels}},
	file = {Siam J Control Optim 1981 AeyelsGENERIC OBSERVABILITY OF DIFFERENTIABLE SYSTEMS:/home/user/Zotero/storage/QWD7R326/Siam J Control Optim 1981 AeyelsGENERIC OBSERVABILITY OF DIFFERENTIABLE SYSTEMS.pdf:application/pdf}
}

@inproceedings{aziz-welfare-2015,
	title = {Welfare {Maximization} in {Fractional} {Hedonic} {Games}},
	booktitle = {{IJCAI}},
	author = {Aziz, Haris and Gaspers, Serge and Gudmundsson, Joachim},
	year = {2015},
	pages = {461--467},
	file = {IJCAI15-071:/home/user/Zotero/storage/TE3546BS/IJCAI15-071.pdf:application/pdf}
}

@article{aur-evolving-2000,
	title = {Evolving {GPU} {Machine} {Code} .},
	volume = {1},
	author = {Aur, Marco},
	year = {2000},
	pages = {1--22},
	file = {dasilva15a:/home/user/Zotero/storage/33FUKFBC/dasilva15a.pdf:application/pdf}
}

@article{arrigoni-spectral-nodate,
	title = {Spectral {Motion} {Synchronization} in {SE}(3)},
	number = {3},
	author = {Arrigoni, Federica and Fusiello, Andrea and Scienze, Via and Rossi, Beatrice},
	file = {1506.08765v1:/home/user/Zotero/storage/TN9ZARWM/1506.08765v1.pdf:application/pdf}
}

@article{archambeau-incremental-2015,
	title = {Incremental {Variational} {Inference} for {Latent} {Dirichlet} {Allocation}},
	url = {http://arxiv.org/abs/1507.05016},
	abstract = {We introduce incremental variational inference and apply it to latent Dirichlet allocation (LDA). Incremental variational inference is inspired by incremental EM and provides an alternative to stochastic variational inference. Incremental LDA can process massive document collections, does not require to set a learning rate, con- verges faster to a local optimum of the variational bound and enjoys the attractive property of monotonically increasing it. We study the performance of incremental LDA on large bench- mark data sets. We further introduce a stochastic approximation of incremental variational inference which extends to the asynchronous distributed setting. The resulting distributed algorithm achieves comparable performance as single host incremental variational inference, but with a significant speed-up.},
	author = {Archambeau, Cedric and Ermis, Beyza},
	year = {2015},
	file = {1507.05016v1:/home/user/Zotero/storage/9AJA5SR5/1507.05016v1.pdf:application/pdf;Archambeau, Ermis - 2015 - Incremental Variational Inference for Latent Dirichlet Allocation:/home/user/Zotero/storage/D3CCIVPE/Archambeau, Ermis - 2015 - Incremental Variational Inference for Latent Dirichlet Allocation.pdf:application/pdf}
}

@article{arora-provable-2013,
	title = {Provable {Bounds} for {Learning} {Some} {Deep} {Representations}},
	issn = {9781634393973},
	url = {http://arxiv.org/abs/1310.6343},
	abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an \$n\$ node multilayer neural net that has degree at most \$n{\textasciicircum}{\gamma}\$ for some \$\gamma {\textless}1\$ and each edge has a random edge weight in \$[-1,1]\$. Our algorithm learns {\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
	journal = {arXiv preprint arXiv:1310.6343},
	author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
	year = {2013},
	pages = {18--18},
	file = {Arora et al. - 2013 - Provable Bounds for Learning Some Deep Representations:/home/user/Zotero/storage/AWU7ITMU/Arora et al. - 2013 - Provable Bounds for Learning Some Deep Representations.pdf:application/pdf}
}

@article{arne-parsing-nodate,
	title = {Parsing {Morphologically} {Rich} {Languages} with ( {Mostly} ) {Off}-{The}-{Shelf} {Software} and {Word} {Vectors}},
	author = {Arne, K},
	file = {Arne - Unknown - Parsing Morphologically Rich Languages with ( Mostly ) Off-The-Shelf Software and Word Vectors:/home/user/Zotero/storage/PJ6HDAMP/Arne - Unknown - Parsing Morphologically Rich Languages with ( Mostly ) Off-The-Shelf Software and Word Vectors.pdf:application/pdf}
}

@article{archer-bayesian-2013,
	title = {Bayesian {Entropy} {Estimation} for {Countable} {Discrete} {Distributions}},
	volume = {15},
	issn = {1532-4435},
	url = {http://93.93.131.168:8080/xmlui/handle/123456789/1233},
	author = {Archer, Evan and Park, Il Memming and Pillow, J},
	year = {2013},
	keywords = {Information theory, bayesian estimation, bayesian nonparametrics, dirichlet, entropy, neural coding, pitman, process, yor process},
	pages = {1--35},
	file = {Archer, Park, Pillow - 2013 - Bayesian Entropy Estimation for Countable Discrete Distributions:/home/user/Zotero/storage/NNA5RU48/Archer, Park, Pillow - 2013 - Bayesian Entropy Estimation for Countable Discrete Distributions.pdf:application/pdf}
}

@inproceedings{anthony-generating-1997,
	title = {Generating {Numerical} {Literals} {During} {Refinement}},
	booktitle = {Inductive {Logic} {Programming}. {Proceedings} of {ILP} 97},
	publisher = {Springer},
	author = {Anthony, Simon and Frisch, Alan M},
	editor = {Lavrac, Nada and D??eroski, Sa??o},
	year = {1997},
	keywords = {ILP},
	pages = {61--61}
}

@inproceedings{anshelevich-strategic-2015,
	title = {Strategic {Network} {Formation} through an {Intermediary} ∗},
	booktitle = {{IJCAI}},
	author = {Anshelevich, Elliot and Bhardwaj, Onkar and Kar, Koushik and Ny, Troy},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {447--453},
	file = {IJCAI15-069:/home/user/Zotero/storage/GEWRN2RX/IJCAI15-069.pdf:application/pdf}
}

@article{al-shedivat-supervised-2014,
	title = {Supervised {Transfer} {Sparse} {Coding} {S} / {T}},
	author = {Al-shedivat, Maruan and Wang, Jim Jing-yan and Alzahrani, Majed and Huang, Jianhua Z and Gao, Xin},
	year = {2014},
	keywords = {Novel Machine Learning Algorithms},
	pages = {1665--1672},
	file = {Al-shedivat et al. - 2014 - Supervised Transfer Sparse Coding S T:/home/user/Zotero/storage/M5QZHSP2/Al-shedivat et al. - 2014 - Supervised Transfer Sparse Coding S T.pdf:application/pdf}
}

@article{agarwal-lower-2015,
	title = {A {Lower} {Bound} for the {Optimization} of {Finite} {Sums}},
	volume = {37},
	journal = {International Conference on Machine Learning},
	author = {Agarwal, Alekh and Org, Leon Bottou},
	year = {2015},
	file = {agarwal15:/home/user/Zotero/storage/SF7GMGH8/agarwal15.pdf:application/pdf}
}

@article{angeli-philosophers-2013,
	title = {Philosophers are {Mortal}: {Inferring} the {Truth} of {Unseen} {Facts}},
	url = {http://www.aclweb.org/anthology/W13-3515},
	journal = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
	author = {Angeli, Gabor and Manning, Christopher},
	year = {2013},
	pages = {133--142},
	file = {Angeli, Manning - 2013 - Philosophers are Mortal Inferring the Truth of Unseen Facts:/home/user/Zotero/storage/6V2RD446/Angeli, Manning - 2013 - Philosophers are Mortal Inferring the Truth of Unseen Facts.pdf:application/pdf}
}

@article{an-how-2015,
	title = {How {Can} {Deep} {Rectifier} {Networks} {Achieve} {Linear} {Separability} and {Preserve} {Distances} ?},
	volume = {37},
	journal = {Journal of Machine Learning Research},
	author = {An, Senjian and Bennamoun, Mohammed and Bennamoun, Mohammed and Edu, U W a},
	year = {2015},
	file = {an15:/home/user/Zotero/storage/NJPDI7AN/an15.pdf:application/pdf}
}

@article{amari-natural-1998,
	title = {Natural {Gradient} {Works} {Ef} ciently in {Learning}},
	volume = {276},
	journal = {Neural Computation},
	author = {Amari, Shun-ichi and Amari, Shun-ichi},
	year = {1998},
	pages = {251--276},
	file = {Amari1998a:/home/user/Zotero/storage/45MQUV7T/Amari1998a.pdf:application/pdf}
}

@article{ali-active-2014,
	title = {Active {Learning} with {Model} {Selection}},
	issn = {9781577356790},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8547/8793},
	journal = {Aaai},
	author = {Ali, Alnur and Caruana, Rich and Kapoor, Ashish},
	year = {2014},
	keywords = {Novel Machine Learning Algorithms},
	pages = {1673--1679},
	file = {Ali, Caruana, Kapoor - 2014 - Active Learning with Model Selection:/home/user/Zotero/storage/WQHNXWPV/Ali, Caruana, Kapoor - 2014 - Active Learning with Model Selection.pdf:application/pdf}
}

@article{adams-probabilistic-2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	volume = {37},
	author = {Adams, Ryan P},
	year = {2015},
	file = {hernandez-lobatoc15:/home/user/Zotero/storage/Z6KKVIAB/hernandez-lobatoc15.pdf:application/pdf}
}

@article{ostrowski-place-2015,
	title = {Place memory retention in {Drosophila}},
	volume = {123},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1074742715001215},
	doi = {10.1016/j.nlm.2015.06.015},
	journal = {Neurobiology of Learning and Memory},
	author = {Ostrowski, Daniela and Kahsai, Lily and Kramer, Elizabeth F. and Knutson, Patrick and Zars, Troy},
	year = {2015},
	pages = {217--224},
	file = {1-s2.0-S1074742715001215-main:/home/user/Zotero/storage/W33MFEUV/1-s2.0-S1074742715001215-main.pdf:application/pdf}
}

@article{murphy-mapping-2015,
	title = {Mapping {Functional} {Topography} in the {Macaque} {Ventral} {Visual} {Pathway}},
	volume = {35},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2108-15.2015},
	doi = {10.1523/JNEUROSCI.2108-15.2015},
	number = {32},
	journal = {Journal of Neuroscience},
	author = {Murphy, a. P.},
	year = {2015},
	pages = {11171--11173},
	file = {11171.full:/home/user/Zotero/storage/ZEDEATHX/11171.full.pdf:application/pdf}
}

@article{mendoza-drosophila-2014-1,
	title = {Drosophila {FoxP} mutants are deficient in operant self-learning},
	volume = {9},
	doi = {10.1371/journal.pone.0100648},
	abstract = {Intact function of the Forkhead Box P2 (FOXP2) gene is necessary for normal development of speech and language. This important role has recently been extended, first to other forms of vocal learning in animals and then also to other forms of motor learning. The homology in structure and in function among the FoxP gene members raises the possibility that the ancestral FoxP gene may have evolved as a crucial component of the neural circuitry mediating motor learning. Here we report that genetic manipulations of the single Drosophila orthologue, dFoxP, disrupt operant self-learning, a form of motor learning sharing several conceptually analogous features with language acquisition. Structural alterations of the dFoxP locus uncovered the role of dFoxP in operant self-learning and habit formation, as well as the dispensability of dFoxP for operant world-learning, in which no motor learning occurs. These manipulations also led to subtle alterations in the brain anatomy, including a reduced volume of the optic glomeruli. RNAi-mediated interference with dFoxP expression levels copied the behavioral phenotype of the mutant flies, even in the absence of mRNA degradation. Our results provide evidence that motor learning and language acquisition share a common ancestral trait still present in extant invertebrates, manifest in operant self-learning. This 'deep' homology probably traces back to before the split between vertebrate and invertebrate animals.},
	number = {6},
	journal = {PLoS ONE},
	author = {Mendoza, Ezequiel and Colomb, Julien and Rybak, Jürgen and Pflüger, Hans Joachim and Zars, Troy and Scharff, Constance and Brembs, Björn},
	year = {2014},
	file = {foxP-2014:/home/user/Zotero/storage/FKJ42HEE/foxP-2014.pdf:application/pdf}
}

@article{garlapow-quantitative-2015,
	title = {Quantitative genetics of food intake in {Drosophila} melanogaster},
	doi = {10.1371/journal.pone.0138129},
	abstract = {Food intake is an essential animal activity, regulated by signaling pathways, nutrient perception, taste perception, and other processes. To identify factors associated with food intake and its variation in a model system, we performed genome wide association studies (GWAS) in the Drosophila melanogaster Genetic Reference Panel (DGRP). The DGRP population consists of 205 wild-derived, inbred, fully sequenced D. melanogaster lines, enabling GWAS where all variants are known. We assessed food intake in 182 DGRP lines, finding highly significant genetic variation among the lines, sexual dimorphism in feeding behavior, and genetic variation in sexual dimorphism. The top hits were significantly enriched for components of the Epidermal Growth Factor signaling (EGFs) pathway. The role of EGFs in invertebrate food intake remains largely unexplored. Next we computed the coefficient of environmental variation (CVE) of food intake within each DGRP line. CVE quantifies the variability of food intake between individuals in genetically uniform lines. We performed a GWAS of CVE and found significant genetic variation and a distinct genetic basis from average food intake, indicating genetic control of variance in food intake. Individuals within some DGRP lines consistently consume similar amounts of food, while other lines’ individuals consume widely varying volumes of food. Validation experiments using RNAi mutants have enabled us to confirm genes affecting food intake and CVE of food intake that previously had not been described in D. melanogaster while DGRP-based SNP confirmations have allowed us to confirm individual SNPs predicted by the GWAS. The DGRP allows us to explore the fundamental genetic underpinnings of food intake and its CVE in a genetically tractable population.},
	journal = {56th Annual Drosophila Research Conference},
	author = {Garlapow, M and Mackay, Trudy F C},
	year = {2015},
	pages = {1--25},
	file = {journal.pone.0138129:/home/user/Zotero/storage/IPJC7X67/journal.pone.0138129.pdf:application/pdf}
}

@article{brembs-spontaneous-2011-1,
	title = {Spontaneous decisions and operant conditioning in fruit flies},
	volume = {87},
	issn = {1872-8308 (Electronic)\n0376-6357 (Linking)},
	doi = {10.1016/j.beproc.2011.02.005},
	abstract = {Already in the 1930s Skinner, Konorskiand colleagues debated the commonalities, differences and interactions among the processes underlying what was then known as "conditioned reflexes type I and II", but which is today more well-known as classical (Pavlovian) and operant (instrumental) conditioning. Subsequent decades of research have confirmed that the interactions between the various learning systems engaged during operant conditioning are complex and difficult to disentangle. Today, modern neurobiological tools allow us to dissect the biological processes underlying operant conditioning and study their interactions. These processes include initiating spontaneous behavioral variability, world-learning and self-learning. The data suggest that behavioral variability is generated actively by the brain, rather than as a by-product of a complex, noisy input-output system. The function of this variability, in part, is to detect how the environment responds to such actions. World-learning denotes the biological process by which value is assigned to environmental stimuli. Self-learning is the biological process which assigns value to a specific action or movement. In an operant learning situation using visual stimuli for flies, world-learning inhibits self-learning via a prominent neuropil region, the mushroom-bodies. Only extended training can overcome this inhibition and lead to habit formation by engaging the self-learning mechanism. Self-learning transforms spontaneous, flexible actions into stereotyped, habitual responses. © 2011 Elsevier B.V.},
	journal = {Behavioural Processes},
	author = {Brembs, Björn},
	year = {2011},
	keywords = {Learning, Memory, Drosophila, Insect, Multiple memory systems, Operant, Self-learning, World-learning},
	pages = {157--164},
	file = {bproc_2011:/home/user/Zotero/storage/QHSDJG7A/bproc_2011.pdf:application/pdf}
}

@book{xiang-semantic-nodate,
	title = {semantic and pragmatic processes in the comprehension},
	isbn = {7-7383-4092-4},
	author = {Xiang, Ming and Grove, Julian and Giannakidou, Anastasia},
	file = {Xiang-GroveGiannakidou2015:/home/user/Zotero/storage/345TJ59K/Xiang-GroveGiannakidou2015.pdf:application/pdf}
}

@article{politzer-ahles-involvement-2015,
	title = {Involvement of prefrontal cortex in scalar implicatures: evidence from magnetoencephalography},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1027235},
	doi = {10.1080/23273798.2015.1027235},
	number = {July},
	journal = {Language, Cognition and Neuroscience},
	author = {Politzer-Ahles, Stephen and Gwilliams, Laura},
	year = {2015},
	keywords = {scalar implicature, 1, quantifiers, Pragmatics, magnetoencephalography, 1a, but in many contexts, comprehending a natural utterance, for exam, generally involves infer-, is, not explicitly expressed, ple, prefrontal cortex, ring messages that were, the literal meaning of},
	pages = {1--14}
}

@article{xiang-reversing-2014,
	title = {Reversing expectations during discourse comprehension},
	doi = {10.1080/23273798.2014.995679},
	author = {Xiang, Ming and Kuperberg, Gina},
	year = {2014},
	keywords = {erp, Discourse processing, N400, concessive connectives, event structures, late negativity, p600, prediction},
	file = {Xiang&Kuperberg_LCN_2014:/home/user/Zotero/storage/322XBEC6/Xiang&Kuperberg_LCN_2014.pdf:application/pdf}
}

@article{parr-semantic-2012,
	title = {Semantic bootstrapping and the role of meta-cognition},
	url = {http://discovery.ucl.ac.uk/1337008/},
	author = {Parr, N and Breheny, Ret},
	year = {2012},
	file = {BUCLD 2011:/home/user/Zotero/storage/DZQ8XVSB/BUCLD 2011.pdf:application/pdf}
}

@article{noveck-when-2001,
	title = {When children are more logical than adults: {Experimental} investigations of scalar implicature},
	volume = {78},
	issn = {3343791121},
	doi = {10.1016/S0010-0277(00)00114-1},
	abstract = {A conversational implicature is an inference that consists of attributing to a speaker an implicit meaning that goes beyond the explicit linguistic meaning of an utterance. This paper experimentally investigates scalar implicature, a paradigmatic case of implicature in which a speaker's use of a term like Some indicates that the speaker had reasons not to use a more informative term from the same scale, e.g. All; thus, Some implicates Not all. Pragmatic theorists like Grice would predict that a pragmatic interpretation is determined only after its explicit, logical meaning is incorporated (e.g. where Some means at least one). The present work aims to developmentally examine this prediction by showing how younger, albeit competent, reasoners initially treat a relatively weak term logically before becoming aware of its pragmatic potential. Three experiments are presented. Experiment 1 presents a modal reasoning scenario offering an exhaustive set of conclusions; critical among these is participants' evaluation of a statement expressing Might be x when the context indicates that the stronger Must be x is true. The conversationally-infelicitous Might be x can be understood logically (e.g. as compatible with Must) or pragmatically (as exclusive to Must). Results from 5-, 7-, and 9-year-olds as well as adults revealed that (a) 7-year-olds are the youngest to demonstrate modal competence overall and that (b) 7- and 9-year-olds treat the infelicitous Might logically significantly more often than adults do. Experiment 2 showed how training with the modal task can suspend the implicatures for adults. Experiment 3 provides converging evidence of the developmental pragmatic effect with the French existential quantifier Certains (Some). While linguistically-sophisticated children (8- and 10-year-olds olds) typically treat Certains as compatible with Tous (All), adults are equivocal. These results, which are consistent with unanticipated findings in classic developmental papers, reveal a consistent ordering in which representations of weak scalar terms tend to be treated logically by young competent participants and more pragmatically by older ones. This work is also relevant to the treatment of scalar implicatures in the reasoning literature. Copyright (C) 2001 Elsevier Science B.V.},
	journal = {Cognition},
	author = {Noveck, Ira a.},
	year = {2001},
	keywords = {Reasoning, Pragmatics, implicature, Development},
	pages = {165--188},
	file = {Children_are_more_logical:/home/user/Zotero/storage/JCV6VHSD/Children_are_more_logical.pdf:application/pdf}
}

@article{katsos-interaction-2005,
	title = {The {Interaction} of {Structural} and {Contextual} {Constraints} {During} the {On}-line {Generation} of {Scalar} {Inferences}},
	url = {http://discovery.ucl.ac.uk/136032/},
	author = {Katsos, N and Breheny, R and Williams, J},
	year = {2005},
	keywords = {psycholinguistics, Pragmatics, implicatures},
	file = {CogSci Katsos et al final:/home/user/Zotero/storage/JVRJA33V/CogSci Katsos et al final.pdf:application/pdf}
}

@article{kao-all-2014,
	title = {all not all},
	author = {Kao, Justine T and Degen, Judith and Goodman, Noah D},
	year = {2014},
	pages = {1--2},
	file = {kao-xprag2015-final:/home/user/Zotero/storage/AATTP2P6/kao-xprag2015-final.pdf:application/pdf}
}

@article{cummins-granularity-2012,
	title = {Granularity and scalar implicature in numerical expressions},
	volume = {35},
	doi = {10.1007/s10988-012-9114-0},
	abstract = {It has been generally assumed that certain categories of numerical expressions, such as ‘more than n ’, ‘at least n ’, and ‘fewer than n ’, systematically fail to give rise to scalar implicatures in unembedded declarative contexts. Various proposals have been developed to explain this perceived absence. In this paper, we consider the relevance of scale granularity to scalar implicature, and make two novel predictions: first, that scalar implicatures are in fact available from these numerical expressions at the appropriate granularity level, and second, that these implicatures are attenuated if the numeral has been previously mentioned or is otherwise salient in the context. We present novel experimental data in support of both of these predictions, and discuss the implications of this for recent accounts of numerical quantifier usage.},
	journal = {Linguistics and Philosophy},
	author = {Cummins, Chris and Sauerland, Uli and Solt, Stephanie},
	year = {2012},
	keywords = {quantifiers, Pragmatics, implicature, relevance, salience, numerals, Constraints, Granularity},
	pages = {135--169}
}

@article{hochstein-ignorance-nodate-1,
	title = {ignorance and inference gricean children scalar implicature},
	journal = {under review},
	author = {{Hochstein} and {Bale} and {Fox} and {Barner}},
	file = {Hochstein, Bale, Fox, & Barner (under review):/home/user/Zotero/storage/7P2A9VRK/Hochstein, Bale, Fox, & Barner (under review).pdf:application/pdf}
}

@article{degen-availability-2015,
	title = {Availability of {Alternatives} and the {Processing} of {Scalar} {Implicatures}: {A} {Visual} {World} {Eye}-{Tracking} {Study}},
	url = {http://doi.wiley.com/10.1111/cogs.12227},
	doi = {10.1111/cogs.12227},
	journal = {Cognitive Science},
	author = {Degen, Judith and Tanenhaus, Michael K.},
	year = {2015},
	keywords = {scalar implicature, eye-tracking, quantifiers, Pragmatics, alternatives},
	pages = {n/a--n/a},
	file = {DegenTanenhaus2015:/home/user/Zotero/storage/F3JMS9HA/DegenTanenhaus2015.pdf:application/pdf}
}

@article{degen-processing-nodate,
	title = {Processing scalar implicature: a {Constraint}-{Based} approach},
	abstract = {2 Abstract Three experiments investigated the processing of the implicature associated with some using a " gumball paradigm " . On each trial participants saw an image of a gumball machine with an upper chamber with 13 gumballs and an empty lower chamber. Gumballs then dropped to the lower chamber and participants evaluated statements, such as " You got X of the gumballs " . Experiment 1 established that some is less natural for reference to small sets (1, 2 and 3 of the 13 gumballs) and unpartitioned sets (all 13 gumballs) compared to intermediate sets (6-8). Partitive some of was less natural than simple some when used with the unpartitioned set. In Experiment 2, including exact number descriptions lowered naturalness ratings for some with small sets but not for intermediate size sets and the unpartitioned set. In Experiment 3 the naturalness ratings from Experiment 2 predicted response times. The results are interpreted as evidence for a Constraint-Based account of scalar implicature processing and against both two-stage, Literal-First models and pragmatic Default models.},
	author = {Degen, Judith and Tanenhaus, Michael K},
	file = {DegenTanenhaus_resubmitted_a:/home/user/Zotero/storage/2ZZRPQVK/DegenTanenhaus_resubmitted_a.pdf:application/pdf}
}

@article{cummins-no-2012,
	title = {No {Title}},
	author = {Cummins, Chris},
	year = {2012},
	file = {Cummins_XPrag_Fractions:/home/user/Zotero/storage/SAK8P3F8/Cummins_XPrag_Fractions.pdf:application/pdf}
}

@article{cummins-granularity-2012-1,
	title = {Granularity and scalar implicature in numerical expressions},
	volume = {35},
	doi = {10.1007/s10988-012-9114-0},
	abstract = {It has been generally assumed that certain categories of numerical expressions, such as ‘more than n ’, ‘at least n ’, and ‘fewer than n ’, systematically fail to give rise to scalar implicatures in unembedded declarative contexts. Various proposals have been developed to explain this perceived absence. In this paper, we consider the relevance of scale granularity to scalar implicature, and make two novel predictions: first, that scalar implicatures are in fact available from these numerical expressions at the appropriate granularity level, and second, that these implicatures are attenuated if the numeral has been previously mentioned or is otherwise salient in the context. We present novel experimental data in support of both of these predictions, and discuss the implications of this for recent accounts of numerical quantifier usage.},
	journal = {Linguistics and Philosophy},
	author = {Cummins, Chris and Sauerland, Uli and Solt, Stephanie},
	year = {2012},
	keywords = {quantifiers, Pragmatics, implicature, relevance, salience, numerals, Constraints, Granularity},
	pages = {135--169}
}

@article{cummins-comparative-2010,
	title = {Comparative and superlative quantifiers: {Pragmatic} effects of comparison type},
	volume = {27},
	doi = {10.1093/jos/ffq006},
	abstract = {It has historically been assumed that comparative ('more than', 'fewer/less than') and superlative ('at most', 'at least') quantifiers can be semantically analysed in accordance with their core logical-mathematical properties. However, recent theoretical and experimental work has cast doubt on the validity of this assumption. Geurts \& Nouwen (2007) have claimed that superlative quantifiers possess an additional modal component in their semantics that is absent from comparative quantifiers and that this accounts for the previously neglected differences in usage and interpretation between the two types of quantifier that they identify. Their semantically modal hypothesis has received additional support from empirical investigations. In this article, we further corroborate that superlative quantifiers have additional modal interpretations. However, we propose an alternative analysis, whereby these quantifiers possess the semantics postulated by the classical model and the additional aspects of meaning arise as a consequence of psychological complexity and pragmatic implicature. We explain how this model is consistent with the existing empirical findings. Additionally, we present the findings of four novel experiments that support our model above the semantically modal account.},
	journal = {Journal of Semantics},
	author = {Cummins, Chris and Katsos, Napoleon},
	year = {2010},
	pages = {271--305},
	file = {Comparative_and_superlative_quantifiers:/home/user/Zotero/storage/UZE8N3I2/Comparative_and_superlative_quantifiers.pdf:application/pdf}
}

@article{cummins-computational-2014,
	title = {Computational {Approaches} to the {Pragmatics} {Problem}},
	volume = {8},
	doi = {10.1111/lnc3.12072},
	journal = {Linguistics and Language Compass},
	author = {Cummins, Chris and de Ruiter, Jan P.},
	year = {2014},
	pages = {133--143},
	file = {Cummins_De_Ruiter_LLC:/home/user/Zotero/storage/MAREZW5N/Cummins_De_Ruiter_LLC.pdf:application/pdf}
}

@article{chemla-experimental-2011,
	title = {Experimental evidence for embedded scalar implicatures},
	volume = {28},
	doi = {10.1093/jos/ffq023},
	abstract = {Scalar implicatures are traditionally viewed as pragmatic inferences which result from a reasoning about speakers’ communicative intentions (Grice 1989). This view has been challenged in recent years by theories which propose that scalar implica- tures are a grammatical phenomenon. Such theories claim that scalar implicatures can be computed in embedded positions and enter into the recursive computation of meaning—something that is not expected under the traditional, pragmatic view. Recently, Geurts and Pouscoulous (2009) presented an experimental study in which embedded scalar implicatures were not detected. Using a novel version of the truth value judgment tasks, we provide evidence that subjects sometimes compute embed- ded scalar implicatures.},
	number = {1},
	journal = {Journal of Semantics},
	author = {Chemla, Emmanuel and Spector, Benjamin},
	year = {2011},
	keywords = {Pragmatics, Scalar implicatures, experiment, globalism, localism},
	pages = {359--400},
	file = {Chemla-Spector-eSI:/home/user/Zotero/storage/VK5PQ6JP/Chemla-Spector-eSI.pdf:application/pdf}
}

@article{breheny-lexical-2003,
	title = {A {Lexical} {Account} of {Implicit} ({Bound}) {Contextual} {Dependence}},
	journal = {Proceedings of {SALT XIII}},
	author = {Breheny, Richard},
	year = {2003},
	pages = {55--72},
	file = {breheny-salt13:/home/user/Zotero/storage/SWBGRMNZ/breheny-salt13.pdf:application/pdf}
}

@article{breheny-pragmatic-2002,
	title = {{PRAGMATIC} {ANALYSES} {OF} {ANAPHORIC} {PRONOUNS}: {DO} {THINGS} {LOOK} {BETTER} {IN} 2-{D}? * {Richard} {Breheny} {RCEAL}, {University} of {Cambridge}},
	number = {2001},
	author = {Breheny, Richard},
	year = {2002},
	pages = {1--14},
	file = {anaphors in 2D:/home/user/Zotero/storage/GQ8EJW92/anaphors in 2D.pdf:application/pdf}
}

@article{breheny-experimental-2011,
	title = {Experimental pragmatics},
	issn = {1403903506},
	doi = {10.1057/9780230524125},
	abstract = {From the Publisher Palgrave Studies in Pragmatics, Language and Cognition is a new series of high quality research monographs and edited collections of essays focusing on the human pragmatic capacity and its interaction with natural language semantics and other faculties of mind. About the Author Ira A. Noveck is a full-time research scientist at the Institut des Sciences Cognitives in Lyon, France. Dan Sperber is a French social and cognitive scientist at the Centre National de la Recherche Scientifique (CNRS) in Paris.},
	journal = {Handbook of Pragmatics: Volume 1 Foundations of Pragmatics},
	author = {Breheny, Richard},
	year = {2011},
	pages = {561--585},
	file = {experimental prags:/home/user/Zotero/storage/H599R5UX/experimental prags.pdf:application/pdf}
}

@article{breheny-are-2006,
	title = {Are generalised scalar implicatures generated by default? {An} on-line investigation into the role of context in generating pragmatic inferences},
	volume = {100},
	issn = {0010-0277},
	doi = {10.1016/j.cognition.2005.07.003},
	abstract = {Recent research in semantics and pragmatics has revived the debate about whether there are two cognitively distinct categories of conversational implicatures: generalised and particularised. Generalised conversational implicatures are so-called because they seem to arise more or less independently of contextual support. Particularised implicatures are more context-bound. The Default view is that generalised implicatures are default inferences and that their computation is relatively autonomous - being computed by some default mechanism and only being open to cancellation at a second stage when contextual assumptions are taken into consideration (Chierchia, 2004; Horn, 1984; Levinson, 2000 i.a.). It is at that second stage where contextual assumptions are considered that particularised implications are computed. By contrast, Context-Driven theorists claim that both generalised and particularised implicatures are generated by the same process and only where there is contextual support (Carston, 1998; Sperber \& Wilson, 1986 i.a.). In this paper, we present three on-line studies of the prototypical cases of generalised implicatures: the scalar implicatures 'some of the Fs'{\textgreater}'not all the Fs' and 'X or Y'{\textgreater}'either X or Y but not both'. These studies were designed to test the context-dependence and autonomy of the implicatures. Our results suggest that these scalar implicatures are dependent on the conversational context and that they show none of the autonomy predicted by the Default view. We conclude with a discussion of the degree to which such implicatures are purely context-driven and whether an interactionist default position may also be plausible. ?? 2005 Elsevier B.V. All rights reserved.},
	journal = {Cognition},
	author = {Breheny, Richard and Katsos, Napoleon and Williams, John},
	year = {2006},
	keywords = {scalar implicature, Language processing, Pragmatics, Conversational implicature, Defaultness in linguistic processing},
	pages = {434--463},
	file = {BKW Are SIs generated by default:/home/user/Zotero/storage/BZEHDXS5/BKW Are SIs generated by default.pdf:application/pdf}
}

@article{breheny-are-2006-1,
	title = {Are generalised scalar implicatures generated by default? {An} on-line investigation into the role of context in generating pragmatic inferences},
	volume = {100},
	issn = {0010-0277},
	doi = {10.1016/j.cognition.2005.07.003},
	abstract = {Recent research in semantics and pragmatics has revived the debate about whether there are two cognitively distinct categories of conversational implicatures: generalised and particularised. Generalised conversational implicatures are so-called because they seem to arise more or less independently of contextual support. Particularised implicatures are more context-bound. The Default view is that generalised implicatures are default inferences and that their computation is relatively autonomous - being computed by some default mechanism and only being open to cancellation at a second stage when contextual assumptions are taken into consideration (Chierchia, 2004; Horn, 1984; Levinson, 2000 i.a.). It is at that second stage where contextual assumptions are considered that particularised implications are computed. By contrast, Context-Driven theorists claim that both generalised and particularised implicatures are generated by the same process and only where there is contextual support (Carston, 1998; Sperber \& Wilson, 1986 i.a.). In this paper, we present three on-line studies of the prototypical cases of generalised implicatures: the scalar implicatures 'some of the Fs'{\textgreater}'not all the Fs' and 'X or Y'{\textgreater}'either X or Y but not both'. These studies were designed to test the context-dependence and autonomy of the implicatures. Our results suggest that these scalar implicatures are dependent on the conversational context and that they show none of the autonomy predicted by the Default view. We conclude with a discussion of the degree to which such implicatures are purely context-driven and whether an interactionist default position may also be plausible. ?? 2005 Elsevier B.V. All rights reserved.},
	journal = {Cognition},
	author = {Breheny, Richard and Katsos, Napoleon and Williams, John},
	year = {2006},
	keywords = {scalar implicature, Language processing, Pragmatics, Conversational implicature, Defaultness in linguistic processing},
	pages = {434--463},
	file = {BKW Are SIs generated by default:/home/user/Zotero/storage/CI34ASZ2/BKW Are SIs generated by default.pdf:application/pdf}
}

@article{breheny-commentary-2005,
	title = {Commentary on 'exaption and linguistic explanation'},
	volume = {115},
	issn = {0024-3841},
	doi = {10.1016/j.lingua.2004.12.001},
	abstract = {Bouchard (this volume) advances criticisms of the research program and practice of current generative grammarians working within the Minimalist Program. We argue that his claim that the research program is essentially dualist is misconceived, and that his claim that current theories involve just lists, and hence just descriptions of the phenomena, is incorrect. ?? 2005 Elsevier B.V. All rights reserved.},
	journal = {Lingua},
	author = {Breheny, Richard and Adger, David},
	year = {2005},
	keywords = {Interfaces, Minimalism, Narrow syntax},
	pages = {1673--1677},
	file = {comments on Bouchard:/home/user/Zotero/storage/HW9KX94Q/comments on Bouchard.pdf:application/pdf}
}

@article{breheny-communication-2006,
	title = {Communication and folk psychology},
	volume = {21},
	doi = {10.1111/j.1468-0017.2006.00307.x},
	abstract = {Prominent accounts of language use (those of Grice, Lewis, Stalnaker, Sperber \& Wilson among others) have viewed basic communicative acts as essentially involving the attitudes of the participating agents. Developmental data poses a dilemma for these accounts, since it suggests children below age four are competent communicators but would lack the ability to conceptualise communication if philosophers and linguists are right about what communication is. This paper argues that this dilemma is quite serious and that these prominent accounts would be undermined if an adequate more minimal alternative were available. Just such a minimalist account of communication is offered, drawing on ideas from relevance theory and situation theory.},
	number = {1},
	journal = {Mind and Language},
	author = {Breheny, Richard},
	year = {2006},
	pages = {74--107},
	file = {comm & folk psy:/home/user/Zotero/storage/PUAUD876/comm & folk psy.pdf:application/pdf}
}

@article{besserman-anticipatory-2013,
	title = {Anticipatory processes in language comprehension: {The} {English} existential as an indicator of newness},
	issn = {9780511812309},
	doi = {10.1002/wcs.1234.Kaiser},
	journal = {Xprag 2015},
	author = {Besserman, Ana and Love, Tracy and Shapiro, Lew},
	year = {2013},
	file = {bessermanetal:/home/user/Zotero/storage/2V9EC5U7/bessermanetal.pdf:application/pdf}
}

@article{noauthor-[jorge-almeida]-finite-semigroups-and-universal-albookzz-nodate,
	title = {[{Jorge}\_Almeida]\_Finite\_Semigroups\_and\_Universal\_Al({BookZZ}},
	file = {[Jorge_Almeida]_Finite_Semigroups_and_Universal_Al(BookZZ.org):/home/user/Zotero/storage/JG987HSX/[Jorge_Almeida]_Finite_Semigroups_and_Universal_Al(BookZZ.org).djvu:image/vnd.djvu}
}

@article{noauthor-minimal-2007,
	title = {Minimal clones ´},
	year = {2007},
	file = {Unknown - 2007 - Minimal clones ´:/home/user/Zotero/storage/X2GIQH8M/Unknown - 2007 - Minimal clones ´.pdf:application/pdf}
}

@article{noauthor-wreath-nodate,
	title = {{WREATH} {PRODUCTS} {OF} {FOREST} {ALGEBRAS}, {WITH} {APPLICATIONS} {TO} {TREE} {LOGICS}},
	file = {Unknown - Unknown - WREATH PRODUCTS OF FOREST ALGEBRAS, WITH APPLICATIONS TO TREE LOGICS:/home/user/Zotero/storage/Q7KBUSKT/Unknown - Unknown - WREATH PRODUCTS OF FOREST ALGEBRAS, WITH APPLICATIONS TO TREE LOGICS.pdf:application/pdf}
}

@article{noauthor-circuit-nodate,
	title = {circuit lower bounds via ehrenfeucht-fraisse},
	file = {Unknown - Unknown - circuit lower bounds via ehrenfeucht-fraisse:/home/user/Zotero/storage/VIFSGV75/Unknown - Unknown - circuit lower bounds via ehrenfeucht-fraisse.ps:application/postscript}
}

@inproceedings{noauthor-proceedings-1987,
	title = {Proceedings of the {Nineteenth} {Annual} {ACM} {Symposium} on {Theory} of {Computing}, 25-27 {May} 1987, {New} {York} {City}, {NY}, {USA}},
	booktitle = {{STOC}},
	publisher = {ACM},
	year = {1987}
}

@article{noauthor-computational-nodate-1,
	title = {Computational {Complexity} of {Propositional} {Dynamic} {Logics} {Summary} of {Dissertation}},
	pages = {2--5},
	file = {abstract.goeller:/home/user/Zotero/storage/3AHM9NWU/abstract.goeller.pdf:application/pdf}
}

@article{noauthor-institut-1995,
	title = {{INSTITUT} {F} {UR} {INFORMATIK} {Report} on the {Program} {AMoRE}},
	number = {9507},
	year = {1995},
	file = {Unknown - 1995 - INSTITUT F UR INFORMATIK Report on the Program AMoRE:/home/user/Zotero/storage/EVHG7R8M/Unknown - 1995 - INSTITUT F UR INFORMATIK Report on the Program AMoRE.pdf:application/pdf}
}

@article{noauthor-jalc-lac-nodate,
	title = {{JALC}-{LAC}},
	file = {Unknown - Unknown - JALC-LAC:/home/user/Zotero/storage/3ZDMIKQF/Unknown - Unknown - JALC-LAC.ps:application/postscript}
}

@article{noauthor-cloom-nodate,
	title = {cloom},
	file = {cloom:/home/user/Zotero/storage/A4KGN9WI/cloom.ps:application/postscript}
}

@inproceedings{noauthor-15th-1974,
	title = {15th {Annual} {Symposium} on {Switching} and {Automata} {Theory}, 14-16 {October} 1974, {The} {University} of {New} {Orleans}, {USA}},
	booktitle = {15th {Annual} {Symposium} on {Switching} and {Automata} {Theory}},
	publisher = {IEEE},
	year = {1974}
}

@article{noauthor-residually-small-varieties-generated-by-simple-algebras.pdf-nodate,
	title = {Residually\_Small\_Varieties\_Generated\_by\_Simple\_Algebras.pdf},
	file = {Unknown - Unknown - Residually_Small_Varieties_Generated_by_Simple_Algebras.pdf:/home/user/Zotero/storage/9QUX682D/Unknown - Unknown - Residually_Small_Varieties_Generated_by_Simple_Algebras.pdf.pdf:application/pdf}
}

@article{noauthor-algebraic-2006,
	title = {An {Algebraic} {Theory} for {Regular} {Languages} of {Unranked} {Trees}},
	year = {2006},
	file = {Unknown - 2006 - An Algebraic Theory for Regular Languages of Unranked Trees:/home/user/Zotero/storage/CEB5X6H4/Unknown - 2006 - An Algebraic Theory for Regular Languages of Unranked Trees.pdf:application/pdf}
}

@inproceedings{noauthor-14th-1973,
	title = {14th {Annual} {Symposium} on {Switching} and {Automata} {Theory}, {Iowa} {City}, {Iowa}, {USA}, {October} 15-17, 1973},
	url = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4569717},
	publisher = {{IEEE} Computer Society},
	year = {1973}
}

@inproceedings{noauthor-proceedings-1986,
	title = {Proceedings of the {Eighteenth} {Annual} {ACM} {Symposium} on {Theory} of {Computing}, 28-30 {May} 1986, {Berkeley}, {California}, {USA}},
	booktitle = {{STOC}},
	publisher = {ACM},
	year = {1986}
}

@article{noauthor-commutator-nodate,
	title = {Commutator {Theory} for {Congruence} {Modular} {Varieties} {Ralph} {Freese} and {Ralph} {McKenzie}},
	file = {Unknown - Unknown - Commutator Theory for Congruence Modular Varieties Ralph Freese and Ralph McKenzie:/home/user/Zotero/storage/WNFQA4XF/Unknown - Unknown - Commutator Theory for Congruence Modular Varieties Ralph Freese and Ralph McKenzie.pdf:application/pdf}
}

@article{noauthor-learning-nodate,
	title = {learning programs over monoids},
	file = {Unknown - Unknown - learning programs over monoids:/home/user/Zotero/storage/7H85KMBJ/Unknown - Unknown - learning programs over monoids.ps:application/postscript}
}

@article{noauthor-pighi-nodate,
	title = {pighi},
	file = {pighi:/home/user/Zotero/storage/TIES5UR2/pighi.dvi:application/x-dvi}
}

@inproceedings{noauthor-proceedings-2010,
	title = {Proceedings of the 25th {Annual} {{IEEE}} {Symposium} on {Logic} in {Computer} {Science}, {{LICS}} 2010, 11-14 {July} 2010, {Edinburgh}, {United} {Kingdom}},
	isbn = {978-0-7695-4114-3},
	url = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5570020},
	publisher = {{IEEE} Computer Society},
	year = {2010}
}

@inproceedings{noauthor-19th-2004,
	title = {19th {Annual} {{IEEE}} {Conference} on {Computational} {Complexity} {({CCC}} 2004), 21-24 {June} 2004, {Amherst}, {MA}, {{USA}}},
	isbn = {0-7695-2120-7},
	publisher = {{IEEE} Computer Society},
	year = {2004}
}

@article{noauthor-hilbert10.pdf-nodate,
	title = {hilbert10.pdf},
	file = {Unknown - Unknown - hilbert10.pdf:/home/user/Zotero/storage/4G3M43UE/Unknown - Unknown - hilbert10.pdf.pdf:application/pdf}
}

@inproceedings{noauthor-proceedings-2008,
	title = {Proceedings of the {Twenty}-{Third} {Annual} {{IEEE}} {Symposium} on {Logic} in {Computer} {Science}, {{LICS}} 2008, 24-27 {June} 2008, {Pittsburgh}, {PA}, {{USA}}},
	isbn = {978-0-7695-3183-0},
	url = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4557886},
	publisher = {{IEEE} Computer Society},
	year = {2008}
}

@article{noauthor-eq-final-nodate,
	title = {eq-final},
	file = {eq-final:/home/user/Zotero/storage/XACA3G47/eq-final.ps:application/postscript}
}

@article{zimmermann-delay-2014,
	title = {Delay {Games} with {WMSO}+{U} {Winning} {Conditions}},
	url = {http://arxiv.org/abs/1412.3978},
	abstract = {Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent's moves. We consider delay games with winning conditions expressed in weak monadic second order logic with the unbounding quantifier, which is able to express (un)boundedness properties. We show that it is decidable whether the delaying player has a winning strategy using bounded lookahead and give a doubly-exponential upper bound on the necessary lookahead. In contrast, we show that bounded lookahead is not always sufficient to win such a game.},
	author = {Zimmermann, Martin},
	year = {2014},
	pages = {1--17},
	file = {1412.3978v3:/home/user/Zotero/storage/7SHDBSGM/1412.3978v3.pdf:application/pdf}
}

@article{zeitoun-join-nodate,
	title = {on the join of two pseudovarieties},
	author = {Zeitoun, Marc},
	pages = {1--6},
	file = {Z-sal96:/home/user/Zotero/storage/SMRU274I/Z-sal96.pdf:application/pdf}
}

@inproceedings{yao-separating-1985,
	title = {Separating the {Polynomial}-{Time} {Hierarchy} by {Oracles} ({Preliminary} {Version})},
	booktitle = {26th {Annual} {Symposium} on {Foundations} of {Computer} {Science}, {Portland}, {Oregon}, {USA}, 21-23 {October} 1985},
	publisher = {IEEE Computer Society},
	author = {Yao, Andrew Chi-Chih},
	year = {1985},
	pages = {1--10}
}

@article{willard-quick-2007,
	title = {Quick course in {Universal} {Algebra} and {Tame} {Congruence} {Theory}},
	volume = {2007},
	number = {June},
	author = {Willard, Ross},
	year = {2007},
	pages = {1--35},
	file = {Willard - 2007 - Quick course in Universal Algebra and Tame Congruence Theory:/home/user/Zotero/storage/6FKUHRU6/Willard - 2007 - Quick course in Universal Algebra and Tame Congruence Theory.pdf:application/pdf}
}

@article{willard-overview-nodate,
	title = {An overview of modern universal algebra},
	author = {Willard, Ross},
	keywords = {and phrases, universal algebra, nserc of canada, survey, work supported by the},
	pages = {1--23},
	file = {Willard - Unknown - An overview of modern universal algebra:/home/user/Zotero/storage/7KS3B6SU/Willard - Unknown - An overview of modern universal algebra.pdf:application/pdf}
}

@inproceedings{noauthor-conference-1979,
	title = {Conference {Record} of the {Eleventh} {Annual} {ACM} {Symposium} on {Theory} of {Computing}, 30 {April}-2 {May}, 1979, {Atlanta}, {Georgia}, {USA}},
	booktitle = {{STOC}},
	publisher = {ACM},
	year = {1979}
}

@inproceedings{noauthor-21st-2006,
	title = {21st {Annual} {{IEEE}} {Conference} on {Computational} {Complexity} {({CCC}} 2006), 16-20 {July} 2006, {Prague}, {Czech} {Republic}},
	isbn = {0-7695-2596-2},
	publisher = {{IEEE} Computer Society},
	year = {2006}
}

@inproceedings{yao-separating-1985-1,
	title = {Separating the {Polynomial}-{Time} {Hierarchy} by {Oracles} ({Preliminary} {Version})},
	booktitle = {{FOCS}},
	author = {Yao, Andrew Chi-Chih},
	year = {1985},
	pages = {1--10}
}

@inproceedings{winkler-algebraic-2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Algebraic {Informatics} - 4th {International} {Conference}, {CAI} 2011, {Linz}, {Austria}, {June} 21-24, 2011. {Proceedings}},
	volume = {6742},
	isbn = {978-3-642-21492-9},
	booktitle = {{CAI}},
	publisher = {Springer},
	editor = {Winkler, Franz},
	year = {2011}
}

@book{noauthor-nagel-nodate,
	title = {Nagel {One}-parameter {Semigroups} of {Positive} {Operators} (1).pdf},
	file = {Unknown - Unknown - Nagel One-parameter Semigroups of Positive Operators (1).pdf:/home/user/Zotero/storage/BJIHET7I/Unknown - Unknown - Nagel One-parameter Semigroups of Positive Operators (1).pdf.pdf:application/pdf}
}

@inproceedings{williams-non-uniform-2011,
	title = {Non-uniform {ACC} {Circuit} {Lower} {Bounds}},
	booktitle = {Proceedings of the 26th {Annual} {IEEE} {Conference} on {Computational} {Complexity}, {CCC} 2011, {San} {Jose}, {California}, {June} 8-10, 2011},
	publisher = {IEEE Computer Society},
	author = {Williams, Ryan},
	year = {2011},
	pages = {115--125},
	file = {Williams - 2011 - Non-uniform ACC Circuit Lower Bounds:/home/user/Zotero/storage/C2I8RPBV/Williams - 2011 - Non-uniform ACC Circuit Lower Bounds.pdf:application/pdf}
}

@article{williams-natural-2012,
	title = {Natural {Proofs} {Versus} {Derandomization}},
	volume = {1049268},
	url = {http://arxiv.org/abs/1212.1891},
	abstract = {We study connections between Natural Proofs, derandomization, and the problem of proving "weak" circuit lower bounds such as \${\sf NEXP} \not\subset {\sf TC{\textasciicircum}0}\$. Natural Proofs have three properties: they are constructive (an efficient algorithm \$A\$ is embedded in them), have largeness (\$A\$ accepts a large fraction of strings), and are useful (\$A\$ rejects all strings which are truth tables of small circuits). Strong circuit lower bounds that are "naturalizing" would contradict present cryptographic understanding, yet the vast majority of known circuit lower bound proofs are naturalizing. So it is imperative to understand how to pursue un-Natural Proofs. Some heuristic arguments say constructivity should be circumventable: largeness is inherent in many proof techniques, and it is probably our presently weak techniques that yield constructivity. We prove: \$\bullet\$ Constructivity is unavoidable, even for \$\sf NEXP\$ lower bounds. Informally, we prove for all "typical" non-uniform circuit classes \${\cal C}\$, \${\sf NEXP} \not\subset {\cal C}\$ if and only if there is a polynomial-time algorithm distinguishing some function from all functions computable by \${\cal C}\$-circuits. Hence \${\sf NEXP} \not\subset {\cal C}\$ is equivalent to exhibiting a constructive property useful against \${\cal C}\$. \$\bullet\$ There are no \$\sf P\$-natural properties useful against \${\cal C}\$ if and only if randomized exponential time can be "derandomized" using truth tables of circuits from \${\cal C}\$ as random seeds. Therefore the task of proving there are no \$\sf P\$-natural properties is inherently a derandomization problem, weaker than but implied by the existence of strong pseudorandom functions. These characterizations are applied to yield several new results, including improved \${\sf ACC}{\textasciicircum}0\$ lower bounds and new unconditional derandomizations.},
	author = {Williams, Ryan},
	year = {2012},
	file = {1212.1891v3:/home/user/Zotero/storage/VVF37DIV/1212.1891v3.pdf:application/pdf}
}

@article{noauthor-space-nodate,
	title = {Space and {Circuit} {Complexity} of {Monadic} {Second}-{Order} {Definable} {Problems} on {Tree}-{Decomposable} {Structures} {Michael} {Elberfeld}},
	file = {Unknown - Unknown - Space and Circuit Complexity of Monadic Second-Order Definable Problems on Tree-Decomposable Structures Michael Elbe:/home/user/Zotero/storage/BR7K9933/Unknown - Unknown - Space and Circuit Complexity of Monadic Second-Order Definable Problems on Tree-Decomposable Structures Michael Elbe.pdf:application/pdf}
}

@article{wilke-classifying-1999,
	title = {Classifying discrete temporal properties},
	url = {http://www.springerlink.com/index/G1PYYG7ARYR8T5R8.pdf},
	journal = {Lecture Notes in Computer Science},
	author = {Wilke, T. and Wilke, T.},
	year = {1999},
	pages = {32--46},
	file = {10.1.1.30.8942:/home/user/Zotero/storage/KDJAHG7T/10.1.1.30.8942.pdf:application/pdf}
}

@article{weil-profinite-2002,
	title = {Profinite {Methods} in {Semigroup} {Theory}},
	volume = {12},
	url = {http://dx.doi.org/10.1142/S0218196702000912},
	doi = {10.1142/S0218196702000912},
	number = {1-2},
	journal = {IJAC},
	author = {Weil, Pascal},
	year = {2002},
	pages = {137--177}
}

@article{weil-profinite-nodate,
	title = {profinite methods},
	author = {Weil, Pascal},
	file = {Weil - Unknown - profinite methods:/home/user/Zotero/storage/XG2CX6UB/Weil - Unknown - profinite methods.pdf:application/pdf}
}

@book{werner-funktionalanalysis-2011,
	title = {funktionalanalysis},
	isbn = {978-3-642-21016-7},
	author = {{Werner}},
	year = {2011},
	file = {Werner - 2011 - funktionalanalysis:/home/user/Zotero/storage/W6SQXSGC/Werner - 2011 - funktionalanalysis.pdf:application/pdf}
}

@article{weil-algebra-2010,
	title = {From algebra to logic : {There} and back again {The} story of a hierarchy ⋆},
	author = {Weil, Pascal},
	year = {2010},
	pages = {2010--2012},
	file = {Weil - 2010 - From algebra to logic There and back again The story of a hierarchy ⋆:/home/user/Zotero/storage/8E76RKHG/Weil - 2010 - From algebra to logic There and back again The story of a hierarchy ⋆.pdf:application/pdf}
}

@article{weil-pro-2000,
	title = {Pro nite methods in semigroup theory},
	number = {May},
	author = {Weil, Pascal},
	year = {2000},
	pages = {1--35},
	file = {Weil - 2000 - Pro nite methods in semigroup theory:/home/user/Zotero/storage/SFRQF337/Weil - 2000 - Pro nite methods in semigroup theory.pdf:application/pdf}
}

@article{weil-decomposition-1989,
	title = {{DECOMPOSITION} {TECHNIQUES} {USING} {CATEGORIES} {II} {John} {RHODES} * {All} the semigroups considered here are finite . {In} [ {S} ] the first author introduced or m . p . s .‘ s , under a slightly different name , mean- ing non-factorizable semigroup surmorphisms , and f},
	volume = {62},
	author = {Weil, Pascal},
	year = {1989},
	pages = {285--312},
	file = {Weil - 1989 - DECOMPOSITION TECHNIQUES USING CATEGORIES II John RHODES All the semigroups considered here are finite . In S the fir:/home/user/Zotero/storage/6GFUUSEC/Weil - 1989 - DECOMPOSITION TECHNIQUES USING CATEGORIES II John RHODES All the semigroups considered here are finite . In S the fir.pdf:application/pdf}
}

@article{vollmer-gap-language-technique-nodate,
	title = {the gap-language-technique revisited},
	author = {{Vollmer}},
	file = {Vollmer - Unknown - the gap-language-technique revisited:/home/user/Zotero/storage/BAX8B9BG/Vollmer - Unknown - the gap-language-technique revisited.pdf:application/pdf}
}

@article{van-leeuwen-membership-1975,
	title = {The {Membership} {Question} for {ET}0L-{Languages} is {Polynomially} {Complete}},
	volume = {3},
	url = {http://dx.doi.org/10.1016/0020-0190(75)90027-7},
	doi = {10.1016/0020-0190(75)90027-7},
	number = {5},
	journal = {Inf. Process. Lett.},
	author = {van Leeuwen, Jan},
	year = {1975},
	pages = {138--143}
}

@article{tilson-categories-1987,
	title = {Categories as algebra: {An} essential ingredient in the theory of monoids},
	volume = {48},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0022404987901083},
	doi = {10.1016/0022-4049(87)90108-3},
	number = {1-2},
	journal = {Journal of Pure and Applied Algebra},
	author = {Tilson, Bret},
	month = sep,
	year = {1987},
	pages = {83--198},
	file = {Tilson - 1987 - Categories as algebra An essential ingredient in the theory of monoids:/home/user/Zotero/storage/VRKMEAND/Tilson - 1987 - Categories as algebra An essential ingredient in the theory of monoids.pdf:application/pdf}
}

@article{thomas-path-2009,
	title = {Path {Logics} with {Synchronization}},
	author = {Thomas, Wolfgang},
	year = {2009},
	file = {tho09a:/home/user/Zotero/storage/SMV8GIS2/tho09a.pdf:application/pdf}
}

@article{thomas-theory-1978,
	title = {The {Theory} of {Successor} with an {Extra} {Predicate}},
	volume = {237},
	journal = {Math. Annalen},
	author = {Thomas, Wolfgang},
	year = {1978},
	pages = {121--132}
}

@article{walukiewicz-forest-2007,
	title = {Forest {Algebrs} {The} goal},
	number = {November},
	author = {Walukiewicz, Igor and Bojanczyk, Mikolaj and Straubing, Howard},
	year = {2007},
	file = {Walukiewicz, Bojanczyk, Straubing - 2007 - Forest Algebrs The goal:/home/user/Zotero/storage/5BNN2762/Walukiewicz, Bojanczyk, Straubing - 2007 - Forest Algebrs The goal.pdf:application/pdf}
}

@article{waldhauser-minimal-2011,
	title = {Minimal clones with weakly abelian representations},
	url = {http://arxiv.org/abs/1102.2081},
	abstract = {We show that a minimal clone has a nontrivial weakly abelian representation iff it has a nontrivial abelian representation, and that in this case all representations are weakly abelian.},
	author = {Waldhauser, Tamás},
	month = feb,
	year = {2011},
	keywords = {and phrases, abelian algebra, clone, groupoid, minimal clone, weakly},
	pages = {12--12},
	file = {Waldhauser - 2011 - Minimal clones with weakly abelian representations:/home/user/Zotero/storage/AWNZ3VPU/Waldhauser - 2011 - Minimal clones with weakly abelian representations.pdf:application/pdf}
}

@inproceedings{von-braunmuhl-input-driven-1983,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Input-{Driven} {Languages} are {Recognized} in log n {Space}},
	volume = {158},
	isbn = {3-540-12689-9},
	url = {http://dx.doi.org/10.1007/3-540-12689-9\_92},
	doi = {10.1007/3-540-12689-9\_92},
	booktitle = {Fundamentals of {Computation} {Theory}, {Proceedings} of the 1983 {International} {FCT}-{Conference}, {Borgholm}, {Sweden}, {August} 21-27, 1983},
	publisher = {Springer},
	author = {von Braunmühl, Burchard and Verbeek, Rutger},
	editor = {Karpinski, Marek},
	year = {1983},
	pages = {40--51}
}

@book{vollmer-introduction-1999,
	series = {Texts in theoretical computer science},
	title = {Introduction to circuit complexity - a uniform approach},
	isbn = {978-3-540-64310-4},
	publisher = {Springer},
	author = {Vollmer, Heribert},
	year = {1999}
}

@inproceedings{vollmer-gap-language-technique-1990,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Gap}-{Language}-{Technique} {Revisited}},
	volume = {533},
	isbn = {3-540-54487-9},
	booktitle = {{CSL}},
	publisher = {Springer},
	author = {Vollmer, Heribert},
	editor = {Börger, Egon and Büning, Hans Kleine and Richter, Michael M and Schönfeld, Wolfgang},
	year = {1990},
	pages = {389--399}
}

@article{vanderwerf-wreath-1996,
	title = {Wreath products of algebras: {Generalizing} the {Krohn}-{Rhodes} theorem to arbitrary algebras},
	volume = {52},
	url = {http://dx.doi.org/10.1007/BF02574084},
	doi = {10.1007/BF02574084},
	number = {1},
	journal = {Semigroup Forum},
	author = {VanderWerf, Joel},
	year = {1996},
	pages = {93--100}
}

@article{van-rooijen-separation-2013,
	title = {The separation problem for regular languages by piecewise testable languages},
	url = {http://arxiv.org/abs/1303.2143\nhttp://www.arxiv.org/pdf/1303.2143.pdf},
	abstract = {Separation is a classical problem in mathematics and computer science. It asks whether, given two sets belonging to some class, it is possible to separate them by another set of a smaller class. We present and discuss the separation problem for regular languages. We then give a direct polynomial time algorithm to check whether two given regular languages are separable by a piecewise testable language, that is, whether a \$B{\Sigma}1({\textless})\$ sentence can witness that the languages are indeed disjoint. The proof is a reformulation and a refinement of an algebraic argument already given by Almeida and the second author.},
	number = {1},
	journal = {arXiv:1303.2143},
	author = {van Rooijen, Lorijn and Zeitoun, Marc},
	year = {2013},
	pages = {1--16},
	file = {1303.2143v1:/home/user/Zotero/storage/UZD5ZH2H/1303.2143v1.pdf:application/pdf}
}

@incollection{van-benthem-higher-order-1983,
	title = {Higher-order logic},
	url = {http://link.springer.com/chapter/10.1007/978-94-009-7066-3\_4},
	booktitle = {Handbook of philosophical logic},
	publisher = {Springer},
	author = {van Benthem, Johan and Doets, Kees and Benthem, J Van},
	year = {1983},
	pages = {275--329},
	file = {Benthem, Doets - 1983 - Higher-order logic:/home/user/Zotero/storage/XBC6GX6E/Benthem, Doets - 1983 - Higher-order logic.pdf:application/pdf}
}

@article{valeriote-vanderbilt-2015,
	title = {Vanderbilt {Open} {Problems} {List} 2015 {Maltsev} conditions},
	volume = {1},
	author = {Valeriote, Matt and Mckenzie, Ralph and Mckenzie, Ralph and Mckenzie, Ralph and Barto, Libor},
	year = {2015},
	pages = {1--11},
	file = {open-problems-list:/home/user/Zotero/storage/863SUHJB/open-problems-list.pdf:application/pdf;Valeriote et al. - 2015 - Vanderbilt Open Problems List 2015 Maltsev conditions:/home/user/Zotero/storage/UWUHAVUX/Valeriote et al. - 2015 - Vanderbilt Open Problems List 2015 Maltsev conditions.pdf:application/pdf}
}

@article{thomas-characterization-nodate,
	title = {Characterization of noncounting context-free languages},
	author = {Thomas, Wolfgang}
}

@inproceedings{thomas-logical-1984,
	title = {Logical aspects in the study of tree languages},
	booktitle = {Ninth {Colloquium} on {Trees} in {Algebra} and in {Programming} ({Proc}. {CAAP} 84)},
	publisher = {Cambridge University Press},
	author = {Thomas, W.},
	editor = {{Courcelle} and {B.}},
	year = {1984},
	pages = {31--51}
}

@article{thomas-locally-2010,
	title = {locally compact groups},
	volume = {05},
	url = {http://arxiv.org/abs/1011.1635},
	doi = {10.1112/0000/000000},
	abstract = {We prove a conjecture of Kontsevich which states that if \$A\$ is an \$E\_{d-1}\$ algebra then the Hochschild cohomology object of \$A\$ is the universal \$E\_d\$ algebra acting on \$A\$. The notion of an \$E\_d\$ algebra acting on an \$E\_{d-1}\$ algebra was defined by Kontsevich using the swiss cheese operad of Voronov. We prove a homotopical property of the swiss cheese operad from which the conjecture follows.},
	author = {Thomas, Justin D.},
	year = {2010},
	pages = {33--33},
	file = {1405.4851:/home/user/Zotero/storage/B3EJVZZN/1405.4851.pdf:application/pdf}
}

@article{therien-temporal-nodate,
	title = {temporal logic characterization},
	author = {{Therien} and {Wilke}},
	file = {Therien, Wilke - Unknown - temporal logic characterization:/home/user/Zotero/storage/9GKZDWTK/Therien, Wilke - Unknown - temporal logic characterization.ps:application/postscript}
}

@inproceedings{therien-over-1998,
	title = {Over {Words}, {Two} {Variables} {Are} as {Powerful} as {One} {Quantifier} {Alternation}},
	booktitle = {{STOC}},
	author = {Thérien, Denis and Wilke, Thomas},
	year = {1998},
	pages = {234--240}
}

@article{therien-two-sided-1991,
	title = {Two-sided wreath product of categories},
	volume = {74},
	url = {http://www.sciencedirect.com/science/article/pii/002240499190119M},
	doi = {10.1016/0022-4049(91)90119-M},
	abstract = {The Krohn-Rhodes theorem describes how an arbitrary finite monoid can be decomposed into a wreath product of groups and aperiodic monoids. New tools have recently been introduced to refine and extend this fundamental result. New theorems can be obtained by considering monoids as a special case of categories, thus allowing more general structures to appear as building blocks in decompositions results. Also, a two-sided version of the wreath product may be used as the connecting operation. This paper combines the two ideas: the new operation, called the block product, is defined directly as acting on categories and basic properties are presented. As an application, an open problem in the theory of regular languages is solved.},
	number = {3},
	journal = {Journal of Pure and Applied Algebra},
	author = {Thérien, Denis and Thkien, Denis},
	month = sep,
	year = {1991},
	pages = {307--315},
	file = {Thkien - 1991 - rea:/home/user/Zotero/storage/52N5TAPT/Thkien - 1991 - rea.pdf:application/pdf}
}

@article{therien-classification-1981,
	title = {Classification of finite monoids: the language approach},
	volume = {14},
	url = {http://www.sciencedirect.com/science/article/pii/0304397581900578},
	number = {198 1},
	journal = {Theoretical Computer Science},
	author = {Thérien, D},
	year = {1981},
	pages = {195--208},
	file = {Thérien - 1981 - Classification of finite monoids the language approach:/home/user/Zotero/storage/WWFZ5R2K/Thérien - 1981 - Classification of finite monoids the language approach.pdf:application/pdf}
}

@inproceedings{thai-computing-2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computing and {Combinatorics}, 16th {Annual} {International} {Conference}, {COCOON} 2010, {Nha} {Trang}, {Vietnam}, {July} 19-21, 2010. {Proceedings}},
	volume = {6196},
	isbn = {978-3-642-14030-3},
	booktitle = {{COCOON}},
	publisher = {Springer},
	editor = {Thai, My T and Sahni, Sartaj},
	year = {2010}
}

@article{tesson-temporal-nodate,
	title = {temporal logic and semidirect},
	author = {{Tesson} and {Therien}},
	file = {Tesson, Therien - Unknown - temporal logic and semidirect:/home/user/Zotero/storage/SH383SAD/Tesson, Therien - Unknown - temporal logic and semidirect.pdf:application/pdf}
}

@inproceedings{tesson-restricted-2005,
	title = {Restricted {Two}-{Variable} {Sentences}, {Circuits} and {Communication} {Complexity}},
	booktitle = {{ICALP}},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2005},
	pages = {526--538}
}

@article{tesson-logic-2007,
	title = {Logic {Meets} {Algebra}: the {Case} of {Regular} {Languages}},
	volume = {3},
	number = {1},
	journal = {Logical Methods in Computer Science},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2007}
}

@article{tesson-monoids-2004,
	title = {Monoids and {Computations}},
	volume = {14},
	doi = {10.1142/S0218196704001979},
	journal = {International Journal of Algebra and Computation},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2004},
	pages = {801--816},
	file = {Tesson, Thérien - 2004 - Monoids and Computations:/home/user/Zotero/storage/7ADV43VT/Tesson, Thérien - 2004 - Monoids and Computations.pdf:application/pdf}
}

@article{tesson-bridges-2006,
	title = {Bridges between algebraic automata theory and complexity theory},
	volume = {88},
	journal = {Computation Complexity Column, EACTS Bull.},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2006},
	pages = {1--1}
}

@article{therien-nesting-2004,
	title = {Nesting {Until} and {Since} in {Linear} {Temporal} {Logic}},
	volume = {37},
	number = {1},
	journal = {Theory Comput. Syst.},
	author = {Thérien, Denis and Wilke, Thomas},
	year = {2004},
	pages = {111--131}
}

@article{therien-temporal-nodate-1,
	title = {temporal logic and semidirect products},
	author = {Therien, Denis and Wilke, Thomas},
	file = {Therien, Wilke - Unknown - temporal logic and semidirect products:/home/user/Zotero/storage/J94SRHGW/Therien, Wilke - Unknown - temporal logic and semidirect products.pdf:application/pdf}
}

@article{therien-learning-2001,
	title = {Learning {Expressions} and {Programs} over {Monoids} 1 {Introduction}},
	volume = {14186},
	author = {Therien, Denis and Gavalda, Ricard and Tesson, Pascal},
	year = {2001},
	pages = {1--41},
	file = {Therien, Gavalda, Tesson - 2001 - Learning Expressions and Programs over Monoids 1 Introduction:/home/user/Zotero/storage/DIMB8J2R/Therien, Gavalda, Tesson - 2001 - Learning Expressions and Programs over Monoids 1 Introduction.pdf:application/pdf}
}

@article{thatcher-generalized-1968,
	title = {Generalized {Finite} {Automata} {Theory} with an {Application} to a {Decision} {Problem} of {Second}-{Order} {Logic}},
	volume = {2},
	url = {http://dx.doi.org/10.1007/BF01691346},
	doi = {10.1007/BF01691346},
	number = {1},
	journal = {Mathematical Systems Theory},
	author = {Thatcher, James W and Wright, Jesse B},
	year = {1968},
	pages = {57--81}
}

@article{teufel-mathematische-2013,
	title = {Mathematische {Physik} {II}},
	author = {Teufel, Stefan},
	year = {2013},
	file = {Teufel - 2013 - Mathematische Physik II:/home/user/Zotero/storage/C5ZPWA8D/Teufel - 2013 - Mathematische Physik II.pdf:application/pdf}
}

@article{tesson-computing-2002,
	title = {The {Computing} {Power} of {Programs} over {Finite} {Monoids}},
	volume = {7},
	number = {2},
	journal = {Journal of Automata, Languages and Combinatorics},
	author = {{Tesson} and {therien} and Tesson, Pascal and Th{\'e}rien, Denis and Thérien, Denis},
	year = {2002},
	pages = {247--258},
	file = {Tesson, Th 'e rien - 2001 - The Computing Power of Programs over Finite Monoids:/home/user/Zotero/storage/6SRGVQ3R/Tesson, Th 'e rien - 2001 - The Computing Power of Programs over Finite Monoids.pdf:application/pdf;Tesson, therien - Unknown - computing power:/home/user/Zotero/storage/PKZMZGWQ/Tesson, therien - Unknown - computing power.pdf:application/pdf}
}

@article{tesson-monoids-nodate,
	title = {monoids computation},
	author = {{Tesson} and {Therien}},
	file = {Tesson, Therien - Unknown - monoids computation:/home/user/Zotero/storage/Q59QWGCS/Tesson, Therien - Unknown - monoids computation.ps:application/postscript}
}

@article{tesson-diamonds-nodate,
	title = {diamonds are forever: the variety {DA}},
	author = {{Tesson} and Th{\'e}rien, Denis},
	file = {Tesson, Th 'e rien - Unknown - diamonds are forever the variety DA:/home/user/Zotero/storage/SW2D7MF3/Tesson, Th 'e rien - Unknown - diamonds are forever the variety DA.pdf:application/pdf}
}

@article{tesson-computing-2001,
	title = {The {Computing} {Power} of {Programs} over {Finite} {Monoids}},
	volume = {8},
	number = {5},
	journal = {Electronic Colloquium on Computational Complexity (ECCC)},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2001}
}

@inproceedings{tesson-diamonds-2002,
	title = {Diamonds are {Forever}: {The} {Variety} {{DA}}},
	booktitle = {Semigroups, {Algorithms}, {Automata} and {Languages}, {Coimbra} ({Portugal}) 2001},
	publisher = {World Scientific},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2002},
	pages = {475--500}
}

@article{tesson-logic-2007-1,
	title = {Logic {Meets} {Algebra}: the {Case} of {Regular} {Languages}},
	volume = {3},
	number = {1},
	journal = {Logical Methods in Computer Science},
	author = {Tesson, Pascal and Thérien, Denis},
	year = {2007}
}

@article{tesson-computing-2001-1,
	title = {The {Computing} {Power} of {Programs} over {Finite} {Monoids}},
	volume = {5},
	number = {5},
	author = {Tesson, Pascal and Th{\'e}rien, Denis},
	year = {2001},
	file = {Tesson, Th 'e rien - 2001 - The Computing Power of Programs over Finite Monoids:/home/user/Zotero/storage/FQJBXGBG/Tesson, Th 'e rien - 2001 - The Computing Power of Programs over Finite Monoids.pdf:application/pdf}
}

@article{tesson-logic-2007-2,
	title = {Logic meets algebra: the case of regular languages ∗},
	volume = {3},
	doi = {10.2168/LMCS-3},
	author = {Tesson, Pascal and Erien, Denis T H},
	year = {2007},
	keywords = {and phrases, descriptive complexity, regular languages, semigroup theory},
	pages = {1--37},
	file = {Tesson, Erien - 2007 - Logic meets algebra the case of regular languages ∗:/home/user/Zotero/storage/EVQSTUFS/Tesson, Erien - 2007 - Logic meets algebra the case of regular languages ∗.pdf:application/pdf}
}

@article{tantau-existential-2015,
	title = {Existential {Second}-order {Logic} over {Graphs} : {A} {Complete} {Complexity}-theoretic {Classification}},
	number = {Stacs},
	author = {Tantau, Till},
	year = {2015},
	keywords = {descriptive complexity, 4230, digital object identifier 10, lipics, stacs, 2015, Logic, 703, and phrases existential second-order, logarithmic space},
	pages = {703--715},
	file = {Tantau - 2015 - Existential Second-order Logic over Graphs A Complete Complexity-theoretic Classification:/home/user/Zotero/storage/EC2V7QQF/Tantau - 2015 - Existential Second-order Logic over Graphs A Complete Complexity-theoretic Classification.pdf:application/pdf}
}

@book{takeuti-proof-1987,
	address = {Amsterdam, New York, Oxford, Tokyo},
	edition = {2},
	series = {Studies in {Logic}},
	title = {Proof {{T}}heory},
	volume = {81},
	publisher = {North-Holland},
	author = {Takeuti, Gaisi},
	year = {1987}
}

@article{szendrei-set-1813,
	title = {The set of types of a nitely generated variety},
	author = {Szendrei, A},
	year = {1813},
	file = {Szendrei - 1813 - The set of types of a nitely generated variety:/home/user/Zotero/storage/7X2UQZDQ/Szendrei - 1813 - The set of types of a nitely generated variety.pdf:application/pdf}
}

@article{sudborough-tape-bounded-1975,
	title = {On {Tape}-{Bounded} {Complexity} {Classes} and {Multihead} {Finite} {Automata}},
	volume = {10},
	url = {http://dx.doi.org/10.1016/S0022-0000(75)80014-6},
	doi = {10.1016/S0022-0000(75)80014-6},
	number = {1},
	journal = {J. Comput. Syst. Sci.},
	author = {Sudborough, Ivan Hal},
	year = {1975},
	pages = {62--76}
}

@article{sudborough-note-1975,
	title = {A {Note} on {Tape}-{Bounded} {Complexity} {Classes} and {Linear} {Context}-{Free} languages},
	volume = {22},
	url = {http://doi.acm.org/10.1145/321906.321913},
	doi = {10.1145/321906.321913},
	number = {4},
	journal = {J. {ACM}},
	author = {Sudborough, Ivan Hal},
	year = {1975},
	pages = {499--500}
}

@article{straubing-logics-nodate,
	title = {logics for regular languages, finite monoids, and circuit complexity},
	author = {{Straubing} and {Therien} and {Thomas}},
	file = {Straubing, Therien, Thomas - Unknown - logics for regular languages, finite monoids, and circuit complexity:/home/user/Zotero/storage/GBRJJKRF/Straubing, Therien, Thomas - Unknown - logics for regular languages, finite monoids, and circuit complexity.pdf:application/pdf}
}

@article{straubing-forest-nodate,
	title = {Forest 1},
	volume = {i},
	author = {{Straubing} and {krebs}},
	pages = {1--6},
	file = {Straubing, krebs - Unknown - Forest 1:/home/user/Zotero/storage/HSQ9X2MI/Straubing, krebs - Unknown - Forest 1.pdf:application/pdf}
}

@incollection{straubing-varieties-2015,
	title = {Varieties},
	booktitle = {Handbook of automata theory},
	author = {Straubing, Howard and Weil, Pascal},
	editor = {Pin, Jean-Eric},
	year = {2015},
	file = {Straubing, Weil - Unknown - varieties:/home/user/Zotero/storage/9NJVRPC9/Straubing, Weil - Unknown - varieties.pdf:application/pdf}
}

@article{straubing-regular-2003,
	title = {Regular {Languages} {Defined} by {Generalized} {First}-{Order} {Formulas} with a {Bounded} {Number} of {Bound} {Variables}},
	volume = {36},
	number = {1},
	journal = {Theory Comput. Syst.},
	author = {Straubing, Howard and Thérien, Denis},
	year = {2003},
	pages = {29--69}
}

@inproceedings{straubing-weakly-2002,
	title = {Weakly {Iterated} {Block} {Products} of {Finite} {Monoids}},
	booktitle = {{LATIN}},
	author = {Straubing, Howard and Thérien, Denis},
	year = {2002},
	pages = {91--104}
}

@book{straubing-finite-1994,
	title = {Finite {Automata}, {Formal} {Logic}, and {Circuit} {Complexity}},
	publisher = {Birkh{ä}user, Boston},
	author = {Straubing, Howard},
	year = {1994}
}

@article{szendrei-strictly-nodate,
	title = {strictly simple algebras and minimal varieties},
	author = {Szendrei, A},
	file = {Szendrei - Unknown - strictly simple algebras and minimal varieties:/home/user/Zotero/storage/HXD6B2DV/Szendrei - Unknown - strictly simple algebras and minimal varieties.pdf:application/pdf}
}

@article{sudborough-tape-1978,
	title = {On the {Tape} {Complexity} of {Deterministic} {Context}-{Free} {Languages}},
	volume = {25},
	url = {http://doi.acm.org/10.1145/322077.322083},
	doi = {10.1145/322077.322083},
	number = {3},
	journal = {J. {ACM}},
	author = {Sudborough, Ivan Hal},
	year = {1978},
	pages = {405--414}
}

@article{straubing-regular-1995,
	title = {Regular {Languages} {Defined} with {Generalized} {Quanifiers}},
	volume = {118},
	number = {2},
	journal = {Inf. Comput.},
	author = {Straubing, Howard and Thérien, Denis and Thomas, Wolfgang},
	year = {1995},
	pages = {289--301}
}

@article{straubing-regular-1995-1,
	title = {Regular {Languages} {Defined} with {Generalized} {Quanifiers}},
	volume = {118},
	number = {2},
	journal = {Inf. Comput.},
	author = {Straubing, Howard and Thérien, Denis and Thomas, Wolfgang},
	year = {1995},
	pages = {289--301}
}

@incollection{straubing-regular-1988,
	title = {Regular {Languages} defined with {Generalized} {Quantifiers}},
	booktitle = {{LNCS} 317},
	author = {Straubing, Howard and Therien, Denis and Thomas, Wolfgang},
	year = {1988}
}

@article{straubing-when-nodate,
	title = {When {Can} {One} {Finite} {Monoid} {Simulate} {Another} ? 2 {The} {Classical} {Case} : {Finite} {Automata}},
	author = {Straubing, Howard and Hill, Chestnut},
	file = {Straubing, Hill - Unknown - When Can One Finite Monoid Simulate Another 2 The Classical Case Finite Automata:/home/user/Zotero/storage/HEAZMFAH/Straubing, Hill - Unknown - When Can One Finite Monoid Simulate Another 2 The Classical Case Finite Automata.pdf:application/pdf}
}

@inproceedings{straubing-circuit-1992,
	title = {Circuit {Complexity} and the {Expressive} {Power} of {Generalized} {First}-{Order} {Formulas}},
	booktitle = {{ICALP}},
	author = {Straubing, Howard},
	year = {1992},
	pages = {16--27}
}

@inproceedings{straubing-algebraic-2011,
	series = {{LIPIcs}},
	title = {Algebraic {Characterization} of the {Alternation} {Hierarchy} in {{FO}{\textasciicircum}{\mbox{2}}[{\textless}]} on {Finite} {Words}},
	volume = {12},
	isbn = {978-3-939897-32-3},
	booktitle = {{CSL}},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
	author = {Straubing, Howard},
	editor = {Bezem, Marc},
	year = {2011},
	pages = {525--537}
}

@article{straubing-new-2013,
	title = {New applications of the wreath product of forest algebras},
	volume = {47},
	url = {http://www.rairo-ita.org/10.1051/ita/2013039},
	doi = {10.1051/ita/2013039},
	number = {3},
	journal = {RAIRO - Theoretical Informatics and Applications},
	author = {Straubing, Howard},
	month = jul,
	year = {2013},
	keywords = {and phrases, tree automata, forest algebras, temporal logics},
	pages = {261--291},
	file = {Straubing - 2013 - New applications of the wreath product of forest algebras:/home/user/Zotero/storage/78RMKN6T/Straubing - 2013 - New applications of the wreath product of forest algebras.pdf:application/pdf}
}

@article{straubing-circuit-1991,
	title = {Circuit {Complexity} and the {Expressive} {Power} of {Generalized} {First}-{Order} {Formulas}},
	volume = {1},
	number = {1},
	journal = {Intl.\ J.\ Algebra and Computation (IJAC)},
	author = {Straubing, Howard},
	year = {1991},
	pages = {49--87}
}

@inproceedings{straubing-logical-2002,
	title = {On {Logical} {Descriptions} of {Regular} {Languages}},
	booktitle = {{LATIN}},
	author = {Straubing, Howard},
	year = {2002},
	pages = {528--538}
}

@article{straubing-new-2015,
	title = {A new proof of the locality of {R}},
	volume = {25},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218196715400111},
	doi = {10.1142/S0218196715400111},
	journal = {International Journal of Algebra and Computation},
	author = {Straubing, Howard},
	year = {2015},
	keywords = {1, 20m35, forest algebras, 20m07, 6, finite categories, finite semigroups, mathematics subject classification, on the decomposition, proofs of two old, statement of the results, theorems of stiffler, we give simple new},
	pages = {293--300},
	file = {s0218196715400111:/home/user/Zotero/storage/X9VW6AJX/s0218196715400111.pdf:application/pdf}
}

@book{straubing-finite-1994-1,
	address = {Boston},
	title = {Finite automata, formal logic, and circuit complexity},
	isbn = {Straubing, H. (1994). Finite automata, formal logic, and circuit complexity. Boston: Birkh{"a}user.},
	publisher = {Birkh{\"a}user},
	author = {Straubing, Howard},
	year = {1994},
	file = {Straubing - 1994 - Finite automata, formal logic, and circuit complexity:/home/user/Zotero/storage/ND57Q936/Straubing - 1994 - Finite automata, formal logic, and circuit complexity.djvu:image/vnd.djvu}
}

@inproceedings{straubing-algebraic-2011-1,
	series = {{LIPIcs}},
	title = {Algebraic {Characterization} of the {Alternation} {Hierarchy} in {{FO}{\textasciicircum}{\mbox{2}}[{\textless}]} on {Finite} {Words}},
	volume = {12},
	isbn = {978-3-939897-32-3},
	booktitle = {{CSL}},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
	author = {Straubing, Howard},
	editor = {Bezem, Marc},
	year = {2011},
	pages = {525--537},
	file = {Straubing - 2011 - Algebraic Characterization of the Alternation Hierarchy in FO mbox 2 on Finite Words:/home/user/Zotero/storage/8IT8WPZA/Straubing - 2011 - Algebraic Characterization of the Alternation Hierarchy in FO mbox 2 on Finite Words.pdf:application/pdf}
}

@article{straubing-new-2013-1,
	title = {New applications of the wreath product of forest algebras},
	volume = {47},
	url = {http://dx.doi.org/10.1051/ita/2013039},
	doi = {10.1051/ita/2013039},
	number = {3},
	journal = {{RAIRO} - Theor. Inf. and Applic.},
	author = {Straubing, Howard},
	year = {2013},
	pages = {261--291}
}

@article{straubing-logics-1995,
	title = {Logics for regular languages, finite monoids, and circuit complexity},
	issn = {9780387313412},
	url = {http://www.cs.bc.edu/~straubin/papers/york93.pdf},
	journal = {NATO ASI Series C Mathematical …},
	author = {Straubing, H and Thérien, D and Thomas, W},
	year = {1995},
	file = {Straubing, Thérien, Thomas - 1995 - Logics for regular languages, finite monoids, and circuit complexity:/home/user/Zotero/storage/WU3DC7S2/Straubing, Thérien, Thomas - 1995 - Logics for regular languages, finite monoids, and circuit complexity.pdf:application/pdf}
}

@article{steinberg-thesis-nodate,
	title = {thesis},
	author = {Steinberg, Benjamin},
	file = {Steinberg - Unknown - thesis:/home/user/Zotero/storage/V9RPEWJ3/Steinberg - Unknown - thesis.pdf:application/pdf}
}

@article{steinberg-decidability-nodate,
	title = {decidability},
	author = {{Steinberg}},
	file = {Steinberg - Unknown - decidability:/home/user/Zotero/storage/KX5RSE4A/Steinberg - Unknown - decidability.pdf:application/pdf}
}

@article{steinberg-categories-nodate,
	title = {categories as algebra {II}},
	author = {{Steinberg} and {tilson}},
	file = {Steinberg, tilson - Unknown - categories as algebra:/home/user/Zotero/storage/N86WQUXD/Steinberg, tilson - Unknown - categories as algebra.pdf:application/pdf}
}

@article{steinberg-categories-nodate-1,
	title = {Categories as {Algebra} {II}},
	author = {Steinberg, Benjamin and Tilson, Bret},
	file = {Steinberg, Tilson - Unknown - Categories as Algebra II:/home/user/Zotero/storage/QJMXDX4D/Steinberg, Tilson - Unknown - Categories as Algebra II.ps:application/postscript;Steinberg, Tilson - Unknown - Categories as Algebra II:/home/user/Zotero/storage/3UI9GJ4A/Steinberg, Tilson - Unknown - Categories as Algebra II.pdf:application/pdf}
}

@article{steinberg-delay-2001,
	title = {A delay theorem for pointlikes},
	volume = {63},
	doi = {10.1007/s002330010051},
	journal = {Semigroup Forum},
	author = {Steinberg, Benjamin},
	year = {2001},
	pages = {281--304},
	file = {Steinberg - 2001 - A delay theorem for pointlikes:/home/user/Zotero/storage/WBJ6EC33/Steinberg - 2001 - A delay theorem for pointlikes.pdf:application/pdf}
}

@article{state-wreath-2003,
	title = {Wreath products of algebras : generalizing the {Krohn}-{Rhodes} theorem to arbitrary algebras .},
	author = {State, The Goettingen},
	year = {2003},
	pages = {93--100},
	file = {State - 2003 - Wreath products of algebras generalizing the Krohn-Rhodes theorem to arbitrary algebras:/home/user/Zotero/storage/CFWH6MSC/State - 2003 - Wreath products of algebras generalizing the Krohn-Rhodes theorem to arbitrary algebras .pdf:application/pdf}
}

@article{srba-beyond-2009,
	title = {Beyond {Language} {Equivalence} on {Visibly} {Pushdown} {Automata}},
	volume = {5},
	url = {http://www.lmcs-online.org/ojs/viewarticle.php?id=423},
	doi = {10.2168/LMCS-5(1:2)2009},
	number = {1},
	journal = {Logical Methods in Computer Science},
	author = {Srba, Jiri},
	editor = {Stirling, Colin},
	month = jan,
	year = {2009},
	keywords = {and phrases, bisimilarity checking, mu-calculus, regularity, visibly pushdown automata},
	pages = {1--22},
	file = {Srba - 2009 - Beyond Language Equivalence on Visibly Pushdown Automata:/home/user/Zotero/storage/8RQBQ9QH/Srba - 2009 - Beyond Language Equivalence on Visibly Pushdown Automata.pdf:application/pdf}
}

@article{some-2-2005,
	title = {2 . {The} {Lattice} of {Pseudovarieties}},
	volume = {19},
	number = {1990},
	author = {Some, O N and On, Operators},
	year = {2005},
	pages = {122--127},
	file = {Some, On - 2005 - 2 . The Lattice of Pseudovarieties:/home/user/Zotero/storage/UD6UQR8Q/Some, On - 2005 - 2 . The Lattice of Pseudovarieties.pdf:application/pdf}
}

@article{sreejith-expressive-2011,
	title = {Expressive completeness for {LTL} with modulo counting and group quantifiers},
	volume = {278},
	url = {http://dx.doi.org/10.1016/j.entcs.2011.10.016},
	doi = {10.1016/j.entcs.2011.10.016},
	abstract = {Kamp showed that linear temporal logic is expressively complete for first order logic over words. We give a Gabbay style proof to show that linear temporal logic extended with modulo counting and group quantifiers (introduced by Baziramwabo, McKenzie, Th??rien) is expressively complete for first order logic with modulo counting (introduced by Straubing, Th??rien, Thomas) and group quantifiers (introduced by Barrington, Immerman, Straubing). ?? 2011 Elsevier B.V.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Sreejith, a. V.},
	year = {2011},
	keywords = {linear temporal logic, first order logic, group quantifiers, modulo counting},
	pages = {201--214},
	file = {Sreejith - 2011 - Expressive completeness for LTL with modulo counting and group quantifiers:/home/user/Zotero/storage/XHMZWB2G/Sreejith - 2011 - Expressive completeness for LTL with modulo counting and group quantifiers.pdf:application/pdf}
}

@book{spivak-comprehensive-1979,
	edition = {2},
	title = {A {Comprehensive} {Introduction} to {Differential} {Geometry}},
	publisher = {Publish or Perish},
	author = {Spivak, Mike},
	year = {1979}
}

@inproceedings{smolensky-algebraic-1987,
	title = {Algebraic {Methods} in the {Theory} of {Lower} {Bounds} for {Boolean} {Circuit} {Complexity}},
	booktitle = {{STOC}},
	author = {Smolensky, Roman},
	year = {1987},
	pages = {77--82}
}

@article{skrzypczak-equational-2011,
	title = {Equational theories of profinite structures},
	url = {http://arxiv.org/abs/1111.0476},
	abstract = {In this paper we consider a general way of constructing profinite struc- tures based on a given framework - a countable family of objects and a countable family of recognisers (e.g. formulas). The main theorem states: A subset of a family of recognisable sets is a lattice if and only if it is definable by a family of profinite equations. This result extends Theorem 5.2 from [GGEP08] expressed only for finite words and morphisms to finite monoids. One of the applications of our theorem is the situation where objects are finite relational structures and recognisers are first order sentences. In that setting a simple characterisation of lattices of first order formulas arise.},
	author = {Skrzypczak, Michał},
	month = nov,
	year = {2011},
	file = {Skrzypczak - 2011 - Equational theories of profinite structures:/home/user/Zotero/storage/M8FVFAG6/Skrzypczak - 2011 - Equational theories of profinite structures.pdf:application/pdf}
}

@article{sistla-complexity-1985,
	title = {"{Complexity} of {Propositional} {Temporal} {Logics},"},
	volume = {32},
	journal = {Journal of the ACM},
	author = {Sistla, a P and Clarke, E M},
	year = {1985},
	pages = {733--749},
	file = {Sistla, Clarke - 1985 - Complexity of Propositional Temporal Logics,:/home/user/Zotero/storage/NZJ5SBBV/Sistla, Clarke - 1985 - Complexity of Propositional Temporal Logics,.pdf:application/pdf}
}

@article{sin-spectral-nodate,
	title = {Spectral {Properties} of},
	issn = {9076560350634},
	author = {Sin, Ryoma},
	file = {Sin - Unknown - Spectral Properties of:/home/user/Zotero/storage/7RDAIZ5R/Sin - Unknown - Spectral Properties of.pdf:application/pdf}
}

@book{shevrin-sverdlovsk-1989,
	address = {Sverdlovsk},
	edition = {3},
	title = {The {Sverdlovsk} {Notebook}. {Unsolved} {Problems} of the {Theory} of {Semigroups}},
	editor = {Shevrin, L.N.},
	year = {1989}
}

@book{shapiro-foundations-1991,
	series = {Oxford {Logic} {Guides}},
	title = {Foundations {Without} {Foundationalism}: {A} {Case} for {Second}-{Order} {Logic}},
	isbn = {978-0-19-853391-7},
	url = {http://books.google.de/books?id=btx10Q8GQ7oC},
	publisher = {Oxford University Press, Incorporated},
	author = {Shapiro, S},
	year = {1991}
}

@article{shallit-open-nodate,
	title = {Open {Problems} in {Automata} {Theory} and {Formal} {Languages} {An} {Advertisement}},
	author = {Shallit, Jeffrey},
	keywords = {()},
	pages = {1--50},
	file = {bc4:/home/user/Zotero/storage/WZ4PJNIS/bc4.pdf:application/pdf}
}

@inproceedings{seidl-foundations-2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Foundations of {Software} {Science} and {Computational} {Structures}, 10th {International} {Conference}, {FOSSACS} 2007, {Held} as {Part} of the {Joint} {European} {Conferences} on {Theory} and {Practice} of {Software}, {ETAPS} 2007, {Braga}, {Portugal}, {March} 24-{April} 1, 2007, {Proceedings}},
	volume = {4423},
	isbn = {978-3-540-71388-3},
	booktitle = {{FoSSaCS}},
	publisher = {Springer},
	editor = {Seidl, Helmut},
	year = {2007}
}

@article{segoufin-validating-2002,
	title = {Validating {Streaming} {XML} {Documents}},
	issn = {1581135076},
	url = {http://portal.acm.org/citation.cfm?doid=543613.543622},
	doi = {10.1145/543621.543622},
	abstract = {This paper investigates the on-line validation of streaming XML documents with respect to a DTD, under memory constraints. We first consider validation using constant memory, formalized by a finite-state automaton (FSA). We examine two flavors of the problem, depending on whether or not the XML document is assumed to be well-formed. The main results of the paper provide conditions on the DTDs under which validation of either flavor can be done using an FSA. For DTDs that cannot be validated by an FSA, we investigate two alternatives. The first relaxes the constant memory requirement by allowing a stack bounded in the depth of the XML document, while maintaining the deterministic, one-pass requirement. The second approach consists in refining the DTD to provide additional information that allows validation by an FSA.},
	journal = {Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems - PODS '02},
	author = {Segoufin, Luc and Vianu, Victor},
	year = {2002},
	pages = {53--53},
	file = {Segoufin, Vianu - 2002 - Validating Streaming XML Documents:/home/user/Zotero/storage/7RQPI9JQ/Segoufin, Vianu - 2002 - Validating Streaming XML Documents.pdf:application/pdf}
}

@article{smolensky-methods-1987,
	title = {{METHODS} {IN} {THE} {THEORY} {CIRCUIT} {FOR} {BOOLEAN} {COMPLEXITY}},
	author = {Smolensky, Roman},
	year = {1987},
	pages = {77--82},
	file = {Smolensky - 1987 - METHODS IN THE THEORY CIRCUIT FOR BOOLEAN COMPLEXITY:/home/user/Zotero/storage/IWDDI4C3/Smolensky - 1987 - METHODS IN THE THEORY CIRCUIT FOR BOOLEAN COMPLEXITY.pdf:application/pdf}
}

@inproceedings{smolensky-algebraic-1987-1,
	title = {Algebraic methods in the theory of lower bounds for {Boolean} circuit complexity},
	isbn = {0-89791-221-7},
	url = {http://dblp.uni-trier.de/db/conf/stoc/stoc87.html#Smolensky87},
	doi = {10.1145/28395.28404},
	booktitle = {Proceedings of the nineteenth annual {ACM} conference on {Theory} of computing - {STOC} '87},
	publisher = {ACM Press},
	author = {Smolensky, Roman},
	year = {1987},
	pages = {77--82}
}

@book{siegelmann-neural-nodate,
	title = {neural networks},
	isbn = {978-1-4612-6875-8},
	author = {{Siegelmann}},
	file = {[Hava_T._Siegelmann__(auth.)]_Neural_Networks_and_(BookZZ.org):/home/user/Zotero/storage/E9T9VIZW/[Hava_T._Siegelmann__(auth.)]_Neural_Networks_and_(BookZZ.org).pdf:application/pdf}
}

@article{semenov-presburgerness-1977,
	title = {Presburgerness of predicates regular in two number systems},
	volume = {18},
	url = {http://dx.doi.org/10.1007/BF00967164},
	number = {2},
	journal = {Siberian Mathematical Journal},
	author = {Semenov, A L},
	year = {1977},
	pages = {289--300}
}

@article{jiang-abstraction-2015,
	title = {Abstraction {Selection} in {Model}-{Based} {Reinforcement} {Learning}},
	volume = {37},
	author = {Jiang, Nan},
	year = {2015},
	file = {jiang15:/home/user/Zotero/storage/WAHR8XZK/jiang15.pdf:application/pdf}
}

@article{jernite-fast-2015,
	title = {A {Fast} {Variational} {Approach} for {Learning} {Markov} {Random} {Field} {Language} {Models}},
	volume = {37},
	author = {Jernite, Yacine and Street, Mercer and York, New and Rush, Alexander M},
	year = {2015},
	file = {jernite15:/home/user/Zotero/storage/DCGFE5JA/jernite15.pdf:application/pdf}
}

@article{iwata-warped-2012-1,
	title = {Warped {Mixtures} for {Nonparametric} {Cluster} {Shapes}},
	url = {http://arxiv.org/abs/1206.1846},
	abstract = {A mixture of Gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters. To produce more appropriate clusterings, we introduce a model which warps a latent mixture of Gaussians to produce nonparametric cluster shapes. The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data. The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets.},
	author = {Iwata, Tomoharu and Duvenaud, David and Ghahramani, Zoubin},
	year = {2012},
	pages = {10--10},
	file = {Iwata, Duvenaud, Ghahramani - 2012 - Warped Mixtures for Nonparametric Cluster Shapes:/home/user/Zotero/storage/2VXIHJ7S/Iwata, Duvenaud, Ghahramani - 2012 - Warped Mixtures for Nonparametric Cluster Shapes.pdf:application/pdf}
}

@article{janzing-telling-2015,
	title = {Telling {Cause} from {Effect} in {Deterministic} {Linear} {Dynamical} {Systems}},
	volume = {37},
	author = {Janzing, Dominik and Sch, Bernhard},
	year = {2015},
	file = {shajarisales15:/home/user/Zotero/storage/Q87SU3ZM/shajarisales15.pdf:application/pdf}
}

@book{hyvarinen-natural-2009,
	title = {Natural {Image} {Statistics}},
	isbn = {978-1-84882-490-4},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2941908&tool=pmcentrez&rendertype=abstract},
	author = {{Hyvarinen} and {Hurri} and {Hoyer}},
	year = {2009},
	file = {nis_preprintFeb2009:/home/user/Zotero/storage/H57SU24S/nis_preprintFeb2009.pdf:application/pdf}
}

@article{iscen-memory-nodate,
	title = {Memory vectors for similarity search in high-dimensional spaces},
	author = {Iscen, Ahmet and Furon, Teddy and Gripon, Vincent and Rabbat, Michael},
	file = {1412.3328v4:/home/user/Zotero/storage/52G9VZJI/1412.3328v4.pdf:application/pdf}
}

@article{irvine-ebb-nodate,
	title = {The {Ebb} and {Flow} of {Deep} {Learning} : a {Theory} of {Local} {Learning}},
	author = {Irvine, Irvine},
	keywords = {neural networks, machine learning, deep learning, agation, backprop-, hebbian learning, learning rules, supervised learning, unsupervised learning},
	pages = {1--40},
	file = {1506.06472v1:/home/user/Zotero/storage/IJMINXC3/1506.06472v1.pdf:application/pdf}
}

@article{huang-new-2015,
	title = {A {New} {Input} {Method} for {Human} {Translators} : {Integrating} {Machine} {Translation} {Effectively} and {Imperceptibly}},
	number = {Ijcai},
	author = {Huang, Guoping and Zhang, Jiajun and Zhou, Yu and Zong, Chengqing},
	year = {2015},
	keywords = {Technical Papers — Multidisciplinary Topics and Ap},
	pages = {1163--1169},
	file = {IJCAI15-168:/home/user/Zotero/storage/X74IKCGP/IJCAI15-168.pdf:application/pdf}
}

@article{hong-decoupled-nodate,
	title = {Decoupled {Deep} {Neural} {Network} for {Semi}-supervised {Semantic} {Segmentation}},
	author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
	pages = {1--9},
	file = {1506.04924v2:/home/user/Zotero/storage/G28S4UIQ/1506.04924v2.pdf:application/pdf}
}

@article{huang-convolutional-nodate,
	title = {Convolutional {Dictionary} {Learning} through {Tensor} {Factorization}},
	author = {Huang, Furong and Anandkumar, Animashree},
	pages = {1--12},
	file = {1506.03509v1:/home/user/Zotero/storage/T2U7RJWD/1506.03509v1.pdf:application/pdf}
}

@article{hosseini-manifold-nodate,
	title = {Manifold {Optimization} for {Gaussian} {Mixture} {Models}},
	author = {Hosseini, Reshad},
	file = {1506.07677v1:/home/user/Zotero/storage/W6TRKQSF/1506.07677v1.pdf:application/pdf}
}

@article{hong-decoupled-nodate-1,
	title = {Decoupled {Deep} {Neural} {Network} for {Semi}-supervised {Semantic} {Segmentation}},
	author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
	pages = {1--9},
	file = {1506.04924v1:/home/user/Zotero/storage/PFT2FKMG/1506.04924v1.pdf:application/pdf}
}

@article{hinton-deep-2012,
	title = {Deep {Neural} {Networks} for {Acoustic} {Modeling} in {Speech} {Recognition}},
	volume = {29},
	doi = {10.1109/MSP.2012.2205597},
	abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
	number = {november},
	journal = {Signal Processing Magazine, IEEE},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingbury, Brian},
	year = {2012},
	pages = {82--97},
	file = {DNN-2012-proof:/home/user/Zotero/storage/D95WHD8C/DNN-2012-proof.pdf:application/pdf}
}

@article{hinton-learning-2007,
	title = {Learning multiple layers of representation},
	volume = {11},
	issn = {1364-6613 (Print)\r1364-6613 (Linking)},
	doi = {10.1016/j.tics.2007.09.004},
	abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time. ?? 2007 Elsevier Ltd. All rights reserved.},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Hinton, Geoffrey E.},
	year = {2007},
	pages = {428--434},
	file = {tics:/home/user/Zotero/storage/JUJ4K847/tics.pdf:application/pdf}
}

@article{hern-learning-nodate-1,
	title = {Learning the {Semantics} of {Discrete} {Random} {Variables} : {Ordinal} or {Categorical} ?},
	author = {Hern, Daniel and Lloyd, James Robert},
	pages = {1--5},
	file = {14:/home/user/Zotero/storage/A923VGXN/14.pdf:application/pdf}
}

@article{herbrich-machine-nodate,
	title = {Machine learning in bioinformatics {Machine} learning in bioinformatics},
	issn = {9780470397428 047039742X 0470397411  9780470397411},
	author = {Herbrich, Ralf},
	pages = {1--16},
	file = {herbrich:/home/user/Zotero/storage/C42CCF7H/herbrich.pdf:application/pdf}
}

@article{hennig-probabilistic-2014,
	title = {Probabilistic {Interpretation} of {Linear} {Solvers}},
	volume = {25},
	url = {http://arxiv.org/abs/1402.2058},
	doi = {10.1137/140955501},
	abstract = {This manuscript proposes a probabilistic framework for algorithms that iteratively solve unconstrained linear problems Bx = b with positive definite B for x. The goal is to retain, at any time, instead of a point estimate, a Gaussian posterior belief over the elements of the inverse of B. Extending recent probabilistic interpretations of the secant family of quasi-Newton numerical optimization algorithms, and combining them with properties of the conjugate gradient algorithm, leads to uncertainty-calibrated methods that have very limited cost overhead over conjugate gradients, a self-contained novel interpretation of the quasi-Newton and conjugate gradient algorithms, and a foundation for new nonlinear optimization methods.},
	number = {1},
	author = {Hennig, Philipp},
	year = {2014},
	keywords = {ams subject classifications, 49m15, 60g15, 65k05, conjugate gradient, gaussian inference, linear programming, quasi-newton methods},
	pages = {234--260},
	file = {Hennig - 2014 - Probabilistic Interpretation of Linear Solvers:/home/user/Zotero/storage/ZNP8TUQK/Hennig - 2014 - Probabilistic Interpretation of Linear Solvers.pdf:application/pdf}
}

@article{heinrich-smooth-2015,
	title = {Smooth {UCT} {Search} in {Computer} {Poker}},
	number = {Ijcai},
	author = {Heinrich, Johannes and Silver, David},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {554--560},
	file = {IJCAI15-084:/home/user/Zotero/storage/8QWRVRCZ/IJCAI15-084.pdf:application/pdf}
}

@article{hennig-quasi-newton-2012,
	title = {Quasi-{Newton} {Methods}: {A} {New} {Direction}},
	issn = {978-1-4503-1285-1},
	journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
	author = {Hennig, Philipp and Kiefel, Martin},
	year = {2012},
	pages = {25--32},
	file = {Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction:/home/user/Zotero/storage/V6AKP3U6/Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction.pdf:application/pdf}
}

@article{hennig-quasi-newton-2012-1,
	title = {Quasi-{Newton} {Methods}: {A} {New} {Direction}},
	issn = {978-1-4503-1285-1},
	journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
	author = {Hennig, Philipp and Kiefel, Martin},
	year = {2012},
	pages = {25--32},
	file = {Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction:/home/user/Zotero/storage/4GFUPEG5/Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction.pdf:application/pdf}
}

@article{hennig-fast-2013,
	title = {Fast probabilistic optimization from noisy gradients},
	volume = {28},
	url = {http://jmlr.org/proceedings/papers/v28/hennig13.html},
	number = {1},
	journal = {Icml},
	author = {Hennig, Philipp},
	year = {2013},
	file = {Hennig - 2013 - Fast probabilistic optimization from noisy gradients:/home/user/Zotero/storage/AZWAQUJW/Hennig - 2013 - Fast probabilistic optimization from noisy gradients.pdf:application/pdf}
}

@article{hauberg-random-nodate,
	title = {A {Random} {Riemannian} {Metric} for {Probabilistic} {Shortest}-{Path} {Tractography}},
	author = {Hauberg, Søren and Schober, Michael and Liptrot, Matthew and Hennig, Philipp},
	pages = {1--8},
	file = {Haubergetal_MICCAI2015:/home/user/Zotero/storage/3HGG6WTD/Haubergetal_MICCAI2015.pdf:application/pdf}
}

@article{gu-neural-2015,
	title = {Neural {Adaptive} {Sequential} {Monte} {Carlo}},
	author = {Gu, Shixiang and Turner, Richard E and Carlo, Sequential Monte},
	year = {2015},
	pages = {1--10},
	file = {1506.03338v2:/home/user/Zotero/storage/2HW3I223/1506.03338v2.pdf:application/pdf}
}

@article{grosse-exploiting-2012,
	title = {Exploiting compositionality to explore a large space of model structures},
	issn = {9780974903989},
	abstract = {The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and U.S. Senate votes, all using exactly the same code.},
	journal = {Conference on Uncertainty in Artificial Intelligence},
	author = {Grosse, Roger B and Salakhutdinov, Ruslan and Freeman, William T and Tenenbaum, Joshua B},
	year = {2012},
	file = {uai2012-matrix:/home/user/Zotero/storage/WAGSJ869/uai2012-matrix.pdf:application/pdf}
}

@article{gretton-kernel-2015,
	title = {Kernel methods for hypothesis testing and inference},
	author = {Gretton, Arthur and Unit, Gatsby},
	year = {2015},
	file = {intro:/home/user/Zotero/storage/F6PTIVFR/intro.pdf:application/pdf}
}

@article{gretton-lecture-2014,
	title = {Lecture 2 : {Mappings} of {Probabilities} to {RKHS} and {Applications}},
	author = {Gretton, Arthur and Unit, Gatsby},
	year = {2014},
	file = {part_2:/home/user/Zotero/storage/MVBB389G/part_2.pdf:application/pdf}
}

@article{gretton-lecture-2014-1,
	title = {Lecture 3 : {Dependence} measures using {RKHS} embeddings},
	author = {Gretton, Arthur},
	year = {2014},
	file = {part_3:/home/user/Zotero/storage/56S9ZIW6/part_3.pdf:application/pdf}
}

@article{greco-structural-2015,
	title = {Structural {Tractability} of {Shapley} and {Banzhaf} {Values} in {Allocation} {Games}},
	number = {Ijcai},
	author = {Greco, Gianluigi and Lupia, Francesco and Scarcello, Francesco},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {547--553},
	file = {IJCAI15-083:/home/user/Zotero/storage/7FNM2EM5/IJCAI15-083.pdf:application/pdf}
}

@article{grandi-equilibrium-2015,
	title = {Equilibrium {Refinement} through {Negotiation} in {Binary} {Voting}},
	number = {Ijcai},
	author = {Grandi, Umberto and Grossi, Davide and Turrini, Paolo},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {540--546},
	file = {IJCAI15-082:/home/user/Zotero/storage/IUUQ2HZA/IJCAI15-082.pdf:application/pdf}
}

@article{gormley-factor-based-nodate-1,
	title = {Factor-based {Compositional} {Embedding} {Models}},
	number = {Table 1},
	author = {Gormley, Matthew R and Dredze, Mark},
	pages = {1--5},
	file = {5:/home/user/Zotero/storage/H79NM8QX/5.pdf:application/pdf}
}

@article{gopnik-theory-2004,
	title = {A theory of causal learning in children: causal maps and {Bayes} nets.},
	volume = {111},
	issn = {0033-295X},
	doi = {10.1037/0033-295X.111.1.3},
	abstract = {The authors outline a cognitive and computational account of causal learning in children. They propose that children use specialized cognitive systems that allow them to recover an accurate "causal map" of the world: an abstract, coherent, learned representation of the causal relations among events. This kind of knowledge can be perspicuously understood in terms of the formalism of directed graphical causal models, or Bayes nets. Children's causal learning and inference may involve computations similar to those for learning causal Bayes nets and for predicting with them. Experimental results suggest that 2- to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism.},
	journal = {Psychological review},
	author = {Gopnik, Alison and Glymour, Clark and Sobel, David M and Schulz, Laura E and Kushnir, Tamar and Danks, David},
	year = {2004},
	pages = {3--32},
	file = {Gopnik et al. - 2004 - A theory of causal learning in children causal maps and Bayes nets:/home/user/Zotero/storage/3Q5W32BH/Gopnik et al. - 2004 - A theory of causal learning in children causal maps and Bayes nets.pdf:application/pdf}
}

@article{google-show-nodate,
	title = {Show and {Tell}: {A} {Neural} {Image} {Caption} {Generator}},
	abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
	author = {Google, Oriol Vinyals and Google, Alexander Toshev and Google, Samy Bengio and Google, Dumitru Erhan},
	file = {1411.4555v2:/home/user/Zotero/storage/MI38UUF6/1411.4555v2.pdf:application/pdf}
}

@article{giryes-deep-nodate,
	title = {Deep {Neural} {Networks} with {Random} {Gaussian} {Weights} : {A} {Universal} {Classification} {Strategy} ?},
	author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M},
	pages = {1--9},
	file = {1504.08291v3:/home/user/Zotero/storage/W4P9VA4X/1504.08291v3.pdf:application/pdf}
}

@article{ghahramani-automatic-2015,
	title = {The {Automatic} {Statistician} and {Future} {Directions} in {Probabilistic} {Machine} {Learning}},
	author = {Ghahramani, Zoubin},
	year = {2015},
	file = {mlss15future:/home/user/Zotero/storage/T2E8ZT58/mlss15future.pdf:application/pdf}
}

@article{ghahramani-probabilistic-2013,
	title = {Probabilistic {Modelling} and {Bayesian} {Inference}},
	author = {Ghahramani, Zoubin},
	year = {2013},
	file = {lect1bayes:/home/user/Zotero/storage/MRGGGM42/lect1bayes.pdf:application/pdf}
}

@article{geiger-estimating-2013,
	title = {Estimating {Causal} {Effects} by {Bounding} {Confounding}},
	author = {Geiger, Philipp and Janzing, Dominik and Sch, Bernhard},
	year = {2013},
	file = {paper303:/home/user/Zotero/storage/R96I3ZUU/paper303.pdf:application/pdf}
}

@article{geiger-algorithmic-2015,
	title = {Algorithmic {Exam} {Generation}},
	number = {Ijcai},
	author = {Geiger, Omer and Markovitch, Shaul and Israel, Technion},
	year = {2015},
	keywords = {Technical Papers — Multidisciplinary Topics and Ap},
	pages = {1149--1155},
	file = {IJCAI15-166:/home/user/Zotero/storage/J9PV3HRH/IJCAI15-166.pdf:application/pdf}
}

@article{geurts-reasoning-2003-1,
	title = {Reasoning with quantifiers},
	volume = {86},
	issn = {0010-0277, Print},
	doi = {10.1016/S0010-0277(02)00180-4},
	abstract = {In the semantics of natural language, quantification may have received more attention than any other subject, and one of the main topics in psychological studies on deductive reasoning is syllogistic inference, which is just a restricted form of reasoning with quantifiers. But thus far the semantical and psychological enterprises have remained disconnected. This paper aims to show how our understanding of syllogistic reasoning may benefit from semantical research on quantification. I present a very simple logic that pivots on the monotonicity properties of quantified statements - properties that are known to be crucial not only to quantification but to a much wider range of semantical phenomena. This logic is shown to account for the experimental evidence available in the literature as well as for the data from a new experiment with cardinal quantifiers ("at least n" and "at most n"), which cannot be explained by any other theory of syllogistic reasoning. ?? 2002 Elsevier Science B.V. All rights reserved.},
	journal = {Cognition},
	author = {Geurts, Bart},
	year = {2003},
	keywords = {generalized quantifiers, Quantification, Semantics, syllogistic reasoning},
	pages = {223--251},
	file = {Geurts - 2003 - Reasoning with quantifiers:/home/user/Zotero/storage/EDWP9T7B/Geurts - 2003 - Reasoning with quantifiers.pdf:application/pdf}
}

@article{gandomi-bat-2013,
	title = {Bat algorithm for constrained optimization tasks},
	volume = {22},
	issn = {0941-0643},
	doi = {10.1007/s00521-012-1028-9},
	journal = {Neural Computing and Applications},
	author = {Gandomi, Amir Hossein and Yang, Xin She and Alavi, Amir Hossein and Talatahari, Siamak},
	year = {2013},
	keywords = {Bat algorithm, Constraint optimization, Metaheuristic algorithm},
	pages = {1239--1255}
}

@article{friedman-being-2003,
	title = {Being {Bayesian} about network structure. {A} {Bayesian} approach to structure discovery in {Bayesian} networks},
	volume = {50},
	issn = {9781605589428},
	doi = {10.1023/A:1020249912095},
	abstract = {In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orders rather than over network structures. The space of orders is smaller and more regular than the space of structures, and has much a smoother posterior "landscape". We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.},
	journal = {Machine Learning},
	author = {Friedman, Nir and Koller, Daphne},
	year = {2003},
	keywords = {Bayesian model averaging, Bayesian networks, MCMC, Structure learning},
	pages = {95--125},
	file = {Friedman+Koller\:MLJ03-IMPORT:/home/user/Zotero/storage/IRRPJ4HJ/Friedman+KollerMLJ03-IMPORT.pdf:application/pdf}
}

@inproceedings{et-al.-generalizing-1999,
	title = {Generalizing {Refinement} {Operators} to {Learn} {Prenex} {Conjunctive} {Normal} {Forms}},
	booktitle = {Inductive {Logic} {Programming}. {Proceedings} of {ILP} 99},
	publisher = {Springer},
	author = {et al., Shan-Hwei Nienhuys-Cheng},
	editor = {D??eroski, Sa??o and Flach, Peter},
	year = {1999},
	keywords = {ILP},
	pages = {245--245}
}

@incollection{et-al.-discovery-1998,
	title = {Discovery of pharmacophores using {Inductive} {Logic} {Programming}},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	author = {et al., P Finn},
	editor = {et al., U Fayyad},
	year = {1998},
	keywords = {ILP}
}

@inproceedings{et-al.-sufficient-1997,
	title = {On a sufficient {Condition} for the {Existence} of {Most} {Specific} {Hypothesis} in {Progol}},
	booktitle = {Inductive {Logic} {Programming}. {Proceedings} of {ILP} 97},
	publisher = {Springer},
	author = {et al., Koichi Furukawa},
	editor = {Lavrac, Nada and D??eroski, Sa??o},
	year = {1997},
	keywords = {ILP},
	pages = {157--157}
}

@article{elkind-gibbard-2015,
	title = {Gibbard – {Satterthwaite} {Games}},
	number = {Ijcai},
	author = {Elkind, Edith and Grandi, Umberto and Rossi, Francesca},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {533--539},
	file = {IJCAI15-081:/home/user/Zotero/storage/T2IB4FE2/IJCAI15-081.pdf:application/pdf}
}

@article{gal-dropout-nodate,
	title = {Dropout as a {Bayesian} {Approximation} : {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	author = {Gal, Yarin},
	pages = {1--10},
	file = {1506.02142v1:/home/user/Zotero/storage/DQWPV3W2/1506.02142v1.pdf:application/pdf}
}

@article{forster-neural-nodate,
	title = {Neural {Simpletrons} – {Minimalistic} {Probabilistic} {Networks} for {Learning} {With} {Few} {Labels}},
	number = {i},
	author = {Forster, Dennis and Sheikh, Abdul-saboor and Lücke, Jörg},
	pages = {1--15},
	file = {1506.08448v1:/home/user/Zotero/storage/V6RHMKVX/1506.08448v1.pdf:application/pdf}
}

@article{elberfeld-algorithmic-2012-1,
	title = {Algorithmic meta theorems for circuit classes of constant and logarithmic depth},
	volume = {128},
	issn = {9783939897354},
	url = {http://hal.archives-ouvertes.fr/hal-00678176/},
	doi = {10.4230/LIPIcs.STACS.2012.66},
	number = {128},
	journal = {… on Theoretical Aspects of …},
	author = {Elberfeld, Michael and Jakoby, a and Tantau, T},
	year = {2012},
	keywords = {circuit complexity, monadic second-order logic, tree, algorithmic meta theorem},
	pages = {1--37},
	file = {Elberfeld, Jakoby, Tantau - 2012 - Algorithmic meta theorems for circuit classes of constant and logarithmic depth:/home/user/Zotero/storage/M5D79Z6P/Elberfeld, Jakoby, Tantau - 2012 - Algorithmic meta theorems for circuit classes of constant and logarithmic depth.pdf:application/pdf}
}

@article{duvenaud-avoiding-2014,
	title = {Avoiding pathologies in very deep networks},
	url = {http://arxiv.org/abs/1402.5836},
	abstract = {Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.},
	author = {Duvenaud, David and Rippel, Oren and Adams, Ryan P and Ghahramani, Zoubin},
	year = {2014},
	pages = {9--9},
	file = {Duvenaud et al. - 2014 - Avoiding pathologies in very deep networks:/home/user/Zotero/storage/7HM7DBA2/Duvenaud et al. - 2014 - Avoiding pathologies in very deep networks.pdf:application/pdf}
}

@article{duvenaud-additive-2011,
	title = {Additive {Gaussian} {Processes}},
	issn = {9781618395993},
	url = {http://eprints.pascal-network.org/archive/00008445/},
	abstract = {We introduce a Gaussian process model of functions which are additive.  An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels.  Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL).  We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension.  The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.},
	author = {Duvenaud, David and Nickisch, Hannes and Rasmussen, Carl Edward},
	year = {2011},
	keywords = {Learning/Statistics \& Optimisation},
	pages = {1--9},
	file = {Duvenaud, Nickisch, Rasmussen - 2011 - Additive Gaussian Processes:/home/user/Zotero/storage/57BEJBSA/Duvenaud, Nickisch, Rasmussen - 2011 - Additive Gaussian Processes.pdf:application/pdf}
}

@article{duvenaud-introduction-nodate,
	title = {Introduction to probabilistic programming},
	author = {Duvenaud, David and Lloyd, James},
	file = {Duvenaud, Lloyd - Unknown - Introduction to probabilistic programming:/home/user/Zotero/storage/UCAIJIF5/Duvenaud, Lloyd - Unknown - Introduction to probabilistic programming.pdf:application/pdf}
}

@article{durkota-optimal-2015,
	title = {Optimal {Network} {Security} {Hardening} {Using} {Attack} {Graph} {Games}},
	number = {Ijcai},
	author = {Durkota, Karel and Lis, Viliam and Boˇ, Branislav},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {526--532},
	file = {IJCAI15-080:/home/user/Zotero/storage/BNNNHZJK/IJCAI15-080.pdf:application/pdf}
}

@article{dinuzzo-representer-2012,
	title = {The representer theorem for {Hilbert} spaces: a necessary and sufficient condition.},
	issn = {9781627480031},
	url = {https://papers.nips.cc/paper/4841-the-representer-theorem-for-hilbert-spaces-a-necessary-and-sufficient-condition.pdf\nhttp://arxiv.org/abs/1205.1928},
	abstract = {A family of regularization functionals is said to admit a linear repre- senter theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. In this paper, we show that a general class of extended real-valued regularization functionals admits a linear rep- resenter theorem if and only if the regularizer is a non-decreasing function of the norm. In this way, we extend a recent characterization stating that such condition is necessary and sufficient for differentiable regularizers.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dinuzzo, Francesco and Schölkopf, B},
	year = {2012},
	pages = {1--8},
	file = {Dinuzzo, Schölkopf - 2012 - The representer theorem for Hilbert spaces a necessary and sufficient condition:/home/user/Zotero/storage/9NTI2JGH/Dinuzzo, Schölkopf - 2012 - The representer theorem for Hilbert spaces a necessary and sufficient condition.pdf:application/pdf}
}

@article{drummond-sat-2015,
	title = {{SAT} {Is} an {Effective} and {Complete} {Method} for {Solving} {Stable} {Matching} {Problems} with {Couples}},
	number = {Ijcai},
	author = {Drummond, Joanna and Perrault, Andrew and Bacchus, Fahiem},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {518--525},
	file = {IJCAI15-079:/home/user/Zotero/storage/5QIDKVP9/IJCAI15-079.pdf:application/pdf}
}

@article{ding-comprehensive-2015,
	title = {A {Comprehensive} {Survey} on {Pose}-{Invariant} {Face} {Recognition}},
	number = {2014},
	author = {Ding, Changxing and Tao, Dacheng and Systems, Intelligent and Technology, Information},
	year = {2015},
	keywords = {face synthesis, multi-view learning, pose-invariant face recognition, pose-robust feature},
	pages = {1--40},
	file = {1502.04383v2:/home/user/Zotero/storage/MWVW3UNF/1502.04383v2.pdf:application/pdf}
}

@article{deng-new-2013-1,
	title = {New {Types} of {Deep} {Neural} {Network} {Learning} for {Speech} {Recognition} and {Related} {Applications} : an {Overview}},
	issn = {9781479903566},
	author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
	year = {2013},
	pages = {8599--8603},
	file = {overview13:/home/user/Zotero/storage/E8TI4AMX/overview13.pdf:application/pdf}
}

@article{deisenroth-distributed-2015,
	title = {Distributed {Gaussian} {Processes}},
	volume = {37},
	journal = {Journal of Machine Learning Research},
	author = {Deisenroth, Marc Peter},
	year = {2015},
	file = {deisenroth15:/home/user/Zotero/storage/FC5RITNG/deisenroth15.pdf:application/pdf}
}

@article{darrell-constrained-nodate,
	title = {Constrained {Convolutional} {Neural} {Networks} for {Weakly} {Supervised} {Segmentation}},
	author = {Darrell, Trevor},
	file = {1506.03648v1:/home/user/Zotero/storage/7D2ISIDJ/1506.03648v1.pdf:application/pdf}
}

@article{de-rooij-follow-2014,
	title = {Follow the leader if you can, hedge if you must},
	volume = {15},
	issn = {1532-4435},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84901638727&partnerID=tZOtx3y1},
	abstract = {Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains. © 2014 Steven de Rooij, Tim van Erven, Peter D. Grnwald and Wouter M.},
	journal = {Journal of Machine Learning Research},
	author = {De Rooij, Steven and Van Erven, Tim and Grünwald, Peter D. and Koolen, Wouter M.},
	year = {2014},
	keywords = {Hedge, Learning rate, Mixability, Online learning, Prediction with expert advice},
	pages = {1281--1316},
	file = {De Rooij et al. - 2014 - Follow the leader if you can, hedge if you must:/home/user/Zotero/storage/4NMZ4CWQ/De Rooij et al. - 2014 - Follow the leader if you can, hedge if you must.pdf:application/pdf}
}

@article{datta-influence-2015,
	title = {Influence in {Classification} via {Cooperative} {Game} {Theory}},
	number = {IJCAI},
	author = {Datta, Amit and Datta, Anupam and Procaccia, Ariel D and Zick, Yair},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {511--517},
	file = {IJCAI15-078:/home/user/Zotero/storage/W5SMQSQR/IJCAI15-078.pdf:application/pdf}
}

@article{dann-policy-2014,
	title = {Policy {Evaluation} with {Temporal} {Differences}: {A} {Survey} and {Comparison}},
	volume = {15},
	issn = {1532-4435},
	url = {http://jmlr.org/papers/v15/dann14a.html},
	abstract = {Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches. This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSTD, LSPE, FPKF, the residual- gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance.},
	journal = {Journal of Machine Learning Research},
	author = {Dann, Christoph and Neumann, Gerhard and Peters, Jan},
	year = {2014},
	keywords = {Journal Presentation Track},
	pages = {809--883},
	file = {10635-46220-1-PB:/home/user/Zotero/storage/4NMFFEUZ/10635-46220-1-PB.pdf:application/pdf}
}

@article{dann-policy-2014-1,
	title = {Policy {Evaluation} with {Temporal} {Differences}: {A} {Survey} and {Comparison}},
	volume = {15},
	issn = {1532-4435},
	url = {http://jmlr.org/papers/v15/dann14a.html},
	abstract = {Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches. This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSTD, LSPE, FPKF, the residual- gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance.},
	journal = {Journal of Machine Learning Research},
	author = {Dann, Christoph and Neumann, Gerhard and Peters, Jan},
	year = {2014},
	keywords = {policy evaluation, reinforce-, temporal differences, value function estimation},
	pages = {809--883},
	file = {dann14a:/home/user/Zotero/storage/UZD84XE9/dann14a.pdf:application/pdf}
}

@techreport{daelemans-timbl:-2007,
	title = {{TiMBL}: {Tilburg} {Memory}-{Based} {Learner}. {Reference} {Guide}},
	author = {Daelemans, Walter and Zavrel, Jakub and van der Sloot, Ko and van den Bosch, Antal},
	year = {2007}
}

@inproceedings{d??eroski-experiments-1999,
	title = {Experiments in {Predicting} {Biodegradability} with {Nonmonotonic} {Inductive} {Learning}},
	booktitle = {Inductive {Logic} {Programming}. {Proceedings} of {ILP} 99},
	publisher = {Springer},
	author = {D??eroski, Sa??o},
	editor = {D??eroski, Sa??o and Flach, Peter},
	year = {1999},
	keywords = {ILP},
	pages = {80--80}
}

@article{czumaj-approximate-2015,
	title = {Approximate {Nash} {Equilibria} with {Near} {Optimal} {Social} {Welfare} ∗},
	number = {Ijcai},
	author = {Czumaj, Artur and Fasoulakis, Michail and Jurdzi, Marcin},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {504--510},
	file = {IJCAI15-077:/home/user/Zotero/storage/2HP7U5B5/IJCAI15-077.pdf:application/pdf}
}

@article{cortes-structural-2015,
	title = {Structural {Maxent} {Models}},
	volume = {37},
	journal = {Journal of Machine Learning Research},
	author = {Cortes, Corinna and York, New and Com, Usyed Google},
	year = {2015},
	file = {cortes15:/home/user/Zotero/storage/4U49A5ZD/cortes15.pdf:application/pdf}
}

@article{cohen-harmonic-2015,
	title = {Harmonic {Exponential} {Families} on {Manifolds}},
	volume = {37},
	journal = {Journal of Machine Learning Research},
	author = {Cohen, Taco S and Welling, Max and Nl, M Welling U V a},
	year = {2015},
	file = {cohenb15:/home/user/Zotero/storage/9G2HQTBW/cohenb15.pdf:application/pdf}
}

@inproceedings{cussens-morphosyntactic-1999,
	title = {Morphosyntactic {Tagging} of {Slovene} using {Progol}},
	booktitle = {Inductive {Logic} {Programming}. {Proceedings} of {ILP} 99},
	publisher = {Springer},
	author = {Cussens, J},
	editor = {D??eroski, Sa??o and Flach, Peter},
	year = {1999},
	keywords = {ILP},
	pages = {68--68}
}

@inproceedings{courville-spike-2011,
	title = {A spike and slab restricted {Boltzmann} machine},
	volume = {15},
	booktitle = {{AISTATS}},
	author = {Courville, Ac},
	year = {2011},
	pages = {233--241},
	file = {courville11a-IMPORT:/home/user/Zotero/storage/CZ92UKX4/courville11a-IMPORT.pdf:application/pdf}
}

@article{cole-simple-2015,
	title = {A {Simple} {Spectral} {Algorithm} for {Recovering} {Planted} {Partitions}},
	url = {http://arxiv.org/abs/1503.00423},
	abstract = {In this paper, we consider the planted partition model, in which \$n = ks\$ vertices of a random graph are partitioned into \$k\$ "clusters," each of size \$s\$. Edges between vertices in the same cluster and different clusters are included with constant probability \$p\$ and \$q\$, respectively (where \$0 \le q {\textless} p \le 1\$). We give an efficient algorithm that, with high probability, recovers the clustering as long as the cluster sizes are are least \$\Omega(\sqrt{n})\$. Our algorithm is based on projecting the graph's adjacency matrix onto the space spanned by its largest eigenvalues and using the result to recover one cluster at a time. While certainly not the first to use this approach, our algorithm has the advantage of being simple, and we employ a novel technique to prove its correctness.},
	author = {Cole, Sam and Friedland, Shmuel and Reyzin, Lev},
	year = {2015},
	pages = {2009--2016},
	file = {LevReyzinCV:/home/user/Zotero/storage/CZJHV4X5/LevReyzinCV.pdf:application/pdf}
}

@article{clark-training-2015,
	title = {Training {Deep} {Convolutional} {Neural} {Networks} to {Play} {Go}},
	journal = {Journal of Machine Learning Research},
	author = {Clark, Christopher and Org, Chrisc Allenai and Ed, a Storkey and Uk, a C},
	year = {2015},
	file = {clark15:/home/user/Zotero/storage/TRCUC4CK/clark15.pdf:application/pdf}
}

@article{claassen-logical-2011,
	title = {A {Logical} {Characterization} of {Constraint}-{Based} {Causal} {Discovery}},
	issn = {978-0-9749039-7-2},
	journal = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence},
	author = {Claassen, Tom and Heskes, Tom},
	year = {2011},
	file = {UAI2011_LCCausD:/home/user/Zotero/storage/NQAFD7F9/UAI2011_LCCausD.pdf:application/pdf}
}

@article{claassen-learning-2000,
	title = {Learning {Sparse} {Causal} {Models} is not {NP}-hard},
	author = {Claassen, Tom and Mooij, Joris M and Heskes, Tom},
	year = {2000},
	file = {UAI2013_NPHard_final:/home/user/Zotero/storage/4NKDUMA4/UAI2013_NPHard_final.pdf:application/pdf}
}

@article{chen-joint-2015,
	title = {Joint {Learning} of {Character} and {Word} {Embeddings}},
	number = {Ijcai},
	author = {Chen, Xinxiong and Xu, Lei and Liu, Zhiyuan and Sun, Maosong and Luan, Huanbo},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1236--1242},
	file = {IJCAI15-178:/home/user/Zotero/storage/FPR6UFR4/IJCAI15-178.pdf:application/pdf}
}

@article{chen-compressing-2015,
	title = {Compressing {Neural} {Networks} with the {Hashing} {Trick}},
	volume = {37},
	author = {Chen, Wenlin and Edu, J Wilson Wustl and Cse, Chen and Edu, Wustl},
	year = {2015},
	file = {chenc15:/home/user/Zotero/storage/S37AR76F/chenc15.pdf:application/pdf}
}

@article{chen-learning-2014,
	title = {Learning {Deep} {Structured} {Models}},
	volume = {37},
	url = {http://arxiv.org/abs/1407.2538},
	abstract = {In recent years the performance of deep learning algorithms has been demonstrated in a variety of application domains. The goal of this paper is to enrich deep learning to be able to predict a set of random variables while taking into account their dependencies. Towards this goal, we propose an efficient algorithm that is able to learn structured models with non-linear functions. We demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from noisy images, and show that by jointly learning multilayer perceptrons and pairwise features, significant gains in performance can be obtained.},
	author = {Chen, Liang-Chieh and Schwing, Alexander G. and Yuille, Alan L. and Urtasun, Raquel},
	year = {2014},
	pages = {10--10},
	file = {chenb15:/home/user/Zotero/storage/H8SS2UDW/chenb15.pdf:application/pdf}
}

@article{chen-fast-nodate-1,
	title = {A {Fast} and {Accurate} {Dependency} {Parser} using {Neural} {Networks}},
	abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2\% improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2\% unlabeled attachment score on the English Penn Treebank.},
	number = {i},
	author = {Chen, Danqi and Manning, Christopher D},
	file = {Chen, Manning - Unknown - A Fast and Accurate Dependency Parser using Neural Networks:/home/user/Zotero/storage/B3PERVH3/Chen, Manning - Unknown - A Fast and Accurate Dependency Parser using Neural Networks.pdf:application/pdf}
}

@article{ciresan-deep-2012,
	title = {Deep {Neural} {Networks} {Segment} {Neuronal} {Membranes} in {Electron} {Microscopy} {Images}},
	issn = {9781627480031},
	url = {https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
	abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efﬁciently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artiﬁcial neural network as a pixel classiﬁer. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classiﬁer is trained by plain gradient descent on a 512  512  30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-speciﬁc postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
	journal = {Nips},
	author = {Ciresan, Dc and Giusti, Alessandro and Gambardella, Lm and Schmidhuber, J},
	year = {2012},
	pages = {1--9},
	file = {Ciresan et al. - 2012 - Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images:/home/user/Zotero/storage/2X2JC9M2/Ciresan et al. - 2012 - Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images.pdf:application/pdf}
}

@article{christoffel-convex-2015,
	title = {Convex {Formulation} for {Learning} from {Positive} and {Unlabeled} {Data}},
	journal = {Journal of Machine Learning Research},
	author = {Christoffel, Marthinus},
	year = {2015},
	file = {plessis15:/home/user/Zotero/storage/Q8FPN2V6/plessis15.pdf:application/pdf}
}

@article{cherian-riemannian-2015,
	title = {Riemannian {Dictionary} {Learning} and {Sparse} {Coding} for {Positive} {Definite} {Matrices}},
	url = {http://arxiv.org/abs/1507.02772},
	abstract = {Data encoded as symmetric positive definite (SPD) matrices frequently arise in many areas of computer vision and machine learning. While these matrices form an open subset of the Euclidean space of symmetric matrices, viewing them through the lens of non-Euclidean Riemannian geometry often turns out to be better suited in capturing several desirable data properties. However, formulating classical machine learning algorithms within such a geometry is often non-trivial and computationally expensive. Inspired by the great success of dictionary learning and sparse coding for vector-valued data, our goal in this paper is to represent data in the form of SPD matrices as sparse conic combinations of SPD atoms from a learned dictionary via a Riemannian geometric approach. To that end, we formulate a novel Riemannian optimization objective for dictionary learning and sparse coding in which the representation loss is characterized via the affine invariant Riemannian metric. We also present a computationally simple algorithm for optimizing our model. Experiments on several computer vision datasets demonstrate superior classification and retrieval performance using our approach when compared to sparse coding via alternative non-Riemannian formulations.},
	number = {i},
	author = {Cherian, Anoop and Sra, Suvrit},
	year = {2015},
	pages = {1--14},
	file = {1507.02772v1:/home/user/Zotero/storage/MDX4U7NI/1507.02772v1.pdf:application/pdf}
}

@article{chang-learning-2015,
	title = {Learning to {Search} {Better} than {Your} {Teacher}},
	volume = {37},
	author = {Chang, Kai-wei and Daum, Hal and Langford, John},
	year = {2015},
	file = {changb15:/home/user/Zotero/storage/HZ8BFGX2/changb15.pdf:application/pdf}
}

@article{cemgil-bayesian-2009,
	title = {Bayesian inference for nonnegative matrix factorisation models},
	volume = {2009},
	issn = {9780769541099},
	doi = {10.1155/2009/785152},
	abstract = {We describe nonnegative matrix factorisation (NMF) with a Kullback-Leibler (KL) error measure in a statistical framework, with a hierarchical generative model consisting of an observation and a prior component. Omitting the prior leads to the standard KL-NMF algorithms as special cases, where maximum likelihood parameter estimation is carried out via the Expectation-Maximisation (EM) algorithm. Starting from this view, we develop full Bayesian inference via variational Bayes or Monte Carlo. Our construction retains conjugacy and enables us to develop more powerful models while retaining attractive features of standard NMF such as monotonic convergence and easy implementation. We illustrate our approach on model order selection and image reconstruction.},
	journal = {Computational Intelligence and Neuroscience},
	author = {Cemgil, Ali Taylan},
	year = {2009},
	file = {785152:/home/user/Zotero/storage/TV4A55WP/785152.pdf:application/pdf;Cemgil - 2009 - Bayesian inference for nonnegative matrix factorisation models:/home/user/Zotero/storage/Q3X386NZ/Cemgil - 2009 - Bayesian inference for nonnegative matrix factorisation models.pdf:application/pdf}
}

@article{carey-manifold-1998,
	title = {Manifold {Spanning} {Graphs}},
	author = {Carey, C J and Mahadevan, Sridhar},
	year = {1998},
	keywords = {Novel Machine Learning Algorithms},
	pages = {1708--1714},
	file = {Carey, Mahadevan - 1998 - Manifold Spanning Graphs:/home/user/Zotero/storage/K6Q23UPB/Carey, Mahadevan - 1998 - Manifold Spanning Graphs.pdf:application/pdf}
}

@article{carbonara-incentivizing-2015,
	title = {Incentivizing {Peer} {Grading} in {MOOCS} : {An} {Audit} {Game} {Approach}},
	number = {Ijcai},
	author = {Carbonara, Alejandro},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {497--503},
	file = {IJCAI15-076:/home/user/Zotero/storage/E6J9QJBU/IJCAI15-076.pdf:application/pdf}
}

@article{calandra-manifold-2014,
	title = {Manifold {Gaussian} {Processes} for {Regression}},
	url = {http://arxiv.org/abs/1402.5876\nhttp://arxiv.org/pdf/1402.5876v3.pdf},
	journal = {arXiv preprint arXiv:1402.5876},
	author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	year = {2014},
	file = {1402.5876v3:/home/user/Zotero/storage/EZSPGE52/1402.5876v3.pdf:application/pdf}
}

@article{cai-hierarchical-2013,
	title = {Hierarchical {Dirichlet} {Processes}},
	author = {Cai, Diana and Miller, Andrew},
	year = {2013},
	keywords = {clustering, hierarchical, markov chain monte carlo, mixture models, models, nonparametric bayesian statistics},
	pages = {1--30},
	file = {hdp:/home/user/Zotero/storage/JHI53IHE/hdp.pdf:application/pdf}
}

@article{brown-simultaneous-2015,
	title = {Simultaneous {Abstraction} and {Equilibrium} {Finding} in {Games}},
	number = {Ijcai},
	author = {Brown, Noam},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {489--496},
	file = {IJCAI15-075:/home/user/Zotero/storage/NBZTBEI2/IJCAI15-075.pdf:application/pdf}
}

@article{broderick-nonparametric-nodate,
	title = {Nonparametric {Bayesian} {Statistics} : {Part} {III} {Recall} : {Part} {I}},
	author = {Broderick, Tamara},
	file = {broderick_mlss2015_part3:/home/user/Zotero/storage/P959224K/broderick_mlss2015_part3.pdf:application/pdf}
}

@article{braun-structure-2010,
	title = {Structure learning in action},
	volume = {206},
	issn = {0166-4328},
	doi = {10.1016/j.bbr.2009.08.031},
	abstract = {'Learning to learn' phenomena have been widely investigated in cognition, perception and more recently also in action. During concept learning tasks, for example, it has been suggested that characteristic features are abstracted from a set of examples with the consequence that learning of similar tasks is facilitated-a process termed 'learning to learn'. From a computational point of view such an extraction of invariants can be regarded as learning of an underlying structure. Here we review the evidence for structure learning as a 'learning to learn' mechanism, especially in sensorimotor control where the motor system has to adapt to variable environments. We review studies demonstrating that common features of variable environments are extracted during sensorimotor learning and exploited for efficient adaptation in novel tasks. We conclude that structure learning plays a fundamental role in skill learning and may underlie the unsurpassed flexibility and adaptability of the motor system. ?? 2009 Elsevier B.V. All rights reserved.},
	journal = {Behavioural Brain Research},
	author = {Braun, Daniel a. and Mehring, Carsten and Wolpert, Daniel M.},
	year = {2010},
	keywords = {Structure learning, Adaptive motor control, Dimensionality reduction, Learning-to-learn, Variability, Visuomotor learning},
	pages = {157--165},
	file = {Braun, Mehring, Wolpert - 2010 - Structure learning in action:/home/user/Zotero/storage/956XRBC3/Braun, Mehring, Wolpert - 2010 - Structure learning in action.pdf:application/pdf}
}

@article{broderick-nonparametric-nodate-1,
	title = {Nonparametric {Bayesian} {Statistics} : {Part} {II} {Recall} : {Part} {I}},
	author = {Broderick, Tamara},
	file = {broderick_mlss2015_part2:/home/user/Zotero/storage/CDJ3ARGZ/broderick_mlss2015_part2.pdf:application/pdf}
}

@article{braun-structure-2010-1,
	title = {Structure learning in action},
	volume = {206},
	issn = {0166-4328},
	url = {http://dx.doi.org/10.1016/j.bbr.2009.08.031},
	doi = {10.1016/j.bbr.2009.08.031},
	abstract = {'Learning to learn' phenomena have been widely investigated in cognition, perception and more recently also in action. During concept learning tasks, for example, it has been suggested that characteristic features are abstracted from a set of examples with the consequence that learning of similar tasks is facilitated-a process termed 'learning to learn'. From a computational point of view such an extraction of invariants can be regarded as learning of an underlying structure. Here we review the evidence for structure learning as a 'learning to learn' mechanism, especially in sensorimotor control where the motor system has to adapt to variable environments. We review studies demonstrating that common features of variable environments are extracted during sensorimotor learning and exploited for efficient adaptation in novel tasks. We conclude that structure learning plays a fundamental role in skill learning and may underlie the unsurpassed flexibility and adaptability of the motor system. ?? 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Behavioural Brain Research},
	author = {Braun, Daniel a. and Mehring, Carsten and Wolpert, Daniel M.},
	year = {2010},
	keywords = {Structure learning, Adaptive motor control, Dimensionality reduction, Learning-to-learn, Variability, Visuomotor learning},
	pages = {157--165},
	file = {Braun, Mehring, Wolpert - 2010 - Structure learning in action(2):/home/user/Zotero/storage/7GFTIFUT/Braun, Mehring, Wolpert - 2010 - Structure learning in action(2).pdf:application/pdf}
}

@inproceedings{bratko-inductive-1993,
	title = {Inductive {Learning} {Applied} to {Program} {Construction} and {Verification}},
	isbn = {0-444-81541-4},
	url = {http://portal.acm.org/citation.cfm?id=646677.756917},
	booktitle = {Extended {Papers} from the {IFIP} {TC}12 {Workshop} on {Artificial} {Intelligence} from the {Information} {Processing} {Perspective}: {Knowledge} {Oriented} {Software} {Design}},
	publisher = {North-Holland Publishing Co.},
	author = {Bratko, Ivan and Grobelnik, Marko},
	year = {1993},
	keywords = {ILP},
	pages = {169--182}
}

@inproceedings{bratko-refining-1999,
	series = {{ILP} '99},
	title = {Refining {Complete} {Hypotheses} in {ILP}},
	isbn = {3-540-66109-3},
	url = {http://portal.acm.org/citation.cfm?id=647999.742923},
	booktitle = {Proceedings of the 9th {International} {Workshop} on {Inductive} {Logic} {Programming}},
	publisher = {Springer-Verlag},
	author = {Bratko, Ivan},
	year = {1999},
	keywords = {ILP},
	pages = {44--55}
}

@book{boyd-convex-2004,
	address = {Cambridge},
	title = {Convex {Optimization}},
	isbn = {978-0-511-80444-1},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511804441},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	year = {2004},
	file = {Boyd, Vandenberghe - 2004 - Convex Optimization:/home/user/Zotero/storage/CNMMG4UX/Boyd, Vandenberghe - 2004 - Convex Optimization.pdf:application/pdf}
}

@article{bowman-natural-nodate-1,
	title = {Natural {Logic} {Reasoning}},
	author = {Bowman, Samuel R},
	pages = {1--4},
	file = {Bowman - Unknown - Natural Logic Reasoning:/home/user/Zotero/storage/GGCWR6WD/Bowman - Unknown - Natural Logic Reasoning.pdf:application/pdf}
}

@article{bottou-counterfactual-2013,
	title = {Counterfactual {Reasoning} and {Learning} {Systems}: {The} {Example} of {Computational} {Advertising}},
	volume = {14},
	issn = {1532-4435},
	url = {http://jmlr.org/papers/v14/bottou13a.html},
	abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
	journal = {Journal of Machine Learning Research},
	author = {Bottou, Léon and Peters, Jonas and Quiñonero-Candela, Joaquin and Charles, Denis X. and Chickering, D. Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
	year = {2013},
	keywords = {causation, computational advertising, counterfactual reasoning},
	pages = {3207--3260},
	file = {Bottou et al. - 2013 - Counterfactual Reasoning and Learning Systems The Example of Computational Advertising:/home/user/Zotero/storage/QJIFN2FJ/Bottou et al. - 2013 - Counterfactual Reasoning and Learning Systems The Example of Computational Advertising.pdf:application/pdf}
}

@book{boyd-convex-2010,
	title = {Convex {Optimization}},
	volume = {25},
	isbn = {978-0-521-83378-3},
	url = {https://web.stanford.edu/~boyd/cvxbook/bv\_cvxbook.pdf},
	abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57\%, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	year = {2010},
	file = {Boyd, Vandenberghe - 2010 - Convex Optimization:/home/user/Zotero/storage/I6Z26PMU/Boyd, Vandenberghe - 2010 - Convex Optimization.pdf:application/pdf}
}

@article{boutsidis-spectral-2007,
	title = {Spectral {Clustering} via the {Power} {Method} - {Provably}},
	author = {Boutsidis, Christos and Street, West and York, New and Gittens, Alex},
	year = {2007},
	file = {boutsidis15:/home/user/Zotero/storage/IR5A8KI2/boutsidis15.pdf:application/pdf}
}

@article{bourhis-reasonable-2015,
	title = {Reasonable {Highly} {Expressive} {Query} {Languages}},
	number = {Ijcai},
	author = {Bourhis, Pierre and Krötzsch, Markus and Rudolph, Sebastian and Dresden, Technische Universität},
	year = {2015},
	keywords = {Special Track on Knowledge Representation and Reas},
	pages = {2826--2832},
	file = {IJCAI15-400:/home/user/Zotero/storage/ES96CVEV/IJCAI15-400.pdf:application/pdf}
}

@article{bollegala-embedding-2015,
	title = {Embedding {Semantic} {Relations} into {Word} {Representations}},
	number = {Ijcai},
	author = {Bollegala, Danushka and Maehara, Takanori},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1222--1228},
	file = {IJCAI15-176:/home/user/Zotero/storage/DU7MZW2W/IJCAI15-176.pdf:application/pdf}
}

@article{bian-asymptotic-nodate,
	title = {Asymptotic {Generalization} {Bound} of {Fisher} ’ s {Linear} {Discriminant} {Analysis}},
	author = {Bian, Wei and Tao, Dacheng and Member, Senior},
	pages = {1--30},
	file = {Bian, Tao, Member - Unknown - Asymptotic Generalization Bound of Fisher ’ s Linear Discriminant Analysis:/home/user/Zotero/storage/KR89QDFC/Bian, Tao, Member - Unknown - Asymptotic Generalization Bound of Fisher ’ s Linear Discriminant Analysis.pdf:application/pdf}
}

@article{johnson-laird-response-2015,
	title = {Response to {Baratgin} et al.: {Mental} {Models} {Integrate} {Probability} and {Deduction}},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Response to {Baratgin} et al.},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315001515},
	doi = {10.1016/j.tics.2015.06.014},
	number = {10},
	urldate = {2015-10-21},
	journal = {Trends in Cognitive Sciences},
	author = {Johnson-Laird, P. N. and Khemlani, Sangeet S. and Goodwin, Geoffrey P.},
	month = oct,
	year = {2015},
	pages = {548--549},
	file = {Johnson-Laird et al_2015_Response to Baratgin et al.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Johnson-Laird et al_2015_Response to Baratgin et al.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/QTZX526R/S1364661315001515.html:text/html}
}

@article{anders-evidence-2015,
	title = {Evidence accumulation as a model for lexical selection},
	volume = {82},
	issn = {0010-0285},
	url = {http://www.sciencedirect.com/science/article/pii/S0010028515000572},
	doi = {10.1016/j.cogpsych.2015.07.002},
	abstract = {We propose and demonstrate evidence accumulation as a plausible theoretical and/or empirical model for the lexical selection process of lexical retrieval. A number of current psycholinguistic theories consider lexical selection as a process related to selecting a lexical target from a number of alternatives, which each have varying activations (or signal supports), that are largely resultant of an initial stimulus recognition. We thoroughly present a case for how such a process may be theoretically explained by the evidence accumulation paradigm, and we demonstrate how this paradigm can be directly related or combined with conventional psycholinguistic theory and their simulatory instantiations (generally, neural network models). Then with a demonstrative application on a large new real data set, we establish how the empirical evidence accumulation approach is able to provide parameter results that are informative to leading psycholinguistic theory, and that motivate future theoretical development.},
	urldate = {2015-10-21},
	journal = {Cognitive Psychology},
	author = {Anders, R. and Riès, S. and van Maanen, L. and Alario, F. -X.},
	month = nov,
	year = {2015},
	keywords = {Lexical retrieval, Neural network models, Psychometrics, Response time analysis, Sequential sampling, Shifted Wald},
	pages = {57--73},
	file = {Anders et al_2015_Evidence accumulation as a model for lexical selection.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Anders et al_2015_Evidence accumulation as a model for lexical selection.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/NSG5WCZI/S0010028515000572.html:text/html}
}

@article{sloutsky-conceptual-2015,
	title = {Conceptual influences on induction: {A} case for a late onset},
	volume = {82},
	issn = {0010-0285},
	shorttitle = {Conceptual influences on induction},
	url = {http://www.sciencedirect.com/science/article/pii/S0010028515000638},
	doi = {10.1016/j.cogpsych.2015.08.005},
	abstract = {This research examines the mechanism of early induction, the development of induction, and the ways attentional and conceptual factors contribute to induction across development. Different theoretical views offer different answers to these questions. Six experiments with 4- and 5-year-olds, 7-year-olds and adults (N = 208) test these competing theories by teaching categories for which category membership and perceptual similarity are in conflict, and varying orthogonally conceptual and attentional factors that may potentially affect inductive inference. The results suggest that early induction is similarity-based; conceptual information plays a negligible role in early induction, but its role increases gradually, with the 7-year-olds being a transitional group. And finally, there is substantial contribution of attention to the development of induction: only adults, but not children, could perform category-based induction without attentional support. Therefore, category-based induction exhibits protracted development, with attentional factors contributing early in development and conceptual factors contributing later in development. These results are discussed in relation to existing theories of development of inductive inference and broader theoretical views on cognitive development.},
	urldate = {2015-10-21},
	journal = {Cognitive Psychology},
	author = {Sloutsky, Vladimir M. and (Sophia) Deng, Wei and Fisher, Anna V. and Kloos, Heidi},
	month = nov,
	year = {2015},
	keywords = {Induction, Learning, Categorization, Cognitive development, Similarity},
	pages = {1--31},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/XEG4DAM8/S0010028515000638.html:text/html;Sloutsky et al_2015_Conceptual influences on induction.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Sloutsky et al_2015_Conceptual influences on induction.pdf:application/pdf}
}

@article{ball-semantic-2015,
	title = {Semantic {Relations} between {Visual} {Objects} {Can} {Be} {Unconsciously} {Processed} but {Not} {Reported} under {Change} {Blindness}},
	volume = {27},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00860},
	doi = {10.1162/jocn\_a\_00860},
	abstract = {Change blindness—the failure to detect changes in visual scenes—has often been interpreted as a result of impoverished visual information encoding or as a failure to compare the prechange and postchange scene. In the present electroencephalography study, we investigated whether semantic features of prechange and postchange information are processed unconsciously, even when observers are unaware that a change has occurred. We presented scenes composed of natural objects in which one object changed from one presentation to the next. Object changes were either semantically related (e.g., rail car changed to rail) or unrelated (e.g., rail car changed to sausage). Observers were first asked to detect whether any change had occurred and then to judge the semantic relation of the two objects involved in the change. We found a semantic mismatch ERP effect, that is, a more negative-going ERP for semantically unrelated compared to related changes, originating from a cortical network including the left middle temporal gyrus and occipital cortex and resembling the N400 effect, albeit at longer latencies. Importantly, this semantic mismatch effect persisted even when observers were unaware of the change and the semantic relationship of prechange and postchange object. This finding implies that change blindness does not preclude the encoding of the prechange and postchange objects' identities and possibly even the comparison of their semantic content. Thus, change blindness cannot be interpreted as resulting from impoverished or volatile visual representations or as a failure to process the prechange and postchange object. Instead, change detection appears to be limited at a later, postperceptual stage.},
	number = {11},
	urldate = {2015-10-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {Ball, Felix and Bernasconi, Fosco and Busch, Niko A.},
	month = aug,
	year = {2015},
	pages = {2253--2268},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/8GK3WVU8/jocn_a_00860.html:text/html}
}

@article{cohen-visual-2015,
	title = {Visual {Awareness} {Is} {Limited} by the {Representational} {Architecture} of the {Visual} {System}},
	volume = {27},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00855},
	doi = {10.1162/jocn\_a\_00855},
	abstract = {Visual perception and awareness have strict limitations. We suggest that one source of these limitations is the representational architecture of the visual system. Under this view, the extent to which items activate the same neural channels constrains the amount of information that can be processed by the visual system and ultimately reach awareness. Here, we measured how well stimuli from different categories (e.g., faces and cars) blocked one another from reaching awareness using two distinct paradigms that render stimuli invisible: visual masking and continuous flash suppression. Next, we used fMRI to measure the similarity of the neural responses elicited by these categories across the entire visual hierarchy. Overall, we found strong brain–behavior correlations within the ventral pathway, weaker correlations in the dorsal pathway, and no correlations in early visual cortex (V1–V3). These results suggest that the organization of higher level visual cortex constrains visual awareness and the overall processing capacity of visual cognition.},
	number = {11},
	urldate = {2015-10-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {Cohen, Michael A. and Nakayama, Ken and Konkle, Talia and Stantić, Mirta and Alvarez, George A.},
	month = jul,
	year = {2015},
	pages = {2240--2252},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/CZT2DCTN/jocn_a_00855.html:text/html}
}

@article{hoversten-language-2015,
	title = {Language {Membership} {Identification} {Precedes} {Semantic} {Access}: {Suppression} during {Bilingual} {Word} {Recognition}},
	volume = {27},
	issn = {0898-929X},
	shorttitle = {Language {Membership} {Identification} {Precedes} {Semantic} {Access}},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00844},
	doi = {10.1162/jocn\_a\_00844},
	abstract = {Previous research suggests that bilingual comprehenders access lexical representations of words in both languages nonselectively. However, it is unclear whether global language suppression plays a role in guiding attention to target language representations during ongoing lexico-semantic processing. To help clarify this issue, this study examined the relative timing of language membership and meaning activation during visual word recognition. Spanish–English bilinguals performed simultaneous semantic and language membership classification tasks on single words during EEG recording. Go/no-go ERP latencies provided evidence that language membership information was accessed before semantic information. Furthermore, N400 frequency effects indicated that the depth of processing of words in the nontarget language was reduced compared to the target language. These results suggest that the bilingual brain can rapidly identify the language to which a word belongs and subsequently use this information to selectively modulate the degree of processing in each language accordingly.},
	number = {11},
	urldate = {2015-10-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {Hoversten, Liv J. and Brothers, Trevor and Swaab, Tamara Y. and Traxler, Matthew J.},
	month = jun,
	year = {2015},
	pages = {2108--2116},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/4RATQ93M/jocn_a_00844.html:text/html}
}

@article{bastiaansen-frequency-based-2015,
	title = {Frequency-based {Segregation} of {Syntactic} and {Semantic} {Unification} during {Online} {Sentence} {Level} {Language} {Comprehension}},
	volume = {27},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00829},
	doi = {10.1162/jocn\_a\_00829},
	abstract = {During sentence level language comprehension, semantic and syntactic unification are functionally distinct operations. Nevertheless, both recruit roughly the same brain areas (spatially overlapping networks in the left frontotemporal cortex) and happen at the same time (in the first few hundred milliseconds after word onset). We tested the hypothesis that semantic and syntactic unification are segregated by means of neuronal synchronization of the functionally relevant networks in different frequency ranges: gamma (40 Hz and up) for semantic unification and lower beta (10–20 Hz) for syntactic unification. EEG power changes were quantified as participants read either correct sentences, syntactically correct though meaningless sentences (syntactic prose), or sentences that did not contain any syntactic structure (random word lists). Other sentences contained either a semantic anomaly or a syntactic violation at a critical word in the sentence. Larger EEG gamma-band power was observed for semantically coherent than for semantically anomalous sentences. Similarly, beta-band power was larger for syntactically correct sentences than for incorrect ones. These results confirm the existence of a functional dissociation in EEG oscillatory dynamics during sentence level language comprehension that is compatible with the notion of a frequency-based segregation of syntactic and semantic unification.},
	number = {11},
	urldate = {2015-10-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {Bastiaansen, Marcel and Hagoort, Peter},
	month = jun,
	year = {2015},
	pages = {2095--2107},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/6KX4SQSA/jocn_a_00829.html:text/html}
}

@article{ellis-language-2015,
	title = {Language and culture modulate online semantic processing},
	volume = {10},
	issn = {1749-5016, 1749-5024},
	url = {http://scan.oxfordjournals.org/content/10/10/1392},
	doi = {10.1093/scan/nsv028},
	abstract = {Language has been shown to influence non-linguistic cognitive operations such as colour perception, object categorization and motion event perception. Here, we show that language also modulates higher level processing, such as semantic knowledge. Using event-related brain potentials, we show that highly fluent Welsh–English bilinguals require significantly less processing effort when reading sentences in Welsh which contain factually correct information about Wales, than when reading sentences containing the same information presented in English. Crucially, culturally irrelevant information was processed similarly in both Welsh and English. Our findings show that even in highly proficient bilinguals, language interacts with factors associated with personal identity, such as culture, to modulate online semantic processing.},
	language = {en},
	number = {10},
	urldate = {2015-10-21},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Ellis, Ceri and Kuipers, Jan R. and Thierry, Guillaume and Lovett, Victoria and Turnbull, Oliver and Jones, Manon W.},
	month = oct,
	year = {2015},
	pmid = {25767190},
	keywords = {Semantics, linguistic relativity, bilingualism, culture},
	pages = {1392--1396},
	file = {Ellis et al_2015_Language and culture modulate online semantic processing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ellis et al_2015_Language and culture modulate online semantic processing.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XKGE9I97/1392.html:text/html}
}

@article{meyer-social-2015,
	title = {Social working memory and its distinctive link to social cognitive ability: an {fMRI} study},
	volume = {10},
	issn = {1749-5016, 1749-5024},
	shorttitle = {Social working memory and its distinctive link to social cognitive ability},
	url = {http://scan.oxfordjournals.org/content/10/10/1338},
	doi = {10.1093/scan/nsv065},
	abstract = {Engaging social working memory (SWM) during effortful social cognition has been associated with neural activation in two neurocognitive systems: the medial frontoparietal system and the lateral frontoparietal system. However, the respective roles played by these systems in SWM remain unknown. Results from this study demonstrate that only the medial frontoparietal system supports the social cognitive demands managed in SWM. In contrast, the lateral frontoparietal system supports the non-social cognitive demands that are needed for task performance, but that are independent of the social cognitive computations. Moreover, parametric increases in the medial frontoparietal system, but not the lateral frontoparietal system, in response to SWM load predicted performance on a challenging measure of perspective-taking. Thus, the medial frontoparietal system may uniquely support social cognitive processes in working memory and the working memory demands afforded by effortful social cognition, such as the need to track another person’s perspective in mind.},
	language = {en},
	number = {10},
	urldate = {2015-10-21},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Meyer, Meghan L. and Taylor, Shelley E. and Lieberman, Matthew D.},
	month = oct,
	year = {2015},
	pmid = {25987597},
	keywords = {working memory, default network, fMRI, social cognition},
	pages = {1338--1347},
	file = {Meyer et al_2015_Social working memory and its distinctive link to social cognitive ability.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Meyer et al_2015_Social working memory and its distinctive link to social cognitive ability.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/65AER3B7/1338.html:text/html}
}

@article{finnerty-time-2015,
	title = {Time in {Cortical} {Circuits}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/35/41/13912},
	doi = {10.1523/JNEUROSCI.2654-15.2015},
	abstract = {Time is central to cognition. However, the neural basis for time-dependent cognition remains poorly understood. We explore how the temporal features of neural activity in cortical circuits and their capacity for plasticity can contribute to time-dependent cognition over short time scales. This neural activity is linked to cognition that operates in the present or anticipates events or stimuli in the near future. We focus on deliberation and planning in the context of decision making as a cognitive process that integrates information across time. We progress to consider how temporal expectations of the future modulate perception. We propose that understanding the neural basis for how the brain tells time and operates in time will be necessary to develop general models of cognition.
SIGNIFICANCE STATEMENT Time is central to cognition. However, the neural basis for time-dependent cognition remains poorly understood. We explore how the temporal features of neural activity in cortical circuits and their capacity for plasticity can contribute to time-dependent cognition over short time scales. We propose that understanding the neural basis for how the brain tells time and operates in time will be necessary to develop general models of cognition.},
	language = {en},
	number = {41},
	urldate = {2015-10-21},
	journal = {The Journal of Neuroscience},
	author = {Finnerty, Gerald T. and Shadlen, Michael N. and Jazayeri, Mehrdad and Nobre, Anna C. and Buonomano, Dean V.},
	month = oct,
	year = {2015},
	pmid = {26468192},
	pages = {13912--13916},
	file = {Finnerty et al_2015_Time in Cortical Circuits.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Finnerty et al_2015_Time in Cortical Circuits.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/NAMNHSKG/13912.html:text/html}
}

@article{emiliani-all-optical-2015,
	title = {All-{Optical} {Interrogation} of {Neural} {Circuits}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/35/41/13917},
	doi = {10.1523/JNEUROSCI.2916-15.2015},
	abstract = {There have been two recent revolutionary advances in neuroscience: First, genetically encoded activity sensors have brought the goal of optical detection of single action potentials in vivo within reach. Second, optogenetic actuators now allow the activity of neurons to be controlled with millisecond precision. These revolutions have now been combined, together with advanced microscopies, to allow “all-optical” readout and manipulation of activity in neural circuits with single-spike and single-neuron precision. This is a transformational advance that will open new frontiers in neuroscience research. Harnessing the power of light in the all-optical approach requires coexpression of genetically encoded activity sensors and optogenetic probes in the same neurons, as well as the ability to simultaneously target and record the light from the selected neurons. It has recently become possible to combine sensors and optical strategies that are sufficiently sensitive and cross talk free to enable single-action-potential sensitivity and precision for both readout and manipulation in the intact brain. The combination of simultaneous readout and manipulation from the same genetically defined cells will enable a wide range of new experiments as well as inspire new technologies for interacting with the brain. The advances described in this review herald a future where the traditional tools used for generations by physiologists to study and interact with the brain—stimulation and recording electrodes—can largely be replaced by light. We outline potential future developments in this field and discuss how the all-optical strategy can be applied to solve fundamental problems in neuroscience.
SIGNIFICANCE STATEMENT This review describes the nexus of dramatic recent developments in optogenetic probes, genetically encoded activity sensors, and novel microscopies, which together allow the activity of neural circuits to be recorded and manipulated entirely using light. The optical and protein engineering strategies that form the basis of this “all-optical” approach are now sufficiently advanced to enable single-neuron and single-action potential precision for simultaneous readout and manipulation from the same functionally defined neurons in the intact brain. These advances promise to illuminate many fundamental challenges in neuroscience, including transforming our search for the neural code and the links between neural circuit activity and behavior.},
	language = {en},
	number = {41},
	urldate = {2015-10-21},
	journal = {The Journal of Neuroscience},
	author = {Emiliani, Valentina and Cohen, Adam E. and Deisseroth, Karl and Häusser, Michael},
	month = oct,
	year = {2015},
	pmid = {26468193},
	keywords = {calcium imaging, genetically encoded calcium sensor, genetically encoded voltage sensor, optogenetics, two-photon microscopy, wavefront shaping},
	pages = {13917--13926},
	file = {Emiliani et al_2015_All-Optical Interrogation of Neural Circuits.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Emiliani et al_2015_All-Optical Interrogation of Neural Circuits.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3H3HJFF8/13917.html:text/html}
}

@article{xylouris-three-dimensional-2015,
	title = {A three-dimensional mathematical model for the signal propagation on a neuron's membrane},
	volume = {9},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2015.00094/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Neuroscience-w43-2015},
	doi = {10.3389/fncom.2015.00094},
	abstract = {In order to be able to examine the extracellular potential's influence on network activity and to better understand dipole properties of the extracellular potential, we present and analyze a three-dimensional formulation of the cable equation which facilitates numeric simulations. When the neuron's intra- and extracellular space is assumed to be purely resistive (i.e., no free charges), the balance law of electric fluxes leads to the Laplace equation for the distribution of the intra- and extracellular potential. Moreover, the flux across the neuron's membrane is continuous. This observation already delivers the three dimensional cable equation. The coupling of the intra- and extracellular potential over the membrane is not trivial. Here, we present a continuous extension of the extracellular potential to the intracellular space and combine the resulting equation with the intracellular problem. This approach makes the system numerically accessible. On the basis of the assumed pure resistive intra- and extracellular spaces, we conclude that a cell's out-flux balances out completely. As a consequence neurons do not own any current monopoles. We present a rigorous analysis with spherical harmonics for the extracellular potential by approximating the neuron's geometry to a sphere. Furthermore, we show with first numeric simulations on idealized circumstances that the extracellular potential can have a decisive effect on network activity through ephaptic interactions.},
	urldate = {2015-10-21},
	journal = {Frontiers in Computational Neuroscience},
	author = {Xylouris, Konstantinos and Wittum, Gabriel},
	year = {2015},
	keywords = {Models, 3D-modeling, cable equation, detailed 3D-modeling, dipole effect, ephaptic coupling, theoretical},
	pages = {94},
	file = {Full Text PDF:/home/user/Zotero/storage/B7DQXAPX/Xylouris and Wittum - 2015 - A three-dimensional mathematical model for the sig.pdf:application/pdf}
}

@article{du-castel-pattern-2015,
	title = {Pattern activation/recognition theory of mind},
	volume = {9},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2015.00090/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Neuroscience-w43-2015},
	doi = {10.3389/fncom.2015.00090},
	abstract = {In his 2012 book How to Create a Mind, Ray Kurzweil defines a “Pattern Recognition Theory of Mind” that states that the brain uses millions of pattern recognizers, plus modules to check, organize, and augment them. In this article, I further the theory to go beyond pattern recognition and include also pattern activation, thus encompassing both sensory and motor functions. In addition, I treat checking, organizing, and augmentation as patterns of patterns instead of separate modules, therefore handling them the same as patterns in general. Henceforth I put forward a unified theory I call “Pattern Activation/Recognition Theory of Mind.” While the original theory was based on hierarchical hidden Markov models, this evolution is based on their precursor: stochastic grammars. I demonstrate that a class of self-describing stochastic grammars allows for unifying pattern activation, recognition, organization, consistency checking, metaphor, and learning, into a single theory that expresses patterns throughout. I have implemented the model as a probabilistic programming language specialized in activation/recognition grammatical and neural operations. I use this prototype to compute and present diagrams for each stochastic grammar and corresponding neural circuit. I then discuss the theory as it relates to artificial network developments, common coding, neural reuse, and unity of mind, concluding by proposing potential paths to validation.},
	urldate = {2015-10-21},
	journal = {Frontiers in Computational Neuroscience},
	author = {du Castel, Bertrand},
	year = {2015},
	keywords = {grammar, metaphor, autapse, hylomorphism, neural, recurrent, self-description, stochastic},
	pages = {90},
	file = {Full Text PDF:/home/user/Zotero/storage/TPBIA8W7/du Castel - 2015 - Pattern activationrecognition theory of mind.pdf:application/pdf}
}

@article{bubeck-geometric-2015,
	title = {A geometric alternative to {Nesterov}'s accelerated gradient descent},
	url = {http://arxiv.org/abs/1506.08187},
	abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov's accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov's accelerated gradient descent.},
	urldate = {2015-10-20},
	journal = {arXiv:1506.08187 [cs, math]},
	author = {Bubeck, Sébastien and Lee, Yin Tat and Singh, Mohit},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08187},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Computer Science - Data Structures and Algorithms, Computer Science - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9XGHMMZA/1506.html:text/html;Bubeck et al_2015_A geometric alternative to Nesterov's accelerated gradient descent.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bubeck et al_2015_A geometric alternative to Nesterov's accelerated gradient descent.pdf:application/pdf}
}

@article{mcgann-associative-2015,
	title = {Associative learning and sensory neuroplasticity: how does it happen and what is it good for?},
	volume = {22},
	issn = {1072-0502, 1549-5485},
	shorttitle = {Associative learning and sensory neuroplasticity},
	url = {http://learnmem.cshlp.org/content/22/11/567},
	doi = {10.1101/lm.039636.115},
	abstract = {Historically, the body's sensory systems have been presumed to provide the brain with raw information about the external environment, which the brain must interpret to select a behavioral response. Consequently, studies of the neurobiology of learning and memory have focused on circuitry that interfaces between sensory inputs and behavioral outputs, such as the amygdala and cerebellum. However, evidence is accumulating that some forms of learning can in fact drive stimulus-specific changes very early in sensory systems, including not only primary sensory cortices but also precortical structures and even the peripheral sensory organs themselves. This review synthesizes evidence across sensory modalities to report emerging themes, including the systems’ flexibility to emphasize different aspects of a sensory stimulus depending on its predictive features and ability of different forms of learning to produce similar plasticity in sensory structures. Potential functions of this learning-induced neuroplasticity are discussed in relation to the challenges faced by sensory systems in changing environments, and evidence for absolute changes in sensory ability is considered. We also emphasize that this plasticity may serve important nonsensory functions, including balancing metabolic load, regulating attentional focus, and facilitating downstream neuroplasticity.},
	language = {en},
	number = {11},
	urldate = {2015-10-20},
	journal = {Learning \& Memory},
	author = {McGann, John P.},
	month = nov,
	year = {2015},
	pmid = {26472647},
	pages = {567--576},
	file = {McGann_2015_Associative learning and sensory neuroplasticity.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/McGann_2015_Associative learning and sensory neuroplasticity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Q7786Q5C/567.html:text/html}
}

@article{batterink-functional-2015,
	title = {Functional differences between statistical learning with and without explicit training},
	volume = {22},
	issn = {1072-0502, 1549-5485},
	url = {http://learnmem.cshlp.org/content/22/11/544},
	doi = {10.1101/lm.037986.114},
	abstract = {Humans are capable of rapidly extracting regularities from environmental input, a process known as statistical learning. This type of learning typically occurs automatically, through passive exposure to environmental input. The presumed function of statistical learning is to optimize processing, allowing the brain to more accurately predict and prepare for incoming input. In this study, we ask whether the function of statistical learning may be enhanced through supplementary explicit training, in which underlying regularities are explicitly taught rather than simply abstracted through exposure. Learners were randomly assigned either to an explicit group or an implicit group. All learners were exposed to a continuous stream of repeating nonsense words. Prior to this implicit training, learners in the explicit group received supplementary explicit training on the nonsense words. Statistical learning was assessed through a speeded reaction-time (RT) task, which measured the extent to which learners used acquired statistical knowledge to optimize online processing. Both RTs and brain potentials revealed significant differences in online processing as a function of training condition. RTs showed a crossover interaction; responses in the explicit group were faster to predictable targets and marginally slower to less predictable targets relative to responses in the implicit group. P300 potentials to predictable targets were larger in the explicit group than in the implicit group, suggesting greater recruitment of controlled, effortful processes. Taken together, these results suggest that information abstracted through passive exposure during statistical learning may be processed more automatically and with less effort than information that is acquired explicitly.},
	language = {en},
	number = {11},
	urldate = {2015-10-20},
	journal = {Learning \& Memory},
	author = {Batterink, Laura J. and Reber, Paul J. and Paller, Ken A.},
	month = nov,
	year = {2015},
	pmid = {26472644},
	pages = {544--556},
	file = {Batterink et al_2015_Functional differences between statistical learning with and without explicit.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Batterink et al_2015_Functional differences between statistical learning with and without explicit.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JD5R6KJN/544.html:text/html}
}

@article{luke-predicting-2015,
	title = {Predicting inflectional morphology from context},
	volume = {30},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1009918},
	doi = {10.1080/23273798.2015.1009918},
	abstract = {The present studies investigated the influence of the semantic and syntactic predictability of an inflectional morpheme on word recognition and morphological processing. In two eye-tracking experiments, we examined the effect of syntactic and semantic context on the processing of letter transpositions in inflected words. Participants experienced greater and earlier disruption from cross-morpheme letter transpositions when target verbs appeared in a context that syntactically predicted the presence of a past-tense suffix. Further, internal transpositions caused greater and earlier disruption even in monomorphemic verbs when syntactic context created an expectation of morphological complexity. No effect of semantic predictability was observed, potentially because the semantic manipulation was insufficiently strong. The results reveal that syntactic contexts typical of most English sentences can lead readers to make predictions about the morphological structure of upcoming words.},
	number = {6},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Luke, Steven G. and Christianson, Kiel},
	month = jul,
	year = {2015},
	pages = {735--748},
	file = {Luke_Christianson_2015_Predicting inflectional morphology from context.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Luke_Christianson_2015_Predicting inflectional morphology from context.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/A9FI8ZTR/23273798.2015.html:text/html}
}

@article{ros-aiming-2015,
	title = {Aiming at shorter dependencies: the role of agreement morphology},
	volume = {30},
	issn = {2327-3798},
	shorttitle = {Aiming at shorter dependencies},
	url = {http://dx.doi.org/10.1080/23273798.2014.994009},
	doi = {10.1080/23273798.2014.994009},
	abstract = {This study examined word order preferences as a function of phrasal length in Basque. Basque is an OV language with flexible sentence word order and rich verb agreement. Contrary to the universal short-before-long preference predicted by availability models, Hawkins has argued that short-before-long orders are preferred in VO languages such as English, whereas long-before-short orders are preferred in OV languages such as Japanese. However, it is unclear how length affects word order preferences when an OV language has rich verb agreement and allows post-verbal arguments. We found a general long-before-short preference, and a tendency to place the verb in a sentence-medial position when one constituent is long. We argue that since agreement morphology signals the thematic role and case of surrounding phrases, it contributes to speeding up sentence processing. We conclude that morphologically rich languages employ both general adjacency mechanisms and language-specific resources to enhance language efficiency.},
	number = {9},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Ros, Idoia and Santesteban, Mikel and Fukumura, Kumiko and Laka, Itziar},
	month = oct,
	year = {2015},
	pages = {1156--1174},
	file = {Ros et al_2015_Aiming at shorter dependencies.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ros et al_2015_Aiming at shorter dependencies.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/KZJWA8C3/23273798.2014.html:text/html}
}

@article{norcliffe-word-2015,
	title = {Word order affects the time course of sentence formulation in {Tzeltal}},
	volume = {30},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1006238},
	doi = {10.1080/23273798.2015.1006238},
	abstract = {The scope of planning during sentence formulation is known to be flexible, as it can be influenced by speakers' communicative goals and language production pressures (among other factors). Two eye-tracked picture description experiments tested whether the time course of formulation is also modulated by grammatical structure and thus whether differences in linear word order across languages affect the breadth and order of conceptual and linguistic encoding operations. Native speakers of Tzeltal [a primarily verb–object–subject (VOS) language] and Dutch [a subject–verb–object (SVO) language] described pictures of transitive events. Analyses compared speakers' choice of sentence structure across events with more accessible and less accessible characters as well as the time course of formulation for sentences with different word orders. Character accessibility influenced subject selection in both languages in subject-initial and subject-final sentences, ruling against a radically incremental formulation process. In Tzeltal, subject-initial word orders were preferred over verb-initial orders when event characters had matching animacy features, suggesting a possible role for similarity-based interference in influencing word order choice. Time course analyses revealed a strong effect of sentence structure on formulation: In subject-initial sentences, in both Tzeltal and Dutch, event characters were largely fixated sequentially, while in verb-initial sentences in Tzeltal, relational information received priority over encoding of either character during the earliest stages of formulation. The results show a tight parallelism between grammatical structure and the order of encoding operations carried out during sentence formulation.},
	number = {9},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Norcliffe, Elisabeth and Konopka, Agnieszka E. and Brown, Penelope and Levinson, Stephen C.},
	month = oct,
	year = {2015},
	pages = {1187--1208},
	file = {Norcliffe et al_2015_Word order affects the time course of sentence formulation in Tzeltal.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Norcliffe et al_2015_Word order affects the time course of sentence formulation in Tzeltal.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HJDNBWA2/23273798.2015.html:text/html}
}

@article{santesteban-effects-2015,
	title = {Effects of case-marking and head position on language production? {Evidence} from an ergative {OV} language},
	volume = {30},
	issn = {2327-3798},
	shorttitle = {Effects of case-marking and head position on language production?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1065335},
	doi = {10.1080/23273798.2015.1065335},
	abstract = {How ease of access to semantic, lexical, morphological, and syntactic information affects constituent structure selection has been investigated exclusively in nominative/accusative head-initial (VO) languages. We investigated whether these findings can be generalised to ergative head-final (OV) languages like Basque. Using the structural priming paradigm, we studied Basque native speakers’ choice of description of events involving psychological-verbs (intransitive [NPABS-PP-VPSYCH] vs. transitive [NPERG-NPABS-VPSYCH] structures). Experiment 1 showed structural priming and lexical boost effects: more intransitive [NPABS-PP-VPSYCH] descriptions produced after intransitive [NPABS-PP-VPSYCH] than transitive [NPERG-NPABS-VPSYCH] primes, and stronger effects with verb repetition. Experiment 3 showed that structural similarity between prime and target enhances priming. Finally, Experiments 2 and 4 revealed no case-marking repetition boost effects to structural priming: intransitive [NPABS-PP-VPSYCH] structures were equally primed by intransitive structures with absolutive-marked ([NPABS-VINTR]) or ergative-marked ([NPERG-VINTR]) subjects. We conclude that sentence production is verb-based in both VO and OV languages, and that case-marking occurs after structural selection.},
	number = {9},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Santesteban, Mikel and Pickering, Martin J. and Laka, Itziar and Branigan, Holly P.},
	month = oct,
	year = {2015},
	pages = {1175--1186},
	file = {Santesteban et al_2015_Effects of case-marking and head position on language production.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Santesteban et al_2015_Effects of case-marking and head position on language production.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RT7R3NZ7/23273798.2015.html:text/html}
}

@article{kgolo-role-2015,
	title = {The role of morphological structure in the processing of complex forms: evidence from {Setswana} deverbative nouns},
	volume = {30},
	issn = {2327-3798},
	shorttitle = {The role of morphological structure in the processing of complex forms},
	url = {http://dx.doi.org/10.1080/23273798.2015.1053813},
	doi = {10.1080/23273798.2015.1053813},
	abstract = {The morphological structure of poly-morphemic words (e.g. government) can affect processing, but it is unclear whether this effect is due to morphological structure or combined formal/orthographic and semantic effects. Setswana, a Bantu language, allows us to explore morphological, formal, and semantic effects: It has a noun derivation with an agglutinative agentive affix (Class-1, mo-rer-i “preacher”) and a noun derivation with vowel changes and little form-overlap (Class-9, ther-o “sermon”) that both apply to verbs (rer-a “preach”). In our masked-priming experiments (SOA = 60 ms), Class-9-forms were even more effective as primes for verbs than Class-1-forms, despite reduced formal overlap, suggesting that abstract morphological structure affects processing independently of formal or semantic relationships. Moreover, Class-1-targets showed reduced priming, indicating that unprimed morphological target material (the affix) reduces priming.},
	number = {9},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Kgolo, Naledi and Eisenbeiss, Sonja},
	month = oct,
	year = {2015},
	pages = {1116--1133},
	file = {Kgolo_Eisenbeiss_2015_The role of morphological structure in the processing of complex forms.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kgolo_Eisenbeiss_2015_The role of morphological structure in the processing of complex forms.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TURI3H4R/23273798.2015.html:text/html}
}

@article{wiener-syllable-specific-2015,
	title = {Do syllable-specific tonal probabilities guide lexical access? {Evidence} from {Mandarin}, {Shanghai} and {Cantonese} speakers},
	volume = {30},
	issn = {2327-3798},
	shorttitle = {Do syllable-specific tonal probabilities guide lexical access?},
	url = {http://dx.doi.org/10.1080/23273798.2014.946934},
	doi = {10.1080/23273798.2014.946934},
	abstract = {An eye-tracking study investigated how the interaction between syllable frequency and syllable-specific tonal probability guides online lexical access in speakers of mutually unintelligible Chinese dialects with three disparate tonal systems. Mono-dialectal Mandarin speakers, bi-dialectal Shanghai–Mandarin speakers and bi-dialectal Cantonese–Mandarin speakers searched for target Mandarin syllable–tone combinations while their eye movements and mouse clicks were recorded. The results showed dialectal differences in online eye fixation patterns but not in offline mouse responses. For all groups, mouse clicks were fastest for infrequent syllables with most probable tones and slowest for infrequent syllables with least probable tones. In online eye movement responses, only mono-dialectal Mandarin speakers showed an interaction between syllable frequency and tonal probability. Mono-dialectal Mandarin speakers’ fixations were fastest for infrequent syllables with probable tones and slowest for infrequent syllables with improbable tones. Mono-dialectal speakers also showed a greater amount of competition from the more probable segmental competitor when hearing improbable tones. Bi-dialectal speakers showed different timing in their integration of tonal probabilities. These findings suggest that highly bilingual speakers track and use Mandarin tonal probabilities, but their sensitivity to L2 tonal information may lag behind monolinguals for online word recognition.},
	number = {9},
	urldate = {2015-10-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Wiener, Seth and Ito, Kiwako},
	month = oct,
	year = {2015},
	pages = {1048--1060},
	file = {Snapshot:/home/user/Zotero/storage/KV4DNDAD/23273798.2014.html:text/html;Wiener_Ito_2015_Do syllable-specific tonal probabilities guide lexical access.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Wiener_Ito_2015_Do syllable-specific tonal probabilities guide lexical access.pdf:application/pdf}
}

@article{greenhill-hebbian-nodate,
	title = {Hebbian and {Homeostatic} {Plasticity} {Mechanisms} in {Regular} {Spiking} and {Intrinsic} {Bursting} {Cells} of {Cortical} {Layer} 5},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627315008144/abstract},
	doi = {10.1016/j.neuron.2015.09.025},
	abstract = {Layer 5 contains the major projection neurons of the neocortex and is composed of two major cell types: regular spiking (RS) cells, which have cortico-cortical projections, and intrinsic bursting cells (IB), which have subcortical projections. Little is known about the plasticity processes and specifically the molecular mechanisms by which these two cell classes develop and maintain their unique integrative properties. In this study, we find that RS and IB cells show fundementally different experience-dependent plasticity processes and integrate Hebbian and homeostatic components of plasticity differently. Both RS and IB cells showed TNFα-dependent homeostatic plasticity in response to sensory deprivation, but IB cells were capable of a much faster synaptic depression and homeostatic rebound than RS cells. Only IB cells showed input-specific potentiation that depended on CaMKII autophosphorylation. Our findings demonstrate that plasticity mechanisms are not uniform within the neocortex, even within a cortical layer, but are specialized within subcircuits.},
	language = {English},
	number = {0},
	urldate = {2015-10-20},
	journal = {Neuron},
	author = {Greenhill, Stuart David and Ranson, Adam and Fox, Kevin},
	file = {Greenhill et al_Hebbian and Homeostatic Plasticity Mechanisms in Regular Spiking and Intrinsic.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Greenhill et al_Hebbian and Homeostatic Plasticity Mechanisms in Regular Spiking and Intrinsic.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZP4AZFI7/S0896-6273(15)00814-4.html:text/html}
}

@article{moraglio-principled-2015,
	title = {Principled {Design} and {Runtime} {Analysis} of {Abstract} {Convex} {Evolutionary} {Search}*},
	issn = {1063-6560},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/EVCO\_a\_00169},
	doi = {10.1162/EVCO\_a\_00169},
	abstract = {Geometric crossover is a formal class of crossovers which includes many well-known recombination operators across representations. In previous work, it was shown that all evolutionary algorithms with geometric crossover (but with no mutation) do the same form of convex search regardless of the underlying representation, the specific selection mechanism, offspring distribution, search space, and problem at hand. Furthermore, it was suggested that the generalised convex search could perform well on generalised forms of concave and approximately concave fitness landscapes, regardless of the underlying space and representation. In this article, we deepen this line of enquiry and study the runtime of generalised convex search on concave fitness landscapes. This is a first step towards linking a geometric theory of representations and runtime analysis in the attempt to (i) set the basis for a more general, unified approach for the runtime analysis of evolutionary algorithms across representations, and (ii) identify the essential matching features of evolutionary search behaviour and landscape topography that cause polynomial performance. We present a general runtime result that can be systematically instantiated to specific search spaces and representations, and present its specifications to three search spaces. As a corollary, we obtain that the convex search algorithm optimises LeadingOnes in O (n log n) fitness evaluations, which is faster than all unbiased unary black-box algorithms.},
	urldate = {2015-10-20},
	journal = {Evolutionary Computation},
	author = {Moraglio, Alberto and Sudholt, Dirk},
	month = oct,
	year = {2015},
	file = {Evolutionary Computation Snapshot:/home/user/Zotero/storage/I6JEFURE/EVCO_a_00169.html:text/html}
}

@article{lopez-garcia-open-nodate,
	title = {Open {Questions} on the {Origin} of {Eukaryotes}},
	issn = {0169-5347},
	url = {http://www.sciencedirect.com/science/article/pii/S0169534715002384},
	doi = {10.1016/j.tree.2015.09.005},
	abstract = {Despite recent progress, the origin of the eukaryotic cell remains enigmatic. It is now known that the last eukaryotic common ancestor was complex and that endosymbiosis played a crucial role in eukaryogenesis at least via the acquisition of the alphaproteobacterial ancestor of mitochondria. However, the nature of the mitochondrial host is controversial, although the recent discovery of an archaeal lineage phylogenetically close to eukaryotes reinforces models proposing archaea-derived hosts. We argue that, in addition to improved phylogenomic analyses with more comprehensive taxon sampling to pinpoint the closest prokaryotic relatives of eukaryotes, determining plausible mechanisms and selective forces at the origin of key eukaryotic features, such as the nucleus or the bacterial-like eukaryotic membrane system, is essential to constrain existing models.},
	urldate = {2015-10-20},
	journal = {Trends in Ecology \& Evolution},
	author = {López-García, Purificación and Moreira, David},
	keywords = {archaea, eukaryogenesis, membrane, metabolism, origin of nucleus, symbiosis, syntrophy},
	file = {López-García_Moreira_Open Questions on the Origin of Eukaryotes.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/López-García_Moreira_Open Questions on the Origin of Eukaryotes.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/GMSHA6GI/S0169534715002384.html:text/html}
}

@article{caruso-plants-nodate,
	title = {Do {Plants} {Eavesdrop} on {Floral} {Scent} {Signals}?},
	issn = {1360-1385},
	url = {http://www.sciencedirect.com/science/article/pii/S1360138515002307},
	doi = {10.1016/j.tplants.2015.09.001},
	abstract = {Plants emit a diverse array of volatile organic compounds that can function as cues to other plants. Plants can use volatiles emitted by neighbors to gain information about their environment, and respond by adjusting their phenotype. Less is known about whether the many different volatile signals that plants emit are all equally likely to function as cues to other plants. We review evidence for the function of floral volatile signals and conclude that plants are as likely to perceive and respond to floral volatiles as to other, better-studied volatiles. We propose that eavesdropping on floral volatile cues is particularly likely to be adaptive because plants can respond to these cues by adjusting traits that directly affect pollination and mating.},
	urldate = {2015-10-20},
	journal = {Trends in Plant Science},
	author = {Caruso, Christina M. and Parachnowitsch, Amy L.},
	keywords = {floral volatile, phenotypic plasticity, plant–plant communication, pollination, volatile organic compound},
	file = {Caruso_Parachnowitsch_Do Plants Eavesdrop on Floral Scent Signals.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Caruso_Parachnowitsch_Do Plants Eavesdrop on Floral Scent Signals.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/IKIISRSM/S1360138515002307.html:text/html}
}

@article{begum-ali-human-2015,
	title = {Human infants’ ability to perceive touch in external space develops postnatally},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S0960982215010714/abstract},
	doi = {10.1016/j.cub.2015.08.055},
	abstract = {Arriving in the outside world, the newborn infant has to determine how the tactile stimulation experienced in utero relates to the spatial environment newly offered up by vision, hearing and olfaction. We investigated this developmental process by tracing the origins of the influence of external spatial representation on young infants’ orienting responses to tactile stimuli. When adults cross their hands or feet they typically make more tactile localization errors than otherwise, and this has been attributed to the conflicts between skin-based and external frames of reference and/or the usual and current locations of touches in external space  [1,2] . Here, we report that a group of six-month-olds, like adults, showed a tactile localisation deficit with their feet crossed, indicating external spatial coding of touch; in striking contrast, four-month-olds outperformed the older infants showing no crossed-feet deficit. Thus, in the first months of life, infants perceive touches solipsistically, and only come to locate them in the external world after significant postnatal experience.},
	language = {English},
	number = {20},
	urldate = {2015-10-20},
	journal = {Current Biology},
	author = {Begum Ali, Jannath and Spence, Charles and Bremner, Andrew J.},
	month = oct,
	year = {2015},
	pages = {R978--R979},
	file = {Begum Ali et al_2015_Human infants’ ability to perceive touch in external space develops postnatally.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Begum Ali et al_2015_Human infants’ ability to perceive touch in external space develops postnatally.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/KURSCAKM/S0960-9822(15)01071-4.html:text/html}
}

@article{jazayeri-neural-2015,
	title = {A {Neural} {Mechanism} for {Sensing} and {Reproducing} a {Time} {Interval}},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S096098221501009X/abstract},
	doi = {10.1016/j.cub.2015.08.038},
	language = {English},
	number = {20},
	urldate = {2015-10-20},
	journal = {Current Biology},
	author = {Jazayeri, Mehrdad and Shadlen, Michael N.},
	month = oct,
	year = {2015},
	pages = {2599--2609},
	file = {Jazayeri_Shadlen_2015_A Neural Mechanism for Sensing and Reproducing a Time Interval.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Jazayeri_Shadlen_2015_A Neural Mechanism for Sensing and Reproducing a Time Interval.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EBA2V5JT/S0960-9822(15)01009-X.html:text/html}
}

@misc{noauthor-semcompanionjaeger.pdf-nodate,
	title = {{semCompanionJaeger}.pdf},
	url = {http://www.sfs.uni-tuebingen.de/~gjaeger/publications/semCompanionJaeger.pdf},
	urldate = {2015-10-20},
	file = {semCompanionJaeger.pdf:/home/user/Zotero/storage/UKBEMN42/semCompanionJaeger.pdf:application/pdf}
}

@misc{noauthor-degenfrankejaeger2013.pdf-nodate,
	title = {{DegenFrankeJaeger}2013.pdf},
	url = {http://www.bcs.rochester.edu/people/jdegen/docs/DegenFrankeJaeger2013.pdf},
	urldate = {2015-10-20},
	file = {DegenFrankeJaeger2013.pdf:/home/user/Zotero/storage/H5GDQR94/DegenFrankeJaeger2013.pdf:application/pdf}
}

@article{baayen-no-nodate-1,
	title = {No {Title}},
	number = {Dcc},
	author = {baayen},
	keywords = {bilingual word recognition, address for correspondence, cognate representation, cross-language, similarity, task demands},
	pages = {1--79},
	file = {DijkstraMiwaBrummelhuisSappellieBaayenJML2010:/home/user/Zotero/storage/PTEX2N5C/DijkstraMiwaBrummelhuisSappellieBaayenJML2010.pdf:application/pdf}
}

@article{baayen-no-nodate-2,
	title = {No {Title}},
	volume = {1},
	number = {780},
	author = {baayen},
	pages = {1--39},
	file = {MiwaLibbenBaayen_LCP110510:/home/user/Zotero/storage/T5TMI65N/MiwaLibbenBaayen_LCP110510.pdf:application/pdf}
}

@article{baayen-no-nodate-3,
	title = {No {Title}},
	author = {baayen},
	pages = {1--37},
	file = {TremblayBaayen2010:/home/user/Zotero/storage/VJUAQQWF/TremblayBaayen2010.pdf:application/pdf}
}

@article{gong-studying-2012,
	title = {Studying {Language} {Change} {Using} {Price} {Equation} and {Pólya}-urn {Dynamics}},
	volume = {7},
	url = {http://dx.doi.org/10.1371/journal.pone.0033171},
	doi = {10.1371/journal.pone.0033171},
	abstract = {Language change takes place primarily via diffusion of linguistic variants in a population of individuals. Identifying selective pressures on this process is important not only to construe and predict changes, but also to inform theories of evolutionary dynamics of socio-cultural factors. In this paper, we advocate the Price equation from evolutionary biology and the Pólya-urn dynamics from contagion studies as efficient ways to discover selective pressures. Using the Price equation to process the simulation results of a computer model that follows the Pólya-urn dynamics, we analyze theoretically a variety of factors that could affect language change, including variant prestige, transmission error, individual influence and preference, and social structure. Among these factors, variant prestige is identified as the sole selective pressure, whereas others help modulate the degree of diffusion only if variant prestige is involved. This multidisciplinary study discerns the primary and complementary roles of linguistic, individual learning, and socio-cultural factors in language change, and offers insight into empirical studies of language change.},
	number = {3},
	urldate = {2015-10-20},
	journal = {PLoS ONE},
	author = {Gong, Tao and Shuai, Lan and Tamariz, Mónica and Jäger, Gerhard},
	month = mar,
	year = {2012},
	pages = {e33171},
	file = {Gong et al_2012_Studying Language Change Using Price Equation and Pólya-urn Dynamics.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Gong et al_2012_Studying Language Change Using Price Equation and Pólya-urn Dynamics.pdf:application/pdf}
}

@article{jager-support-2015,
	title = {Support for linguistic macrofamilies from weighted sequence alignment},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/112/41/12752},
	doi = {10.1073/pnas.1500331112},
	abstract = {Computational phylogenetics is in the process of revolutionizing historical linguistics. Recent applications have shed new light on controversial issues, such as the location and time depth of language families and the dynamics of their spread. So far, these approaches have been limited to single-language families because they rely on a large body of expert cognacy judgments or grammatical classifications, which is currently unavailable for most language families. The present study pursues a different approach. Starting from raw phonetic transcription of core vocabulary items from very diverse languages, it applies weighted string alignment to track both phonetic and lexical change. Applied to a collection of ∼1,000 Eurasian languages and dialects, this method, combined with phylogenetic inference, leads to a classification in excellent agreement with established findings of historical linguistics. Furthermore, it provides strong statistical support for several putative macrofamilies contested in current historical linguistics. In particular, there is a solid signal for the Nostratic/Eurasiatic macrofamily.},
	language = {en},
	number = {41},
	urldate = {2015-10-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jäger, Gerhard},
	month = oct,
	year = {2015},
	pmid = {26403857},
	keywords = {cultural evolution, historical linguistics, linguistic macrofamilies, mass lexical comparison, phylogenetic methods},
	pages = {12752--12757},
	file = {Jäger_2015_Support for linguistic macrofamilies from weighted sequence alignment.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Jäger_2015_Support for linguistic macrofamilies from weighted sequence alignment.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XE5HN9AK/12752.html:text/html}
}

@article{jager-evolution-2008,
	title = {The evolution of convex categories},
	volume = {30},
	issn = {0165-0157, 1573-0549},
	url = {http://link.springer.com/article/10.1007/s10988-008-9024-3},
	doi = {10.1007/s10988-008-9024-3},
	abstract = {Gärdenfors (Conceptual spaces, 2000) argues that the semantic domains that natural language deals with have a geometrical structure. He gives evidence that simple natural language adjectives usually denote natural properties, where a natural property is a convex region of such a “conceptual space.” In this paper I will show that this feature of natural categories need not be stipulated as basic. In fact, it can be shown to be the result of evolutionary dynamics of communicative strategies under very general assumptions.},
	language = {en},
	number = {5},
	urldate = {2015-10-20},
	journal = {Linguistics and Philosophy},
	author = {Jäger, Gerhard},
	month = mar,
	year = {2008},
	keywords = {Syntax, Artificial Intelligence (incl. Robotics), Computational Linguistics, Semantics, Conceptual spaces, Philosophy of Language, Cognitive semantics, Evolutionary game theory, Language evolution},
	pages = {551--564},
	file = {Jäger_2008_The evolution of convex categories.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Jäger_2008_The evolution of convex categories.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/48THM7ZP/10.html:text/html}
}

@misc{jaeger-gerhard-abstractevolang6.pdf-nodate,
	title = {{abstractEvolang}6.pdf},
	url = {http://www.sfs.uni-tuebingen.de/~gjaeger/publications/abstractEvolang6.pdf},
	urldate = {2015-10-20},
	author = {Jaeger, Gerhard},
	file = {abstractEvolang6.pdf:/home/user/Zotero/storage/A8RN6Q9V/abstractEvolang6.pdf:application/pdf}
}

@unpublished{jaeger-gerhard-proceedings-ac03.pdf-nodate,
	title = {proceedings\_ac03.pdf},
	url = {http://www.sfs.uni-tuebingen.de/~gjaeger/publications/proceedings\_ac03.pdf},
	urldate = {2015-10-20},
	author = {Jaeger, Gerhard},
	file = {proceedings_ac03.pdf:/home/user/Zotero/storage/ITTKJBFM/proceedings_ac03.pdf:application/pdf}
}

@article{baayen-comprehension-nodate,
	title = {Comprehension without segmentation: {A} proof of concept with naive discriminative learning {R}. {Harald} {Baayen}},
	author = {Baayen},
	keywords = {Auditory comprehension, word segmentation, corla-wagner equations, discriminative learning, phonotactics, res-},
	pages = {1--32},
	file = {BaayenShaoulWillitsRamscarLCN2015:/home/user/Zotero/storage/VF85MZR2/BaayenShaoulWillitsRamscarLCN2015.pdf:application/pdf}
}

@article{blacoe-quantum-2015,
	title = {On {Quantum} {Generalizations} of {Information}-{Theoretic} {Measures} and their {Contribution} to {Distributional} {Semantics}},
	url = {http://arxiv.org/abs/1506.00578},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1506.00578},
	author = {Blacoe, William},
	year = {2015},
	file = {1506.00578v1.pdf:/home/user/Zotero/storage/FT5Q7TRR/1506.00578v1.pdf:application/pdf}
}

@article{goldberg-primer-2015,
	title = {A {Primer} on {Neural} {Network} {Models} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1510.00726},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1510.00726},
	author = {Goldberg, Yoav},
	year = {2015},
	file = {nnlp.pdf:/home/user/Zotero/storage/87IVHNKI/nnlp.pdf:application/pdf}
}

@article{harmon-reinforcement-1996,
	title = {Reinforcement learning: {A} tutorial},
	volume = {45433},
	shorttitle = {Reinforcement learning},
	url = {http://www.csc.kth.se/utbildning/kth/kurser/DD2432/ann12/forelasningsanteckningar/old-notes/rltutorial.pdf},
	urldate = {2015-10-20},
	journal = {WL/AAFC, WPAFB Ohio},
	author = {Harmon, Mance E. and Harmon, Stephanie S.},
	year = {1996},
	file = {Harmon96.pdf:/home/user/Zotero/storage/8R5V88CS/Harmon96.pdf:application/pdf}
}

@book{heller-catch-22:-1999,
	title = {Catch-22: a novel},
	volume = {1120},
	shorttitle = {Catch-22},
	url = {https://books.google.com/books?hl=en&lr=&id=Xfze51E7TEoC&oi=fnd&pg=PA11&dq=%22were+brought+to+him+in+bed.+There+were+extra+rations+of+fresh+meat,+and+during+the%22+%22day+he+had+a+better+idea.+To+everyone+he+knew+he+wrote+that+he+was+going+on+a%22+&ots=uWA0fCFAJF&sig=48z8D94koc-9s0E10uEDyTPP2yM},
	urldate = {2015-10-20},
	publisher = {Simon and Schuster},
	author = {Heller, Joseph},
	year = {1999},
	file = {[Joseph_Heller]_Catch-22(BookZZ.org).pdf:/home/user/Zotero/storage/6HE4XT6V/[Joseph_Heller]_Catch-22(BookZZ.org).pdf:application/pdf}
}

@article{gilens-testing-2014,
	title = {Testing {Theories} of {American} {Politics}: {Elites}, {Interest} {Groups}, and {Average} {Citizens}},
	volume = {12},
	issn = {1537-5927, 1541-0986},
	shorttitle = {Testing {Theories} of {American} {Politics}},
	url = {http://www.journals.cambridge.org/abstract\_S1537592714001595},
	doi = {10.1017/S1537592714001595},
	language = {en},
	number = {03},
	urldate = {2015-10-20},
	journal = {Perspectives on Politics},
	author = {Gilens, Martin and Page, Benjamin I.},
	month = sep,
	year = {2014},
	pages = {564--581},
	file = {gilens_and_page_2014_-testing_theories_of_american_politics.doc.pdf:/home/user/Zotero/storage/VNRQDIU3/gilens_and_page_2014_-testing_theories_of_american_politics.doc.pdf:application/pdf}
}

@article{place-tale-2015,
	title = {The tale of the quantifier alternation hierarchy of first-order logic over words},
	volume = {2},
	url = {http://dl.acm.org/citation.cfm?id=2815495},
	number = {3},
	urldate = {2015-10-20},
	journal = {ACM SIGLOG News},
	author = {Place, Thomas and Zeitoun, Marc},
	year = {2015},
	pages = {4--17},
	file = {surveyqalt.pdf:/home/user/Zotero/storage/CEE6PRBE/surveyqalt.pdf:application/pdf}
}

@article{lee-morphological-2015,
	title = {Morphological {Phylogenetics} in the {Genomic} {Age}},
	volume = {25},
	issn = {09609822},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S096098221500812X},
	doi = {10.1016/j.cub.2015.07.009},
	language = {en},
	number = {19},
	urldate = {2015-10-20},
	journal = {Current Biology},
	author = {Lee, Michael S.Y. and Palci, Alessandro},
	month = oct,
	year = {2015},
	pages = {R922--R929},
	file = {PIIS096098221500812X.pdf:/home/user/Zotero/storage/775EHVZZ/PIIS096098221500812X.pdf:application/pdf}
}

@article{lee-morphological-2015-1,
	title = {Morphological {Phylogenetics} in the {Genomic} {Age}},
	volume = {25},
	issn = {09609822},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S096098221500812X},
	doi = {10.1016/j.cub.2015.07.009},
	language = {en},
	number = {19},
	urldate = {2015-10-20},
	journal = {Current Biology},
	author = {Lee, Michael S.Y. and Palci, Alessandro},
	month = oct,
	year = {2015},
	pages = {R922--R929},
	file = {PIIS096098221500812X.pdf:/home/user/Zotero/storage/HK4MDHGD/PIIS096098221500812X.pdf:application/pdf}
}

@article{voigt-users-nodate,
	title = {The {Users} {Who} {Say} ‘{Ni}’: {Audience} {Identification} in {Chinese}-language {Restaurant} {Reviews}},
	shorttitle = {The {Users} {Who} {Say} ‘{Ni}’},
	url = {http://www.aclweb.org/anthology/P15-2052},
	urldate = {2015-10-20},
	journal = {Volume 2: Short Papers},
	author = {Voigt, Rob and Jurafsky, Dan},
	pages = {314},
	file = {P15-2052.pdf:/home/user/Zotero/storage/4MWR7FJP/P15-2052.pdf:application/pdf}
}

@article{konstas-semantic-nodate,
	title = {Semantic {Role} {Labeling} {Improves} {Incremental} {Parsing}},
	url = {http://anthology.aclweb.org/P/P15/P15-1115.pdf},
	urldate = {2015-10-20},
	author = {Konstas, Ioannis and Keller, Frank},
	file = {P15-1115:/home/user/Zotero/storage/FEDQAKGB/P15-1115:application/pdf}
}

@inproceedings{blacoe-quantum-theoretic-2013,
	title = {A {Quantum}-{Theoretic} {Approach} to {Distributional} {Semantics}.},
	url = {http://www.aclweb.org/website/old\_anthology/N/N13/N13-1.pdf#page=885},
	urldate = {2015-10-20},
	booktitle = {{HLT}-{NAACL}},
	author = {Blacoe, William and Kashefi, Elham and Lapata, Mirella},
	year = {2013},
	pages = {847--857},
	file = {N13-1105.pdf:/home/user/Zotero/storage/XHWPMNPM/N13-1105.pdf:application/pdf}
}

@article{liu-textual-2015,
	title = {Textual {Analysis} for {Studying} {Chinese} {Historical} {Documents} and {Literary} {Novels}},
	url = {http://arxiv.org/abs/1510.03021},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1510.03021},
	author = {Liu, Chao-Lin and Jin, Guan-Tao and Wang, Hongsu and Liu, Qing-Feng and Cheng, Wen-Huei and Chiu, Wei-Yun and Tsai, Richard Tzong-Han and Wang, Yu-Chun},
	year = {2015},
	file = {1510.03021v1.pdf:/home/user/Zotero/storage/XES88DKA/1510.03021v1.pdf:application/pdf}
}

@article{spaak-decoding-nodate,
	title = {Decoding {Rich} {Spatial} {Information} with {High} {Temporal} {Resolution}},
	url = {http://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613(15)00207-7.pdf},
	urldate = {2015-10-20},
	author = {Spaak, Eelke},
	file = {1-s2.0-S1364661315002077-main.pdf:/home/user/Zotero/storage/T5B7BK5B/1-s2.0-S1364661315002077-main.pdf:application/pdf}
}

@article{pynte--line-2009,
	title = {On-line contextual influences during reading normal text: {The} role of nouns, verbs and adjectives},
	volume = {49},
	issn = {00426989},
	shorttitle = {On-line contextual influences during reading normal text},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698908006123},
	doi = {10.1016/j.visres.2008.12.016},
	language = {en},
	number = {5},
	urldate = {2015-10-20},
	journal = {Vision Research},
	author = {Pynte, Joel and New, Boris and Kennedy, Alan},
	month = mar,
	year = {2009},
	pages = {544--552},
	file = {1-s2.0-S0042698908006123-main.pdf:/home/user/Zotero/storage/MCZTTESU/1-s2.0-S0042698908006123-main.pdf:application/pdf}
}

@inproceedings{keller-cognitively-2010,
	title = {Cognitively plausible models of human language processing},
	url = {http://dl.acm.org/citation.cfm?id=1858854},
	urldate = {2015-10-20},
	booktitle = {Proceedings of the {ACL} 2010 {Conference} {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Keller, Frank},
	year = {2010},
	pages = {60--67},
	file = {P10-2012:/home/user/Zotero/storage/RW52IIFU/P10-2012:application/pdf}
}

@article{sayeed-vector-space-nodate,
	title = {Vector-space calculation of semantic surprisal for predicting word pronunciation duration},
	url = {http://anthology.aclweb.org/P/P15/P15-1074.pdf},
	urldate = {2015-10-20},
	author = {Sayeed, Asad and Fischer, Stefan and Demberg, Vera},
	file = {P15-1074:/home/user/Zotero/storage/TQ2VHF73/P15-1074:application/pdf}
}

@article{algorithmic-multiagent-2008,
	title = {Multiagent {Systems}},
	url = {http://www.eecs.harvard.edu/cs286r/courses/fall08/files/SLB.pdf},
	urldate = {2015-10-20},
	author = {Algorithmic, Game-Theoretic},
	year = {2008},
	file = {mas.pdf:/home/user/Zotero/storage/C589TXPV/mas.pdf:application/pdf}
}

@article{de-grijs-ten-2015,
	title = {Ten {Simple} {Rules} for {Establishing} {International} {Research} {Collaborations}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004311},
	doi = {10.1371/journal.pcbi.1004311},
	language = {en},
	number = {10},
	urldate = {2015-10-20},
	journal = {PLOS Computational Biology},
	author = {de Grijs, Richard},
	editor = {Markel, Scott},
	month = oct,
	year = {2015},
	pages = {e1004311},
	file = {journal.pcbi.1004311.pdf:/home/user/Zotero/storage/7VS9CF34/journal.pcbi.1004311.pdf:application/pdf}
}

@inproceedings{place-separating-2015,
	title = {Separating {Regular} {Languages} with {Two} {Quantifiers} {Alternations}},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=7174882},
	urldate = {2015-10-20},
	booktitle = {Logic in {Computer} {Science} ({LICS}), 2015 30th {Annual} {ACM}/{IEEE} {Symposium} on},
	publisher = {IEEE},
	author = {Place, Thomas},
	year = {2015},
	pages = {202--213},
	file = {LICS15.pdf:/home/user/Zotero/storage/5WUH4469/LICS15.pdf:application/pdf}
}

@inproceedings{konstas-incremental-2014-1,
	title = {Incremental {Semantic} {Role} {Labeling} with {Tree} {Adjoining} {Grammar}},
	volume = {301},
	url = {http://www.research.ed.ac.uk/portal/files/18904911/Konstas\_Keller\_ET\_AL\_2014\_Incremental\_Semantic\_Role\_Labeling\_with\_Tree\_Adjoining\_Grammar.pdf},
	urldate = {2015-10-20},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Konstas, Ioannis and Keller, Frank and Demberg, Vera and Lapata, Mirella},
	year = {2014},
	file = {emnlp14.pdf:/home/user/Zotero/storage/MGMN4A4P/emnlp14.pdf:application/pdf}
}

@article{demberg-incremental-2013-1,
	title = {Incremental, predictive parsing with psycholinguistically motivated tree-adjoining grammar},
	volume = {39},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00160},
	number = {4},
	urldate = {2015-10-20},
	journal = {Computational Linguistics},
	author = {Demberg, Vera and Keller, Frank and Koller, Alexander},
	year = {2013},
	pages = {1025--1066},
	file = {coli_a_00160.pdf:/home/user/Zotero/storage/PS72ZVA8/coli_a_00160.pdf:application/pdf}
}

@article{demberg-data-2008-1,
	title = {Data from eye-tracking corpora as evidence for theories of syntactic processing complexity},
	volume = {109},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027708001741},
	number = {2},
	urldate = {2015-10-20},
	journal = {Cognition},
	author = {Demberg, Vera and Keller, Frank},
	year = {2008},
	pages = {193--210},
	file = {CognitionDembergKeller08.pdf:/home/user/Zotero/storage/N5RJUZPI/CognitionDembergKeller08.pdf:application/pdf}
}

@book{eilenberg-automata-1976-2,
	address = {New York},
	title = {Automata, languages, and machines},
	isbn = {978-0-12-234002-4},
	publisher = {Academic Pr},
	author = {Eilenberg, S. and Tilson, B.},
	year = {1976},
	file = {[Samuel_Eilenberg,_Bret_Tilson_(Eds.)]_Automata,_L(BookZZ.org).pdf:/home/user/Zotero/storage/7HCX2CM7/[Samuel_Eilenberg,_Bret_Tilson_(Eds.)]_Automata,_L(BookZZ.org).pdf:application/pdf}
}

@article{cadilhac-value-2015,
	title = {Value {Automata} with {Filters}},
	url = {http://arxiv.org/abs/1510.02393},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1510.02393},
	author = {Cadilhac, Michaël and Krebs, Andreas and Limaye, Nutan},
	year = {2015},
	file = {1510.02393v1.pdf:/home/user/Zotero/storage/2NZ296JD/1510.02393v1.pdf:application/pdf}
}

@article{pado-probabilistic-2009,
	title = {A probabilistic model of semantic plausibility in sentence processing},
	volume = {33},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01033.x/pdf},
	number = {5},
	urldate = {2015-10-20},
	journal = {Cognitive Science},
	author = {Padó, Ulrike and Crocker, Matthew W. and Keller, Frank},
	year = {2009},
	pages = {794--838},
	file = {cogsci_journal09.pdf:/home/user/Zotero/storage/C8PFKU5P/cogsci_journal09.pdf:application/pdf}
}

@article{wilson-controlled-2015,
	title = {Controlled {Experiments} for {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1510.02675},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1510.02675},
	author = {Wilson, Benjamin J. and Schakel, Adriaan MJ},
	year = {2015},
	file = {1510.02675v1.pdf:/home/user/Zotero/storage/9ET9QS75/1510.02675v1.pdf:application/pdf}
}

@article{liu-textual-2015-1,
	title = {Textual {Analysis} for {Studying} {Chinese} {Historical} {Documents} and {Literary} {Novels}},
	url = {http://arxiv.org/abs/1510.03021},
	urldate = {2015-10-20},
	journal = {arXiv preprint arXiv:1510.03021},
	author = {Liu, Chao-Lin and Jin, Guan-Tao and Wang, Hongsu and Liu, Qing-Feng and Cheng, Wen-Huei and Chiu, Wei-Yun and Tsai, Richard Tzong-Han and Wang, Yu-Chun},
	year = {2015},
	file = {1510.03021v1.pdf:/home/user/Zotero/storage/KX5U8RRB/1510.03021v1.pdf:application/pdf}
}

@article{qian-involuntary-2015,
	title = {Involuntary attention in the absence of visual awareness},
	volume = {0},
	issn = {1350-6285},
	url = {http://dx.doi.org/10.1080/13506285.2015.1093249},
	doi = {10.1080/13506285.2015.1093249},
	abstract = {Previous studies have found a dissociation between voluntary attention and awareness. Here, we examined the relationship between involuntary attention and awareness. We presented a masked cue such that participants were unaware of the cue. This was followed by a search array, for which participants detected the presence of a shape target. Even though the cue was task irrelevant and unconscious, we observed faster reaction times when either the cue’s location or colour was congruent with shape target. We conclude an unconscious stimulus can elicit both space- and feature-based attention. These results provide further evidence for the dissociation between attention and consciousness.},
	number = {0},
	urldate = {2015-10-20},
	journal = {Visual Cognition},
	author = {Qian, Cheng and Liu, Taosheng},
	month = oct,
	year = {2015},
	pages = {1--4},
	file = {Qian_Liu_2015_Involuntary attention in the absence of visual awareness.pdf:/home/user/Zotero/storage/2MII5T27/Qian_Liu_2015_Involuntary attention in the absence of visual awareness.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IS7BQIZ3/13506285.2015.html:text/html}
}

@article{goncharov-p-doubling-2015,
	title = {P-{Doubling} in {Split} {PPs} and {Information} {Structure}},
	volume = {46},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00199},
	doi = {10.1162/LING\_a\_00199},
	number = {4},
	urldate = {2015-10-20},
	journal = {Linguistic Inquiry},
	author = {Goncharov, Julie},
	month = oct,
	year = {2015},
	pages = {731--742},
	file = {Snapshot:/home/user/Zotero/storage/5D86QMRT/LING_a_00199.html:text/html}
}

@article{mccready-against-2015,
	title = {Against {Lexical} {Self}-{Reference}},
	volume = {46},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00200},
	doi = {10.1162/LING\_a\_00200},
	number = {4},
	urldate = {2015-10-20},
	journal = {Linguistic Inquiry},
	author = {McCready, Eric},
	month = oct,
	year = {2015},
	pages = {742--752},
	file = {Snapshot:/home/user/Zotero/storage/MAA26RGC/LING_a_00200.html:text/html}
}

@article{lopez-parallel-2015,
	title = {Parallel {Computation} in {Word} {Formation}},
	volume = {46},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00197},
	doi = {10.1162/LING\_a\_00197},
	abstract = {Taking the Distributed Morphology model as a starting point, this article presents and develops the hypothesis that parallel computations drive some word formation processes. Along the way, some Distributed Morphology assumptions, particularly those concerning contextual allomorphy, are revised. It is argued that event structure is a syntactic head independent of the presence of a vP. Nominalizations in Spanish, which often exhibit verbal thematic vowels between the root and the nominalizing affix, turn out to be an ideal testing ground for theoretical hypotheses.},
	number = {4},
	urldate = {2015-10-20},
	journal = {Linguistic Inquiry},
	author = {López, Luis},
	month = oct,
	year = {2015},
	pages = {657--701},
	file = {Snapshot:/home/user/Zotero/storage/65UIGH62/LING_a_00197.html:text/html}
}

@article{belder-how-2015,
	title = {How to {Merge} a {Root}},
	volume = {46},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00196},
	doi = {10.1162/LING\_a\_00196},
	abstract = {The main goal of this article is to show that four properties of roots can be derived in a principled manner from the theory of Merge. The properties in question are the following: (a) roots have no grammatical features, (b) roots have no syntactic category, (c) roots are defined structurally rather than lexically, and (d) roots are dominated by functional material (rather than the other way around). We argue that the first Merge operation in each cyclic domain creates a radically empty structural position at the foot of the structure in which a root can be inserted at the level of Vocabulary Insertion. The four abovementioned properties of roots can then be shown to follow straightforwardly from this theory.},
	number = {4},
	urldate = {2015-10-20},
	journal = {Linguistic Inquiry},
	author = {Belder, Marijke De and Craenenbroeck, Jeroen van},
	month = oct,
	year = {2015},
	pages = {625--655},
	file = {Snapshot:/home/user/Zotero/storage/46A53NJG/LING_a_00196.html:text/html}
}

@article{dalessandro-modular-2015,
	title = {Modular {PIC}},
	volume = {46},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00195},
	doi = {10.1162/LING\_a\_00195},
	abstract = {This article argues that there can only be one chunk-defining device in grammar: a theory cannot afford to have the same work done twice, once by phases, a second time by prosodic constituency. As it stands, however, phase theory is unable to describe all phonologically relevant chunks; these are too small and too diverse to be delineated. To qualify as the only chunk-defining device in grammar, phase theory therefore needs to be made more flexible—that is, to be adapted to the demands of phonology. To allow phase theory to describe all phonologically relevant chunks, we propose the separation of the Spell-Out operation from the Phase Impenetrability Condition (PIC). When Spell-Out occurs, every access point may or may not be associated with a PIC at PF, and the same optional endowment with a PIC holds for syntax. This is what we call Modular PIC. Empirically, on the basis of Abruzzese raddoppiamento fonosintattico and data from Bantu, we show that PIC effects in syntax and phonology are entirely independent: a given Spell-Out operation may leave traces in both modules, in either one, or in neither.},
	number = {4},
	urldate = {2015-10-20},
	journal = {Linguistic Inquiry},
	author = {D’Alessandro, Roberta and Scheer, Tobias},
	month = oct,
	year = {2015},
	pages = {593--624},
	file = {Snapshot:/home/user/Zotero/storage/XB3DTEM8/LING_a_00195.html:text/html}
}

@article{noauthor-leibniz-collectanea-hottentot.pdf-nodate,
	title = {leibniz-collectanea-hottentot.pdf},
	file = {Unknown - Unknown - leibniz-collectanea-hottentot.pdf:/home/user/Zotero/storage/GH7JGKVT/Unknown - Unknown - leibniz-collectanea-hottentot.pdf.pdf:application/pdf}
}

@article{noauthor-leer-nodate,
	title = {Leer jouself {Nama} {Teach} yourself {Nama} {Aitsama} {Namagowaba} ║ kh ā ║ kh ā sen},
	pages = {1--19},
	file = {Unknown - Unknown - Leer jouself Nama Teach yourself Nama Aitsama Namagowaba ║ kh ā ║ kh ā sen:/home/user/Zotero/storage/4DUTZVH3/Unknown - Unknown - Leer jouself Nama Teach yourself Nama Aitsama Namagowaba ║ kh ā ║ kh ā sen.pdf:application/pdf}
}

@article{noauthor-khoekhoe-nodate,
	title = {khoekhoe texts},
	file = {Unknown - Unknown - khoekhoe texts:/home/user/Zotero/storage/C37DDRWN/Unknown - Unknown - khoekhoe texts.txt:text/plain}
}

@article{noauthor-khoekhoe-2005,
	title = {The {Khoekhoe} language, a member of the {Khoisan} family, was widely spoken by {Southern} {Africa} pastoralists and hunters-gatherers a few centuries ago. {Apart} from varieties still spoken in the 20},
	volume = {34},
	number = {2},
	year = {2005},
	pages = {66--84},
	file = {Unknown - 2005 - The Khoekhoe language, a member of the Khoisan family, was widely spoken by Southern Africa pastoralists and hunters-ga:/home/user/Zotero/storage/BP4AXV4E/Unknown - 2005 - The Khoekhoe language, a member of the Khoisan family, was widely spoken by Southern Africa pastoralists and hunters-ga.pdf:application/pdf}
}

@article{witzlack-makarevich-aspects-1978,
	title = {Aspects of {Information} {Structure} in {Richtersveld} {Nama}},
	author = {Witzlack-makarevich, Alena},
	year = {1978},
	pages = {0--96},
	file = {Witzlack-makarevich - 1978 - Aspects of Information Structure in Richtersveld Nama:/home/user/Zotero/storage/HJNNXEBH/Witzlack-makarevich - 1978 - Aspects of Information Structure in Richtersveld Nama.pdf:application/pdf}
}

@phdthesis{witzlack-maka-revich-aspects-2006,
	title = {Aspects of {{I}}nformation {{S}}tructure in {{R}}ichtersveld {{N}}ama},
	author = {Witzlack-Maka\-revich, Alena},
	year = {2006}
}

@article{washburn-minimalist-2001,
	title = {A {{M}}inimalist {Approach} to {{K}}hoekhoe {Declaratives}},
	volume = {18},
	journal = {Cornell Working Papers in Linguistics},
	author = {Washburn, P},
	year = {2001},
	pages = {28--56}
}

@book{wagner-robertz-liedtexte-2002,
	title = {Liedtexte der {Dama} ({S}{ü}dwestafrika / {Namibia})},
	publisher = {Köppe},
	author = {Wagner-Robertz, Dagmar},
	year = {2002}
}

@book{wagner-robertz-heilungsritual-2000,
	address = {Köln},
	title = {Ein {Heilungsritual} der {Dama} ({S}{ü}dwestafrika / {Namibia})},
	publisher = {Köppe},
	author = {Wagner-Robertz, Dagmar},
	year = {2000}
}

@book{schultze-aus-1907,
	address = {Jena},
	title = {Aus {{N}}amaland und {{K}}alahari},
	url = {http://archive.org/details/ausnamalandundk00berlgoog},
	publisher = {Gustav Fischer},
	author = {Schultze, Leonhard},
	year = {1907}
}

@book{rust-nama-1969,
	address = {Pietermaritzburg},
	title = {Nama {{W}}{ö}rterbuch},
	publisher = {University of Natal Press},
	author = {Rust, F},
	year = {1969}
}

@book{planert-handbuch-1905,
	title = {Handbuch der {Nama}-{Sprache} in {Deutsch}-{S}{ü}dwestafrika},
	publisher = {Reimer},
	author = {Planert, W},
	year = {1905}
}

@book{olpp-nama-grammatika-1977,
	address = {Windhoek},
	title = {Nama-grammatika, soos verwerk deur {H}. {J}. {Krüger}},
	publisher = {Inboorlingtaalburo van die Departement van Bantoe-onderwys},
	author = {Olpp, Johannes},
	year = {1977}
}

@book{martin-namib-1998,
	edition = {Translated},
	title = {Namib !nâ //hai//hâ},
	publisher = {Gamsberg Macmillan},
	author = {Martin, Henno},
	year = {1998}
}

@book{stopa-teksty-1936,
	address = {Warsaw},
	title = {Teksty {Hotentockie}},
	author = {Stopa, Roman},
	year = {1936}
}

@article{schaar-nama-fabeln-1917,
	title = {Nama-{Fabeln}},
	volume = {8},
	journal = {Zeitschrift für Kolonialsprachen},
	author = {Schaar, W},
	year = {1917},
	pages = {81--109}
}

@book{rust-praktische-1965,
	title = {Praktische {{N}}amagrammatik},
	author = {Rust, F},
	year = {1965}
}

@book{namaseb-khoekhoegowab-2003,
	address = {Windhoek},
	title = {Khoekhoegowab di i??sedi ge {Khoekhoegowab} !na?? ra //gamhe},
	publisher = {Out of Africa Publishers},
	author = {Namaseb, Levi},
	year = {2003}
}

@book{meinhof-lehrbuch-1909,
	title = {Lehrbuch der {Nama}-{Sprache}},
	publisher = {Georg Reimer},
	author = {Meinhof, Carl},
	year = {1909}
}

@article{linguistics-tuu-2011,
	title = {A {Tuu} substrate in {Khoekhoe} ? {The} case of compound verbs},
	number = {July},
	author = {Linguistics, Historical and Rapold, Christian J and Leiden, Universiteit},
	year = {2011},
	pages = {1--18},
	file = {Linguistics, Rapold, Leiden - 2011 - A Tuu substrate in Khoekhoe The case of compound verbs:/home/user/Zotero/storage/9I97NZPF/Linguistics, Rapold, Leiden - 2011 - A Tuu substrate in Khoekhoe The case of compound verbs.pdf:application/pdf}
}

@book{kronlein-wortschatz-1889,
	address = {Berlin},
	title = {Wortschatz der {{K}}hoi-{{K}}hoin},
	publisher = {Deutsche Kolonialgesellschaft},
	author = {Krönlein, Johann Georg},
	year = {1889},
	file = {Krönlein - Unknown - No Title:/home/user/Zotero/storage/48MXEKGG/Krönlein - Unknown - No Title.pdf:application/pdf}
}

@article{klein-tense-1976,
	title = {Tense and aspect in the {{D}}amara verbal system},
	volume = {35},
	journal = {African Studies},
	author = {Klein, Harriet E M},
	year = {1976},
	pages = {207--227}
}

@article{hoymann-questions-2010,
	title = {Questions and responses in ????khoe {Hai}????om},
	journal = {Journal of Pragmatics},
	author = {Hoymann, Gertie},
	year = {2010},
	file = {Hoymann - 2010 - Questions and responses in khoe Haiom:/home/user/Zotero/storage/323FAP5Q/Hoymann - 2010 - Questions and responses in khoe Haiom.pdf:application/pdf}
}

@article{heinrich-linguistic-2008,
	title = {Linguistic hypotheses on the origin of {Namibian} {Khoekhoe} speakers},
	volume = {20},
	number = {2007},
	author = {Heinrich, Wilfrid and Haacke, Gerhard},
	year = {2008},
	keywords = {a synopsis of the, among, damara, findings of a dialect, hai, khoekhoegowab, khoisan, kwadi, linguistic, members of the department, of african languages of, om, survey conducted by, the dialects of khoekhoegowab, the present paper presents, the university of namibia},
	pages = {163--177},
	file = {Heinrich, Haacke - 2008 - Linguistic hypotheses on the origin of Namibian Khoekhoe speakers:/home/user/Zotero/storage/BQQC9V94/Heinrich, Haacke - 2008 - Linguistic hypotheses on the origin of Namibian Khoekhoe speakers.pdf:application/pdf}
}

@article{job--2014,
	title = {" {FEATURES} {OF} {A} {SESFONTEIN} {DIALECT} {OF} {KHOEKHOEGOWAB} "},
	author = {Job, Sylvanus},
	year = {2014},
	pages = {1--23},
	file = {Job - 2014 - FEATURES OF A SESFONTEIN DIALECT OF KHOEKHOEGOWAB:/home/user/Zotero/storage/PT53UXZJ/Job - 2014 - FEATURES OF A SESFONTEIN DIALECT OF KHOEKHOEGOWAB.pdf:application/pdf}
}

@inproceedings{hahn-predication-2014,
	title = {Predication and {NP} {Structure} in an {Omnipredicative} {Language}: {The} {Case} of {Khoekhoe}},
	url = {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2014/hahn.pdf},
	booktitle = {Proceedings of the 21st {{I}}nternational {{C}}onference on {{H}}ead-{{D}}riven {{P}}hrase {{S}}tructure {{G}}rammar},
	publisher = {CSLI Publications},
	author = {Hahn, Michael},
	editor = {{Stefan Müller}},
	year = {2014},
	file = {hpsg2014-slides-1:/home/user/Zotero/storage/R9NM6JM3/hpsg2014-slides-1.pdf:application/pdf}
}

@inproceedings{hahn-word-2013,
	title = {Word {Order} {Variation} in {Khoekhoe}},
	url = {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2013/hahn.pdf},
	booktitle = {Proceedings of the 20th {{I}}nternational {{C}}onference on {{H}}ead-{{D}}riven {{P}}hrase {{S}}tructure {{G}}rammar},
	author = {Hahn, Michael},
	editor = {Müller, Stefan},
	year = {2013},
	pages = {48--68},
	file = {Hahn - 2013 - Word Order Variation in Khoekhoe:/home/user/Zotero/storage/R4EKTHPN/Hahn - 2013 - Word Order Variation in Khoekhoe.pdf:application/pdf}
}

@book{hagman-nama-1977,
	title = {Nama {Hottentot} grammar},
	volume = {15},
	publisher = {Indiana University Bloomington},
	author = {Hagman, Roy S.},
	year = {1977}
}

@article{haacke-naro-2003,
	title = {{NARO} {SYNTAX} {FROM} {THE} {PERSPECTIVE} {OF} {THE} {DESENTENTIAL}},
	author = {Haacke, Wilfrid and Hardly, Introduction and Khoesaan, Central and Khoesaan, Southern and Khoisan, Central},
	year = {2003},
	pages = {1--22},
	file = {Haacke et al. - 2003 - NARO SYNTAX FROM THE PERSPECTIVE OF THE DESENTENTIAL:/home/user/Zotero/storage/2QJVWWEN/Haacke et al. - 2003 - NARO SYNTAX FROM THE PERSPECTIVE OF THE DESENTENTIAL.pdf:application/pdf}
}

@incollection{haacke-compound-1992,
	address = {Hatfield},
	title = {Compound noun phrases in {Nama}},
	booktitle = {African linguistic contributions},
	author = {Haacke, Wilfrid H G},
	editor = {Gowlett, Derek F.},
	year = {1992},
	pages = {189--194}
}

@article{haacke-syntactic-2006,
	title = {Syntactic focus marking in {Khoekhoe} ("{Nama}/{Damara}")},
	volume = {46},
	journal = {ZAS Papers in Linguistics},
	author = {Haacke, Wilfrid H G},
	year = {2006},
	pages = {105 ??? 127--105 ??? 127},
	file = {Haacke - 2006 - Syntactic focus marking in Khoekhoe (NamaDamara):/home/user/Zotero/storage/M66CAZEB/Haacke - 2006 - Syntactic focus marking in Khoekhoe (NamaDamara).pdf:application/pdf}
}

@article{deoskar-multiple-2003,
	title = {Multiple {Verb} {Constructions} in {Khoekhoe}},
	author = {Deoskar, Tejaswini},
	year = {2003},
	pages = {1--32},
	file = {Deoskar - 2003 - Multiple Verb Constructions in Khoekhoe 1 Introduction:/home/user/Zotero/storage/I5CSWCP4/Deoskar - 2003 - Multiple Verb Constructions in Khoekhoe 1 Introduction.pdf:application/pdf}
}

@phdthesis{haacke-subject-1978,
	address = {Colchester},
	title = {Subject deposition in {{N}}ama},
	author = {Haacke, Wilfrid H G},
	year = {1978},
	file = {Deposition, Of - 1978 - University of:/home/user/Zotero/storage/G42W4928/Deposition, Of - 1978 - University of.pdf:application/pdf;Heinrich, Haacke - 1978 - SUBJECT DEPuSITION IN NAMA:/home/user/Zotero/storage/GRU6XQAC/Heinrich, Haacke - 1978 - SUBJECT DEPuSITION IN NAMA.pdf:application/pdf}
}

@incollection{haacke-nama-1980,
	address = {Pretoria},
	title = {Nama coreferential copulative sentences re-assessed},
	booktitle = {Bushman and {Hottentot} linguistic {Studies}},
	publisher = {University of South Africa},
	author = {Haacke, Wilfrid H G},
	editor = {Snyman, Jannie Winston},
	year = {1980},
	pages = {80--106}
}

@article{haacke-instances-1995,
	title = {Instances of incorporation and compounding in {Khoekhoegowab} ({Nama}/{Damara})},
	journal = {The complete linguist: papers in memory of Patrick J. Dickens. K{ö}ln: R{ü}diger K{ö}ppe},
	author = {Haacke, Wilfrid H G},
	year = {1995},
	pages = {339--361}
}

@article{haacke-noun-1985,
	title = {Noun phrase accessibility to relativization in {Herero} and {Nama}},
	volume = {5},
	journal = {South African journal of African languages},
	author = {Haacke, Wilfrid H G},
	year = {1985},
	pages = {43--48}
}

@article{den-besten-khoekhoe-2002,
	title = {Khoekhoe {Syntax} and {Its} {Implications} for {{L}}2 {Acquisition} of {{D}}utch and {{A}}frikaans},
	volume = {14.1},
	journal = {Journal of Germanic Linguistics},
	author = {den Besten, Hans},
	year = {2002},
	pages = {3--56}
}

@article{dempwolff-einfuhrung-nodate,
	title = {Einführung in die {{S}}prache der {Nama}-{{H}}ottentotten},
	journal = {Zeitschrift für Eingeborenen-Sprachen},
	author = {Dempwolff, Otto}
}

@book{bollig-heilungsritual-nodate,
	title = {Heilungsritual},
	isbn = {3-89645-351-3},
	author = {Bollig, Michael},
	file = {Bollig - Unknown - Heilungsritual:/home/user/Zotero/storage/92B5WVTJ/Bollig - Unknown - Heilungsritual.pdf:application/pdf}
}

@book{bollig-liedtexte-nodate,
	title = {Liedtexte},
	isbn = {3-89645-354-8},
	author = {Bollig, Michael},
	file = {Bollig - Unknown - Liedtexte:/home/user/Zotero/storage/MWHGCE7X/Bollig - Unknown - Liedtexte.pdf:application/pdf}
}

@article{besten-khoekhoe-2002,
	title = {Khoekhoe {Syntax} and {Its} {Implications} for {L}2 {Acquisition} of {Dutch} and {Afrikaans}},
	volume = {1},
	author = {Besten, Hans Den},
	year = {2002},
	pages = {3--56},
	file = {Besten - 2002 - Khoekhoe Syntax and Its Implications for L2 Acquisition of Dutch and Afrikaans:/home/user/Zotero/storage/H7FFVCCR/Besten - 2002 - Khoekhoe Syntax and Its Implications for L2 Acquisition of Dutch and Afrikaans.pdf:application/pdf}
}

@article{besten-isaac-nodate,
	title = {Isaac le {Long} ’ s {German} {Version} of {Grevenbroek} ’ s {Khoekhoe} {Glossaries} as {Published} by {Juncker} in 1710},
	volume = {5},
	number = {1},
	author = {Besten, Hans D E N},
	keywords = {abstract 1, brethren, by way of an, cape dutch, cape khoekhoe, christian juncker, have hit upon two, herrnhut ar-, isaac le long, j, khoekhoe documents in the, moravian, over the years researchers, peter kolb, van grevenbroek, w},
	pages = {7--46},
	file = {Besten - Unknown - Isaac le Long ’ s German Version of Grevenbroek ’ s Khoekhoe Glossaries as Published by Juncker in 1710:/home/user/Zotero/storage/ACD942AX/Besten - Unknown - Isaac le Long ’ s German Version of Grevenbroek ’ s Khoekhoe Glossaries as Published by Juncker in 1710.pdf:application/pdf}
}

@article{baumann-nama-texte-1915,
	title = {Nama-{Texte}},
	volume = {6},
	journal = {Zeitschrift für Kolonialsprachen},
	author = {Baumann, Chr.},
	year = {1915},
	pages = {55--78}
}

@article{symonds-gender-2006,
	title = {Gender differences in publication output: {Towards} an unbiased metric of research performance},
	volume = {1},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0000127},
	abstract = {We examined the publication records of a cohort of 168 life scientists in the field of ecology and evolutionary biology to assess gender differences in research performance. Clear discrepancies in publication rate between men and women appear very early in their careers and this has consequences for the subsequent citation of their work. We show that a recently proposed index designed to rank scientists fairly is in fact strongly biased against female researchers, and advocate a modified index to assess men and women on a more equitable basis.},
	number = {1},
	journal = {PLoS ONE},
	author = {Symonds, M. R E and Gemmell, Neil J. and Braisher, Tamsin L. and Gorringe, Kylie L. and Elgar, Mark a.},
	year = {2006},
	pages = {1--5},
	file = {Symonds et al. - 2006 - Gender differences in publication output Towards an unbiased metric of research performance:/home/user/Zotero/storage/2V7HBZ6Q/Symonds et al. - 2006 - Gender differences in publication output Towards an unbiased metric of research performance.pdf:application/pdf}
}

@article{steel-nature-2007,
	title = {The nature of procrastination: a meta-analytic and theoretical review of quintessential self-regulatory failure.},
	volume = {133},
	issn = {0033-2909 (Print)\n0033-2909 (Linking)},
	doi = {10.1037/0033-2909.133.1.65},
	abstract = {Procrastination is a prevalent and pernicious form of self-regulatory failure that is not entirely understood. Hence, the relevant conceptual, theoretical, and empirical work is reviewed, drawing upon correlational, experimental, and qualitative findings. A meta-analysis of procrastination's possible causes and effects, based on 691 correlations, reveals that neuroticism, rebelliousness, and sensation seeking show only a weak connection. Strong and consistent predictors of procrastination were task aversiveness, task delay, self-efficacy, and impulsiveness, as well as conscientiousness and its facets of self-control, distractibility, organization, and achievement motivation. These effects prove consistent with temporal motivation theory, an integrative hybrid of expectancy theory and hyperbolic discounting. Continued research into procrastination should not be delayed, especially because its prevalence appears to be growing.},
	number = {1},
	journal = {Psychological bulletin},
	author = {Steel, Piers},
	year = {2007},
	keywords = {95, a way of life, although virtually all of, estimates indicate that 80, have at least dallied, in pro-, irrational delay, meta-analysis, of college students engage, pathological decision making, procrastination, procrastination is extremely prevalent, some have made it, us, with dallying},
	pages = {65--94},
	file = {Procrastination:/home/user/Zotero/storage/MM5FWCT3/Procrastination.pdf:application/pdf}
}

@article{simmons-false-positive-2011,
	title = {False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant.},
	volume = {22},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22006061},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	number = {11},
	journal = {Psychological science},
	author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
	month = nov,
	year = {2011},
	keywords = {Humans, Adult, Young Adult, Computer Simulation, Data Collection, Data Collection: standards, Data Interpretation, Statistical, Peer Review, Research, Peer Review, Research: standards, Practice Guidelines as Topic, Publications, Publications: standards, Research Design, Research Design: standards, Research Personnel, Research Personnel: psychology, Research Personnel: standards, Statistics as Topic},
	pages = {1359--66},
	file = {Simmons, Nelson, Simonsohn - 2011 - False-positive psychology undisclosed flexibility in data collection and analysis allows presenting:/home/user/Zotero/storage/KSQZKBVI/Simmons, Nelson, Simonsohn - 2011 - False-positive psychology undisclosed flexibility in data collection and analysis allows presenting.pdf:application/pdf}
}

@article{holman-evidence-2015,
	title = {Evidence of {Experimental} {Bias} in the {Life} {Sciences} : {Why} {We} {Need} {Blind} {Data} {Recording}},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002190},
	doi = {10.5281/zenodo.13147},
	journal = {PLoS biology},
	author = {Holman, Luke and Head, Megan L and Lanfear, Robert and Jennions, Michael D},
	year = {2015},
	pages = {1--12},
	file = {journal.pbio.1002190:/home/user/Zotero/storage/72QERKI3/journal.pbio.1002190.pdf:application/pdf}
}

@article{francois-arbitrariness-2015,
	title = {Arbitrariness of peer review: {A} {Bayesian} analysis of the {NIPS} experiment},
	volume = {25},
	url = {http://arxiv.org/abs/1507.06411},
	abstract = {The principle of peer review is central to the evaluation of research, by ensuring that only high-quality items are funded or published. But peer review has also received criticism, as the selection of reviewers may introduce biases in the system. In 2014, the organizers of the ``Neural Information Processing Systems\rq\rq{} conference conducted an experiment in which \$10\\%\$ of submitted manuscripts (166 items) went through the review process twice. Arbitrariness was measured as the conditional probability for an accepted submission to get rejected if examined by the second committee. This number was equal to \$60\\%\$, for a total acceptance rate equal to \$22.5\\%\$. Here we present a Bayesian analysis of those two numbers, by introducing a hidden parameter which measures the probability that a submission meets basic quality criteria. The standard quality criteria usually include novelty, clarity, reproducibility, correctness and no form of misconduct, and are met by a large proportions of submitted items. The Bayesian estimate for the hidden parameter was equal to \$56\\%\$ (\$95\\%\$CI: \$ I = (0.34, 0.83)\$), and had a clear interpretation. The result suggested the total acceptance rate should be increased in order to decrease arbitrariness estimates in future review processes.},
	author = {Francois, Olivier},
	year = {2015},
	keywords = {arbitrariness, corresponding author, nips experiment, olivier fran, peer review},
	pages = {1--9},
	file = {1507.06411v1:/home/user/Zotero/storage/KIGSHSC3/1507.06411v1.pdf:application/pdf}
}

@book{david-sustainable-2009,
	title = {Sustainable {Energy} — without the hot air {This} {Cover}-sheet must not appear in the printed book .},
	volume = {78},
	isbn = {978-0-9544529-3-3},
	url = {www.withouthotair.com},
	abstract = {Addressing the sustainable energy crisis in an objective manner, this enlightening book analyzes the relevant numbers and organizes a plan for change on both a personal level and an international scalefor Europe, the Untied States, and the world. In case study format, this informative reference answers questions surrounding nuclear energy, the potential of sustainable fossil fuels, and the possibilities of sharing renewable power with foreign countries. While underlining the difficulty of minimizing consumption, the tone remains positive as it debunks misinformation and clearly explains the calculations of expenditure per person to encourage people to make individual changes that will benefit the world at large.},
	author = {David, Copyright and Mackay, J C},
	year = {2009},
	file = {David, Mackay - 2009 - Sustainable Energy — without the hot air This Cover-sheet must not appear in the printed book:/home/user/Zotero/storage/CEGFAFT7/David, Mackay - 2009 - Sustainable Energy — without the hot air This Cover-sheet must not appear in the printed book .pdf:application/pdf}
}

@article{brembs-deep-2013,
	title = {Deep impact: unintended consequences of journal rank.},
	volume = {7},
	issn = {1662-5161 (Electronic)\r1662-5161 (Linking)},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3690355&tool=pmcentrez&rendertype=abstract},
	doi = {10.3389/fnhum.2013.00291},
	abstract = {Most researchers acknowledge an intrinsic hierarchy in the scholarly journals ("journal rank") that they submit their work to, and adjust not only their submission but also their reading strategies accordingly. On the other hand, much has been written about the negative effects of institutionalizing journal rank as an impact measure. So far, contributions to the debate concerning the limitations of journal rank as a scientific impact assessment tool have either lacked data, or relied on only a few studies. In this review, we present the most recent and pertinent data on the consequences of our current scholarly communication system with respect to various measures of scientific quality (such as utility/citations, methodological soundness, expert ratings or retractions). These data corroborate previous hypotheses: using journal rank as an assessment tool is bad scientific practice. Moreover, the data lead us to argue that any journal rank (not only the currently-favored Impact Factor) would have this negative impact. Therefore, we suggest that abandoning journals altogether, in favor of a library-based scholarly communication system, will ultimately be necessary. This new system will use modern information technology to vastly improve the filter, sort and discovery functions of the current journal system.},
	journal = {Frontiers in human neuroscience},
	author = {Brembs, Björn and Button, Katherine and Munafò, Marcus},
	year = {2013},
	pages = {291--291},
	file = {45406_Brembs_ProvisionalPDF:/home/user/Zotero/storage/BTKXTGPF/45406_Brembs_ProvisionalPDF.pdf:application/pdf}
}

@article{noauthor-chinese-nodate,
	title = {chinese zero pronoun resolution},
	pages = {1622--1628},
	file = {Unknown - Unknown - chinese zero pronoun resolution:/home/user/Zotero/storage/GQ65HBBX/Unknown - Unknown - chinese zero pronoun resolution.pdf:application/pdf}
}

@article{noauthor-svm-nodate,
	title = {svm},
	file = {Unknown - Unknown - svm:/home/user/Zotero/storage/TCQ75G9N/Unknown - Unknown - svm.ps:application/postscript}
}

@article{noauthor-cuckersmale-nodate,
	title = {cuckersmale},
	file = {Unknown - Unknown - cuckersmale:/home/user/Zotero/storage/6VG2WHNQ/Unknown - Unknown - cuckersmale.ps:application/postscript}
}

@article{noauthor-parametric-nodate,
	title = {Parametric vs non-parametric methods for data analysis.pdf},
	file = {gp-neural-nets15:/home/user/Zotero/storage/9T7AE4V8/gp-neural-nets15.pdf:application/pdf}
}

@article{zweig-speech-2011,
	title = {Speech recognitionwith segmental conditional random fields: {A} summary of the {JHU} {CLSP} 2010 {Summer} {Workshop}},
	issn = {9781457705397},
	doi = {10.1109/ICASSP.2011.5947490},
	abstract = {This paper summarizes the 2010 CLSP Summer Workshop on speech recognition at Johns Hopkins University. The key theme of the workshop was to improve on state-of-the-art speech recognition systems by using Segmental Conditional Random Fields (SCRFs) to integrate multiple types of information. This approach uses a state-of-the-art baseline as a springboard from which to add a suite of novel features including ones derived from acoustic templates, deep neural net phoneme detections, duration models, modulation features, and whole word point-process models. The SCRF framework is able to appropriately weight these different information sources to produce significant gains on both the Broadcast News and Wall Street Journal tasks.},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	author = {Zweig, G. and Nguyen, P. and Van Compernolle, D. and Demuynck, K. and Atlas, L. and Clark, P. and Sell, G. and Wang, M. and Sha, F. and Hermansky, H. and Karakos, D. and Jansen, a. and Thomas, S. and S, G. S V S and Bowman, S. and Kao, J.},
	year = {2011},
	keywords = {speech recognition, CRF, Segmental Conditional Random Field},
	pages = {5044--5047},
	file = {icassp2011_scarf:/home/user/Zotero/storage/SRBZCZPD/icassp2011_scarf.pdf:application/pdf}
}

@inproceedings{zheng-character-based-2015,
	title = {Character-{Based} {Parsing} with {Convolutional} {Neural} {Network}},
	booktitle = {{IJCAI}},
	author = {Zheng, Xiaoqing and Peng, Haoyuan and Chen, Yi and Zhang, Pengjing and Zhang, Wenqiang},
	year = {2015},
	keywords = {Technical Papers — Machine Learning},
	pages = {1054--1060},
	file = {IJCAI15-153:/home/user/Zotero/storage/9D8TRBEW/IJCAI15-153.pdf:application/pdf}
}

@article{zhang-mind-2014,
	title = {Mind the {Gap}: {Machine} {Translation} by {Minimizing} the {Semantic} {Gap} in {Embedding} {Space}},
	issn = {9781577356783},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewPDFInterstitial/8247/8623},
	journal = {Twenty-Eighth AAAI Conference on …},
	author = {Zhang, Jiajun and Liu, Shujie and Li, Mu and Zhou, Ming and Zong, Chengqing},
	year = {2014},
	keywords = {NLP and Text Mining},
	pages = {1657--1663},
	file = {Zhang et al. - 2014 - Mind the Gap Machine Translation by Minimizing the Semantic Gap in Embedding Space:/home/user/Zotero/storage/QAUKF5C9/Zhang et al. - 2014 - Mind the Gap Machine Translation by Minimizing the Semantic Gap in Embedding Space.pdf:application/pdf}
}

@inproceedings{zelle-learning-1996,
	title = {Learning to {Parse} {Database} {Queries} {Using} {Inductive} {Logic} {Programming}},
	booktitle = {In {Proceedings} of the {Thirteenth} {National} {Conference} on {Artificial} {Intelligence}},
	author = {Zelle, John M and Mooney, Raymond J},
	year = {1996},
	keywords = {ILP},
	pages = {1050--1055}
}

@inproceedings{zelle-comparative-1996,
	title = {Comparative {Results} on {Using} {Inductive} {Logic} {Programming} for {Corpus}-based {Parser} {Construction}},
	booktitle = {In {Wermter}, {S}., {Riloff}, {E}., \& {Scheler}, {G}. ({Eds}.), {Connectionist}, {Statistical}, and {Symbolic} {Approaches} to {Learning} for {Natural} {Language} {Processing}},
	publisher = {Springer},
	author = {Zelle, John M and Mooney, Raymond J},
	year = {1996},
	keywords = {ILP},
	pages = {355--369}
}

@article{zhao-stochastic-2014,
	title = {Stochastic {Optimization} with {Importance} {Sampling}},
	volume = {37},
	journal = {2arXiv stat.ML},
	author = {Zhao, Peilin and Zhang, Tong},
	year = {2014},
	file = {zhaoa15:/home/user/Zotero/storage/ZKSTTRP2/zhaoa15.pdf:application/pdf}
}

@article{zhao-revisiting-2015,
	title = {Revisiting {Gaussian} {Process} {Dynamical} {Models}},
	number = {Ijcai},
	author = {Zhao, Jing and Sun, Shiliang},
	year = {2015},
	keywords = {Technical Papers — Machine Learning},
	pages = {1047--1053},
	file = {IJCAI15-152:/home/user/Zotero/storage/UN9WWXQ2/IJCAI15-152.pdf:application/pdf}
}

@article{zhang-deterministic-2015,
	title = {Deterministic convergence of chaos injection-based gradient method for training feedforward neural networks},
	url = {http://dx.doi.org/10.1007/s11571-014-9323-z},
	doi = {10.1007/s11571-014-9323-z},
	journal = {Cognitive Neurodynamics},
	author = {Zhang, Huisheng and Zhang, Ying},
	year = {2015},
	keywords = {Online learning, corresponding author, batch learning, chaos injection-based gradient method, convergence, d, dalian maritime university, dr, feedforward neural networks, huisheng zhang, ph, s institution},
	pages = {331--340}
}

@article{yogatama-learning-2014,
	title = {Learning {Word} {Representations} with {Hierarchical} {Sparse} {Coding}},
	url = {http://arxiv.org/abs/1406.2035},
	abstract = {We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We apply an efficient online and distributed learning method. Experiments on various benchmark tasks---word similarity ranking, analogies, sentence completion, and sentiment analysis---demonstrate that the method outperforms or is competitive with state-of-the-art neural network representations. Our word representations are available at \url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}},
	author = {Yogatama, Dani and Faruqui, Manaal and Dyer, Chris and Smith, Noah a.},
	year = {2014},
	file = {yogatama15:/home/user/Zotero/storage/PGGGI6C5/yogatama15.pdf:application/pdf}
}

@article{xu-security-2015,
	title = {Security {Games} with {Information} {Leakage} : {Modeling} and {Computation}},
	number = {Ijcai},
	author = {Xu, Haifeng and Jiang, Albert X},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {674--680},
	file = {IJCAI15-101:/home/user/Zotero/storage/DV5X8SX8/IJCAI15-101.pdf:application/pdf}
}

@article{xie-music-2011,
	title = {Music tagging with regularized logistic regression},
	issn = {9780615548654},
	number = {Ismir},
	journal = {International Conference on Music Information Retrieval},
	author = {Xie, Bo and Bian, Wei and Tao, Dacheng and Chordia, Parag},
	year = {2011},
	pages = {711--716},
	file = {PS6-10:/home/user/Zotero/storage/FAI82PNA/PS6-10.pdf:application/pdf;Xie et al. - 2011 - Music tagging with regularized logistic regression:/home/user/Zotero/storage/JX4FC23C/Xie et al. - 2011 - Music tagging with regularized logistic regression.pdf:application/pdf}
}

@article{wood-1-1995,
	title = {From 1},
	volume = {2541},
	number = {1037},
	journal = {Shock},
	author = {Wood, Wassennan Bernard and Davies, Alastair},
	year = {1995},
	file = {Wood, Davies - 1995 - From 1:/home/user/Zotero/storage/RWUJWPBS/Wood, Davies - 1995 - From 1.pdf:application/pdf}
}

@article{yin-computing-2015,
	title = {Computing {Optimal} {Mixed} {Strategies} for {Security} {Games} with {Dynamic} {Payoffs}},
	number = {Ijcai},
	author = {Yin, Yue and Xu, Haifeng and Gan, Jiarui and An, Bo and Jiang, Albert Xin},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {681--687},
	file = {IJCAI15-102:/home/user/Zotero/storage/K362U9RC/IJCAI15-102.pdf:application/pdf}
}

@article{xie-modeling-2015,
	title = {Modeling {Quantum} {Entanglements} in {Quantum} {Language} {Models} ∗},
	number = {Ijcai},
	author = {Xie, Mengjiao and Hou, Yuexian and Zhang, Peng and Li, Jingfei and Li, Wenjie and Song, Dawei},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1362--1368},
	file = {IJCAI15-196:/home/user/Zotero/storage/G5WIBUG6/IJCAI15-196.pdf:application/pdf}
}

@article{weston-memory-2015,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	journal = {Iclr 2015},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
	pages = {1--14},
	file = {1410.3916v10:/home/user/Zotero/storage/AFGGWEQH/1410.3916v10.pdf:application/pdf}
}

@article{watkins-reinforcement-2015,
	title = {Reinforcement {Learning} : {Part} 2 {TD} ( 0 ) learning},
	author = {Watkins, Chris},
	year = {2015},
	file = {Lecture2:/home/user/Zotero/storage/SB6PXMFJ/Lecture2.pdf:application/pdf}
}

@article{watkins-reinforcement-2015-1,
	title = {Reinforcement {Learning} : {Part} 3 {Evolution} {Cross}-entropy method for {TSP}},
	author = {Watkins, Chris},
	year = {2015},
	pages = {1--27},
	file = {Lecture3:/home/user/Zotero/storage/IDDXU6S9/Lecture3.pdf:application/pdf}
}

@article{wood-new-2014-1,
	title = {A {New} {Approach} to {Probabilistic} {Programming} {Inference}},
	volume = {33},
	journal = {Aistats},
	author = {Wood, Frank and Meent, Jan Willem Van De},
	year = {2014},
	pages = {1024--1032},
	file = {1507.00996v1:/home/user/Zotero/storage/MZHCUBQN/1507.00996v1.pdf:application/pdf;1507.00996v2:/home/user/Zotero/storage/K4C524IX/1507.00996v2.pdf:application/pdf}
}

@article{welling-exploiting-2014,
	title = {Exploiting the {Statistics} of {Learning} and {Inference}},
	url = {http://arxiv.org/abs/1402.7025},
	abstract = {When dealing with datasets containing a billion instances or with simulations that require a supercomputer to execute, computational resources become part of the equation. We can improve the efficiency of learning and inference by exploiting their inherent statistical nature. We propose algorithms that exploit the redundancy of data relative to a model by subsampling data-cases for every update and reasoning about the uncertainty created in this process. In the context of learning we propose to test for the probability that a stochastically estimated gradient points more than 180 degrees in the wrong direction. In the context of MCMC sampling we use stochastic gradients to improve the efficiency of MCMC updates, and hypothesis tests based on adaptive mini-batches to decide whether to accept or reject a proposed parameter update. Finally, we argue that in the context of likelihood free MCMC one needs to store all the information revealed by all simulations, for instance in a Gaussian process. We conclude that Bayesian methods will remain to play a crucial role in the era of big data and big simulations, but only if we overcome a number of computational challenges.},
	journal = {Proceedings of the International Conference on Machine Learning},
	author = {Welling, Max},
	year = {2014},
	file = {LargeScaleLearning:/home/user/Zotero/storage/H2GHTQ86/LargeScaleLearning.pdf:application/pdf}
}

@article{watkins-reinforcement-2015-2,
	title = {Reinforcement {Learning} {Plan} 1},
	author = {Watkins, Chris},
	year = {2015},
	file = {Lecture1:/home/user/Zotero/storage/T8U2B39C/Lecture1.pdf:application/pdf}
}

@article{waniek-spiteful-2015,
	title = {Spiteful {Bidding} in the {Dollar} {Auction}},
	number = {Ijcai},
	author = {Waniek, Marcin and Nie, Agata},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {667--673},
	file = {IJCAI15-100:/home/user/Zotero/storage/4ISE4D29/IJCAI15-100.pdf:application/pdf}
}

@article{wang-reduced-2014,
	title = {Reduced multiple empirical kernel learning machine},
	volume = {9},
	url = {http://link.springer.com/10.1007/s11571-014-9304-2},
	doi = {10.1007/s11571-014-9304-2},
	journal = {Cognitive Neurodynamics},
	author = {Wang, Zhe and Lu, MingZhe and Gao, Daqi},
	year = {2014},
	keywords = {á, empirical kernel, learning á orthonormal basis, mapping á reduced kernel, multiple kernel learning á},
	pages = {63--73}
}

@article{wang-experimental-2013,
	title = {Experimental comparison of representation methods and distance measures for time series data},
	volume = {26},
	doi = {10.1007/s10618-012-0250-5},
	abstract = {The previous decade has brought a remarkable increase of the interest in applications that deal with querying and mining of time series data. Many of the research efforts in this context have focused on introducing new representation methods for dimensionality reduction or novel similarity measures for the underlying data. In the vast majority of cases, each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive experimental study re-implementing eight different time series representations and nine similarity measures and their variants, and testing their effectiveness on thirty-eight time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. In addition to providing a unified validation of some of the existing achievements, our experiments also indicate that, in some cases, certain claims in the literature may be unduly optimistic.},
	journal = {Data Mining and Knowledge Discovery},
	author = {Wang, Xiaoyue and Mueen, Abdullah and Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Keogh, Eamonn},
	year = {2013},
	keywords = {representation, Distance measure, Experimental comparison, Time series},
	pages = {275--309}
}

@article{wang-deep-2015,
	title = {On {Deep} {Multi}-{View} {Representation} {Learning}},
	volume = {37},
	author = {Wang, Weiran and Livescu, Karen and Bilmes, Jeff},
	year = {2015},
	keywords = {multi-view learning, deep, representation learning},
	file = {wangb15:/home/user/Zotero/storage/ATTU894A/wangb15.pdf:application/pdf}
}

@article{wang-syntax-based-2015,
	title = {Syntax-{Based} {Deep} {Matching} of {Short} {Texts}},
	number = {Ijcai},
	author = {Wang, Mingxuan and Lu, Zhengdong and Li, Hang and Liu, Qun},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1354--1361},
	file = {IJCAI15-195:/home/user/Zotero/storage/VHESGABJ/IJCAI15-195.pdf:application/pdf}
}

@article{wang-information-2015,
	title = {Information {Geometry} and {Minimum} {Description} {Length} {Networks}},
	journal = {International Conference on Machine Learning},
	author = {Wang, Jun and Alousis, a Lexandros K and Ch, Hesge},
	year = {2015},
	file = {suna15:/home/user/Zotero/storage/H465M2DB/suna15.pdf:application/pdf}
}

@article{wang-online-2011,
	title = {Online {Variational} {Inference} for the {Hierarchical} {Dirichlet} {Process}},
	volume = {15},
	url = {http://www.cs.cmu.edu/~chongw/papers/WangPaisleyBlei2011.pdf},
	abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a poten- tially infinite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the ...},
	journal = {International Conference on Artificial Intelligence and Statistics},
	author = {Wang, Chong and Blei, David M},
	year = {2011},
	pages = {752--760},
	file = {WangPaisleyBlei2011:/home/user/Zotero/storage/CAH3MGAC/WangPaisleyBlei2011.pdf:application/pdf}
}

@article{walsh-possible-2015,
	title = {Possible and {Necessary} {Allocations} via {Sequential} {Mechanisms}},
	number = {Ijcai},
	author = {Walsh, Toby},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {468--474},
	file = {IJCAI15-072:/home/user/Zotero/storage/Q4JXS85N/IJCAI15-072.pdf:application/pdf}
}

@article{visin-renet-2015,
	title = {{ReNet} : {A} {Recurrent} {Neural} {Network} {Based} {Alternative} to {Convolutional} {Networks}},
	author = {Visin, Francesco and Milano, Politecnico and Courville, Aaron and Matteucci, Matteo and Milano, Politecnico},
	year = {2015},
	pages = {1--9},
	file = {1505.00393v2:/home/user/Zotero/storage/S8DM5G4J/1505.00393v2.pdf:application/pdf}
}

@article{vidali-characterization-2015,
	title = {A {Characterization} of n-{Player} {Strongly} {Monotone} {Scheduling} {Mechanisms} ∗},
	number = {Ijcai},
	author = {Vidali, Angelina},
	year = {2015},
	pages = {568--574},
	file = {IJCAI15-086:/home/user/Zotero/storage/8DW7V3ZR/IJCAI15-086.pdf:application/pdf}
}

@article{wang-dense-2013,
	title = {Dense trajectories and motion boundary descriptors for action recognition},
	volume = {103},
	issn = {1126301205},
	doi = {10.1007/s11263-012-0594-8},
	abstract = {This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of the dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the context of action classification on eight datasets, namely KTH, YouTube, Hollywood2, UCF sports, IXMAS, UIUC, Olympic Sports and UCF50. On all datasets our approach outperforms current state-of-the-art results.},
	journal = {International Journal of Computer Vision},
	author = {Wang, Heng and Kläser, Alexander and Schmid, Cordelia and Liu, Cheng Lin},
	year = {2013},
	keywords = {Action recognition, Dense trajectories, Motion boundary histograms},
	pages = {60--79}
}

@article{visin-renet-2015-1,
	title = {{ReNet} : {A} {Recurrent} {Neural} {Network} {Based} {Alternative} to {Convolutional} {Networks}},
	author = {Visin, Francesco and Milano, Politecnico and Milano, Politecnico and Courville, Aaron},
	year = {2015},
	pages = {1--9},
	file = {1505.00393v3:/home/user/Zotero/storage/XU8XKXM8/1505.00393v3.pdf:application/pdf}
}

@article{vasserman-implementing-2015,
	title = {Implementing the {Wisdom} of {Waze} ∗},
	number = {Ijcai},
	author = {Vasserman, Shoshana and Feldman, Michal},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {660--666},
	file = {IJCAI15-099:/home/user/Zotero/storage/XA7WN382/IJCAI15-099.pdf:application/pdf}
}

@article{van-merrienboer-blocks-nodate,
	title = {Blocks and {Fuel} : {Frameworks} for deep learning},
	url = {http://arxiv.org/pdf/1506.00619v1.pdf},
	author = {van Merrienboer, Bart and Bahdanau, Dzmitry and Dumoulin, Vincent and Serdyuk, Dmitriy and Warde-farley, David and Chorowski, Jan and Bengio, Yoshua},
	keywords = {neural networks, gpgpu, large-scale machine learning},
	pages = {1--5},
	file = {van Merrienboer et al. - Unknown - Blocks and Fuel Frameworks for deep learning:/home/user/Zotero/storage/AJKJ5GQ6/van Merrienboer et al. - Unknown - Blocks and Fuel Frameworks for deep learning.pdf:application/pdf}
}

@article{van-erven-fast-2015,
	title = {Fast rates in statistical and online learning},
	volume = {2014},
	url = {http://arxiv.org/abs/1507.02592},
	abstract = {The pursuit of fast rates in online and statistical learning has led to the conception of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that, under surprisingly weak conditions, both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov-Mammen margin condition, which has played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov, and Vovk's notion of mixability. Our unifying conditions thus provide a significant step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
	author = {van Erven, Tim and Grünwald, Peter D. and Mehta, Nishant a. and Reid, Mark D. and Williamson, Robert C.},
	year = {2015},
	file = {1507.02592v1:/home/user/Zotero/storage/JR3HGM9M/1507.02592v1.pdf:application/pdf}
}

@article{unit-lecture-2014,
	title = {Lecture 1 : {Introduction} to {RKHS} {Kernels} and feature space ( 1 ): {XOR} example {No} linear classifier separates red from blue {Map} points to higher dimensional feature space :},
	author = {Unit, Gatsby},
	year = {2014},
	file = {part_1:/home/user/Zotero/storage/JSZGUX9M/part_1.pdf:application/pdf}
}

@article{van-de-meent-black-box-2015,
	title = {Black-{Box} {Policy} {Search} with {Probabilistic} {Programs}},
	url = {http://arxiv.org/abs/1507.04635},
	abstract = {In this work, we explore how probabilistic programs can be used to represent policies in sequential decision problems. In this formulation, a probabilistic program is a black-box stochastic simulator for both the problem domain and the agent. We relate classic policy gradient techniques to recently introduced black-box variational methods which generalize to probabilistic program inference. We present case studies in the Canadian traveler problem, Rock Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study illustrates how programs can efficiently represent policies using moderate numbers of parameters.},
	author = {van de Meent, Jan-Willem and Tolpin, David and Paige, Brooks and Wood, Frank},
	year = {2015},
	pages = {1--22},
	file = {1507.04635v2:/home/user/Zotero/storage/SDPF6AU3/1507.04635v2.pdf:application/pdf;van de Meent et al. - 2015 - Black-Box Policy Search with Probabilistic Programs:/home/user/Zotero/storage/TEPJFBNZ/van de Meent et al. - 2015 - Black-Box Policy Search with Probabilistic Programs.pdf:application/pdf}
}

@article{titov-bayesian-2012,
	title = {A {Bayesian} approach to unsupervised semantic role induction},
	issn = {978-1-937284-19-0},
	url = {http://dl.acm.org/citation.cfm?id=2380821},
	number = {c},
	journal = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics},
	author = {Titov, Ivan and Klementiev, a},
	year = {2012},
	pages = {12--22},
	file = {eacl12:/home/user/Zotero/storage/EEW57RX9/eacl12.pdf:application/pdf}
}

@article{tang-envy-free-2015,
	title = {Envy-{Free} {Sponsored} {Search} {Auctions} with {Budgets}},
	number = {Ijcai},
	author = {Tang, Bo and Zhang, Jinshan},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {653--659},
	file = {IJCAI15-098:/home/user/Zotero/storage/E64H2IDV/IJCAI15-098.pdf:application/pdf}
}

@article{ter-sarkisov-incremental-2003,
	title = {Incremental {Adaptation} {Strategies} for {Neural} {Network} {Language} {Models}},
	author = {Ter-sarkisov, Aram and Schwenk, Holger and Bougares, Fethi and Mans, Le},
	year = {2003},
	file = {1412.6650v4:/home/user/Zotero/storage/DXTMNFAP/1412.6650v4.pdf:application/pdf}
}

@article{tammelin-solving-2015,
	title = {Solving {Heads}-{Up} {Limit} {Texas} {Hold} ’ em},
	number = {Ijcai},
	author = {Tammelin, Oskari and Burch, Neil and Johanson, Michael and Bowling, Michael},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {645--652},
	file = {IJCAI15-097:/home/user/Zotero/storage/DQUT65JH/IJCAI15-097.pdf:application/pdf}
}

@article{sutskever-modelling-2009,
	title = {Modelling {Relational} {Data} using {Bayesian} {Clustered} {Tensor} {Factorization}},
	volume = {22},
	url = {http://www.ece.duke.edu/~lcarin/pmfcrp.pdf},
	abstract = {We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance thatmatches or beats previous probabilisticmodels for relational data.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sutskever, Ilya},
	year = {2009},
	pages = {1--8},
	file = {pmfcrp:/home/user/Zotero/storage/ET8VUTPW/pmfcrp.pdf:application/pdf}
}

@article{srivastava-unsupervised-2015,
	title = {Unsupervised {Learning} of {Video} {Representations} using {LSTMs}},
	volume = {37},
	author = {Srivastava, Nitish},
	year = {2015},
	file = {srivastava15:/home/user/Zotero/storage/BR49A6H4/srivastava15.pdf:application/pdf}
}

@article{sordoni-learning-2013,
	title = {Learning {Concept} {Embeddings} for {Query} {Expansion} by {Quantum} {Entropy} {Minimization}},
	journal = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
	author = {Sordoni, Alessandro and Bengio, Yoshua and Nie, Jian-yun},
	year = {2013},
	keywords = {NLP and Machine Learning},
	pages = {1586--1592},
	file = {Sordoni, Bengio, Nie - 2013 - Learning Concept Embeddings for Query Expansion by Quantum Entropy Minimization:/home/user/Zotero/storage/RHIFACKZ/Sordoni, Bengio, Nie - 2013 - Learning Concept Embeddings for Query Expansion by Quantum Entropy Minimization.pdf:application/pdf}
}

@article{stuhlmueller-learning-2013,
	title = {Learning {Stochastic} {Inverses}},
	abstract = {We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model’s joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets.},
	journal = {Advances in Neural Information Processing Systems 27 (NIPS 2013)},
	author = {Stuhlmueller, Andreas and Taylor, Jessica and Goodman, Noah D},
	year = {2013},
	pages = {1--9},
	file = {inverses-nips-2013:/home/user/Zotero/storage/I2K3ZIKG/inverses-nips-2013.pdf:application/pdf}
}

@article{stadie-incentivizing-nodate,
	title = {Incentivizing {Exploration} {In} {Reinforcement} {Learning} {With} {Deep} {Predictive} {Models}},
	author = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
	pages = {1--10},
	file = {1507.00814v1:/home/user/Zotero/storage/PZ73N839/1507.00814v1.pdf:application/pdf}
}

@article{srivastava-training-2015,
	title = {Training {Very} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1507.06228},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	year = {2015},
	pages = {1--9},
	file = {1507.06228v1:/home/user/Zotero/storage/UCJRG8FZ/1507.06228v1.pdf:application/pdf}
}

@article{sohl-dickstein-deep-2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	author = {Sohl-dickstein, Jascha and Weiss, Eric A and Edu, Eweiss Berkeley and Edu, Nirum Stanford and Edu, Sganguli Stanford},
	year = {2015},
	file = {sohl-dickstein15:/home/user/Zotero/storage/HCVF459F/sohl-dickstein15.pdf:application/pdf}
}

@article{socher-recursive-2014,
	title = {Recursive {Deep} {Learning} for {Natural} {Language} {Processing} and {Computer} {Vision}},
	number = {August},
	author = {Socher, Richard},
	year = {2014},
	file = {thesis(1):/home/user/Zotero/storage/2M237T4P/thesis(1).pdf:application/pdf}
}

@article{socher-spectral-2011,
	title = {Spectral {Chinese} {Restaurant} {Processes}: {Nonparametric} {Clustering} {Based} on {Similarities}},
	volume = {15},
	abstract = {We introduce a new nonparametric clustering model which combines the recently proposed distance-dependent Chinese restaurant pro- cess (dd-CRP) and non-linear, spectral meth- ods for dimensionality reduction. Our model retains the ability of nonparametric methods to learn the number of clusters from data. At the same time it addresses two key limi- tations of nonparametric Bayesian methods: modeling data that are not exchangeable and have many correlated features. Spec- tral methods use the similarity between doc- uments to map them into a low-dimensional spectral space where we then compare sev- eral clustering methods. Our experiments on handwritten digits and text documents show that nonparametric methods such as the CRP or dd-CRP can perform as well as or better than k-means and also recover the true number of clusters. We improve the per- formance of the dd-CRP in spectral space by incorporating the original similarity matrix in its prior. This simple modification results in better performance than all other meth- ods we compared to. We offer a new formu- lation and first experimental evaluation of a general Gibbs sampler for mixture modeling with distance-dependent CRPs.},
	journal = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	author = {Socher, Richard and Maas, Andrew and Manning, Christopher D.},
	year = {2011},
	pages = {698--706},
	file = {Socher, Maas, Manning - 2011 - Spectral Chinese Restaurant Processes Nonparametric Clustering Based on Similarities:/home/user/Zotero/storage/54G86VX9/Socher, Maas, Manning - 2011 - Spectral Chinese Restaurant Processes Nonparametric Clustering Based on Similarities.pdf:application/pdf}
}

@article{snoek-scalable-2012,
	title = {Scalable {Bayesian} {Optimization} {Using} {Deep} {Neural} {Networks}},
	author = {Snoek, Jasper and Rippel, Oren and Adams, Ryan P},
	year = {2012},
	file = {snoek15:/home/user/Zotero/storage/SAMCFZXS/snoek15.pdf:application/pdf}
}

@article{socher-parsing-2011,
	title = {Parsing natural scenes and natural language with recursive neural networks},
	issn = {9781450306195},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Socher\_125.pdf},
	abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1\%). The features from the image parse tree outperform Gist descriptors for scene classification by 4\%.},
	journal = {Proceedings of the …},
	author = {Socher, R and Lin, Cc},
	year = {2011},
	pages = {129--136},
	file = {SocherLinNgManning_ICML2011:/home/user/Zotero/storage/I8JNTBIS/SocherLinNgManning_ICML2011.pdf:application/pdf}
}

@article{smith-cross-situational-nodate,
	title = {Cross-situational learning : a mathematical approach},
	author = {Smith, Kenny and Smith, Andrew D M and Blythe, Richard A and Vogt, Paul},
	file = {SSBV06:/home/user/Zotero/storage/DP7IXBK9/SSBV06.pdf:application/pdf}
}

@article{skibski-pseudo-polynomial-2015,
	title = {A {Pseudo}-{Polynomial} {Algorithm} for {Computing} {Power} {Indices} in {Graph}-{Restricted} {Weighted} {Voting} {Games}},
	number = {Ijcai},
	author = {Skibski, Oskar and Michalak, Tomasz P and Sakurai, Yuko and Yokoo, Makoto},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {631--637},
	file = {IJCAI15-095:/home/user/Zotero/storage/APZKWEC5/IJCAI15-095.pdf:application/pdf}
}

@article{shi-sparse-2014,
	title = {Sparse {Compositional} {Metric} {Learning} ∗},
	abstract = {We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.},
	author = {Shi, Yuan},
	year = {2014},
	pages = {1--18},
	file = {Shi - 2014 - Sparse Compositional Metric Learning ∗:/home/user/Zotero/storage/ZW89PZDS/Shi - 2014 - Sparse Compositional Metric Learning ∗.pdf:application/pdf}
}

@article{shi-normalized-2000,
	title = {Normalized {Cuts} and {Image} {Segmentation}},
	volume = {22},
	issn = {0818678224},
	url = {http://www.computer.org/portal/web/csdl/doi?doc=abs/proceedings/cvpr/1997/7822/00/78220731abs.htm\npapers3://publication/uuid/268FC197-AF47-4C7C-887F-BEDB94A81320},
	doi = {10.1109/34.868688},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph},
	number = {8},
	journal = {Ieee Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, J and Malik, J},
	year = {2000},
	pages = {888--905},
	file = {Shi, Malik - 2000 - Normalized Cuts and Image Segmentation:/home/user/Zotero/storage/55UKDICD/Shi, Malik - 2000 - Normalized Cuts and Image Segmentation.pdf:application/pdf;SM-ncut:/home/user/Zotero/storage/BME6KGCM/SM-ncut.pdf:application/pdf}
}

@article{shah-random-2013,
	title = {Random {Intersection} {Trees}},
	volume = {15},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=2627455\nhttp://arxiv.org/abs/1303.6223},
	abstract = {Finding interactions between variables in large and high-dimensional datasets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose at an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order \$p{\textasciicircum}\kappa\$ for a value of \$\kappa\$ that can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent \$s\$ obtained when using a brute force search constrained to order \$s\$ interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.},
	journal = {The Journal of Machine Learning Research},
	author = {Shah, Rajen Dinesh and Meinshausen, Nicolai},
	year = {2013},
	keywords = {interactions, high-dimensional classification, min-wise hashing, sparse data},
	pages = {23--23},
	file = {Shah, Meinshausen - 2013 - Random Intersection Trees:/home/user/Zotero/storage/E3USKDUC/Shah, Meinshausen - 2013 - Random Intersection Trees.pdf:application/pdf}
}

@article{jaderberg-deep-2014,
	title = {Deep {Structured} {Output} {Learning} for {Unconstrained} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1412.5903},
	abstract = {We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length. To this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training uses purely synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable.},
	urldate = {2015-10-27},
	journal = {arXiv:1412.5903 [cs]},
	author = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5903},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/86FVP35U/1412.html:text/html;Jaderberg et al_2014_Deep Structured Output Learning for Unconstrained Text Recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Jaderberg et al_2014_Deep Structured Output Learning for Unconstrained Text Recognition.pdf:application/pdf}
}

@article{mao-deep-2014,
	title = {Deep {Captioning} with {Multimodal} {Recurrent} {Neural} {Networks} (m-{RNN})},
	url = {http://arxiv.org/abs/1412.6632},
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\textasciitilde}junhua.mao/m-RNN.html .},
	urldate = {2015-10-27},
	journal = {arXiv:1412.6632 [cs]},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6632},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.2.6, I.2.7},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RZR7I77R/1412.html:text/html;Mao et al_2014_Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mao et al_2014_Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:application/pdf}
}

@article{vilnis-word-2014,
	title = {Word {Representations} via {Gaussian} {Embedding}},
	url = {http://arxiv.org/abs/1412.6623},
	abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.},
	urldate = {2015-10-27},
	journal = {arXiv:1412.6623 [cs]},
	author = {Vilnis, Luke and McCallum, Andrew},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6623},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BSSTH75A/1412.html:text/html;Vilnis_McCallum_2014_Word Representations via Gaussian Embedding.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Vilnis_McCallum_2014_Word Representations via Gaussian Embedding.pdf:application/pdf}
}

@article{stollenga-deep-2014,
	title = {Deep {Networks} with {Internal} {Selective} {Attention} through {Feedback} {Connections}},
	url = {http://arxiv.org/abs/1407.3068},
	abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.},
	urldate = {2015-10-27},
	journal = {arXiv:1407.3068 [cs]},
	author = {Stollenga, Marijn and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.3068},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, 68T45},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QSIXIPR3/1407.html:text/html;Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections2.pdf:application/pdf}
}

@article{xiao-application-2014,
	title = {The {Application} of {Two}-level {Attention} {Models} in {Deep} {Convolutional} {Neural} {Network} for {Fine}-grained {Image} {Classification}},
	url = {http://arxiv.org/abs/1411.6447},
	abstract = {Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200\_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.},
	urldate = {2015-10-27},
	journal = {arXiv:1411.6447 [cs]},
	author = {Xiao, Tianjun and Xu, Yichong and Yang, Kuiyuan and Zhang, Jiaxing and Peng, Yuxin and Zhang, Zheng},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.6447},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4N4G9G9F/1411.html:text/html;Xiao et al_2014_The Application of Two-level Attention Models in Deep Convolutional Neural.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Xiao et al_2014_The Application of Two-level Attention Models in Deep Convolutional Neural.pdf:application/pdf}
}

@inproceedings{lin-saliency-2014,
	title = {Saliency detection within a deep convolutional architecture},
	url = {http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/view/8725},
	urldate = {2015-10-27},
	booktitle = {Workshops at the {Twenty}-{Eighth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Lin, Yuetan and Kong, Shu and Wang, Donghui and Zhuang, Yueting},
	year = {2014},
	file = {8364.pdf:/home/user/Zotero/storage/T2DPIX4D/8364.pdf:application/pdf}
}

@article{bahdanau-end--end-2015,
	title = {End-to-{End} {Attention}-based {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1508.04395},
	abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
	urldate = {2015-10-27},
	journal = {arXiv:1508.04395 [cs]},
	author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.04395},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/V2G6NXTK/1508.html:text/html;Bahdanau et al_2015_End-to-End Attention-based Large Vocabulary Speech Recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bahdanau et al_2015_End-to-End Attention-based Large Vocabulary Speech Recognition.pdf:application/pdf}
}

@article{chorowski-attention-based-2015,
	title = {Attention-{Based} {Models} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1506.07503},
	abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\% PER in single utterances and 20\% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\% level.},
	urldate = {2015-10-27},
	journal = {arXiv:1506.07503 [cs, stat]},
	author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.07503},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NRSCRE3K/1506.html:text/html;Chorowski et al_2015_Attention-Based Models for Speech Recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Chorowski et al_2015_Attention-Based Models for Speech Recognition.pdf:application/pdf}
}

@article{kumar-ask-2015,
	title = {Ask {Me} {Anything}: {Dynamic} {Memory} {Networks} for {Natural} {Language} {Processing}},
	shorttitle = {Ask {Me} {Anything}},
	url = {http://arxiv.org/abs/1506.07285},
	abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), coreference resolution (Quizbowl dataset) and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.},
	urldate = {2015-10-27},
	journal = {arXiv:1506.07285 [cs]},
	author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Socher, Richard},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.07285},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZAXWNZ8W/1506.html:text/html;Kumar et al_2015_Ask Me Anything.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kumar et al_2015_Ask Me Anything.pdf:application/pdf}
}

@article{gregor-draw:-2015,
	title = {{DRAW}: {A} {Recurrent} {Neural} {Network} {For} {Image} {Generation}},
	shorttitle = {{DRAW}},
	url = {http://arxiv.org/abs/1502.04623},
	abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
	urldate = {2015-10-27},
	journal = {arXiv:1502.04623 [cs]},
	author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.04623},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RG7CKRE2/1502.html:text/html;Gregor et al_2015_DRAW.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Gregor et al_2015_DRAW.pdf:application/pdf}
}

@incollection{tang-learning-2014,
	title = {Learning {Generative} {Models} with {Visual} {Attention}},
	url = {http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention.pdf},
	urldate = {2015-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Yichuan and Srivastava, Nitish and Salakhutdinov, Ruslan R},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1808--1816},
	file = {NIPS Snapshort:/home/user/Zotero/storage/QZT6UG33/5345-learning-generative-models-with-visual-attention.html:text/html;Tang et al_2014_Learning Generative Models with Visual Attention.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Tang et al_2014_Learning Generative Models with Visual Attention.pdf:application/pdf}
}

@incollection{stollenga-deep-2014-1,
	title = {Deep {Networks} with {Internal} {Selective} {Attention} through {Feedback} {Connections}},
	url = {http://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf},
	urldate = {2015-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Stollenga, Marijn F and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Jürgen},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3545--3553},
	file = {NIPS Snapshort:/home/user/Zotero/storage/6E7XG78A/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.html:text/html;Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections.pdf:application/pdf}
}

@article{stoianov-prefrontal-2015-1,
	title = {Prefrontal {Goal} {Codes} {Emerge} as {Latent} {States} in {Probabilistic} {Value} {Learning}},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00886},
	urldate = {2015-10-26},
	journal = {Journal of cognitive neuroscience},
	author = {Stoianov, Ivilin and Genovesio, Aldo and Pezzulo, Giovanni},
	year = {2015},
	file = {560da13308ae2aa0be4a48bd.pdf:/home/user/Zotero/storage/DRK3EFF2/560da13308ae2aa0be4a48bd.pdf:application/pdf}
}

@article{florian-jaeger-redundancy-2010,
	title = {Redundancy and reduction: {Speakers} manage syntactic information density},
	volume = {61},
	issn = {00100285},
	shorttitle = {Redundancy and reduction},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028510000083},
	doi = {10.1016/j.cogpsych.2010.02.002},
	language = {en},
	number = {1},
	urldate = {2015-10-26},
	journal = {Cognitive Psychology},
	author = {Florian Jaeger, T.},
	month = aug,
	year = {2010},
	pages = {23--62},
	file = {Florian Jaeger_2010_Redundancy and reduction.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Florian Jaeger_2010_Redundancy and reduction.pdf:application/pdf}
}

@inproceedings{monsalve-lexical-2012,
	title = {Lexical surprisal as a general predictor of reading time},
	url = {http://dl.acm.org/citation.cfm?id=2380866},
	urldate = {2015-10-26},
	booktitle = {Proceedings of the 13th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Monsalve, Irene Fernandez and Frank, Stefan L. and Vigliocco, Gabriella},
	year = {2012},
	pages = {398--408},
	file = {Monsalve et al_2012_Lexical surprisal as a general predictor of reading time.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Monsalve et al_2012_Lexical surprisal as a general predictor of reading time.pdf:application/pdf}
}

@article{noordman-12-2015,
	title = {12 {Causal} inferences and world knowledge},
	url = {https://books.google.com/books?hl=en&lr=&id=ZYeFBwAAQBAJ&oi=fnd&pg=PA260&dq=%22can+remain+%E2%80%9Cavailable%E2%80%9D+even+if+it+is+mentioned+quite+a+distance+ago.+This+is%22+%22on+the+kind+of+inference,+but+on+the+activation+of+information+in+memory:+%E2%80%9CInferences%22+&ots=AWQuIOCjCb&sig=g8MSLoZWDQqtLuH9rrNo4nEnVi8},
	urldate = {2015-10-26},
	journal = {Inferences during Reading},
	author = {Noordman, Leo and Vonk, Wietske and Cozijn, Reinier and Frank, Stefan},
	year = {2015},
	pages = {260},
	file = {Noordman et al_2015_12 Causal inferences and world knowledge.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Noordman et al_2015_12 Causal inferences and world knowledge.pdf:application/pdf}
}

@article{andrews-reconciling-2014,
	title = {Reconciling embodied and distributional accounts of meaning in language},
	volume = {6},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12096/full},
	number = {3},
	urldate = {2015-10-26},
	journal = {Topics in cognitive science},
	author = {Andrews, Mark and Frank, Stefan and Vigliocco, Gabriella},
	year = {2014},
	pages = {359--370},
	file = {Andrews et al_2014_Reconciling embodied and distributional accounts of meaning in language.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Andrews et al_2014_Reconciling embodied and distributional accounts of meaning in language.pdf:application/pdf}
}

@inproceedings{frank-modelling-2014,
	title = {Modelling reading times in bilingual sentence comprehension},
	url = {http://iworx6.webxtra.net/~stefanfr/pubs/bilingual\_models.pdf},
	urldate = {2015-10-26},
	booktitle = {Proceedings of the 36th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {Frank, Stefan L.},
	year = {2014},
	pages = {1860--1861},
	file = {Frank_2014_Modelling reading times in bilingual sentence comprehension.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_2014_Modelling reading times in bilingual sentence comprehension.pdf:application/pdf}
}

@inproceedings{frank-word-2013,
	title = {Word surprisal predicts {N}400 amplitude during reading.},
	url = {http://anthology.aclweb.org/P/P13/P13-2.pdf#page=926},
	urldate = {2015-10-26},
	booktitle = {{ACL} (2)},
	author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
	year = {2013},
	pages = {878--883},
	file = {Frank et al_2013_Word surprisal predicts N400 amplitude during reading.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank et al_2013_Word surprisal predicts N400 amplitude during reading.pdf:application/pdf}
}

@article{frank-uncertainty-2013-1,
	title = {Uncertainty reduction as a measure of cognitive load in sentence comprehension},
	volume = {5},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12025/full},
	number = {3},
	urldate = {2015-10-26},
	journal = {Topics in cognitive science},
	author = {Frank, Stefan L.},
	year = {2013},
	pages = {475--494},
	file = {Frank_2013_Uncertainty reduction as a measure of cognitive load in sentence comprehension.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_2013_Uncertainty reduction as a measure of cognitive load in sentence comprehension.pdf:application/pdf}
}

@article{thompson-irene-nodate,
	title = {Irene {Fernandez} {Monsalve} {Department} of {Cognitive}, {Perceptual} and {Brain} {Sciences} {University} {College} {London}},
	url = {http://www.academia.edu/download/30392149/BRM.pdf},
	urldate = {2015-10-26},
	author = {Thompson, Robin L. and Vigliocco, Gabriella},
	file = {Thompson_Vigliocco_Irene Fernandez Monsalve Department of Cognitive, Perceptual and Brain Sciences.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Thompson_Vigliocco_Irene Fernandez Monsalve Department of Cognitive, Perceptual and Brain Sciences.pdf:application/pdf}
}

@article{frank-sentence-processing-2010,
	title = {Sentence-processing in echo state networks: a qualitative analysis by finite state machine extraction},
	volume = {22},
	issn = {0954-0091, 1360-0494},
	shorttitle = {Sentence-processing in echo state networks},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09540090903398039},
	doi = {10.1080/09540090903398039},
	language = {en},
	number = {2},
	urldate = {2015-10-26},
	journal = {Connection Science},
	author = {Frank, Stefan L. and Jacobsson, Henrik},
	month = jun,
	year = {2010},
	pages = {135--155},
	file = {Frank_Jacobsson_2010_Sentence-processing in echo state networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_Jacobsson_2010_Sentence-processing in echo state networks.pdf:application/pdf}
}

@article{frank-learn-2006,
	title = {Learn more by training less: systematicity in sentence processing by recurrent networks},
	volume = {18},
	issn = {0954-0091, 1360-0494},
	shorttitle = {Learn more by training less},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09540090600768336},
	doi = {10.1080/09540090600768336},
	language = {en},
	number = {3},
	urldate = {2015-10-26},
	journal = {Connection Science},
	author = {Frank, Stefan L.},
	month = sep,
	year = {2006},
	pages = {287--302},
	file = {Frank_2006_Learn more by training less.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_2006_Learn more by training less.pdf:application/pdf}
}

@inproceedings{frank-self-organizing-2008,
	title = {Self-organizing word representations for fast sentence processing},
	volume = {17},
	booktitle = {From {Associations} to {Rules}: {Connectionist} {Models} of {Behavior} and {Cognition}: {Proceedings} of the {Tenth} {Neural} {Computation} and {Psychology} {Workshop}, {Dijon}, {France}, 12-14 {April} 2007},
	publisher = {World Scientific},
	author = {Frank, Stefan L.},
	year = {2008},
	pages = {78},
	file = {Frank_2008_Self-organizing word representations for fast sentence processing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_2008_Self-organizing word representations for fast sentence processing.pdf:application/pdf}
}

@article{frank-connectionist-2009,
	title = {Connectionist semantic systematicity},
	volume = {110},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027708002837},
	doi = {10.1016/j.cognition.2008.11.013},
	language = {en},
	number = {3},
	urldate = {2015-10-26},
	journal = {Cognition},
	author = {Frank, Stefan L. and Haselager, Willem F.G. and van Rooij, Iris},
	month = mar,
	year = {2009},
	pages = {358--379},
	file = {Frank et al_2009_Connectionist semantic systematicity.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank et al_2009_Connectionist semantic systematicity.pdf:application/pdf}
}

@inproceedings{roark-deriving-2009,
	title = {Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing},
	url = {http://dl.acm.org/citation.cfm?id=1699553},
	urldate = {2015-10-26},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Volume} 1-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
	year = {2009},
	pages = {324--333},
	file = {Roark et al_2009_Deriving lexical and syntactic expectation-based measures for psycholinguistic.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Roark et al_2009_Deriving lexical and syntactic expectation-based measures for psycholinguistic.pdf:application/pdf}
}

@article{levy-expectation-based-2008,
	title = {Expectation-based syntactic comprehension},
	volume = {106},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027707001436},
	doi = {10.1016/j.cognition.2007.05.006},
	language = {en},
	number = {3},
	urldate = {2015-10-26},
	journal = {Cognition},
	author = {Levy, Roger},
	month = mar,
	year = {2008},
	pages = {1126--1177},
	file = {Levy_2008_Expectation-based syntactic comprehension.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Levy_2008_Expectation-based syntactic comprehension.pdf:application/pdf}
}

@article{levy-expectation-based-2008-1,
	title = {Expectation-based syntactic comprehension},
	volume = {106},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027707001436},
	doi = {10.1016/j.cognition.2007.05.006},
	language = {en},
	number = {3},
	urldate = {2015-10-26},
	journal = {Cognition},
	author = {Levy, Roger},
	month = mar,
	year = {2008},
	pages = {1126--1177}
}

@article{frank-insensitivity-2011,
	title = {Insensitivity of the {Human} {Sentence}-{Processing} {System} to {Hierarchical} {Structure}},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/22/6/829},
	doi = {10.1177/0956797611409589},
	abstract = {Although it is generally accepted that hierarchical phrase structures are instrumental in describing human language, their role in cognitive processing is still debated. We investigated the role of hierarchical structure in sentence processing by implementing a range of probabilistic language models, some of which depended on hierarchical structure, and others of which relied on sequential structure only. All models estimated the occurrence probabilities of syntactic categories in sentences for which reading-time data were available. Relating the models’ probability estimates to the data showed that the hierarchical-structure models did not account for variance in reading times over and above the amount of variance accounted for by all of the sequential-structure models. This suggests that a sentence’s hierarchical structure, unlike many other sources of information, does not noticeably affect the generation of expectations about upcoming words.},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Psychological Science},
	author = {Frank, Stefan L. and Bod, Rens},
	month = jun,
	year = {2011},
	pmid = {21586764},
	keywords = {language, psycholinguistics, computer simulation, neural networks, Cognitive Processes},
	pages = {829--834},
	file = {Frank_Bod_2011_Insensitivity of the Human Sentence-Processing System to Hierarchical Structure.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_Bod_2011_Insensitivity of the Human Sentence-Processing System to Hierarchical Structure.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RAJRHRHV/829.html:text/html}
}

@inproceedings{fossum-sequential-2012,
	address = {Stroudsburg, PA, USA},
	series = {{CMCL} '12},
	title = {Sequential vs. {Hierarchical} {Syntactic} {Models} of {Human} {Incremental} {Sentence} {Processing}},
	url = {http://dl.acm.org/citation.cfm?id=2390304.2390313},
	abstract = {Experimental evidence demonstrates that syntactic structure influences human online sentence processing behavior. Despite this evidence, open questions remain: which type of syntactic structure best explains observed behavior--hierarchical or sequential, and lexicalized or unlexicalized? Recently, Frank and Bod (2011) find that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models, relative to a baseline prediction model that takes word-level factors into account. They conclude that the human parser is insensitive to hierarchical syntactic structure. We investigate these claims and find a picture more complicated than the one they present. First, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod (2011) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models. Second, we show that lexicalizing the hierarchical models used in Frank and Bod (2011) significantly improves prediction accuracy relative to the unlexicalized versions. Third, we show that using state-of-the-art lexicalized hierarchical models further improves prediction accuracy. Our results demonstrate that the claim of Frank and Bod (2011) that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization matters for prediction accuracy.},
	urldate = {2015-10-26},
	booktitle = {Proceedings of the 3rd {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Fossum, Victoria and Levy, Roger},
	year = {2012},
	pages = {61--69},
	file = {Fossum_Levy_2012_Sequential vs.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fossum_Levy_2012_Sequential vs.pdf:application/pdf}
}

@article{schotter-task-2014,
	title = {Task {Effects} {Reveal} {Cognitive} {Flexibility} {Responding} to {Frequency} and {Predictability}: {Evidence} from {Eye} {Movements} in {Reading} and {Proofreading}},
	volume = {131},
	issn = {0010-0277},
	shorttitle = {Task {Effects} {Reveal} {Cognitive} {Flexibility} {Responding} to {Frequency} and {Predictability}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3943895/},
	doi = {10.1016/j.cognition.2013.11.018},
	abstract = {It is well-known that word frequency and predictability affect processing time. These effects change magnitude across tasks, but studies testing this use tasks with different response types (e.g., lexical decision, naming, and fixation time during reading; ), preventing direct comparison. Recently,  overcame this problem, comparing fixation times in reading for comprehension and proofreading, showing that the frequency effect was larger in proofreading than in reading. This result could be explained by readers exhibiting substantial cognitive flexibility, and qualitatively changing how they process words in the proofreading task in a way that magnifies effects of word frequency. Alternatively, readers may not change word processing so dramatically, and instead may perform more careful identification generally, increasing the magnitude of many word processing effects (e.g., both frequency and predictability). We tested these possibilities with two experiments: subjects read for comprehension and then proofread for spelling errors (letter transpositions) that produce nonwords (e.g., trcak for track as in Kaakinen \& Hyönä) or that produce real but unintended words (e.g., trial for trail) to compare how the task changes these effects. Replicating Kaakinen and Hyönä, frequency effects increased during proofreading. However, predictability effects only increased when integration with the sentence context was necessary to detect errors (i.e., when spelling errors produced words that were inappropriate in the sentence; trial for trail). The results suggest that readers adopt sophisticated word processing strategies to accommodate task demands.},
	number = {1},
	urldate = {2015-10-26},
	journal = {Cognition},
	author = {Schotter, Elizabeth R. and Bicknell, Klinton and Howard, Ian and Levy, Roger and Rayner, Keith},
	month = apr,
	year = {2014},
	pmid = {24434024},
	pmcid = {PMC3943895},
	pages = {1--27},
	file = {Schotter et al_2014_Task Effects Reveal Cognitive Flexibility Responding to Frequency and.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Schotter et al_2014_Task Effects Reveal Cognitive Flexibility Responding to Frequency and.pdf:application/pdf}
}

@article{creem-regehr-perception-2010,
	title = {Perception and action},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.82/abstract},
	doi = {10.1002/wcs.82},
	abstract = {The phrase perception and action is used widely but in diverse ways in the context of the relationship between perceptual and motor processes. This review describes and integrates five perspectives on perception and action which rely on both neurophysiological and behavioral levels of analysis. The two visual systems view proposes dissociable but interactive systems for conscious processing of objects/space and the visual control of action. The integrative view proposes tightly calibrated but flexible systems for perception and motor control in spatial representation. The embodied view posits that action underlies perception, involving common coding or motor simulation systems, and examines the relationship between action observation, imitation, and the understanding of intention. The ecological view emphasizes environmental information and affordances in perception. The functional view defines the relationship between perception, action planning, and semantics in goal-directed actions. Although some of these views/approaches differ in significant ways, their shared emphasis on the importance of action in perception serves as a useful unifying framework. WIREs Cogn Sci 2010 1 800–810 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Creem-Regehr, Sarah H. and Kunz, Benjamin R.},
	month = nov,
	year = {2010},
	pages = {800--810},
	file = {Creem-Regehr_Kunz_2010_Perception and action.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Creem-Regehr_Kunz_2010_Perception and action.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7UQJG6DQ/abstract.html:text/html}
}

@article{chater-bayesian-2010,
	title = {Bayesian models of cognition},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.79/abstract},
	doi = {10.1002/wcs.79},
	abstract = {There has been a recent explosion in research applying Bayesian models to cognitive phenomena. This development has resulted from the realization that across a wide variety of tasks the fundamental problem the cognitive system confronts is coping with uncertainty. From visual scene recognition to on-line language comprehension, from categorizing stimuli to determining to what degree an argument is convincing, people must deal with the incompleteness of the information they possess to perform these tasks, many of which have important survival-related consequences. This paper provides a review of Bayesian models of cognition, dividing them up by the different aspects of cognition to which they have been applied. The paper begins with a brief review of Bayesian inference. This falls short of a full technical introduction but the reader is referred to the relevant literature for further details. There follows reviews of Bayesian models in Perception, Categorization, Learning and Causality, Language Processing, Inductive Reasoning, Deductive Reasoning, and Argumentation. In all these areas, it is argued that sophisticated Bayesian models are enhancing our understanding of the underlying cognitive computations involved. It is concluded that a major challenge is to extend the evidential basis for these models, especially to accounts of higher level cognition. WIREs Cogn Sci 2010 1 811–823 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Chater, Nick and Oaksford, Mike and Hahn, Ulrike and Heit, Evan},
	month = nov,
	year = {2010},
	pages = {811--823},
	file = {Chater et al_2010_Bayesian models of cognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Chater et al_2010_Bayesian models of cognition.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/4FNB8P5Q/abstract.html:text/html}
}

@article{cheke-mental-2010,
	title = {Mental time travel in animals},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.59/abstract},
	doi = {10.1002/wcs.59},
	abstract = {Twelve years on from Suddendorf and Corballis's mental time travel (MTT) hypothesis, the debate as to whether episodic cognition is unique to humans remains unresolved. In this article, we review the evidence for mental time travel in nonhuman animals and the empirical methods used in this field. Investigation of episodic-like memory has been dominated by ‘What–Where–When’ paradigms, with limited success outside of food-caching corvids, and with only scrub-jays meeting Clayton and colleagues' more specific description of the underlying mnemonics. The recent emergence of an ‘unexpected question’ paradigm tapping recall of unattended aspects of episodes provides a promising new avenue for future studies. Falsification of the Bischof–Köhler hypothesis, that acting to satisfy a future motivational state is beyond the scope of nonhuman animals, has been the ‘holy grail’ of animal future planning research, spawning a plethora of studies. We argue that although the criterion proposed by this hypothesis provides a test for an explicit representation of a future time, it does nothing to get at whether planning for this future is mediated by semantic or episodic processes. WIREs Cogn Sci 2010 1 915–930 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Cheke, Lucy G. and Clayton, Nicola S.},
	month = nov,
	year = {2010},
	pages = {915--930},
	file = {Cheke_Clayton_2010_Mental time travel in animals.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Cheke_Clayton_2010_Mental time travel in animals.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/D3RAVMKR/abstract.html:text/html}
}

@article{garnham-models-2010,
	title = {Models of processing: discourse},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	shorttitle = {Models of processing},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.69/abstract},
	doi = {10.1002/wcs.69},
	abstract = {This article discusses models of discourse processing, primarily from a psycholinguistic perspective, though considerations from the other cognitive sciences are mentioned where appropriate. It also touches on issues of discourse representation, because questions about representation and questions about process are closely intertwined. The origins of an interest in questions about discourse are identified in Bransford's ideas from the early 1970s. Their development into more detailed models of discourse processing is discussed, and detailed descriptions are given of, in particular, anaphor processing and, to a lesser extent the establishment of coherence. Some issues that arise in connection with the production of discourse are briefly discussed, as are their relation to dialog rather than to monolog. WIREs Cogn Sci 2010 1 845–853 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Garnham, Alan},
	month = nov,
	year = {2010},
	pages = {845--853},
	file = {Garnham_2010_Models of processing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Garnham_2010_Models of processing2.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Q2SJ4KNX/abstract.html:text/html}
}

@article{ferreira-language-2010,
	title = {Language production},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.70/abstract},
	doi = {10.1002/wcs.70},
	abstract = {Linguistic expressions are produced through a multicomponent multistage process. This article describes the major components of the language-production process and discusses issues of current research focus. General mechanisms of word and sentence production are described. This is followed by discussion of language production in conversation, different modes of production (written production, sign language, monitoring, bilingualism, aphasia, corpus research), and the relationship of language-production mechanisms to other cognitive systems (eye movements, attention, memory, gesture), before ending with concluding thoughts. WIREs Cogn Sci 2010 1 834–844 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Ferreira, Victor S.},
	month = nov,
	year = {2010},
	pages = {834--844},
	file = {Ferreira_2010_Language production.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ferreira_2010_Language production2.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3MUG69XJ/abstract.html:text/html}
}

@article{smith-decision-2010,
	title = {Decision neuroscience: neuroeconomics},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	shorttitle = {Decision neuroscience},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.73/abstract},
	doi = {10.1002/wcs.73},
	abstract = {Few aspects of human cognition are more personal than the choices we make. Our decisions—from the mundane to the impossibly complex—continually shape the courses of our lives. In recent years, researchers have applied the tools of neuroscience to understand the mechanisms that underlie decision making, as part of the new discipline of decision neuroscience. A primary goal of this emerging field has been to identify the processes that underlie specific decision variables, including the value of rewards, the uncertainty associated with particular outcomes, and the consequences of social interactions. Recent work suggests potential neural substrates that integrate these variables, potentially reflecting a common neural currency for value, to facilitate value comparisons. Despite the successes of decision neuroscience research for elucidating brain mechanisms, significant challenges remain. These include building new conceptual frameworks for decision making, integrating research findings across disparate techniques and species, and extending results from neuroscience to shape economic theory. To overcome these challenges, future research will likely focus on interpersonal variability in decision making, with the eventual goal of creating biologically plausible models for individual choice. WIREs Cogn Sci 2010 1 854–871 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Smith, David V. and Huettel, Scott A.},
	month = nov,
	year = {2010},
	pages = {854--871},
	file = {Smith_Huettel_2010_Decision neuroscience.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Smith_Huettel_2010_Decision neuroscience.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/PM9DS7VS/abstract.html:text/html}
}

@article{romberg-statistical-2010,
	title = {Statistical learning and language acquisition},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.78/abstract},
	doi = {10.1002/wcs.78},
	abstract = {Human learners, including infants, are highly sensitive to structure in their environment. Statistical learning refers to the process of extracting this structure. A major question in language acquisition in the past few decades has been the extent to which infants use statistical learning mechanisms to acquire their native language. There have been many demonstrations showing infants' ability to extract structures in linguistic input, such as the transitional probability between adjacent elements. This paper reviews current research on how statistical learning contributes to language acquisition. Current research is extending the initial findings of infants' sensitivity to basic statistical information in many different directions, including investigating how infants represent regularities, learn about different levels of language, and integrate information across situations. These current directions emphasize studying statistical language learning in context: within language, within the infant learner, and within the environment as a whole. WIREs Cogn Sci 2010 1 906–914 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Romberg, Alexa R. and Saffran, Jenny R.},
	month = nov,
	year = {2010},
	pages = {906--914},
	file = {Romberg_Saffran_2010_Statistical learning and language acquisition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Romberg_Saffran_2010_Statistical learning and language acquisition.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QCXSSCMI/abstract.html:text/html}
}

@article{ferreira-language-2010-1,
	title = {Language production},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.70/abstract},
	doi = {10.1002/wcs.70},
	abstract = {Linguistic expressions are produced through a multicomponent multistage process. This article describes the major components of the language-production process and discusses issues of current research focus. General mechanisms of word and sentence production are described. This is followed by discussion of language production in conversation, different modes of production (written production, sign language, monitoring, bilingualism, aphasia, corpus research), and the relationship of language-production mechanisms to other cognitive systems (eye movements, attention, memory, gesture), before ending with concluding thoughts. WIREs Cogn Sci 2010 1 834–844 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Ferreira, Victor S.},
	month = nov,
	year = {2010},
	pages = {834--844},
	file = {Ferreira_2010_Language production.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ferreira_2010_Language production.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JR3IDP7D/abstract.html:text/html}
}

@article{garnham-models-2010-1,
	title = {Models of processing: discourse},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	shorttitle = {Models of processing},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.69/abstract},
	doi = {10.1002/wcs.69},
	abstract = {This article discusses models of discourse processing, primarily from a psycholinguistic perspective, though considerations from the other cognitive sciences are mentioned where appropriate. It also touches on issues of discourse representation, because questions about representation and questions about process are closely intertwined. The origins of an interest in questions about discourse are identified in Bransford's ideas from the early 1970s. Their development into more detailed models of discourse processing is discussed, and detailed descriptions are given of, in particular, anaphor processing and, to a lesser extent the establishment of coherence. Some issues that arise in connection with the production of discourse are briefly discussed, as are their relation to dialog rather than to monolog. WIREs Cogn Sci 2010 1 845–853 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Garnham, Alan},
	month = nov,
	year = {2010},
	pages = {845--853},
	file = {Garnham_2010_Models of processing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Garnham_2010_Models of processing.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/UDCHCMVT/abstract.html:text/html}
}

@article{vallortigara-animal-2010,
	title = {Animal cognition},
	volume = {1},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.75/abstract},
	doi = {10.1002/wcs.75},
	abstract = {The main topics in the study of animal cognition are reviewed with special reference to direct links to human, and in particular developmental, cognitive sciences. The material is organized with regard to the general idea that biological organisms would be endowed with a small set of separable systems of core knowledge, a prominent hypothesis in the current developmental cognitive sciences. Core knowledge systems would serve to represent inanimate physical objects and their mechanical interactions (natural physics); numbers with their relationships of ordering, addition, and subtraction (natural mathematics); places in the spatial layout with their geometric relationships (natural geometry); and animate psychological objects (agents) with their goal-directed actions (natural psychology). Some advanced forms of animal cognition, such as episodic-like representations and planning for the future, are also discussed. WIREs Cogn Sci 2010 1 882–893 For further resources related to this article, please visit the WIREs website},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Vallortigara, Giorgio and Chiandetti, Cinzia and Rugani, Rosa and Sovrano, Valeria Anna and Regolin, Lucia},
	month = nov,
	year = {2010},
	pages = {882--893},
	file = {Snapshot:/home/user/Zotero/storage/6I8VMHIF/abstract.html:text/html;Vallortigara et al_2010_Animal cognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Vallortigara et al_2010_Animal cognition.pdf:application/pdf}
}

@article{vallortigara-animal-2010-1,
	title = {Animal cognition},
	volume = {1},
	issn = {19395078},
	url = {http://doi.wiley.com/10.1002/wcs.75},
	doi = {10.1002/wcs.75},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Vallortigara, Giorgio and Chiandetti, Cinzia and Rugani, Rosa and Sovrano, Valeria Anna and Regolin, Lucia},
	month = nov,
	year = {2010},
	pages = {882--893}
}

@article{rayner-models-2010,
	title = {Models of the {Reading} {Process}},
	volume = {1},
	issn = {1939-5078},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001687/},
	doi = {10.1002/wcs.68},
	abstract = {Reading is a complex skill involving the orchestration of a number of components. Researchers often talk about a “model of reading” when talking about only one aspect of the reading process (for example, models of word identification are often referred to as “models of reading”). Here, we review prominent models that are designed to account for (1) word identification, (2) syntactic parsing, (3) discourse representations, and (4) how certain aspects of language processing (e.g., word identification), in conjunction with other constraints (e g., limited visual acuity, saccadic error, etc.), guide readers’ eyes. Unfortunately, it is the case that these various models addressing specific aspects of the reading process seldom make contact with models dealing with other aspects of reading. Thus, for example, the models of word identification seldom make contact with models of eye movement control, and vice versa. While this may be unfortunate in some ways, it is quite understandable in other ways because reading itself is a very complex process. We discuss prototypical models of aspects of the reading process in the order mentioned above. We do not review all possible models, but rather focus on those we view as being representative and most highly recognized.},
	number = {6},
	urldate = {2015-10-26},
	journal = {Wiley interdisciplinary reviews. Cognitive science},
	author = {Rayner, Keith and Reichle, Erik D.},
	year = {2010},
	pmid = {21170142},
	pmcid = {PMC3001687},
	pages = {787--799},
	file = {Rayner_Reichle_2010_Models of the Reading Process.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rayner_Reichle_2010_Models of the Reading Process.pdf:application/pdf}
}

@article{reichle-ez-2003,
	title = {The {EZ} {Reader} model of eye-movement control in reading: {Comparisons} to other models},
	volume = {26},
	shorttitle = {The {EZ} {Reader} model of eye-movement control in reading},
	url = {http://journals.cambridge.org/abstract\_S0140525X03000104},
	number = {04},
	urldate = {2015-10-26},
	journal = {Behavioral and brain sciences},
	author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
	year = {2003},
	pages = {445--476},
	file = {Reichle et al_2003_The EZ Reader model of eye-movement control in reading.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Reichle et al_2003_The EZ Reader model of eye-movement control in reading.pdf:application/pdf}
}

@article{reichle-using-2012,
	title = {Using {E}-{Z} {Reader} to simulate eye movements in nonreading tasks: {A} unified framework for understanding the eye–mind link.},
	volume = {119},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Using {E}-{Z} {Reader} to simulate eye movements in nonreading tasks},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0026473},
	doi = {10.1037/a0026473},
	language = {en},
	number = {1},
	urldate = {2015-10-26},
	journal = {Psychological Review},
	author = {Reichle, Erik D. and Pollatsek, Alexander and Rayner, Keith},
	year = {2012},
	pages = {155--185},
	file = {Reichle et al_2012_Using E-Z Reader to simulate eye movements in nonreading tasks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Reichle et al_2012_Using E-Z Reader to simulate eye movements in nonreading tasks.pdf:application/pdf}
}

@article{reichle-testing-2011,
	title = {Testing an assumption of the {E}-{Z} {Reader} model of eye-movement control during reading: {Using} event-related potentials to examine the familiarity check: {E}-{Z} {Reader} and event-related potentials},
	volume = {48},
	issn = {00485772},
	shorttitle = {Testing an assumption of the {E}-{Z} {Reader} model of eye-movement control during reading},
	url = {http://doi.wiley.com/10.1111/j.1469-8986.2011.01169.x},
	doi = {10.1111/j.1469-8986.2011.01169.x},
	language = {en},
	number = {7},
	urldate = {2015-10-26},
	journal = {Psychophysiology},
	author = {Reichle, Erik D. and Tokowicz, Natasha and Liu, Ying and Perfetti, Charles A.},
	month = jul,
	year = {2011},
	pages = {993--1003},
	file = {Reichle et al_2011_Testing an assumption of the E-Z Reader model of eye-movement control during.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Reichle et al_2011_Testing an assumption of the E-Z Reader model of eye-movement control during.pdf:application/pdf}
}

@article{engbert-swift:-2005,
	title = {{SWIFT}: {A} {Dynamical} {Model} of {Saccade} {Generation} {During} {Reading}.},
	volume = {112},
	issn = {1939-1471, 0033-295X},
	shorttitle = {{SWIFT}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.112.4.777},
	doi = {10.1037/0033-295X.112.4.777},
	language = {en},
	number = {4},
	urldate = {2015-10-26},
	journal = {Psychological Review},
	author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
	year = {2005},
	pages = {777--813},
	file = {Engbert et al_2005_SWIFT.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Engbert et al_2005_SWIFT.pdf:application/pdf}
}

@article{engbert-dynamical-2002,
	title = {A dynamical model of saccade generation in reading based on spatially distributed lexical processing},
	volume = {42},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698901003017},
	doi = {10.1016/S0042-6989(01)00301-7},
	abstract = {The understanding of the control of eye movements has greatly benefited from the analysis of mathematical models. Currently most comprehensive models include sequential shifts of visual attention. Here we propose an alternative model of eye movement control, which includes three new principles: spatially distributed lexical processing, a separation of saccade timing from saccade target selection, and autonomous (random) generation of saccades with foveal inhibition. These three features provide a common control mechanism for fixations, refixations, and regressions. Consequently, the model is called SWIFT (Saccade-generation with inhibition by foveal targets). Results from numerical simulations are in good agreement with effects of word frequency on single-fixation, first-fixation, and gaze durations as well as fixation and word skipping probabilities in first-pass analysis. The model inherently produces complex eye movement patterns including refixations and regressions due to its underlying dynamical principles.},
	number = {5},
	urldate = {2015-10-26},
	journal = {Vision Research},
	author = {Engbert, Ralf and Longtin, André and Kliegl, Reinhold},
	month = mar,
	year = {2002},
	keywords = {Reading, Eye movement, Modeling, Saccade},
	pages = {621--636},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/M8HMMUKW/S0042698901003017.html:text/html}
}

@article{rayner-extending-2007,
	title = {Extending the {E}-{Z} {Reader} {Model} of {Eye} {Movement} {Control} to {Chinese} {Readers}},
	volume = {31},
	copyright = {2007 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1080/03640210701703824/abstract},
	doi = {10.1080/03640210701703824},
	abstract = {Chinese readers' eye movements were simulated in the context of the E-Z Reader model, which was developed to account for the eye movements of readers of English. Despite obvious differences between English and Chinese, the model did a fairly good job of simulating the eye movements of Chinese readers. The successful simulation suggests that the control of eye movements in reading Chinese is similar to that in an alphabetic language such as English.},
	language = {en},
	number = {6},
	urldate = {2015-10-26},
	journal = {Cognitive Science},
	author = {Rayner, Keith and Li, Xingshan and Pollatsek, Alexander},
	month = nov,
	year = {2007},
	keywords = {Eye Movements, Reading, Chinese readers},
	pages = {1021--1033},
	file = {Rayner et al_2007_Extending the E-Z Reader Model of Eye Movement Control to Chinese Readers.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rayner et al_2007_Extending the E-Z Reader Model of Eye Movement Control to Chinese Readers.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/U865S6E9/abstract.html:text/html}
}

@article{reichle-e-z-2003,
	title = {The {E}-{Z} reader model of eye-movement control in reading: comparisons to other models},
	volume = {26},
	issn = {0140-525X},
	shorttitle = {The {E}-{Z} reader model of eye-movement control in reading},
	abstract = {The E-Z Reader model (Reichle et al. 1998; 1999) provides a theoretical framework for understanding how word identification, visual processing, attention, and oculomotor control jointly determine when and where the eyes move during reading. In this article, we first review what is known about eye movements during reading. Then we provide an updated version of the model (E-Z Reader 7) and describe how it accounts for basic findings about eye movement control in reading. We then review several alternative models of eye movement control in reading, discussing both their core assumptions and their theoretical scope. On the basis of this discussion, we conclude that E-Z Reader provides the most comprehensive account of eye movement control during reading. Finally, we provide a brief overview of what is known about the neural systems that support the various components of reading, and suggest how the cognitive constructs of our model might map onto this neural architecture.},
	language = {eng},
	number = {4},
	journal = {The Behavioral and Brain Sciences},
	author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
	month = aug,
	year = {2003},
	pmid = {15067951},
	keywords = {Humans, Eye Movements, Reading, Models, Psychological, Fixation, Ocular, Models, Neurological, Oculomotor Muscles, Verbal Behavior, Vision, Binocular},
	pages = {445--476; discussion 477--526}
}

@article{reichle-comparing-2000,
	title = {Comparing the {EZ} reader model to other models of eye movement control in reading},
	url = {http://cogprints.org/1169/},
	urldate = {2015-10-26},
	author = {Reichle, Rayner},
	year = {2000},
	file = {Reichle_2000_Comparing the EZ reader model to other models of eye movement control in reading.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Reichle_2000_Comparing the EZ reader model to other models of eye movement control in reading.pdf:application/pdf}
}

@article{rayner-eye-2009,
	title = {Eye movements in reading: {Models} and data},
	volume = {2},
	shorttitle = {Eye movements in reading},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2906818/},
	number = {5},
	urldate = {2015-10-26},
	journal = {Journal of eye movement research},
	author = {Rayner, Keith},
	year = {2009},
	pages = {1},
	file = {Rayner_2009_Eye movements in reading.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rayner_2009_Eye movements in reading.pdf:application/pdf}
}

@article{wang-elements-1994,
	title = {Elements of {Automata} {Theory}},
	volume = {49},
	url = {http://www.sciencedirect.com/science/article/pii/0009250994800065},
	number = {7},
	urldate = {2015-10-23},
	journal = {Chemical Engineering Science},
	author = {Wang, Y. P. and Smith, Robin},
	year = {1994},
	pages = {981--1006},
	file = {CBO9781139195218A005.pdf:/home/user/Zotero/storage/B654663U/CBO9781139195218A005.pdf:application/pdf}
}

@book{sakarovitch-elements-2009,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A053.pdf:/home/user/Zotero/storage/GAKHKZ5H/CBO9781139195218A053.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-1,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A046.pdf:/home/user/Zotero/storage/6PEZBTRX/CBO9781139195218A046.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-2,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A045.pdf:/home/user/Zotero/storage/PAA5WKSB/CBO9781139195218A045.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-3,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A043.pdf:/home/user/Zotero/storage/H8KKPN8B/CBO9781139195218A043.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-4,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A032.pdf:/home/user/Zotero/storage/TBM4VQZB/CBO9781139195218A032.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-5,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A021.pdf:/home/user/Zotero/storage/RVUXDEUF/CBO9781139195218A021.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-6,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A008.pdf:/home/user/Zotero/storage/MDBGGEE8/CBO9781139195218A008.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-7,
	address = {Cambridge},
	title = {Elements of {Automata} {Theory}},
	isbn = {978-1-139-19521-8},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139195218},
	urldate = {2015-10-23},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques and Thomas, Reuben},
	year = {2009},
	file = {CBO9781139195218A007.pdf:/home/user/Zotero/storage/29AGX33T/CBO9781139195218A007.pdf:application/pdf}
}

@book{sakarovitch-elements-2009-8,
	address = {Cambridge ; New York},
	title = {Elements of automata theory},
	isbn = {978-0-521-84425-3},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Sakarovitch, Jacques},
	year = {2009},
	keywords = {Automata math, Automata theory, Automatentheorie, Formal language, Machine theory},
	file = {CBO9781139195218A004.pdf:/home/user/Zotero/storage/BXESC575/CBO9781139195218A004.pdf:application/pdf}
}

@article{saha-ergc:-2015,
	title = {{ERGC}: an efficient referential genome compression algorithm},
	volume = {31},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{ERGC}},
	url = {http://bioinformatics.oxfordjournals.org/content/31/21/3468},
	doi = {10.1093/bioinformatics/btv399},
	abstract = {Motivation: Genome sequencing has become faster and more affordable. Consequently, the number of available complete genomic sequences is increasing rapidly. As a result, the cost to store, process, analyze and transmit the data is becoming a bottleneck for research and future medical applications. So, the need for devising efficient data compression and data reduction techniques for biological sequencing data is growing by the day. Although there exists a number of standard data compression algorithms, they are not efficient in compressing biological data. These generic algorithms do not exploit some inherent properties of the sequencing data while compressing. To exploit statistical and information-theoretic properties of genomic sequences, we need specialized compression algorithms. Five different next-generation sequencing data compression problems have been identified and studied in the literature. We propose a novel algorithm for one of these problems known as reference-based genome compression.
Results: We have done extensive experiments using five real sequencing datasets. The results on real genomes show that our proposed algorithm is indeed competitive and performs better than the best known algorithms for this problem. It achieves compression ratios that are better than those of the currently best performing algorithms. The time to compress and decompress the whole genome is also very promising.
Availability and implementation: The implementations are freely available for non-commercial purposes. They can be downloaded from http://engr.uconn.edu/∼rajasek/ERGC.zip.
Contact: rajasek@engr.uconn.edu},
	language = {en},
	number = {21},
	urldate = {2015-10-23},
	journal = {Bioinformatics},
	author = {Saha, Subrata and Rajasekaran, Sanguthevar},
	month = nov,
	year = {2015},
	pmid = {26139636},
	pages = {3468--3475},
	file = {Saha_Rajasekaran_2015_ERGC.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Saha_Rajasekaran_2015_ERGC.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/SK9DHM66/3468.html:text/html}
}

@article{yang-dna-2015,
	title = {A {DNA} shape-based regulatory score improves position-weight matrix-based recognition of transcription factor binding sites},
	volume = {31},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/31/21/3445},
	doi = {10.1093/bioinformatics/btv391},
	abstract = {Motivation: The position-weight matrix (PWM) is a useful representation of a transcription factor binding site (TFBS) sequence pattern because the PWM can be estimated from a small number of representative TFBS sequences. However, because the PWM probability model assumes independence between individual nucleotide positions, the PWMs for some TFs poorly discriminate binding sites from non-binding-sites that have similar sequence content. Since the local three-dimensional DNA structure (‘shape’) is a determinant of TF binding specificity and since DNA shape has a significant sequence-dependence, we combined DNA shape-derived features into a TF-generalized regulatory score and tested whether the score could improve PWM-based discrimination of TFBS from non-binding-sites.
Results: We compared a traditional PWM model to a model that combines the PWM with a DNA shape feature-based regulatory potential score, for accuracy in detecting binding sites for 75 vertebrate transcription factors. The PWM + shape model was more accurate than the PWM-only model, for 45\% of TFs tested, with no significant loss of accuracy for the remaining TFs.
Availability and implementation: The shape-based model is available as an open-source R package at that is archived on the GitHub software repository at https://github.com/ramseylab/regshape/.
Contact: stephen.ramsey@oregonstate.edu
Supplementary information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {21},
	urldate = {2015-10-23},
	journal = {Bioinformatics},
	author = {Yang, Jichen and Ramsey, Stephen A.},
	month = nov,
	year = {2015},
	pmid = {26130577},
	pages = {3445--3450},
	file = {Snapshot:/home/user/Zotero/storage/CMGWRFAN/3445.html:text/html;Yang_Ramsey_2015_A DNA shape-based regulatory score improves position-weight matrix-based.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Yang_Ramsey_2015_A DNA shape-based regulatory score improves position-weight matrix-based.pdf:application/pdf}
}

@article{ofer-profet:-2015,
	title = {{ProFET}: {Feature} engineering captures high-level protein functions},
	volume = {31},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{ProFET}},
	url = {http://bioinformatics.oxfordjournals.org/content/31/21/3429},
	doi = {10.1093/bioinformatics/btv345},
	abstract = {Motivation: The amount of sequenced genomes and proteins is growing at an unprecedented pace. Unfortunately, manual curation and functional knowledge lag behind. Homologous inference often fails at labeling proteins with diverse functions and broad classes. Thus, identifying high-level protein functionality remains challenging. We hypothesize that a universal feature engineering approach can yield classification of high-level functions and unified properties when combined with machine learning approaches, without requiring external databases or alignment.
Results: In this study, we present a novel bioinformatics toolkit called ProFET (Protein Feature Engineering Toolkit). ProFET extracts hundreds of features covering the elementary biophysical and sequence derived attributes. Most features capture statistically informative patterns. In addition, different representations of sequences and the amino acids alphabet provide a compact, compressed set of features. The results from ProFET were incorporated in data analysis pipelines, implemented in python and adapted for multi-genome scale analysis. ProFET was applied on 17 established and novel protein benchmark datasets involving classification for a variety of binary and multi-class tasks. The results show state of the art performance. The extracted features’ show excellent biological interpretability. The success of ProFET applies to a wide range of high-level functions such as subcellular localization, structural classes and proteins with unique functional properties (e.g. neuropeptide precursors, thermophilic and nucleic acid binding). ProFET allows easy, universal discovery of new target proteins, as well as understanding the features underlying different high-level protein functions.
Availability and implementation: ProFET source code and the datasets used are freely available at https://github.com/ddofer/ProFET.
Contact: michall@cc.huji.ac.il
Supplementary information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {21},
	urldate = {2015-10-23},
	journal = {Bioinformatics},
	author = {Ofer, Dan and Linial, Michal},
	month = nov,
	year = {2015},
	pmid = {26130574},
	pages = {3429--3436},
	file = {Ofer_Linial_2015_ProFET.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ofer_Linial_2015_ProFET.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TE2XBCV8/3429.html:text/html}
}

@article{ha-dingo:-2015,
	title = {{DINGO}: differential network analysis in genomics},
	volume = {31},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{DINGO}},
	url = {http://bioinformatics.oxfordjournals.org/content/31/21/3413},
	doi = {10.1093/bioinformatics/btv406},
	abstract = {Motivation: Cancer progression and development are initiated by aberrations in various molecular networks through coordinated changes across multiple genes and pathways. It is important to understand how these networks change under different stress conditions and/or patient-specific groups to infer differential patterns of activation and inhibition. Existing methods are limited to correlation networks that are independently estimated from separate group-specific data and without due consideration of relationships that are conserved across multiple groups.
Method: We propose a pathway-based differential network analysis in genomics (DINGO) model for estimating group-specific networks and making inference on the differential networks. DINGO jointly estimates the group-specific conditional dependencies by decomposing them into global and group-specific components. The delineation of these components allows for a more refined picture of the major driver and passenger events in the elucidation of cancer progression and development.
Results: Simulation studies demonstrate that DINGO provides more accurate group-specific conditional dependencies than achieved by using separate estimation approaches. We apply DINGO to key signaling pathways in glioblastoma to build differential networks for long-term survivors and short-term survivors in The Cancer Genome Atlas. The hub genes found by mRNA expression, DNA copy number, methylation and microRNA expression reveal several important roles in glioblastoma progression.
Availability and implementation: R Package at: odin.mdacc.tmc.edu/∼vbaladan.
Contact: veera@mdanderson.org
Supplementary information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {21},
	urldate = {2015-10-23},
	journal = {Bioinformatics},
	author = {Ha, Min Jin and Baladandayuthapani, Veerabhadran and Do, Kim-Anh},
	month = nov,
	year = {2015},
	pmid = {26148744},
	pages = {3413--3420},
	file = {Ha et al_2015_DINGO.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ha et al_2015_DINGO.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/79ITCP94/3413.html:text/html}
}

@article{sieffert-first-2015,
	title = {First principles static and dynamic calculations for the transition metal hydride series \textbf{{M}} {H} $_{\textrm{4}}$ \textbf{{L}} $_{\textrm{3}}$ ( \textbf{{M}} = {Fe}, {Ru} and {Os}; \textbf{{L}} = {NH} $_{\textrm{3}}$ , {PH} $_{\textrm{3}}$ and {PF} $_{\textrm{3}}$ )},
	volume = {44},
	issn = {1477-9226, 1477-9234},
	url = {http://xlink.rsc.org/?DOI=C4DT02475C},
	doi = {10.1039/C4DT02475C},
	language = {en},
	number = {9},
	urldate = {2015-10-23},
	journal = {Dalton Trans.},
	author = {Sieffert, Nicolas and Kendrick, Thomas and Tiana, Davide and Morrison, Carole A.},
	year = {2015},
	pages = {4259--4270},
	file = {First principles static and dynamic calculations for the transition metal hydride series MH4L3 (M = Fe, Ru and Os\; L = NH3, PH3 and PF3) - c4dt02475c:/home/user/Zotero/storage/J29ZIQZB/c4dt02475c.pdf:application/pdf}
}

@inproceedings{shapiro-political-2006,
	title = {Political polarization and the rational public},
	url = {http://themonkeycage.org/Shapiro%2520and%2520Bloch.pdf},
	urldate = {2015-10-23},
	booktitle = {annual conference of the {American} {Association} for {Public} {Opinion} {Research}, {Montreal}, {Canada}},
	author = {Shapiro, Robert Y. and Bloch-Elkon, Yaeli},
	year = {2006},
	file = {Microsoft Word - 78178AbstractContentFile.doc - Shapiro%20and%20Bloch.pdf:/home/user/Zotero/storage/3AGN3Z7I/Shapiro%20and%20Bloch.pdf:application/pdf}
}

@article{opper-learning-2010,
	title = {Learning combinatorial transcriptional dynamics from gene expression data},
	volume = {26},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/26/13/1623},
	doi = {10.1093/bioinformatics/btq244},
	abstract = {Motivation: mRNA transcriptional dynamics is governed by a complex network of transcription factor (TF) proteins. Experimental and theoretical analysis of this process is hindered by the fact that measurements of TF activity in vivo is very challenging. Current models that jointly infer TF activities and model parameters rely on either of the two main simplifying assumptions: either the dynamics is simplified (e.g. assuming quasi-steady state) or the interactions between TFs are ignored, resulting in models accounting for a single TF.
Results: We present a novel approach to reverse engineer the dynamics of multiple TFs jointly regulating the expression of a set of genes. The model relies on a continuous time, differential equation description of transcriptional dynamics where TFs are treated as latent on/off variables and are modelled using a switching stochastic process (telegraph process). The model can not only incorporate both activation and repression, but allows any non-trivial interaction between TFs, including AND and OR gates. By using a factorization assumption within a variational Bayesian treatment we formulate a framework that can reconstruct both the activity profiles of the TFs and the type of regulation from time series gene expression data. We demonstrate the identifiability of the model on a simple but non-trivial synthetic example, and then use it to formulate non-trivial predictions about transcriptional control during yeast metabolism.
Availability: http://homepages.inf.ed.ac.uk/gsanguin/
Contact: g.sanguinetti@ed.ac.uk
Supplementary information: Supplementary data are available at Bioinformatics online.},
	language = {en},
	number = {13},
	urldate = {2015-10-23},
	journal = {Bioinformatics},
	author = {Opper, Manfred and Sanguinetti, Guido},
	month = jul,
	year = {2010},
	pmid = {20444835},
	pages = {1623--1629},
	file = {Opper_Sanguinetti_2010_Learning combinatorial transcriptional dynamics from gene expression data.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Opper_Sanguinetti_2010_Learning combinatorial transcriptional dynamics from gene expression data.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BZIGWNV5/1623.html:text/html}
}

@article{martin-holiday-2015,
	title = {Holiday or vacation? {The} processing of variation in vocabulary across dialects},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Holiday or vacation?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1100750},
	doi = {10.1080/23273798.2015.1100750},
	abstract = {Native speakers with different linguistic backgrounds differ in their usage of language, and particularly in their vocabulary. For instance, British natives would use the word "holiday" when American natives would prefer the word "vacation". This study investigates how cross-dialectal lexical variation impacts lexical processing. Electrophysiological responses were recorded, while British natives listened to British or American speech in which lexical frequency dominance across dialects was manipulated (British versus American vocabulary). Words inconsistent with the dialect of the speaker (British words uttered by American speakers and vice versa) elicited larger negative electrophysiological deflections than consistent words, 700 ms after stimulus onset. Thus, processing of British words was easier when listening to British speakers and processing of American words was easier when listening to American speakers. These results show that listeners integrate their knowledge about cross-dialectal lexical variations in vocabulary as speech unfolds, as it was previously shown for social lexical variations.},
	number = {0},
	urldate = {2015-10-23},
	journal = {Language, Cognition and Neuroscience},
	author = {Martin, Clara D. and Garcia, Xavier and Potter, Douglas and Melinger, Alissa and Costa, Albert},
	month = oct,
	year = {2015},
	pages = {1--16},
	file = {Martin et al_2015_Holiday or vacation.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Martin et al_2015_Holiday or vacation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TEAAXDHD/23273798.2015.html:text/html}
}

@article{bernardi-somatosensory-2015,
	title = {Somatosensory {Contribution} to the {Initial} {Stages} of {Human} {Motor} {Learning}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/35/42/14316},
	doi = {10.1523/JNEUROSCI.1344-15.2015},
	abstract = {The early stages of motor skill acquisition are often marked by uncertainty about the sensory and motor goals of the task, as is the case in learning to speak or learning the feel of a good tennis serve. Here we present an experimental model of this early learning process, in which targets are acquired by exploration and reinforcement rather than sensory error. We use this model to investigate the relative contribution of motor and sensory factors to human motor learning. Participants make active reaching movements or matched passive movements to an unseen target using a robot arm. We find that learning through passive movements paired with reinforcement is comparable with learning associated with active movement, both in terms of magnitude and durability, with improvements due to training still observable at a 1 week retest. Motor learning is also accompanied by changes in somatosensory perceptual acuity. No stable changes in motor performance are observed for participants that train, actively or passively, in the absence of reinforcement, or for participants who are given explicit information about target position in the absence of somatosensory experience. These findings indicate that the somatosensory system dominates learning in the early stages of motor skill acquisition.
SIGNIFICANCE STATEMENT The research focuses on the initial stages of human motor learning, introducing a new experimental model that closely approximates the key features of motor learning outside of the laboratory. The finding indicates that it is the somatosensory system rather than the motor system that dominates learning in the early stages of motor skill acquisition. This is important given that most of our computational models of motor learning are based on the idea that learning is motoric in origin. This is also a valuable finding for rehabilitation of patients with limited mobility as it shows that reinforcement in conjunction with passive movement results in benefits to motor learning that are as great as those observed for active movement training.},
	language = {en},
	number = {42},
	urldate = {2015-10-23},
	journal = {The Journal of Neuroscience},
	author = {Bernardi, Nicolò F. and Darainy, Mohammad and Ostry, David J.},
	month = oct,
	year = {2015},
	pmid = {26490869},
	keywords = {motor learning, motor skill learning, passive movements, reinforcement, somatosensory perception},
	pages = {14316--14326},
	file = {Bernardi et al_2015_Somatosensory Contribution to the Initial Stages of Human Motor Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bernardi et al_2015_Somatosensory Contribution to the Initial Stages of Human Motor Learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RJW8XFMU/14316.html:text/html}
}

@article{cox-there-2015,
	title = {There {Is} a “{U}” in {Clutter}: {Evidence} for {Robust} {Sparse} {Codes} {Underlying} {Clutter} {Tolerance} in {Human} {Vision}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	shorttitle = {There {Is} a “{U}” in {Clutter}},
	url = {http://www.jneurosci.org/content/35/42/14148},
	doi = {10.1523/JNEUROSCI.1211-15.2015},
	abstract = {The ability to recognize objects in clutter is crucial for human vision, yet the underlying neural computations remain poorly understood. Previous single-unit electrophysiology recordings in inferotemporal cortex in monkeys and fMRI studies of object-selective cortex in humans have shown that the responses to pairs of objects can sometimes be well described as a weighted average of the responses to the constituent objects. Yet, from a computational standpoint, it is not clear how the challenge of object recognition in clutter can be solved if downstream areas must disentangle the identity of an unknown number of individual objects from the confounded average neuronal responses. An alternative idea is that recognition is based on a subpopulation of neurons that are robust to clutter, i.e., that do not show response averaging, but rather robust object-selective responses in the presence of clutter. Here we show that simulations using the HMAX model of object recognition in cortex can fit the aforementioned single-unit and fMRI data, showing that the averaging-like responses can be understood as the result of responses of object-selective neurons to suboptimal stimuli. Moreover, the model shows how object recognition can be achieved by a sparse readout of neurons whose selectivity is robust to clutter. Finally, the model provides a novel prediction about human object recognition performance, namely, that target recognition ability should show a U-shaped dependency on the similarity of simultaneously presented clutter objects. This prediction is confirmed experimentally, supporting a simple, unifying model of how the brain performs object recognition in clutter.
SIGNIFICANCE STATEMENT The neural mechanisms underlying object recognition in cluttered scenes (i.e., containing more than one object) remain poorly understood. Studies have suggested that neural responses to multiple objects correspond to an average of the responses to the constituent objects. Yet, it is unclear how the identities of an unknown number of objects could be disentangled from a confounded average response. Here, we use a popular computational biological vision model to show that averaging-like responses can result from responses of clutter-tolerant neurons to suboptimal stimuli. The model also provides a novel prediction, that human detection ability should show a U-shaped dependency on target–clutter similarity, which is confirmed experimentally, supporting a simple, unifying account of how the brain performs object recognition in clutter.},
	language = {en},
	number = {42},
	urldate = {2015-10-23},
	journal = {The Journal of Neuroscience},
	author = {Cox, Patrick H. and Riesenhuber, Maximilian},
	month = oct,
	year = {2015},
	pmid = {26490856},
	keywords = {clutter, HMAX, sparse coding, vision},
	pages = {14148--14159},
	file = {Cox_Riesenhuber_2015_There Is a “U” in Clutter.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Cox_Riesenhuber_2015_There Is a “U” in Clutter.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3NKPKFD5/14148.html:text/html}
}

@article{guderian-hippocampal-2015,
	title = {Hippocampal {Volume} {Reduction} in {Humans} {Predicts} {Impaired} {Allocentric} {Spatial} {Memory} in {Virtual}-{Reality} {Navigation}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/35/42/14123},
	doi = {10.1523/JNEUROSCI.0801-15.2015},
	abstract = {The extent to which navigational spatial memory depends on hippocampal integrity in humans is not well documented. We investigated allocentric spatial recall using a virtual environment in a group of patients with severe hippocampal damage (SHD), a group of patients with “moderate” hippocampal damage (MHD), and a normal control group. Through four learning blocks with feedback, participants learned the target locations of four different objects in a circular arena. Distal cues were present throughout the experiment to provide orientation. A circular boundary as well as an intra-arena landmark provided spatial reference frames. During a subsequent test phase, recall of all four objects was tested with only the boundary or the landmark being present. Patients with SHD were impaired in both phases of this task. Across groups, performance on both types of spatial recall was highly correlated with memory quotient (MQ), but not with intelligence quotient (IQ), age, or sex. However, both measures of spatial recall separated experimental groups beyond what would be expected based on MQ, a widely used measure of general memory function. Boundary-based and landmark-based spatial recall were both strongly related to bilateral hippocampal volumes, but not to volumes of the thalamus, putamen, pallidum, nucleus accumbens, or caudate nucleus. The results show that boundary-based and landmark-based allocentric spatial recall are similarly impaired in patients with SHD, that both types of recall are impaired beyond that predicted by MQ, and that recall deficits are best explained by a reduction in bilateral hippocampal volumes.
SIGNIFICANCE STATEMENT In humans, bilateral hippocampal atrophy can lead to profound impairments in episodic memory. Across species, perhaps the most well-established contribution of the hippocampus to memory is not to episodic memory generally but to allocentric spatial memory. However, the extent to which navigational spatial memory depends on hippocampal integrity in humans is not well documented. We investigated spatial recall using a virtual environment in two groups of patients with hippocampal damage (moderate/severe) and a normal control group. The results showed that patients with severe hippocampal damage are impaired in learning and recalling allocentric spatial information. Furthermore, hippocampal volume reduction impaired allocentric navigation beyond what can be predicted by memory quotient as a widely used measure of general memory function.},
	language = {en},
	number = {42},
	urldate = {2015-10-23},
	journal = {The Journal of Neuroscience},
	author = {Guderian, Sebastian and Dzieciol, Anna M. and Gadian, David G. and Jentschke, Sebastian and Doeller, Christian F. and Burgess, Neil and Mishkin, Mortimer and Vargha-Khadem, Faraneh},
	month = oct,
	year = {2015},
	pmid = {26490854},
	keywords = {memory, amnesia, human, navigation, spatial, virtual reality},
	pages = {14123--14131},
	file = {Guderian et al_2015_Hippocampal Volume Reduction in Humans Predicts Impaired Allocentric Spatial.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Guderian et al_2015_Hippocampal Volume Reduction in Humans Predicts Impaired Allocentric Spatial.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QWS7UE89/14123.html:text/html}
}

@article{moutsiana-human-2015,
	title = {Human {Frontal}–{Subcortical} {Circuit} and {Asymmetric} {Belief} {Updating}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/35/42/14077},
	doi = {10.1523/JNEUROSCI.1120-15.2015},
	abstract = {How humans integrate information to form beliefs about reality is a question that has engaged scientists for centuries, yet the biological system supporting this process is not well understood. One of the most salient attributes of information is valence. Whether a piece of news is good or bad is critical in determining whether it will alter our beliefs. Here, we reveal a frontal–subcortical circuit in the left hemisphere that is simultaneously associated with enhanced integration of favorable information into beliefs and impaired integration of unfavorable information. Specifically, for favorable information, stronger white matter connectivity within this system, particularly between the left inferior frontal gyrus (IFG) and left subcortical regions (including the amygdala, hippocampus, thalamus, putamen, and pallidum), as well as insular cortex, is associated with greater change in belief. However, for unfavorable information, stronger connectivity within this system, particularly between the left IFG and left pallidum, putamen, and insular cortex, is associated with reduced change in beliefs. These novel results are consistent with models suggesting that partially separable processes govern learning from favorable and unfavorable information.
SIGNIFICANCE STATEMENT Beliefs of what may happen in the future are important, because they guide decisions and actions. Here, we illuminate how structural brain connectivity is related to the generation of subjective beliefs. We focus on how the valence of information is related to people's tendency to alter their beliefs. By quantifying the extent to which participants update their beliefs in response to desirable and undesirable information and relating those measures to the strength of white matter connectivity using diffusion tensor imaging, we characterize a left frontal–subcortical system that is associated simultaneously with greater belief updating in response to favorable information and reduced belief updating in response to unfavorable information. This neural architecture may allow valence to be incorporated into belief updating.},
	language = {en},
	number = {42},
	urldate = {2015-10-23},
	journal = {The Journal of Neuroscience},
	author = {Moutsiana, Christina and Charpentier, Caroline J. and Garrett, Neil and Cohen, Michael X. and Sharot, Tali},
	month = oct,
	year = {2015},
	pmid = {26490851},
	keywords = {individual differences, belief updating, DTI, frontal-subcortical circuit, valence, white-matter connectivity},
	pages = {14077--14085},
	file = {Moutsiana et al_2015_Human Frontal–Subcortical Circuit and Asymmetric Belief Updating.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Moutsiana et al_2015_Human Frontal–Subcortical Circuit and Asymmetric Belief Updating.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/D4JGMK4W/14077.html:text/html}
}

@article{white-competing-2015,
	title = {Competing influences of emotion and phonology during picture-word interference},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1101144},
	doi = {10.1080/23273798.2015.1101144},
	abstract = {Speaking is susceptible to distraction, illustrated by slowed picture naming in the presence of taboo distractor words. However, other distractors such as phonologically related words speed picture naming. Two experiments explored the simultaneous influences of these competing factors. Participants named target pictures superimposed with taboo, negative, positive, or neutral distractor words, and filler pictures were presented after every target to investigate emotional carryover effects. Distractors were phonologically related or unrelated to the target (Experiment 1) or filler (Experiment 2). Results showed that taboo, and to a lesser extent negative, distractors slowed picture naming relative to neutral and positive distractors, and slowing from taboo distractors persisted into the filler trial. In contrast, phonological overlap between targets and distractors sped target but not filler picture naming, especially when distractors were taboo. These findings suggest that strong emotional words engage attention to influence phonological encoding during speech production, and interfering effects from taboo words are particularly long lasting. Results are interpreted within existing language production theories, using mechanisms that are sensitive to words’ emotional properties and that regulate distractor interference during speech production.},
	number = {0},
	urldate = {2015-10-23},
	journal = {Language, Cognition and Neuroscience},
	author = {White, Katherine K. and Abrams, Lise and LaBat, Lauren R. and Rhynes, Anne M.},
	month = oct,
	year = {2015},
	pages = {1--19},
	file = {Snapshot:/home/user/Zotero/storage/N5NJ8QXN/23273798.2015.html:text/html;White et al_2015_Competing influences of emotion and phonology during picture-word interference.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/White et al_2015_Competing influences of emotion and phonology during picture-word interference.pdf:application/pdf}
}

@article{johnson-laird-logic-2015,
	title = {Logic, probability, and human reasoning},
	volume = {19},
	issn = {1879-307X},
	doi = {10.1016/j.tics.2015.02.006},
	abstract = {This review addresses the long-standing puzzle of how logic and probability fit together in human reasoning. Many cognitive scientists argue that conventional logic cannot underlie deductions, because it never requires valid conclusions to be withdrawn - not even if they are false; it treats conditional assertions implausibly; and it yields many vapid, although valid, conclusions. A new paradigm of probability logic allows conclusions to be withdrawn and treats conditionals more plausibly, although it does not address the problem of vapidity. The theory of mental models solves all of these problems. It explains how people reason about probabilities and postulates that the machinery for reasoning is itself probabilistic. Recent investigations accordingly suggest a way to integrate probability and deduction.},
	language = {eng},
	number = {4},
	journal = {Trends in Cognitive Sciences},
	author = {Johnson-Laird, P. N. and Khemlani, Sangeet S. and Goodwin, Geoffrey P.},
	month = apr,
	year = {2015},
	pmid = {25770779},
	pages = {201--214},
	file = {Logic, probability, and human reasoning - 1-s2.0-S1364661315000303-main.pdf:/home/user/Zotero/storage/KNPWH52Z/1-s2.0-S1364661315000303-main.pdf:application/pdf}
}

@article{baratgin-new-2015,
	title = {The {New} {Paradigm} and {Mental} {Models}},
	volume = {19},
	issn = {1364-6613},
	url = {http://www.cell.com/article/S1364661315001503/abstract},
	doi = {10.1016/j.tics.2015.06.013},
	language = {English},
	number = {10},
	urldate = {2015-10-23},
	journal = {Trends in Cognitive Sciences},
	author = {Baratgin, Jean and Douven, Igor and Evans, Jonathan St B. T. and Oaksford, Mike and Over, David and Politzer, Guy},
	month = jan,
	year = {2015},
	pmid = {26412091},
	pages = {547--548},
	file = {Baratgin et al_2015_The New Paradigm and Mental Models.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Baratgin et al_2015_The New Paradigm and Mental Models.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/UZTG4Q9M/S1364-6613(15)00150-3.html:text/html}
}

@book{graves-supervised-2012,
	title = {Supervised sequence labelling with recurrent neural networks},
	volume = {385},
	url = {http://link.springer.com/content/pdf/10.1007/978-3-642-24797-2.pdf},
	urldate = {2015-10-22},
	publisher = {Springer},
	author = {Graves, Alex and {others}},
	year = {2012},
	file = {preprint.pdf:/home/user/Zotero/storage/TR7DSQ2S/preprint.pdf:application/pdf}
}

@phdthesis{mikolov-thesis.pdf-nodate,
	title = {thesis.pdf},
	url = {http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf},
	urldate = {2015-10-22},
	author = {mikolov},
	file = {thesis.pdf:/home/user/Zotero/storage/8NAVXQQ6/thesis.pdf:application/pdf}
}

@article{dalege-toward-2015,
	title = {Toward a {Formalized} {Account} of {Attitudes}: {The} {Causal} {Attitude} {Network} ({CAN}) {Model}},
	copyright = {(c) 2015 APA, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	shorttitle = {Toward a {Formalized} {Account} of {Attitudes}},
	doi = {10.1037/a0039802},
	abstract = {This article introduces the Causal Attitude Network (CAN) model, which conceptualizes attitudes as networks consisting of evaluative reactions and interactions between these reactions. Relevant evaluative reactions include beliefs, feelings, and behaviors toward the attitude object. Interactions between these reactions arise through direct causal influences (e.g., the belief that snakes are dangerous causes fear of snakes) and mechanisms that support evaluative consistency between related contents of evaluative reactions (e.g., people tend to align their belief that snakes are useful with their belief that snakes help maintain ecological balance). In the CAN model, the structure of attitude networks conforms to a small-world structure: evaluative reactions that are similar to each other form tight clusters, which are connected by a sparser set of “shortcuts” between them. We argue that the CAN model provides a realistic formalized measurement model of attitudes and therefore fills a crucial gap in the attitude literature. Furthermore, the CAN model provides testable predictions for the structure of attitudes and how they develop, remain stable, and change over time. Attitude strength is conceptualized in terms of the connectivity of attitude networks and we show that this provides a parsimonious account of the differences between strong and weak attitudes. We discuss the CAN model in relation to possible extensions, implication for the assessment of attitudes, and possibilities for further study.},
	journal = {Psychological Review},
	author = {Dalege, Jonas and Borsboom, Denny and van Harreveld, Frenk and van den Berg, Helma and Conner, Mark and J, L.},
	year = {2015},
	pages = {No Pagination Specified}
}

@article{harkin-does-2015,
	title = {Does {Monitoring} {Goal} {Progress} {Promote} {Goal} {Attainment}? {A} {Meta}-{Analysis} of the {Experimental} {Evidence}},
	copyright = {(c) 2015 APA, all rights reserved},
	issn = {1939-1455(Electronic);0033-2909(Print)},
	shorttitle = {Does {Monitoring} {Goal} {Progress} {Promote} {Goal} {Attainment}?},
	doi = {10.1037/bul0000025},
	abstract = {Control theory and other frameworks for understanding self-regulation suggest that monitoring goal progress is a crucial process that intervenes between setting and attaining a goal, and helps to ensure that goals are translated into action. However, the impact of progress monitoring interventions on rates of behavioral performance and goal attainment has yet to be quantified. A systematic literature search identified 138 studies (N = 19,951) that randomly allocated participants to an intervention designed to promote monitoring of goal progress versus a control condition. All studies reported the effects of the treatment on (a) the frequency of progress monitoring and (b) subsequent goal attainment. A random effects model revealed that, on average, interventions were successful at increasing the frequency of monitoring goal progress (d+ = 1.98, 95\% CI [1.71, 2.24]) and promoted goal attainment (d+ = 0.40, 95\% CI [0.32, 0.48]). Furthermore, changes in the frequency of progress monitoring mediated the effect of the interventions on goal attainment. Moderation tests revealed that progress monitoring had larger effects on goal attainment when the outcomes were reported or made public, and when the information was physically recorded. Taken together, the findings suggest that monitoring goal progress is an effective self-regulation strategy, and that interventions that increase the frequency of progress monitoring are likely to promote behavior change.},
	journal = {Psychological Bulletin},
	author = {Harkin, Benjamin and Webb, Thomas L. and I, P. and Prestwich, Andrew and Conner, Mark and Kellar, Ian and Benn, Yael and Sheeran, Paschal},
	year = {2015},
	pages = {No Pagination Specified}
}

@article{liu-calibrated-2015,
	title = {Calibrated {Multivariate} {Regression} with {Application} to {Neural} {Semantic} {Basis} {Discovery}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/liu15b.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Liu, Han and Wang, Lie and Zhao, Tuo},
	year = {2015},
	pages = {1579--1606},
	file = {Liu et al_2015_Calibrated Multivariate Regression with Application to Neural Semantic Basis.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Liu et al_2015_Calibrated Multivariate Regression with Application to Neural Semantic Basis.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/N4G8UJX5/liu15b.html:text/html}
}

@article{moroshko-second-order-2015,
	title = {Second-{Order} {Non}-{Stationary} {Online} {Learning} for {Regression}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/moroshko15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Moroshko, Edward and Vaits, Nina and Crammer, Koby},
	year = {2015},
	pages = {1481--1517},
	file = {Moroshko et al_2015_Second-Order Non-Stationary Online Learning for Regression.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Moroshko et al_2015_Second-Order Non-Stationary Online Learning for Regression2.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JJQU7QJI/moroshko15a.html:text/html}
}

@article{moroshko-second-order-2015-1,
	title = {Second-{Order} {Non}-{Stationary} {Online} {Learning} for {Regression}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/moroshko15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Moroshko, Edward and Vaits, Nina and Crammer, Koby},
	year = {2015},
	pages = {1481--1517},
	file = {Moroshko et al_2015_Second-Order Non-Stationary Online Learning for Regression.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Moroshko et al_2015_Second-Order Non-Stationary Online Learning for Regression.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9JWWFIAR/moroshko15a.html:text/html}
}

@article{sunehag-rationality-2015,
	title = {Rationality, {Optimism} and {Guarantees} in {General} {Reinforcement} {Learning}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/sunehag15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Sunehag, Peter and Hutter, Marcus},
	year = {2015},
	pages = {1345--1390},
	file = {Snapshot:/home/user/Zotero/storage/T6PCKMNU/sunehag15a.html:text/html;Sunehag_Hutter_2015_Rationality, Optimism and Guarantees in General Reinforcement Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Sunehag_Hutter_2015_Rationality, Optimism and Guarantees in General Reinforcement Learning.pdf:application/pdf}
}

@article{j.kiraly-algebraic-2015,
	title = {The {Algebraic} {Combinatorial} {Approach} for {Low}-{Rank} {Matrix} {Completion}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/kiraly15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {J.Király, Franz and Theran, Louis and Tomioka, Ryota},
	year = {2015},
	pages = {1391--1436},
	file = {J.Király et al_2015_The Algebraic Combinatorial Approach for Low-Rank Matrix Completion.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/J.Király et al_2015_The Algebraic Combinatorial Approach for Low-Rank Matrix Completion.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/78MJ8JG8/kiraly15a.html:text/html}
}

@article{garcia-comprehensive-2015,
	title = {A {Comprehensive} {Survey} on {Safe} {Reinforcement} {Learning}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/garcia15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {García, Javier and Fernández, Fernando},
	year = {2015},
	pages = {1437--1480},
	file = {García_Fernández_2015_A Comprehensive Survey on Safe Reinforcement Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/García_Fernández_2015_A Comprehensive Survey on Safe Reinforcement Learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/98XV4JK8/garcia15a.html:text/html}
}

@article{fearnley-learning-2015,
	title = {Learning {Equilibria} of {Games} via {Payoff} {Queries}},
	volume = {16},
	url = {http://jmlr.org/papers/v16/fearnley15a.html},
	urldate = {2015-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Fearnley, John and Gairing, Martin and Goldberg, Paul W. and Savani, Rahul},
	year = {2015},
	pages = {1305--1344},
	file = {Fearnley et al_2015_Learning Equilibria of Games via Payoff Queries.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fearnley et al_2015_Learning Equilibria of Games via Payoff Queries.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VTBFB89G/fearnley15a.html:text/html}
}

@article{sun-complex-2015,
	title = {Complex linguistic rules modulate early auditory brain responses},
	volume = {149},
	issn = {0093-934X},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X15001297},
	doi = {10.1016/j.bandl.2015.06.009},
	abstract = {During speech perception, listeners compensate for phonological rules of their language. For instance, English place assimilation causes green boat to be typically pronounced as greem boat; English listeners, however, perceptually compensate for this rule and retrieve the intended sound (n). Previous research using EEG has focused on rules with clear phonetic underpinnings, showing that perceptual compensation occurs at an early stage of speech perception. We tested whether this early mechanism also accounts for the compensation for more complex rules. We examined compensation for French voicing assimilation, a rule with abstract phonological restrictions on the contexts in which it applies. Our results reveal that perceptual compensation for this rule by French listeners modulates an early ERP component. This is evidence that early stages of speech sound categorization are sensitive to complex phonological rules of the native language.},
	urldate = {2015-10-21},
	journal = {Brain and Language},
	author = {Sun, Yue and Giavazzi, Maria and Adda-Decker, Martine and Barbosa, Leonardo S. and Kouider, Sid and Bachoud-Lévi, Anne-Catherine and Jacquemot, Charlotte and Peperkamp, Sharon},
	month = oct,
	year = {2015},
	keywords = {speech perception, Assimilation, Electroencephalography, Mismatch negativity, Phonological rules},
	pages = {55--65},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/UV84V674/S0093934X15001297.html:text/html;Sun et al_2015_Complex linguistic rules modulate early auditory brain responses.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Sun et al_2015_Complex linguistic rules modulate early auditory brain responses.pdf:application/pdf}
}

@article{hertrich-context-dependent-2015,
	title = {Context-dependent impact of presuppositions on early magnetic brain responses during speech perception},
	volume = {149},
	issn = {0093-934X},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X1500125X},
	doi = {10.1016/j.bandl.2015.06.005},
	abstract = {Discourse structure enables us to generate expectations based upon linguistic material that has already been introduced. The present magnetoencephalography (MEG) study addresses auditory perception of test sentences in which discourse coherence was manipulated by using presuppositions (PSP) that either correspond or fail to correspond to items in preceding context sentences with respect to uniqueness and existence. Context violations yielded delayed auditory M50 and enhanced auditory M200 cross-correlation responses to syllable onsets within an analysis window of 1.5 s following the PSP trigger words. Furthermore, discourse incoherence yielded suppression of spectral power within an expanded alpha band ranging from 6 to 16 Hz. This effect showed a bimodal temporal distribution, being significant in an early time window of 0.0–0.5 s following the PSP trigger and a late interval of 2.0–2.5 s. These findings indicate anticipatory top-down mechanisms interacting with various aspects of bottom-up processing during speech perception.},
	urldate = {2015-10-21},
	journal = {Brain and Language},
	author = {Hertrich, Ingo and Kirsten, Mareike and Tiemann, Sonja and Beck, Sigrid and Wühle, Anja and Ackermann, Hermann and Rolke, Bettina},
	month = oct,
	year = {2015},
	keywords = {speech perception, Alpha suppression, Auditory processing, Magnetoencephalography, Phase locking, Semantics in context},
	pages = {1--12},
	file = {Hertrich et al_2015_Context-dependent impact of presuppositions on early magnetic brain responses.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hertrich et al_2015_Context-dependent impact of presuppositions on early magnetic brain responses.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/9Z8WDQRI/S0093934X1500125X.html:text/html}
}

@article{ecker-he-2015,
	title = {He did it! {She} did it! {No}, she did not! {Multiple} causal explanations and the continued influence of misinformation},
	volume = {85},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001035},
	doi = {10.1016/j.jml.2015.09.002},
	abstract = {Two types of misinformation effects are discussed in the literature—the post-event misinformation effect and the continued influence effect. The former refers to the distorting memorial effects of misleading information that is presented after valid event encoding; the latter refers to information that is initially presented as true but subsequently turns out to be false and continues to affect memory and reasoning despite the correction. In two experiments, using a paradigm that merges elements from both traditions, we investigated the role of presentation order and recency when two competing causal explanations for an event are presented and one is subsequently retracted. Theoretical accounts of misinformation effects make diverging predictions regarding the roles of presentation order and recency. A recency account—derived from time-based models of memory and reading comprehension research suggesting efficient situation model updating—predicts that the more recently presented cause should have a stronger influence on memory and reasoning. By contrast, a primacy account—derived from primacy effects in impression formation and story recall as well as findings of inadequate memory updating—predicts that the initially presented cause should be dominant irrespective of temporal factors. Results indicated that (1) a cause’s recency, rather than its position (i.e., whether it was presented first or last) determined the emphasis that people place on it in their later reasoning, with more recent explanations being preferred; and (2) a retraction was equally effective whether it invalidated the first or the second cause, as long as the cause’s recency was held constant. This provides evidence against the primacy account and supports time-based models of memory such as temporal distinctiveness theory.},
	urldate = {2015-10-21},
	journal = {Journal of Memory and Language},
	author = {Ecker, Ullrich K. H. and Lewandowsky, Stephan and Cheung, Candy S. C. and Maybery, Murray T.},
	month = nov,
	year = {2015},
	keywords = {Continued influence effect, Memory updating, Mental-model theory, Post-event misinformation, Temporal distinctiveness theory},
	pages = {101--115},
	file = {Ecker et al_2015_He did it.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ecker et al_2015_He did it.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CNEZJH9R/S0749596X15001035.html:text/html}
}

@article{mitterer-letters-2015,
	title = {Letters don’t matter: {No} effect of orthography on the perception of conversational speech},
	volume = {85},
	issn = {0749-596X},
	shorttitle = {Letters don’t matter},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001011},
	doi = {10.1016/j.jml.2015.08.005},
	abstract = {It has been claimed that learning to read changes the way we perceive speech, with detrimental effects for words with sound–spelling inconsistencies. Because conversational speech is peppered with segment deletions and alterations that lead to sound–spelling inconsistencies, such an influence would seriously hinder the perception of conversational speech. We hence tested whether the orthographic coding of a segment influences its deletion costs in perception. German glottal stop, a segment that is canonically present but not orthographically coded, allows such a test. The effects of glottal-stop deletion in German were compared to deletion of /h/ in German (grapheme: h) and deletion of glottal stop in Maltese (grapheme: q) in an implicit task with conversational speech and explicit task with careful speech. All segment deletions led to similar reduction costs in the implicit task, while an orthographic effect, with larger effects for orthographically coded segments, emerged in the explicit task. These results suggest that learning to read does not influence how we process speech but mainly how we think about it.},
	urldate = {2015-10-21},
	journal = {Journal of Memory and Language},
	author = {Mitterer, Holger and Reinisch, Eva},
	month = nov,
	year = {2015},
	keywords = {Phonological reduction, Spoken-word recognition, Visual-word recognition},
	pages = {116--134},
	file = {Mitterer_Reinisch_2015_Letters don’t matter.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mitterer_Reinisch_2015_Letters don’t matter2.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/RUSZ2DU6/S0749596X15001011.html:text/html}
}

@article{mitterer-letters-2015-1,
	title = {Letters don’t matter: {No} effect of orthography on the perception of conversational speech},
	volume = {85},
	issn = {0749-596X},
	shorttitle = {Letters don’t matter},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001011},
	doi = {10.1016/j.jml.2015.08.005},
	abstract = {It has been claimed that learning to read changes the way we perceive speech, with detrimental effects for words with sound–spelling inconsistencies. Because conversational speech is peppered with segment deletions and alterations that lead to sound–spelling inconsistencies, such an influence would seriously hinder the perception of conversational speech. We hence tested whether the orthographic coding of a segment influences its deletion costs in perception. German glottal stop, a segment that is canonically present but not orthographically coded, allows such a test. The effects of glottal-stop deletion in German were compared to deletion of /h/ in German (grapheme: h) and deletion of glottal stop in Maltese (grapheme: q) in an implicit task with conversational speech and explicit task with careful speech. All segment deletions led to similar reduction costs in the implicit task, while an orthographic effect, with larger effects for orthographically coded segments, emerged in the explicit task. These results suggest that learning to read does not influence how we process speech but mainly how we think about it.},
	urldate = {2015-10-21},
	journal = {Journal of Memory and Language},
	author = {Mitterer, Holger and Reinisch, Eva},
	month = nov,
	year = {2015},
	keywords = {Phonological reduction, Spoken-word recognition, Visual-word recognition},
	pages = {116--134},
	file = {Mitterer_Reinisch_2015_Letters don’t matter.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mitterer_Reinisch_2015_Letters don’t matter.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/UXWZ688R/S0749596X15001011.html:text/html}
}

@article{annis-list-length-2015,
	title = {The list-length effect does not discriminate between models of recognition memory},
	volume = {85},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15000753},
	doi = {10.1016/j.jml.2015.06.001},
	abstract = {Dennis, Lee, and Kinnell (2008) claimed that they obtained evidence for a null list-length effect (LLE) for recognition memory, and that their finding was consistent with context-noise models and inconsistent with item-noise models of memory. This claim has since been repeated in several articles (e.g., Kinnell \&amp; Dennis, 2011; Turner, Dennis, \&amp; Van Zandt, 2013). However, a more thorough investigation of their data indicates that Dennis et al.’s findings are inconclusive, and their assertion that empirical observations of the LLE may distinguish between item-noise and context-noise models is debatable. In fact, their findings provide very little evidence in favor of a null LLE; there is actually a credible positive LLE in one condition of their experiment, a finding that context-noise models cannot explain. Moreover, we show that Dennis et al.’s findings support an item-noise model like the retrieving effectively from memory (REM) at least as well as a context-noise model. The source of the erroneous conclusions is identified as the measurement model Dennis et al. developed. In the end, we conclude that the list-length effect obtained from present experimental designs is insufficient for competitively testing item-noise and context-noise models of recognition.},
	urldate = {2015-10-21},
	journal = {Journal of Memory and Language},
	author = {Annis, Jeffrey and Lenes, Joshua Guy and Westfall, Holly A. and Criss, Amy H. and Malmberg, Kenneth J.},
	month = nov,
	year = {2015},
	keywords = {Memory models, Bayesian, BCDMEM, List-length effect, Recognition memory, REM},
	pages = {27--41},
	file = {Annis et al_2015_The list-length effect does not discriminate between models of recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Annis et al_2015_The list-length effect does not discriminate between models of recognition.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/U9N9C6PK/S0749596X15000753.html:text/html}
}

@article{pack-seeing-2015,
	title = {Seeing and {Feeling} {Motion}: {Canonical} {Computations} in {Vision} and {Touch}},
	volume = {13},
	shorttitle = {Seeing and {Feeling} {Motion}},
	url = {http://dx.doi.org/10.1371/journal.pbio.1002271},
	doi = {10.1371/journal.pbio.1002271},
	abstract = {A close look at the cortical areas that support vision and touch suggests that the brain uses similar computational strategies to handle different kinds of sensory inputs.},
	number = {9},
	urldate = {2015-10-21},
	journal = {PLoS Biol},
	author = {Pack, Christopher C. and Bensmaia, Sliman J.},
	month = sep,
	year = {2015},
	pages = {e1002271},
	file = {Pack_Bensmaia_2015_Seeing and Feeling Motion.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Pack_Bensmaia_2015_Seeing and Feeling Motion.pdf:application/pdf}
}

@article{fengler-brain-nodate,
	title = {Brain structural correlates of complex sentence comprehension in children},
	issn = {1878-9293},
	url = {http://www.sciencedirect.com/science/article/pii/S1878929315000900},
	doi = {10.1016/j.dcn.2015.09.004},
	abstract = {Prior structural imaging studies found initial evidence for the link between structural gray matter changes and the development of language performance in children. However, previous studies generally only focused on sentence comprehension. Therefore, little is known about the relationship between structural properties of brain regions relevant to sentence processing and more specific cognitive abilities underlying complex sentence comprehension. In this study, whole-brain magnetic resonance images from 59 children between 5 and 8 years were assessed. Scores on a standardized sentence comprehension test determined grammatical proficiency of our participants. A confirmatory factory analysis corroborated a grammar-relevant and a verbal working memory-relevant factor underlying the measured performance. Voxel-based morphometry of gray matter revealed that while children's ability to assign thematic roles is positively correlated with gray matter probability (GMP) in the left inferior temporal gyrus and the left inferior frontal gyrus, verbal working memory-related performance is positively correlated with GMP in the left parietal operculum extending into the posterior superior temporal gyrus. Since these areas are known to be differentially engaged in adults’ complex sentence processing, our data suggest a specific correspondence between children's GMP in language-relevant brain regions and differential cognitive abilities that guide their sentence comprehension.},
	urldate = {2015-10-21},
	journal = {Developmental Cognitive Neuroscience},
	author = {Fengler, Anja and Meyer, Lars and Friederici, Angela D.},
	keywords = {Sentence comprehension, Brain development, Language-relevant brain areas, VBM, Verbal working memory},
	file = {Fengler et al_Brain structural correlates of complex sentence comprehension in children.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fengler et al_Brain structural correlates of complex sentence comprehension in children.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/SUGKGNVE/S1878929315000900.html:text/html}
}

@article{shamir-stochastic-2015,
	title = {A {Stochastic} {PCA} and {SVD} {Algorithm} with an {Exponential} {Convergence} {Rate}},
	journal = {Journal of Machine Learning Research},
	author = {Shamir, Ohad},
	year = {2015},
	file = {shamir15:/home/user/Zotero/storage/92VAN35M/shamir15.pdf:application/pdf}
}

@article{sener-unsupervised-nodate,
	title = {Unsupervised {Semantic} {Parsing} of {Video} {Collections}},
	author = {Sener, Ozan},
	file = {1506.08438v1:/home/user/Zotero/storage/J4G3REMZ/1506.08438v1.pdf:application/pdf}
}

@book{seeger-gaussian-2004,
	title = {Gaussian processes for machine learning.},
	volume = {14},
	isbn = {0-262-18253-X},
	abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
	author = {Seeger, Matthias},
	year = {2004},
	file = {Seeger - 2004 - Gaussian processes for machine learning:/home/user/Zotero/storage/6IVBDHDF/Seeger - 2004 - Gaussian processes for machine learning.pdf:application/pdf}
}

@article{schober-probabilistic-2014,
	title = {Probabilistic {ODE} {Solvers} with {Runge}-{Kutta} {Means}},
	url = {http://arxiv.org/abs/1406.2582},
	abstract = {Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state-of-the-art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.},
	author = {Schober, Michael and Duvenaud, David and Hennig, Philipp},
	year = {2014},
	pages = {18--18},
	file = {Schober, Duvenaud, Hennig - 2014 - Probabilistic ODE Solvers with Runge-Kutta Means:/home/user/Zotero/storage/BK89SMKK/Schober, Duvenaud, Hennig - 2014 - Probabilistic ODE Solvers with Runge-Kutta Means.pdf:application/pdf}
}

@article{schaul-universal-nodate,
	title = {Universal {Value} {Function} {Approximators}},
	author = {Schaul, Tom and Com, Davidsilver Google},
	file = {schaul15:/home/user/Zotero/storage/9K4TF3GA/schaul15.pdf:application/pdf}
}

@article{schoelkopf-causal-2012,
	title = {On {Causal} and {Anticausal} {Learning}},
	issn = {9781450312851},
	url = {http://arxiv.org/abs/1206.6471\nhttp://www.arxiv.org/pdf/1206.6471.pdf},
	abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
	journal = {arXiv:1206.6471},
	author = {Schoelkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
	year = {2012},
	file = {Schoelkopf et al. - 2012 - On Causal and Anticausal Learning:/home/user/Zotero/storage/K5C3CXDC/Schoelkopf et al. - 2012 - On Causal and Anticausal Learning.pdf:application/pdf}
}

@article{schaal-robotics-nodate,
	title = {Robotics {Grand} {Challenge} \# {I} : {The} {Human} {Brain} {How} does the brain learn and control complex motor skills ?},
	author = {Schaal, Stefan},
	file = {schaal-I:/home/user/Zotero/storage/6ZQJ94PD/schaal-I.pdf:application/pdf}
}

@article{schaal-robotics-nodate-1,
	title = {Robotics {Where} {Did} {We} {Stop} ...},
	author = {Schaal, Stefan},
	pages = {1--61},
	file = {schaal-II:/home/user/Zotero/storage/FUNTNISW/schaal-II.pdf:application/pdf}
}

@article{sch-causality-2015,
	title = {Causality},
	number = {July},
	author = {Sch, Bernhard and Peters, Jonas},
	year = {2015},
	file = {peters:/home/user/Zotero/storage/XNEQAPZ9/peters.pdf:application/pdf}
}

@article{sch-computing-2015,
	title = {Computing {Functions} of {Random} {Variables} via {Reproducing} {Kernel} {Hilbert} {Space} {Representations}},
	author = {Sch, Bernhard},
	year = {2015},
	pages = {1--20},
	file = {1501.06794v1:/home/user/Zotero/storage/DD7UMXIX/1501.06794v1.pdf:application/pdf}
}

@article{salimans-markov-2015,
	title = {Markov {Chain} {Monte} {Carlo} and {Variational} {Inference} : {Bridging} the {Gap}},
	journal = {International Conference on Machine Learning},
	author = {Salimans, Tim and Kingma, D P and Nl, M Welling U V a},
	year = {2015},
	file = {salimans15:/home/user/Zotero/storage/B4QXEPTB/salimans15.pdf:application/pdf}
}

@article{salakhutdinov-efficient-2012,
	title = {An {Efficient} {Learning} {Procedure} for {Deep} {Boltzmann} {Machines}},
	volume = {24},
	doi = {10.1162/NECO\_a\_00311},
	abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.},
	journal = {Neural Computation},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	year = {2012},
	pages = {1967--2006},
	file = {efficientDBM:/home/user/Zotero/storage/6322MGDS/efficientDBM.pdf:application/pdf}
}

@article{sachan-active-2015,
	title = {An {Active} {Learning} {Approach} to {Coreference} {Resolution}},
	number = {Ijcai},
	author = {Sachan, Mrinmaya and Hovy, Eduard and Xing, Eric P},
	year = {2015},
	keywords = {Technical Papers — Natural Language Processing},
	pages = {1312--1318},
	file = {IJCAI15-189:/home/user/Zotero/storage/BP5MPJZS/IJCAI15-189.pdf:application/pdf}
}

@book{russel-artificial-2010,
	edition = {2nd},
	title = {Artificial {Intelligence}. {A} {Modern} {Approach}},
	publisher = {Pearson},
	author = {Russel, Stuart and Norvig, Peter},
	year = {2010},
	keywords = {AI}
}

@article{rupp-machine-2015,
	title = {Machine {Learning} for {Quantum} {Mechanical} {Properties} of {Atoms} in {Molecules}},
	author = {Rupp, Matthias and Ramakrishnan, Raghunathan},
	year = {2015},
	pages = {1--5},
	file = {Rupp, Ramakrishnan - 2015 - Machine Learning for Quantum Mechanical Properties of Atoms in Molecules:/home/user/Zotero/storage/XWFIKV48/Rupp, Ramakrishnan - 2015 - Machine Learning for Quantum Mechanical Properties of Atoms in Molecules.pdf:application/pdf}
}

@article{rojas-carulla-causal-2015,
	title = {A {Causal} {Perspective} on {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1507.05333},
	abstract = {From training data from several related domains (or tasks), methods of domain adaptation try to combine knowledge to improve performance. This paper discusses an approach to domain adaptation which is inspired by a causal interpretation of the multi-task problem. We assume that a covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant with respect to shifts in those predictors (covariates). We propose to learn the corresponding conditional expectation in the training domains and use it for estimation in the target domain. We further introduce a method which allows for automatic inference of the above subset in regression and classification. We study the performance of this approach in an adversarial setting, in the case where no additional examples are available in the test domain. If a labeled sample is available, we provide a method for using both the transferred invariant conditional and task specific information. We present results on synthetic data sets and a sentiment analysis problem.},
	author = {Rojas-Carulla, Mateo and Schölkopf, Bernhard and Turner, Richard and Peters, Jonas},
	year = {2015},
	pages = {1--14},
	file = {1507.05333v1:/home/user/Zotero/storage/TJVHRKW4/1507.05333v1.pdf:application/pdf}
}

@article{roux-continuous-2006,
	title = {Continuous {Neural} {Networks}},
	abstract = {This article extends neural networks to the case of an uncountable number of hidden units, in several ways. In the first approach proposed, a finite parametrization is possible, allowing gradient-based learning. While having the same number of parameters as an ordinary neural network, its internal structure suggests that it can represent some smooth functions much more compactly. Under mild assumptions, we also find better error bounds than with ordinary neural networks. Furthermore, this parametrization may help reducing the problem of saturation of the neurons. In a second approach, the input-to-hidden weights arefully non-parametric, yielding a kernel machine for which we demonstrate a simple kernel formula. Interestingly, the resulting kernel machine can be made hyperparameter-free and still generalizes in spite of an absence of explicit regularization.},
	number = {1},
	journal = {Solutions},
	author = {Roux, Nicolas Le and Bengio, Yoshua},
	year = {2006},
	pages = {1--16},
	file = {Roux, Bengio - 2006 - Continuous Neural Networks:/home/user/Zotero/storage/B3ESKIQD/Roux, Bengio - 2006 - Continuous Neural Networks.pdf:application/pdf}
}

@article{rippel-spectral-2015,
	title = {Spectral {Representations} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.03767},
	number = {2013},
	journal = {arXiv preprint arXiv:1506.03767},
	author = {Rippel, O and Snoek, J and Adams, Rp},
	year = {2015},
	pages = {1--10},
	file = {1506.03767v1:/home/user/Zotero/storage/7H4GVPNQ/1506.03767v1.pdf:application/pdf}
}

@article{rindflesch-natural-1996,
	title = {Natural {Language} {Processing}},
	volume = {16},
	issn = {9783662459232},
	doi = {10.1017/S0267190500001446},
	abstract = {ABSTRACT Work in computational linguistics began very soon after the development of the first computers (Booth, Brandwood and Cleave 1958), yet in the intervening four decades there has been a pervasive feeling that progress in computer understanding of natural language has not been commensurate with progress in other computer applications. Recently, a number of prominent researchers in natural language processing met to assess the state of the discipline and discuss future directions (Bates and Weischedel 1993). The consensus of this meeting was that increased attention to large amounts of lexical and domain knowledge was essential for significant progress, and current research efforts in the field reflect this point of view.},
	journal = {Annual Review of Applied Linguistics},
	author = {Rindflesch, Thomas C.},
	year = {1996},
	pages = {70--70},
	file = {Fergus_2:/home/user/Zotero/storage/BAWE7DEH/Fergus_2.pdf:application/pdf}
}

@article{ried-inferring-2014,
	title = {Inferring causal structure: a quantum advantage},
	doi = {10.1038/NPHYS3266},
	abstract = {The problem of using observed correlations to infer causal relations is relevant to a wide variety of scientific disciplines. Yet given correlations between just two classical variables, it is impossible to determine whether they arose from a causal influence of one on the other or a common cause influencing both, unless one can implement a randomized intervention. We here consider the problem of causal inference for quantum variables. We introduce causal tomography, which unifies and generalizes conventional quantum tomography schemes to provide a complete solution to the causal inference problem using a quantum analogue of a randomized trial. We furthermore show that, in contrast to the classical case, observed quantum correlations alone can sometimes provide a solution. We implement a quantum-optical experiment that allows us to control the causal relation between two optical modes, and two measurement schemes -- one with and one without randomization -- that extract this relation from the observed correlations. Our results show that entanglement and coherence, known to be central to quantum information processing, also provide a quantum advantage for causal inference.},
	journal = {arXiv preprint},
	author = {Ried, Katja and Agnew, Megan and Vermeyden, Lydia and Janzing, Dominik and Spekkens, Robert W and Resch, Kevin J},
	year = {2014},
	pages = {1--17},
	file = {1406.5036v1:/home/user/Zotero/storage/TBXNU2V3/1406.5036v1.pdf:application/pdf}
}

@article{richard-socher-and-john-bauer-and-christopher-d.-manning-and-andrew-y.-ng-parsing-2013,
	title = {Parsing {With} {Compositional} {Vector} {Grammars}},
	abstract = {Natural language parsing has typically been done with small sets of discrete categories such as {NP} and {VP}, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar ({CVG)}, which combines {PCFGs} with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The {CVG} improves the {PCFG} of the Stanford Parser by 3.8 \% to obtain an F1 score of 90.4\%. It is fast to train and implemented approximately as an efficient reranker it is about 20 \% faster than the current Stanford factored parser. The {CVG} learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as {PP} attachments. 1},
	journal = {In Proceedings of the {ACL} conference},
	author = {{Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng} and Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
	year = {2013},
	pages = {455--465},
	file = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng et al. - 2013 - Parsing With Compositional Vector Grammars:/home/user/Zotero/storage/F2XMPNXI/Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng et al. - 2013 - Parsing With Compositional Vector Grammars.pdf:application/pdf}
}

@article{reyzin-active-2009,
	title = {Active {Learning} of {Interaction} {Networks}},
	url = {http://www.levreyzin.com/presentations/GATech2010.pdf},
	author = {Reyzin, Lev},
	year = {2009},
	file = {final:/home/user/Zotero/storage/SA38TWAS/final.pdf:application/pdf}
}

@article{reyzin-analyzing-2005,
	title = {Analyzing {Margins} in {Boosting}},
	author = {Reyzin, Lev},
	year = {2005},
	pages = {1--21},
	file = {Reyzin04_siw:/home/user/Zotero/storage/KGBZMNNV/Reyzin04_siw.pdf:application/pdf}
}

@article{redi-like-2015,
	title = {Like {Partying} ? {Your} {Face} {Says} {It} {All} . {Predicting} the {Ambiance} of {Places} with {Profile} {Pictures}},
	author = {Redi, Miriam and Quercia, Daniele and Graham, Lindsay T and Gosling, Samuel D},
	year = {2015},
	file = {1505.07522v1:/home/user/Zotero/storage/ZQBMWIXG/1505.07522v1.pdf:application/pdf}
}

@incollection{raedt-probabilistic-2008,
	series = {Lecture {Notes} in {Artificial} {Intelligence}},
	title = {Probabilistic {Inductive} {Logic} {Programming}},
	booktitle = {Probabilistic {Inductive} {Logic} {Programming}},
	publisher = {Springer},
	author = {Raedt, Luc De and Kersting, Kristian},
	editor = {et al., Luc De Raedt},
	year = {2008},
	keywords = {ILP},
	pages = {1--18}
}

@inproceedings{quinlan-determinate-1991,
	title = {Determinate literals in inductive logic programming},
	isbn = {1-55860-160-0},
	url = {http://portal.acm.org/citation.cfm?id=1631552.1631572},
	booktitle = {Proceedings of the 12th international joint conference on {Artificial} intelligence - {Volume} 2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Quinlan, J R},
	year = {1991},
	keywords = {ILP},
	pages = {746--750}
}

@article{qin-differential-2015,
	title = {Differential {Semantics} of {Intervention} in {Bayesian} {Networks}},
	number = {Ijcai},
	author = {Qin, Biao},
	year = {2015},
	keywords = {Technical Papers — Graphical Models},
	pages = {710--716},
	file = {IJCAI15-106:/home/user/Zotero/storage/IWMJZHM8/IJCAI15-106.pdf:application/pdf}
}

@article{rasmus-semi-supervised-2015,
	title = {Semi-{Supervised} {Learning} with {Ladder} {Network}},
	author = {Rasmus, Antti and Valpola, Harri and Berglund, Mathias},
	year = {2015},
	pages = {1--19},
	file = {1507.02672v1:/home/user/Zotero/storage/XQGWGIQQ/1507.02672v1.pdf:application/pdf}
}

@article{rajkovi-artists-2015,
	title = {The {Artists} who {Forged} {Themselves} : {Detecting} {Creativity} in {Art}},
	author = {Rajkovi, Milan},
	year = {2015},
	pages = {1--26},
	file = {1506.04356v1:/home/user/Zotero/storage/ZJW3UINE/1506.04356v1.pdf:application/pdf}
}

@article{rahmadi-causality-2015,
	title = {Causality on {Cross}-{Sectional} {Data} : {Stable} {Specification} {Search} in {Constrained} {Structural} {Equation} {Modeling}},
	journal = {arXiv preprint},
	author = {Rahmadi, Ridho and Groot, Perry and Heins, Marianne and Knoop, Hans and Heskes, Tom and Optimistic, The},
	year = {2015},
	pages = {1--14},
	file = {1506.05600v1:/home/user/Zotero/storage/5V2IAPC2/1506.05600v1.pdf:application/pdf}
}

@article{rabinovich-efficient-nodate,
	title = {Efficient {Inference} for {Unsupervised} {Semantic} {Parsing}},
	author = {Rabinovich, Maxim},
	pages = {1--5},
	file = {24:/home/user/Zotero/storage/3ZRXRRR4/24.pdf:application/pdf}
}

@article{qiu-learning-2003,
	title = {Learning {Word} {Representation} {Considering} {Proximity} and {Ambiguity}},
	author = {Qiu, Lin and Cao, Yong and Nie, Zaiqing and Rui, Yong},
	year = {2003},
	keywords = {NLP and Machine Learning},
	pages = {1572--1578},
	file = {Qiu et al. - 2003 - Learning Word Representation Considering Proximity and Ambiguity:/home/user/Zotero/storage/M9D5UV4H/Qiu et al. - 2003 - Learning Word Representation Considering Proximity and Ambiguity.pdf:application/pdf}
}

@article{potts-recursive-2014,
	title = {Recursive {Neural} {Networks} for {Learning} {Logical} {Semantics}},
	author = {Potts, Christopher},
	year = {2014},
	pages = {1--10},
	file = {Potts - 2014 - Recursive Neural Networks for Learning Logical Semantics:/home/user/Zotero/storage/N3M2M54F/Potts - 2014 - Recursive Neural Networks for Learning Logical Semantics.pdf:application/pdf}
}

@article{polukarov-convergence-2015,
	title = {Convergence to {Equilibria} in {Strategic} {Candidacy}},
	number = {Ijcai},
	author = {Polukarov, Maria and Jennings, Nicholas R},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {624--630},
	file = {IJCAI15-094:/home/user/Zotero/storage/ZH63CUDG/IJCAI15-094.pdf:application/pdf}
}

@article{peters-identifying-2009,
	title = {Identifying {Cause} and {Effect} on {Discrete} {Data} using {Additive} {Noise} {Models}},
	number = {2008},
	journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {Peters, Jonas and Janzing, Dominik and Sch, Bernhard},
	year = {2009},
	file = {Peters, Janzing, Sch - 2009 - Identifying Cause and Effect on Discrete Data using Additive Noise Models:/home/user/Zotero/storage/S969KIRF/Peters, Janzing, Sch - 2009 - Identifying Cause and Effect on Discrete Data using Additive Noise Models.pdf:application/pdf}
}

@phdthesis{peters-restricted-2012,
	title = {Restricted {Structural} {Equation} {Models} for {Causal} {Inference}},
	author = {Peters, Jonas},
	year = {2012},
	file = {eth-6445-02:/home/user/Zotero/storage/SXINMTV9/eth-6445-02.pdf:application/pdf}
}

@article{peters-diploma-2008,
	title = {Diploma {Thesis} in {Mathematics} {Asymmetries} of {Time} {Series} under {Inverting} their {Direction}},
	author = {Peters, Jonas},
	year = {2008},
	file = {diploma_[0]:/home/user/Zotero/storage/BCD8U5C6/diploma_[0].pdf:application/pdf}
}

@techreport{peters-causality-2015,
	title = {causality},
	author = {Peters, Jonas},
	year = {2015},
	file = {scriptChapter1-4:/home/user/Zotero/storage/AXEUS3TM/scriptChapter1-4.pdf:application/pdf}
}

@article{peters-simple-2015,
	title = {Simple {Causes} of {Complexity} in {Hedonic} {Games}},
	number = {Ijcai},
	author = {Peters, Dominik and Elkind, Edith},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {617--623},
	file = {IJCAI15-093:/home/user/Zotero/storage/IEP2AWM2/IJCAI15-093.pdf:application/pdf}
}

@article{pan-robust-nodate,
	title = {Robust {Non}-{Negative} {Dictionary} {Learning}},
	number = {2},
	author = {Pan, Qihe and Kong, Deguang and Ding, Chris and Luo, Bin},
	keywords = {Novel Machine Learning Algorithms},
	pages = {2027--2033},
	file = {Pan et al. - Unknown - Robust Non-Negative Dictionary Learning:/home/user/Zotero/storage/3ZBCU3TE/Pan et al. - Unknown - Robust Non-Negative Dictionary Learning.pdf:application/pdf}
}

@article{orr-learning-2014,
	title = {Learning {Scripts} as {Hidden} {Markov} {Models}},
	issn = {9781577356783},
	journal = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
	author = {Orr, J Walker and Tadepalli, Prasad and Doppa, Janardhan Rao and Fern, Xiaoli and Dietterich, Thomas G},
	year = {2014},
	keywords = {NLP and Machine Learning},
	pages = {1565--1571},
	file = {Orr et al. - 2014 - Learning Scripts as Hidden Markov Models:/home/user/Zotero/storage/7TQSTXV8/Orr et al. - 2014 - Learning Scripts as Hidden Markov Models.pdf:application/pdf}
}

@article{of-chinese-2004,
	title = {chinese overt pronoun resolution},
	volume = {12},
	author = {Of, Ategories and De, Ategorias},
	year = {2004},
	pages = {3--4},
	file = {Of, De - 2004 - chinese overt pronoun resolution:/home/user/Zotero/storage/PTHPCGMF/Of, De - 2004 - chinese overt pronoun resolution.pdf:application/pdf}
}

@article{palatucci-zero-shot-2009,
	title = {Zero-{Shot} {Learning} with {Semantic} {Output} {Codes}},
	issn = {9781615679119},
	abstract = {We consider the problem of zero-shot learning, where the goal is to\nlearn a clas- sifier f : X �?Y that must predict novel values of\nY that were omitted from the training set. To achieve this, we define\nthe notion of a semantic output code classifier (SOC) which utilizes\na knowledge base of semantic properties of Y to extrapolate to novel\nclasses. We provide a formalism for this type of classifier and study\nits theoretical properties in a PAC framework, showing conditions\nun- der which the classifier can accurately predict novel classes.\nAs a case study, we build a SOC classifier for a neural decoding\ntask and show that it can often predict words that people are thinking\nabout from functional magnetic resonance images (fMRI) of their neural\nactivity, even without training examples for those words.},
	journal = {Neural Information Processing Systems},
	author = {Palatucci, Mark and Hinton, Geoffrey and Pomerleau, Dean and Mitchell, Tom M},
	year = {2009},
	pages = {1--9},
	file = {Palatucci et al. - 2009 - Zero-Shot Learning with Semantic Output Codes:/home/user/Zotero/storage/C63SCFK8/Palatucci et al. - 2009 - Zero-Shot Learning with Semantic Output Codes.pdf:application/pdf}
}

@inproceedings{obraztsova-strategic-2015,
	title = {Strategic {Candidacy} {Games} with {Lazy} {Candidates}},
	booktitle = {{IJCAI}},
	author = {Obraztsova, Svetlana and Aviv, Tel and Elkind, Edith},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {610--616},
	file = {IJCAI15-092:/home/user/Zotero/storage/3VDUHB6W/IJCAI15-092.pdf:application/pdf}
}

@article{nock-rademacher-2015,
	title = {Rademacher {Observations} , {Private} {Data} , and {Boosting}},
	volume = {37},
	author = {Nock, Richard and Patrini, Giorgio and Com, Nicta},
	year = {2015},
	file = {nock15:/home/user/Zotero/storage/32NH9ZNB/nock15.pdf:application/pdf}
}

@book{nienhuys-cheng-foundations-1997,
	series = {Lecture {Notes} in {Artificial} {Intelligence}},
	title = {Foundations of {Inductive} {Logic} {Programming}},
	publisher = {Springer},
	author = {Nienhuys-Cheng, S H and de Wolf., R},
	year = {1997},
	keywords = {ILP}
}

@article{neubig-non-parametric-2011,
	title = {Non-parametric {Bayesian} {Statistics}},
	author = {Neubig, Graham},
	year = {2011},
	file = {broderick_mlss2015_part1:/home/user/Zotero/storage/XFE2HZ9J/broderick_mlss2015_part1.pdf:application/pdf}
}

@article{naim-unsupervised-2014,
	title = {Unsupervised {Alignment} of {Natural} {Language} {Instructions} with {Video} {Segments}},
	issn = {9781577356783},
	url = {http://www.cs.rochester.edu/~inaim/aaai\_2014/naim\_et\_al\_aaai14.pdf},
	journal = {… of the Joint Workshop on Exploiting …},
	author = {Naim, I and Song, Yc and Liu, Q and Kautz, H and Luo, J and Gildea, D},
	year = {2014},
	keywords = {NLP and Machine Learning},
	pages = {1558--1564},
	file = {Naim et al. - 2014 - Unsupervised Alignment of Natural Language Instructions with Video Segments:/home/user/Zotero/storage/K3F5T79T/Naim et al. - 2014 - Unsupervised Alignment of Natural Language Instructions with Video Segments.pdf:application/pdf}
}

@inproceedings{nassif-inductive-2010,
	series = {{ILP}'09},
	title = {An inductive logic programming approach to validate {Hexose} binding biochemical knowledge},
	isbn = {3-642-13839-X 978-3-642-13839-3},
	url = {http://portal.acm.org/citation.cfm?id=1893538.1893552},
	booktitle = {Proceedings of the 19th international conference on {Inductive} logic programming},
	publisher = {Springer-Verlag},
	author = {Nassif, Houssam and Al-Ali, Hassan and Khuri, Sawsan and Keirouz, Walid and Page, David},
	year = {2010},
	keywords = {ILP},
	pages = {149--165}
}

@incollection{muggleton-efficient-1992,
	title = {Efficient induction in logic programs},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Academic Press},
	author = {Muggleton, Stephen and Feng, Cao},
	editor = {Muggleton, S},
	year = {1992},
	keywords = {ILP},
	pages = {281--298}
}

@article{muelling-learning-2014,
	title = {Learning strategies in table tennis using inverse reinforcement learning},
	doi = {10.1007/s00422-014-0599-1},
	abstract = {Learning a complex task such as table tennis is a challenging problem for both robots and humans. Even after acquiring the necessary motor skills, a strategy is needed to choose where and how to return the ball to the opponent's court in order to win the game. The data-driven identification of basic strategies in interactive tasks, such as table tennis, is a largely unexplored problem. In this paper, we suggest a computational model for representing and inferring strategies, based on a Markov decision problem, where the reward function models the goal of the task as well as the strategic information. We show how this reward function can be discovered from demonstrations of table tennis matches using model-free inverse reinforcement learning. The resulting framework allows to identify basic elements on which the selection of striking movements is based. We tested our approach on data collected from players with different playing styles and under different playing conditions. The estimated reward function was able to capture expert-specific strategic information that sufficed to distinguish the expert among players with different skill levels as well as different playing styles.},
	number = {1984},
	journal = {Biological Cybernetics},
	author = {Muelling, Katharina and Boularias, Abdeslam and Mohler, Betty and Schölkopf, Bernhard and Peters, Jan},
	year = {2014},
	keywords = {Computational models of decision processes, Inverse reinforcement learning, Table tennis},
	file = {Muelling_BICY_2014:/home/user/Zotero/storage/6U6MNRQS/Muelling_BICY_2014.pdf:application/pdf}
}

@article{murray-introduction-2008,
	title = {Introduction to {Gaussian} {Processes} {The} problem},
	number = {July},
	author = {Murray, Iain},
	year = {2008},
	file = {lawrence:/home/user/Zotero/storage/RJ2QB74R/lawrence.pdf:application/pdf}
}

@inproceedings{muggleton-progolem:-2010,
	series = {{ILP}'09},
	title = {{ProGolem}: a system based on relative minimal generalisation},
	isbn = {3-642-13839-X 978-3-642-13839-3},
	url = {http://portal.acm.org/citation.cfm?id=1893538.1893551},
	booktitle = {Proceedings of the 19th international conference on {Inductive} logic programming},
	publisher = {Springer-Verlag},
	author = {Muggleton, Stephen and Santos, José and Tamaddoni-Nezhad, Alireza},
	year = {2010},
	keywords = {ILP},
	pages = {131--148}
}

@article{monner-generalized-2012,
	title = {A generalized {LSTM}-like training algorithm for second-order recurrent neural networks},
	volume = {25},
	issn = {18792782},
	doi = {10.1016/j.neunet.2011.07.003},
	abstract = {The Long Short Term Memory (LSTM) is a second-order recurrent neural network architecture that excels at storing sequential short-term memories and retrieving them many time-steps later. LSTM's original training algorithm provides the important properties of spatial and temporal locality, which are missing from other training approaches, at the cost of limiting its applicability to a small set of network architectures. Here we introduce the Generalized Long Short-Term Memory(LSTM-g) training algorithm, which provides LSTM-like locality while being applicable without modification to a much wider range of second-order network architectures. With LSTM-g, all units have an identical set of operating instructions for both activation and learning, subject only to the configuration of their local environment in the network; this is in contrast to the original LSTM training algorithm, where each type of unit has its own activation and training instructions. When applied to LSTM architectures with peephole connections, LSTM-g takes advantage of an additional source of back-propagated error which can enable better performance than the original algorithm. Enabled by the broad architectural applicability of LSTM-g, we demonstrate that training recurrent networks engineered for specific tasks can produce better results than single-layer networks. We conclude that LSTM-g has the potential to both improve the performance and broaden the applicability of spatially and temporally local gradient-based training algorithms for recurrent neural networks. © 2011 Elsevier Ltd.},
	number = {301},
	journal = {Neural Networks},
	author = {Monner, Derek and Reggia, James a.},
	year = {2012},
	keywords = {Gradient-based training, Long Short Term Memory (LSTM), recurrent neural network, Sequential retrieval, Temporal sequence processing},
	pages = {70--83},
	file = {nn2012:/home/user/Zotero/storage/973GKX4G/nn2012.pdf:application/pdf}
}

@inproceedings{mnich-when-2015,
	title = {When {Does} {Schwartz} {Conjecture} {Hold} ?},
	booktitle = {{IJCAI}},
	author = {Mnich, Matthias},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {603--609},
	file = {IJCAI15-091:/home/user/Zotero/storage/DAGW39WK/IJCAI15-091.pdf:application/pdf}
}

@inproceedings{miltersen-dictatorship-2015,
	title = {A {Dictatorship} {Theorem} for {Cake} {Cutting} ∗},
	booktitle = {{IJCAI}},
	author = {Miltersen, Peter Bro},
	year = {2015},
	pages = {482--488},
	file = {IJCAI15-074:/home/user/Zotero/storage/TJIH3KUX/IJCAI15-074.pdf:application/pdf}
}

@article{may-tensor-2015,
	title = {Tensor {Factorization} via {Matrix} {Factorization}},
	volume = {38},
	author = {May, L G},
	year = {2015},
	file = {1501.07320v2:/home/user/Zotero/storage/GQJGMNJ4/1501.07320v2.pdf:application/pdf}
}

@article{maximov-tight-2015,
	title = {Tight {Risk} {Bounds} for {Multi}-{Class} {Margin} {Classifiers}},
	url = {http://arxiv.org/abs/1507.03040},
	abstract = {We consider a problem of risk estimation for large-margin multi-class classifiers. We propose a novel risk bound for the multi-class classification problem. The bound involves the marginal distribution of the classifier and the Rademacher complexity of the hypothesis class. We prove that our bound is tight in the number of classes. Finally, we compare our bound with the related ones and provide a simplified version of the bound for the multi-class classification with kernel based hypotheses.},
	author = {Maximov, Yury and Reshetova, Daria},
	year = {2015},
	pages = {1--11},
	file = {1507.03040v1:/home/user/Zotero/storage/4CIKH3FH/1507.03040v1.pdf:application/pdf}
}

@article{martins-turning-2013,
	title = {Turning on the {Turbo}: {Fast} {Third}-{Order} {Non}-{Projective} {Turbo} {Parsers}},
	url = {http://www.aclweb.org/anthology/P13-2109},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Martins, Andre and Almeida, Miguel and Smith, Noah a},
	year = {2013},
	pages = {617--622},
	file = {Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers:/home/user/Zotero/storage/TDJ33RBS/Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers.pdf:application/pdf}
}

@article{mallat-deep-2013,
	title = {Deep {Learning} by {Scattering}},
	url = {http://arxiv.org/abs/1306.5532},
	abstract = {We introduce general scattering transforms as mathematical models of deep neural networks with l2 pooling. Scattering networks iteratively apply complex valued unitary operators, and the pooling is performed by a complex modulus. An expected scattering defines a contractive representation of a high-dimensional probability distribution, which preserves its mean-square norm. We show that unsupervised learning can be casted as an optimization of the space contraction to preserve the volume occupied by unlabeled examples, at each layer of the network. Supervised learning and classification are performed with an averaged scattering, which provides scattering estimations for multiple classes.},
	journal = {arXiv preprint arXiv:1306.5532},
	author = {Mallat, Stephane},
	year = {2013},
	pages = {1--10},
	file = {1306.5532v2:/home/user/Zotero/storage/MPRHMWI3/1306.5532v2.pdf:application/pdf}
}

@article{meek-toward-nodate,
	title = {Toward {Learning} {Graphical} and {Causal} {Process} {Models}},
	author = {Meek, Christopher},
	file = {uai2014ci_paper8:/home/user/Zotero/storage/PWX3KWJH/uai2014ci_paper8.pdf:application/pdf}
}

@article{maclaurin-gradient-based-2015,
	title = {Gradient-based hyperparameter optimization through reversible learning},
	author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
	year = {2015},
	file = {Maclaurin, Duvenaud, Adams - 2015 - Gradient-based hyperparameter optimization through reversible learning:/home/user/Zotero/storage/RANG76W6/Maclaurin, Duvenaud, Adams - 2015 - Gradient-based hyperparameter optimization through reversible learning.pdf:application/pdf}
}

@book{mackay-information-2003,
	title = {Information {Theory} , {Inference} , and {Learning} {Algorithms}},
	isbn = {0-521-64298-1},
	author = {Mackay, David J C},
	year = {2003},
	file = {Mackay - 2003 - Information Theory , Inference , and Learning Algorithms:/home/user/Zotero/storage/M48UMWV5/Mackay - 2003 - Information Theory , Inference , and Learning Algorithms.pdf:application/pdf}
}

@article{maaten-visualizing-2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1532-4435},
	doi = {10.1007/s10479-011-0841-3},
	abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
	year = {2008},
	keywords = {Dimensionality reduction, embedding algorithms, manifold learning, multidimensional scaling, visualization},
	pages = {2579--2605},
	file = {tsne-IMPORT:/home/user/Zotero/storage/GCG25XAW/tsne-IMPORT.pdf:application/pdf}
}

@article{ma-multimodal-nodate,
	title = {Multimodal {Convolutional} {Neural} {Networks} for {Matching} {Image} and {Sentence}},
	author = {Ma, Lin},
	file = {1504.06063v3:/home/user/Zotero/storage/DWRTZ58A/1504.06063v3.pdf:application/pdf}
}

@article{lukosevicius-reservoir-2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	issn = {1574-0137},
	doi = {10.1016/j.cosrev.2009.03.005},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current "brand-names" of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed "map" of it. © 2009 Elsevier Inc. All rights reserved.},
	journal = {Computer Science Review},
	author = {Lukoševičius, Mantas and Jaeger, Herbert},
	year = {2009},
	keywords = {connectionist, machine learning, recurrent neural network, computational intelligence, echo state network, liquid state machine},
	pages = {127--149},
	file = {2261_LukoseviciusJaeger09:/home/user/Zotero/storage/VE74ZUEJ/2261_LukoseviciusJaeger09.pdf:application/pdf}
}

@article{lu-how-2015,
	title = {How to {Scale} {Up} {Kernel} {Methods} to {Be} {As} {Good} {As} {Deep} {Neural} {Nets}},
	abstract = {The computational complexity of kernel methods has often been a major barrier for applying them to large-scale learning problems. We argue that this barrier can be effectively overcome. In particular, we develop methods to scale up kernel models to successfully tackle large-scale learning problems that are so far only approachable by deep learning architectures. Based on the seminal work by [38] on approximating kernel functions with features derived from random projections, we advance the state-of-the-art by proposing methods that can efficiently train models with hundreds of millions of parameters, and learn optimal representations from multiple kernels. We conduct extensive empirical studies on problems from image recognition and automatic speech recognition, and show that the performance of our kernel models matches that of well-engineered deep neural nets (DNNs). To the best of our knowledge, this is the first time that a direct comparison between these two methods on large-scale problems is reported. Our kernel methods have several appealing properties: training with convex optimization, cost for training a single model comparable to DNNs, and significantly reduced total cost due to fewer hyperparameters to tune for model selection. Our contrastive study between these two very different but equally competitive models sheds light on fundamental questions such as how to learn good representations.},
	author = {Lu, Zhiyun and May, Avner and Liu, Kuan and Garakani, Alireza Bagheri and Guo, Dong and Bellet, Aurélien and Fan, Linxi and Collins, Michael and Kingsbury, Brian and Picheny, Michael and Sha, Fei},
	year = {2015},
	pages = {1--24},
	file = {1411.4000v2:/home/user/Zotero/storage/UTRNPVR6/1411.4000v2.pdf:application/pdf}
}

@inproceedings{lou-equilibrium-2015,
	title = {Equilibrium {Analysis} of {Multi}-{Defender} {Security} {Games}},
	booktitle = {{IJCAI}},
	author = {Lou, Jian and Vorobeychik, Yevgeniy},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {596--602},
	file = {IJCAI15-090:/home/user/Zotero/storage/GW9AG3R6/IJCAI15-090.pdf:application/pdf}
}

@article{lopez-paz-towards-2015,
	title = {Towards a {Learning} {Theory} of {Cause}-{Effect} {Inference}},
	journal = {Journal of Machine Learning Research},
	author = {Lopez-paz, David and Sch, Bernhard},
	year = {2015},
	file = {lopez-paz15:/home/user/Zotero/storage/BVVEGBFS/lopez-paz15.pdf:application/pdf}
}

@article{liang-landmarking-2015,
	title = {Landmarking {Manifolds} with {Gaussian} {Processes}},
	volume = {37},
	journal = {Journal of Machine Learning Research},
	author = {Liang, Dawen and Paisley, John},
	year = {2015},
	file = {liang15:/home/user/Zotero/storage/T4GR5GQ8/liang15.pdf:application/pdf}
}

@article{li-equivalence-2015,
	title = {On the {Equivalence} of {Factorized} {Information} {Criterion} {Regularization} and the {Chinese} {Restaurant} {Process} {Prior}},
	author = {Li, Shaohua},
	year = {2015},
	pages = {1--3},
	file = {1506.09068v1:/home/user/Zotero/storage/9CWUQD9D/1506.09068v1.pdf:application/pdf}
}

@article{lu-combining-2015,
	title = {Combining {Eye} {Movements} and {EEG} to {Enhance} {Emotion} {Recognition}},
	number = {Ijcai},
	author = {Lu, Yifei and Zheng, Wei-long and Li, Binbin and Lu, Bao-liang},
	year = {2015},
	keywords = {Technical Papers — Multidisciplinary Topics and Ap},
	pages = {1170--1176},
	file = {IJCAI15-169:/home/user/Zotero/storage/TNMXKEGZ/IJCAI15-169.pdf:application/pdf}
}

@inproceedings{libkin-how-2015,
	title = {How to {Define} {Certain} {Answers} ∗},
	booktitle = {{IJCAI} 2015},
	author = {Libkin, Leonid},
	year = {2015},
	keywords = {Invited Sister Conference Presentations Track},
	pages = {4282--4288},
	file = {IJCAI15-609:/home/user/Zotero/storage/FDVBBEQX/IJCAI15-609.pdf:application/pdf}
}

@article{liang-codebook-based-2014,
	title = {Codebook-{Based} {Scalable} {Music} {Tagging} {With} {Poisson} {Matrix} {Factorization}},
	url = {http://www.ee.columbia.edu/~dliang/publications/LiangPE14-codebook.pdf},
	number = {Ismir},
	journal = {15th International Society for Music Infor- mation Retrieval Conference, 2014},
	author = {Liang, Dawen and Paisley, John and Ellis, Dpw},
	year = {2014},
	pages = {2--7},
	file = {Liang, Paisley, Ellis - 2014 - Codebook-Based Scalable Music Tagging With Poisson Matrix Factorization:/home/user/Zotero/storage/SJ6WF88P/Liang, Paisley, Ellis - 2014 - Codebook-Based Scalable Music Tagging With Poisson Matrix Factorization.pdf:application/pdf;T030_363_Paper:/home/user/Zotero/storage/2VE9D6C6/T030_363_Paper.pdf:application/pdf}
}

@inproceedings{lehmann-ideal-2010,
	title = {Ideal {Downward} {Refinement} in the {EL} {Description} {Logic}},
	booktitle = {{ILP}'09 {Proceedings} of the 19th international conference on {Inductive} logic programming},
	publisher = {Springer},
	author = {Lehmann, Jens and Haase, Christoph},
	editor = {Raedt, Luc De},
	year = {2010},
	keywords = {ILP},
	pages = {73--87}
}

@article{li-truthful-2015,
	title = {Truthful {Cake} {Cutting} {Mechanisms} with {Externalities} : {Do} {Not} {Make} {Them} {Care} for {Others} {Too} {Much} !},
	number = {Ijcai},
	author = {Li, Minming},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {589--595},
	file = {IJCAI15-089:/home/user/Zotero/storage/HWITUETF/IJCAI15-089.pdf:application/pdf}
}

@article{leite-introduction-2010,
	title = {Introduction to geometric control theory - controllability and {Lie} bracket -},
	author = {Leite, Fátima Silva},
	year = {2010},
	pages = {1--42},
	file = {Fatima_Leite:/home/user/Zotero/storage/2JS4J5QZ/Fatima_Leite.pdf:application/pdf;Leite - 2010 - Introduction to geometric control theory - controllability and Lie bracket -:/home/user/Zotero/storage/CNEK27GS/Leite - 2010 - Introduction to geometric control theory - controllability and Lie bracket -.pdf:application/pdf}
}

@article{lee-convolutional-2009,
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	volume = {2008},
	issn = {9781605585161},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
	doi = {10.1145/1553374.1553453},
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	journal = {Proceedings of the 26th Annual International Conference on Machine Learning ICML 09},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
	year = {2009},
	pages = {1--8},
	file = {icml09-cdbn:/home/user/Zotero/storage/W79PWZVC/icml09-cdbn.pdf:application/pdf}
}

@article{lee-learning-1999,
	title = {Learning the parts of objects by non-negative matrix factorization.},
	volume = {401},
	issn = {0028-0836 (Print)\r0028-0836 (Linking)},
	doi = {10.1038/44565},
	abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	number = {October 1999},
	journal = {Nature},
	author = {Lee, D D and Seung, H S},
	year = {1999},
	pages = {788--791},
	file = {Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization:/home/user/Zotero/storage/XJJSFJ3I/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:application/pdf;nmf_nature:/home/user/Zotero/storage/H432PJHP/nmf_nature.pdf:application/pdf}
}

@article{le-unsupervised-2015,
	title = {Unsupervised {Riemannian} {Metric} {Learning} for {Histograms} {Using} {Aitchison} {Transformations}},
	volume = {37},
	abstract = {Many applications in machine learning handle bags of features or histograms rather than sim-ple vectors. In that context, defining a proper ge-ometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon (2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multino-mial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison (1982) transfor-mations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimen-tal evidence that the metric obtained under our proposal outperforms alternative approaches.},
	author = {Le, Tam and Cuturi, Marco},
	year = {2015},
	file = {le15:/home/user/Zotero/storage/BD7UHEZ4/le15.pdf:application/pdf}
}

@article{lazaridou-unveiling-nodate-1,
	title = {Unveiling the {Dreams} of {Word} {Embeddings}: {Towards} {Language}-{Driven} {Image} {Generation}},
	abstract = {We introduce language-driven image generation, the task of generating an im-age visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general vi-sual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.},
	author = {Lazaridou, Angeliki and Nguyen, Dat Tien and Bernardi, Raffaella and Baroni, Marco},
	pages = {1--11},
	file = {1506.03500v1:/home/user/Zotero/storage/3NST2F8I/1506.03500v1.pdf:application/pdf}
}

@article{laskey-quantum-2007,
	title = {Quantum {Causal} {Networks}},
	issn = {9781577353171},
	url = {http://arxiv.org/abs/0710.1200},
	abstract = {Intervention theories of causality define a relationship as causal if appropriately specified interventions to manipulate a putative cause tend to produce changes in the putative effect. Interventionist causal theories are commonly formalized by using directed graphs to represent causal relationships, local probability models to quantify the relationship between cause and effect, and a special kind of conditioning operator to represent the effects of interventions. Such a formal model represents a family of joint probability distributions, one for each allowable intervention policy. This paper interprets the von Neumann formalization of quantum theory as an interventionist theory of causality, describes its relationship to interventionist theories popular in the artificial intelligence literature, and presents a new family of graphical models that extends causal Bayesian networks to quantum systems.},
	author = {Laskey, Kathryn B.},
	year = {2007},
	pages = {10--10},
	file = {0710.1200v1:/home/user/Zotero/storage/GKUUJC5D/0710.1200v1.pdf:application/pdf}
}

@article{lane-neural-1995,
	title = {Neural networks.},
	volume = {346},
	issn = {0140-6736 (Print)\r0140-6736 (Linking)},
	number = {8988},
	journal = {Lancet},
	author = {Lane, V and Littlejohns, P},
	year = {1995},
	pages = {1501--1501},
	file = {Fergus_1:/home/user/Zotero/storage/Z93PTI4P/Fergus_1.pdf:application/pdf}
}

@article{laffierriere-differential-1998,
	title = {A {Differential} {Geometric} {Approach} to {Motion} {Planning}},
	author = {Laffierriere, G and Sussman, H J},
	year = {1998},
	pages = {1--17},
	file = {1503.01436v5:/home/user/Zotero/storage/NS4M2IP7/1503.01436v5.pdf:application/pdf}
}

@article{kurokawa-impartial-2014,
	title = {Impartial {Peer} {Review}},
	volume = {X},
	number = {X},
	author = {Kurokawa, D. and Lev, O. and Morgenstern, J. and Procaccia, a. D.},
	year = {2014},
	keywords = {Technical Papers — Game Theory},
	pages = {22--22},
	file = {IJCAI15-088:/home/user/Zotero/storage/6EAWMK2R/IJCAI15-088.pdf:application/pdf}
}

@article{lai-how-2015-1,
	title = {How to {Generate} a {Good} {Word} {Embedding}?},
	url = {http://arxiv.org/abs/1507.05523},
	abstract = {We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.},
	author = {Lai, Siwei and Liu, Kang and Xu, Liheng and Zhao, Jun},
	year = {2015},
	keywords = {distributed representation, neural network, word embedding},
	file = {1507.05523v1:/home/user/Zotero/storage/PFSQ4CRV/1507.05523v1.pdf:application/pdf}
}

@article{krylov-1.-2010,
	title = {1. {Motivation}, {Aims} and {Examples}},
	author = {Krylov, N},
	year = {2010},
	pages = {1--4},
	file = {1502.03951v2:/home/user/Zotero/storage/SR635F68/1502.03951v2.pdf:application/pdf}
}

@article{kroer-limited-2015,
	title = {Limited {Lookahead} in {Imperfect}-{Information} {Games}},
	number = {Ijcai},
	author = {Kroer, Christian and Sandholm, Tuomas},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {575--581},
	file = {IJCAI15-087:/home/user/Zotero/storage/JC3J25E9/IJCAI15-087.pdf:application/pdf}
}

@article{koo-dual-2010,
	title = {Dual {Decomposition} for {Parsing} with {Non}-{Projective} {Head} {Automata}},
	issn = {1932432868},
	journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	author = {Koo, Terry and Rush, Alexander M. and Collins, Michael and Jaakkola, Tommi and Sontag, David},
	year = {2010},
	pages = {1288--1298},
	file = {Koo et al. - 2010 - Dual Decomposition for Parsing with Non-Projective Head Automata:/home/user/Zotero/storage/2MDPXUD9/Koo et al. - 2010 - Dual Decomposition for Parsing with Non-Projective Head Automata.pdf:application/pdf}
}

@article{koo-dual-2010-1,
	title = {Dual {Decomposition} for {Parsing} with {Non}-{Projective} {Head} {Automata}},
	issn = {1932432868},
	journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	author = {Koo, Terry and Rush, Alexander M. and Collins, Michael and Jaakkola, Tommi and Sontag, David},
	year = {2010},
	pages = {1288--1298},
	file = {Koo et al. - 2010 - Dual Decomposition for Parsing with Non-Projective Head Automata(2):/home/user/Zotero/storage/HVARAHNB/Koo et al. - 2010 - Dual Decomposition for Parsing with Non-Projective Head Automata(2).pdf:application/pdf}
}

@article{klenske-nonparametric-2013,
	title = {Nonparametric {Dynamics} {Estimation} for {Time} {Periodic} {Systems}},
	author = {Klenske, Edgar D and Zeilinger, Melanie N and Sch, Bernhard and Hennig, Philipp},
	year = {2013},
	file = {Klenske et al. - 2013 - Nonparametric Dynamics Estimation for Time Periodic Systems:/home/user/Zotero/storage/548VCE3X/Klenske et al. - 2013 - Nonparametric Dynamics Estimation for Time Periodic Systems.pdf:application/pdf}
}

@article{king-structure-activity-1996,
	title = {Structure-activity relationships derived by machine learning: {{T}}he use of atoms and their bond connectivities to predict mutagenicity by inductive logic programming},
	volume = {93},
	journal = {Proceedings of the National Academy of Sciences},
	author = {King, R D and Muggleton, S and Srinivasan, A and Sternberg, M J E},
	year = {1996},
	keywords = {ILP},
	pages = {438--442}
}

@article{kim-fixing-2015,
	title = {Fixing {Tournaments} for {Kings} , {Chokers} , and {More} ∗},
	number = {Ijcai},
	author = {Kim, Michael P and Williams, Virginia V},
	year = {2015},
	keywords = {Technical Papers — Game Theory},
	pages = {561--567},
	file = {IJCAI15-085:/home/user/Zotero/storage/55HNEGGH/IJCAI15-085.pdf:application/pdf}
}

@inproceedings{kim-manifold-valued-2015,
	title = {Manifold-valued {Dirichlet} {Processes}},
	volume = {37},
	booktitle = {{ICML}},
	author = {Kim, Hyunwoo J},
	year = {2015},
	file = {kim15:/home/user/Zotero/storage/PG5DSQHH/kim15.pdf:application/pdf}
}

@article{kennedy-bayesian-calibration-of-computer-models-nodate,
	title = {Bayesian\_Calibration\_of\_Computer\_Models},
	author = {{Kennedy} and {Ohagan}},
	file = {Kennedy, Ohagan - Unknown - Bayesian_Calibration_of_Computer_Models:/home/user/Zotero/storage/NFRMGJJ9/Kennedy, Ohagan - Unknown - Bayesian_Calibration_of_Computer_Models.pdf:application/pdf}
}

@article{kalchbrenner-grid-nodate,
	title = {Grid {Long} {Short}-{Term} {Memory}},
	abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. It therefore provides a uni-fied way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as integer addition and determining the parity of random binary vectors. It is able to solve these problems for 15-digit integers and 250-bit vectors respectively. We then give results for three empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia char-acter prediction benchmark, which is state-of-the-art among neural approaches. We also observe that a two-dimensional translation model based on Grid LSTM outperforms a phrase-based reference system on a Chinese-to-English translation task, and that 3D Grid LSTM yields a near state-of-the-art error rate of 0.32\% on MNIST.},
	author = {Kalchbrenner, Nal and Danihelka, Ivo and Deepmind, Google and Graves, Alex},
	pages = {1--14},
	file = {1507.01526:/home/user/Zotero/storage/PH7ECAVJ/1507.01526.pdf:application/pdf}
}

@article{johnson-adaptor-2007,
	title = {Adaptor grammars: {A} framework for specifying compositional nonparametric {Bayesian} models},
	volume = {19},
	issn = {0-262-19568-2},
	url = {papers2://publication/uuid/7CB3D732-3FD2-425A-9341-4FE63D938E6A},
	abstract = {This paper introduces adaptor grammars, a class of probabilistic models of lan- guage that generalize probabilistic context-free grammars (PCFGs).\tAdaptor grammars augment the probabilistic rules of PCFGs with “adaptors” that can in- duce dependencies among successive uses. With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars. We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Johnson, M and Griffiths, T L and Goldwater, S},
	year = {2007},
	pages = {641--641},
	file = {3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models:/home/user/Zotero/storage/4DIWKVXD/3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.pdf:application/pdf}
}

@article{kandasamy-bayesian-2015,
	title = {Bayesian {Active} {Learning} for {Posterior} {Estimation}},
	number = {Ijcai},
	author = {Kandasamy, Kirthevasan and Schneider, Jeff},
	year = {2015},
	pages = {3605--3611},
	file = {IJCAI15-507:/home/user/Zotero/storage/326SZU7U/IJCAI15-507.pdf:application/pdf}
}

@article{jin-dimensionality-2012,
	title = {Dimensionality {Dependent} {PAC}-{Bayes} {Margin} {Bound}},
	issn = {9781627480031},
	url = {http://books.nips.cc/papers/files/nips25/NIPS2012\_0497.pdf},
	journal = {Advances in Neural Information Processing Systems 25},
	author = {Jin, Chi and Wang, Liwei},
	year = {2012},
	pages = {1043--1051},
	file = {Jin, Wang - 2012 - Dimensionality Dependent PAC-Bayes Margin Bound:/home/user/Zotero/storage/RN3QSVWR/Jin, Wang - 2012 - Dimensionality Dependent PAC-Bayes Margin Bound.pdf:application/pdf}
}

@article{jiang-joint-2015,
	title = {Joint {Learning} of {Constituency} and {Dependency} {Grammars} by {Decomposed} {Cross}-{Lingual} {Induction}},
	number = {Ijcai},
	author = {Jiang, Wenbin and Liu, Qun and Supnithi, Thepchai},
	year = {2015},
	keywords = {Technical Papers — Machine Learning},
	pages = {953--959},
	file = {IJCAI15-139:/home/user/Zotero/storage/I2HKP7EI/IJCAI15-139.pdf:application/pdf}
}

@article{yang-stacked-2015,
	title = {Stacked {Attention} {Networks} for {Image} {Question} {Answering}},
	url = {http://arxiv.org/abs/1511.02274},
	urldate = {2015-11-16},
	journal = {arXiv preprint arXiv:1511.02274},
	author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
	year = {2015},
	file = {1511.02274v1.pdf:/home/user/Zotero/storage/KNHR2J4K/1511.02274v1.pdf:application/pdf}
}

@article{andreas-deep-2015,
	title = {Deep {Compositional} {Question} {Answering} with {Neural} {Module} {Networks}},
	url = {http://arxiv.org/abs/1511.02799},
	urldate = {2015-11-16},
	journal = {arXiv preprint arXiv:1511.02799},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	year = {2015},
	file = {1511.02799v1.pdf:/home/user/Zotero/storage/PS8SD8G5/1511.02799v1.pdf:application/pdf}
}

@article{mikolajczyk-deep-2015,
	title = {Deep {Correlation} for {Matching} {Images} and {Text}},
	url = {http://www.cv-foundation.org/openaccess/content\_cvpr\_2015/app/2B\_012.pdf},
	urldate = {2015-11-16},
	author = {Mikolajczyk, Fei Yan Krystian},
	year = {2015},
	file = {Deep Correlation for Matching Images and Text - Yan_Deep_Correlation_for_2015_CVPR_paper.pdf:/home/user/Zotero/storage/RCNX3WV8/Yan_Deep_Correlation_for_2015_CVPR_paper.pdf:application/pdf}
}

@article{chen-profinite-2015,
	title = {Profinite {Monads}, {Profinite} {Equations} and {Reiterman}'s {Theorem}},
	url = {http://arxiv.org/abs/1511.02147},
	abstract = {Profinite equations are an indispensable tool for the algebraic classification of formal languages. Reiterman's theorem states that they precisely specify pseudovarieties, i.e. classes of finite algebras closed under finite products, subalgebras and quotients. In this paper Reiterman's theorem is generalised to finite Eilenberg-Moore algebras for a monad T on a variety D of (ordered) algebras: a class of finite T-algebras is a pseudovariety iff it is presentable by profinite (in-)equations. As an application, quasivarieties of finite algebras are shown to be presentable by profinite implications. Other examples include finite ordered algebras, finite categories, finite infinity-monoids, etc.},
	urldate = {2015-11-13},
	journal = {arXiv:1511.02147 [cs, math]},
	author = {Chen, Liang-Ting and Adamek, Jiri and Milius, Stefan and Urbat, Henning},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02147},
	keywords = {Computer Science - Formal Languages and Automata Theory, Mathematics - Category Theory},
	file = {arXiv\:1511.02147 PDF:/home/user/Zotero/storage/UB4S4DBS/Chen et al. - 2015 - Profinite Monads, Profinite Equations and Reiterma.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/AKRPDVDK/1511.html:text/html}
}

@article{bowman-tree-structured-2015,
	title = {Tree-structured composition in neural networks without tree-structured architectures},
	url = {http://arxiv.org/abs/1506.04834},
	abstract = {Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an LSTM-based sequence model can indeed learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure.},
	urldate = {2015-11-13},
	journal = {arXiv:1506.04834 [cs]},
	author = {Bowman, Samuel R. and Manning, Christopher D. and Potts, Christopher},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04834},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1506.04834 PDF:/home/user/Zotero/storage/H4FPMJRS/Bowman et al. - 2015 - Tree-structured composition in neural networks wit.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MB92RGPX/1506.html:text/html}
}

@article{rohrbach-grounding-2015,
	title = {Grounding of {Textual} {Phrases} in {Images} by {Reconstruction}},
	url = {http://arxiv.org/abs/1511.03745},
	abstract = {Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Although many data sources contain images which are described with sentences or phrases, they typically do not provide the spatial localization of the phrases. This is true for both curated datasets such as MSCOCO or large user generated content as e.g. in the YFCC 100M dataset. Consequently, being able to learn from this data without grounding supervision would allow large amount and variety of training data. For this setting we propose GroundeR, a novel approach, which is able to learn the grounding by aiming to reconstruct a given phrase using an attention mechanism. More specifically, during training time, the model encodes the phrase using an LSTM, and then has to learn to attend to the relevant image region in order to reconstruct the input phrase. At test time the correct attention, i.e. the grounding is evaluated. On the Flickr 30k Entities dataset our approach outperforms prior work which, in contrast to us, trains with the grounding (bounding box) annotations.},
	urldate = {2015-11-13},
	journal = {arXiv:1511.03745 [cs]},
	author = {Rohrbach, Anna and Rohrbach, Marcus and Hu, Ronghang and Darrell, Trevor and Schiele, Bernt},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.03745},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.03745 PDF:/home/user/Zotero/storage/EXHB3EXF/Rohrbach et al. - 2015 - Grounding of Textual Phrases in Images by Reconstr.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/PG3AF3Z4/1511.html:text/html}
}

@article{wang-larger-context-2015,
	title = {Larger-{Context} {Language} {Modelling}},
	url = {http://arxiv.org/abs/1511.03729},
	abstract = {In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.},
	urldate = {2015-11-13},
	journal = {arXiv:1511.03729 [cs]},
	author = {Wang, Tian and Cho, Kyunghyun},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.03729},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1511.03729 PDF:/home/user/Zotero/storage/KD5UDCDA/Wang and Cho - 2015 - Larger-Context Language Modelling.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/5HN6WUXX/1511.html:text/html}
}

@article{williams-simple-1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	url = {http://link.springer.com/article/10.1007/BF00992696},
	number = {3-4},
	urldate = {2015-11-12},
	journal = {Machine learning},
	author = {Williams, Ronald J.},
	year = {1992},
	pages = {229--256},
	file = {Simple statistical gradient-following algorithms for connectionist reinforcement learning - williams-92.pdf:/home/user/Zotero/storage/XWZQJ6EQ/williams-92.pdf:application/pdf}
}

@inproceedings{artzi-broad-coverage-2009,
	title = {Broad-coverage {CCG} {Semantic} {Parsing} with {AMR}},
	url = {http://anthology.aclweb.org/D/D15/D15-1198.pdf},
	urldate = {2015-11-12},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}. {Màrquez}, {Adam} {Meyers}, {Joakim} {Nivre}, {Sebastian} {Padó}, {Jan} Štepánek, {Pavel} {Stranák}, {Mihai} {Surdeanu}, {Nianwen} {Xue}, and {Yi} {Zhang}},
	author = {Artzi, Yoav and Zettlemoyer, Kenton Lee Luke},
	year = {2009},
	file = {Broad-coverage CCG Semantic Parsing with AMR - D15-1198.pdf:/home/user/Zotero/storage/ECZJWV4D/D15-1198.pdf:application/pdf}
}

@article{hill-embedding-2014,
	title = {Embedding {Word} {Similarity} with {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1412.6448},
	abstract = {Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.},
	urldate = {2015-11-12},
	journal = {arXiv:1412.6448 [cs]},
	author = {Hill, Felix and Cho, Kyunghyun and Jean, Sebastien and Devin, Coline and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6448},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZDZAQVS4/1412.html:text/html;Hill et al_2014_Embedding Word Similarity with Neural Machine Translation.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hill et al_2014_Embedding Word Similarity with Neural Machine Translation.pdf:application/pdf}
}

@article{hill-simlex-999:-2014,
	title = {{SimLex}-999: {Evaluating} {Semantic} {Models} with ({Genuine}) {Similarity} {Estimation}},
	shorttitle = {{SimLex}-999},
	url = {http://arxiv.org/abs/1408.3456},
	abstract = {We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.},
	urldate = {2015-11-12},
	journal = {arXiv:1408.3456 [cs]},
	author = {Hill, Felix and Reichart, Roi and Korhonen, Anna},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.3456},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/FGKF2GAQ/1408.html:text/html;Hill et al_2014_SimLex-999.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hill et al_2014_SimLex-999.pdf:application/pdf}
}

@article{hill-goldilocks-2015,
	title = {The {Goldilocks} {Principle}: {Reading} {Children}'s {Books} with {Explicit} {Memory} {Representations}},
	shorttitle = {The {Goldilocks} {Principle}},
	url = {http://arxiv.org/abs/1511.02301},
	abstract = {We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.},
	urldate = {2015-11-12},
	journal = {arXiv:1511.02301 [cs]},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02301},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/B643Q6DF/1511.html:text/html;Hill et al_2015_The Goldilocks Principle.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hill et al_2015_The Goldilocks Principle.pdf:application/pdf}
}

@article{hill-learning-2015,
	title = {Learning to {Understand} {Phrases} by {Embedding} the {Dictionary}},
	url = {http://arxiv.org/abs/1504.00548},
	abstract = {Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: "reverse dictionaries" that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.},
	urldate = {2015-11-12},
	journal = {arXiv:1504.00548 [cs]},
	author = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna and Bengio, Yoshua},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.00548},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/97WCFDPK/1504.html:text/html;Hill et al_2015_Learning to Understand Phrases by Embedding the Dictionary.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hill et al_2015_Learning to Understand Phrases by Embedding the Dictionary.pdf:application/pdf}
}

@inproceedings{zhang-chinese-2014,
	title = {Chinese poetry generation with recurrent neural networks},
	url = {http://www.aclweb.org/old\_anthology/D/D14/D14-1074.pdf},
	urldate = {2015-11-12},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Zhang, Xingxing and Lapata, Mirella},
	year = {2014},
	pages = {670--680},
	file = {Chinese Poetry Generation with Recurrent Neural Networks - D14-1074.pdf:/home/user/Zotero/storage/4DU8P63P/D14-1074.pdf:application/pdf}
}

@article{graves-generating-2013,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2015-11-12},
	journal = {arXiv:1308.0850 [cs]},
	author = {Graves, Alex},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.0850},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZCZFSTVZ/1308.html:text/html;Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:application/pdf}
}

@article{cho-learning-2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2015-11-12},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Q897WBDQ/1406.html:text/html;Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:application/pdf}
}

@article{dicarlo-how-2012,
	title = {How {Does} the {Brain} {Solve} {Visual} {Object} {Recognition}?},
	volume = {73},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731200092X},
	doi = {10.1016/j.neuron.2012.01.010},
	abstract = {Mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.},
	number = {3},
	urldate = {2015-11-11},
	journal = {Neuron},
	author = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C.},
	month = feb,
	year = {2012},
	pages = {415--434},
	file = {DiCarlo et al_2012_How Does the Brain Solve Visual Object Recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/DiCarlo et al_2012_How Does the Brain Solve Visual Object Recognition.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/BSD4PTH7/S089662731200092X.html:text/html}
}

@inproceedings{maas-learning-2011,
	title = {Learning word vectors for sentiment analysis},
	url = {http://dl.acm.org/citation.cfm?id=2002491},
	urldate = {2015-11-11},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	year = {2011},
	pages = {142--150},
	file = {Maas et al_2011_Learning word vectors for sentiment analysis.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Maas et al_2011_Learning word vectors for sentiment analysis.pdf:application/pdf}
}

@misc{maas-large-nodate,
	title = {Large {Movie} {Review} {Dataset}},
	url = {http://ai.stanford.edu/~amaas/data/sentiment/},
	urldate = {2015-11-11},
	author = {Maas, Andrew},
	file = {Sentiment Analysis:/home/user/Zotero/storage/98IKVDXM/sentiment.html:text/html}
}

@misc{noauthor-kyunghyuncho/dl4mt-material-nodate,
	title = {kyunghyuncho/dl4mt-material},
	url = {https://github.com/kyunghyuncho/dl4mt-material},
	abstract = {Contribute to dl4mt-material development by creating an account on GitHub.},
	urldate = {2015-11-11},
	journal = {GitHub},
	file = {Snapshot:/home/user/Zotero/storage/VTQ9MQHI/session3.html:text/html}
}

@article{sukhbaatar-end--end-2015,
	title = {End-{To}-{End} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1503.08895},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	urldate = {2015-11-11},
	journal = {arXiv:1503.08895 [cs]},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.08895},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RN8QFCJI/1503.html:text/html;Sukhbaatar et al_2015_End-To-End Memory Networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Sukhbaatar et al_2015_End-To-End Memory Networks.pdf:application/pdf}
}

@misc{noauthor-introduction-nodate,
	title = {Introduction to {Neural} {Machine} {Translation} with {GPUs} (part 3) {\textbar} {Parallel} {Forall}},
	url = {http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/#more-5633},
	urldate = {2015-11-11},
	file = {Introduction to Neural Machine Translation with GPUs (part 3) | Parallel Forall:/home/user/Zotero/storage/TM8DFTUT/introduction-neural-machine-translation-gpus-part-3.html:text/html}
}

@article{rolfe-discriminative-2013,
	title = {Discriminative recurrent sparse auto-encoders},
	url = {http://arxiv.org/abs/1301.3775},
	urldate = {2015-11-11},
	journal = {arXiv preprint arXiv:1301.3775},
	author = {Rolfe, Jason Tyler and LeCun, Yann},
	year = {2013},
	file = {Rolfe_LeCun_2013_Discriminative recurrent sparse auto-encoders.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rolfe_LeCun_2013_Discriminative recurrent sparse auto-encoders.pdf:application/pdf}
}

@inproceedings{pasa-pre-training-2014,
	title = {Pre-training of {Recurrent} {Neural} {Networks} via {Linear} {Autoencoders}},
	url = {http://papers.nips.cc/paper/5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders},
	urldate = {2015-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pasa, Luca and Sperduti, Alessandro},
	year = {2014},
	pages = {3572--3580},
	file = {Pasa_Sperduti_2014_Pre-training of Recurrent Neural Networks via Linear Autoencoders.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Pasa_Sperduti_2014_Pre-training of Recurrent Neural Networks via Linear Autoencoders.pdf:application/pdf}
}

@inproceedings{maas-recurrent-2012,
	title = {Recurrent {Neural} {Networks} for {Noise} {Reduction} in {Robust} {ASR}.},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.613&rep=rep1&type=pdf},
	urldate = {2015-11-11},
	booktitle = {{INTERSPEECH}},
	publisher = {Citeseer},
	author = {Maas, Andrew L. and Le, Quoc V. and O'Neil, Tyler M. and Vinyals, Oriol and Nguyen, Patrick and Ng, Andrew Y.},
	year = {2012},
	file = {Maas et al_2012_Recurrent Neural Networks for Noise Reduction in Robust ASR.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Maas et al_2012_Recurrent Neural Networks for Noise Reduction in Robust ASR.pdf:application/pdf}
}

@inproceedings{weninger-deep-2014,
	title = {Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6854478},
	urldate = {2015-11-11},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2014 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Weninger, Felix and Watanabe, Shigetaka and Tachioka, Yuuki and Schuller, Bjorn},
	year = {2014},
	pages = {4623--4627},
	file = {Weninger et al_2014_Deep recurrent de-noising auto-encoder and blind de-reverberation for.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Weninger et al_2014_Deep recurrent de-noising auto-encoder and blind de-reverberation for.pdf:application/pdf}
}

@article{le-tutorial-2015,
	title = {A {Tutorial} on {Deep} {Learning} {Part} 2: {Autoencoders}, {Convolutional} {Neural} {Networks} and {Recurrent} {Neural} {Networks}},
	shorttitle = {A {Tutorial} on {Deep} {Learning} {Part} 2},
	url = {http://ai.stanford.edu/~quocle/tutorial2.pdf},
	urldate = {2015-11-11},
	author = {Le, Quoc V.},
	year = {2015},
	file = {Le_2015_A Tutorial on Deep Learning Part 2.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Le_2015_A Tutorial on Deep Learning Part 2.pdf:application/pdf}
}

@article{schmidhuber-deep-2015,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep {Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	urldate = {2015-11-11},
	journal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	month = jan,
	year = {2015},
	note = {arXiv: 1404.7828},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {85--117},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4PI8V5AZ/1404.html:text/html;Schmidhuber_2015_Deep Learning in Neural Networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Schmidhuber_2015_Deep Learning in Neural Networks.pdf:application/pdf}
}

@incollection{larochelle-learning-2010,
	title = {Learning to combine foveal glimpses with a third-order {Boltzmann} machine},
	url = {http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine.pdf},
	urldate = {2015-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Larochelle, Hugo and Hinton, Geoffrey E.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {1243--1251},
	file = {Larochelle_Hinton_2010_Learning to combine foveal glimpses with a third-order Boltzmann machine.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Larochelle_Hinton_2010_Learning to combine foveal glimpses with a third-order Boltzmann machine.pdf:application/pdf;NIPS Snapshort:/home/user/Zotero/storage/952DKIC6/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine.html:text/html}
}

@article{schmidhuber-fovea-nodate,
	title = {fovea trajectories for target detection},
	url = {ftp://ftp.idsia.ch/pub/juergen/attention.pdf},
	urldate = {2015-11-11},
	author = {schmidhuber, huber},
	file = {schmidhuber_fovea trajectories for target detection.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/schmidhuber_fovea trajectories for target detection.pdf:application/pdf}
}

@article{graves-neural-2014,
	title = {Neural {Turing} {Machines}},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2015-11-11},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.5401},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/524F4DRJ/1410.html:text/html;Graves et al_2014_Neural Turing Machines.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Graves et al_2014_Neural Turing Machines.pdf:application/pdf}
}

@misc{noauthor-role-nodate,
	title = {The role of the episodic buffer in working memory for language processing - {Springer}},
	url = {http://link.springer.com/article/10.1007/s10339-007-0183-x},
	urldate = {2015-11-11},
	file = {The role of the episodic buffer in working memory for language processing - Springer:/home/user/Zotero/storage/N89UIZRC/s10339-007-0183-x.html:text/html}
}

@article{fabius-variational-2014,
	title = {Variational {Recurrent} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1412.6581},
	abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
	urldate = {2015-11-11},
	journal = {arXiv:1412.6581 [cs, stat]},
	author = {Fabius, Otto and van Amersfoort, Joost R.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6581},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/SIGC3J4U/1412.html:text/html;Fabius_van Amersfoort_2014_Variational Recurrent Auto-Encoders.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fabius_van Amersfoort_2014_Variational Recurrent Auto-Encoders.pdf:application/pdf}
}

@article{beaty-creative-nodate,
	title = {Creative {Cognition} and {Brain} {Network} {Dynamics}},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002545},
	doi = {10.1016/j.tics.2015.10.004},
	abstract = {Creative thinking is central to the arts, sciences, and everyday life. How does the brain produce creative thought? A series of recently published papers has begun to provide insight into this question, reporting a strikingly similar pattern of brain activity and connectivity across a range of creative tasks and domains, from divergent thinking to poetry composition to musical improvisation. This research suggests that creative thought involves dynamic interactions of large-scale brain systems, with the most compelling finding being that the default and executive control networks, which can show an antagonistic relation, tend to cooperate during creative cognition and artistic performance. These findings have implications for understanding how brain networks interact to support complex cognitive processes, particularly those involving goal-directed, self-generated thought.},
	urldate = {2015-11-11},
	journal = {Trends in Cognitive Sciences},
	author = {Beaty, Roger E. and Benedek, Mathias and Silvia, Paul J. and Schacter, Daniel L.},
	keywords = {creativity, connectivity, expertise, imagination, improvisation, networks},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/XS2I5UFR/Beaty et al. - Creative Cognition and Brain Network Dynamics.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/2G8JXGNW/S1364661315002545.html:text/html}
}

@article{everaert-structures-nodate,
	title = {Structures, {Not} {Strings}: {Linguistics} as {Part} of the {Cognitive} {Sciences}},
	issn = {1364-6613},
	shorttitle = {Structures, {Not} {Strings}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002326},
	doi = {10.1016/j.tics.2015.09.008},
	abstract = {There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what ‘language’ is. Sharpening the core question – what is language? – and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences. Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language.},
	urldate = {2015-11-11},
	journal = {Trends in Cognitive Sciences},
	author = {Everaert, Martin B. H. and Huybregts, Marinus A. C. and Chomsky, Noam and Berwick, Robert C. and Bolhuis, Johan J.},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/ZQXBRU5M/Everaert et al. - Structures, Not Strings Linguistics as Part of th.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/E8PWQEGP/S1364661315002326.html:text/html}
}

@article{ito-predicting-nodate,
	title = {Predicting form and meaning: {Evidence} from brain potentials},
	issn = {0749-596X},
	shorttitle = {Predicting form and meaning},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001242},
	doi = {10.1016/j.jml.2015.10.007},
	abstract = {We used ERPs to investigate the pre-activation of form and meaning in language comprehension. Participants read high-cloze sentence contexts (e.g., “The student is going to the library to borrow a…”), followed by a word that was predictable (book), form-related (hook) or semantically related (page) to the predictable word, or unrelated (sofa). At a 500 ms SOA (Experiment 1), semantically related words, but not form-related words, elicited a reduced N400 compared to unrelated words. At a 700 ms SOA (Experiment 2), semantically related words and form-related words elicited reduced N400 effects, but the effect for form-related words occurred in very high-cloze sentences only. At both SOAs, form-related words elicited an enhanced, post-N400 posterior positivity (Late Positive Component effect). The N400 effects suggest that readers can pre-activate meaning and form information for highly predictable words, but form pre-activation is more limited than meaning pre-activation. The post-N400 LPC effect suggests that participants detected the form similarity between expected and encountered input. Pre-activation of word forms crucially depends upon the time that readers have to make predictions, in line with production-based accounts of linguistic prediction.},
	urldate = {2015-11-11},
	journal = {Journal of Memory and Language},
	author = {Ito, Aine and Corley, Martin and Pickering, Martin J. and Martin, Andrea E. and Nieuwland, Mante S.},
	keywords = {ERPs, Lexical prediction, Semantic processing, SOA, Word form},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/V92MBFMX/Ito et al. - Predicting form and meaning Evidence from brain p.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/PPQPZH5D/S0749596X15001242.html:text/html}
}

@inproceedings{deng-imagenet:-2009,
	title = {Imagenet: {A} large-scale hierarchical image database},
	shorttitle = {Imagenet},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206848},
	urldate = {2015-11-09},
	booktitle = {Computer {Vision} and {Pattern} {Recognition}, 2009. {CVPR} 2009. {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	pages = {248--255},
	file = {imagenet_cvpr09.pdf:/home/user/Zotero/storage/JI29PP83/imagenet_cvpr09.pdf:application/pdf}
}

@inproceedings{andrew-deep-2013,
	title = {Deep canonical correlation analysis},
	url = {http://jmlr.org/proceedings/papers/v28/andrew13.html},
	urldate = {2015-11-09},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
	year = {2013},
	pages = {1247--1255},
	file = {Deep Canonical Correlation Analysis - andrew_icml2013.pdf:/home/user/Zotero/storage/DX2GAV7F/andrew_icml2013.pdf:application/pdf}
}

@inproceedings{canas-attention-2010,
	title = {Attention and reinforcement learning: {Constructing} representations from indirect feedback},
	shorttitle = {Attention and reinforcement learning},
	url = {http://palm.mindmodeling.org/cogsci2010/papers/0358/paper0358.pdf},
	urldate = {2015-11-05},
	booktitle = {Proceedings of the 32nd {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {Canas, Fabián and Jones, Matt},
	year = {2010},
	file = {cogsci_10.dvi - cogsci10cj.pdf:/home/user/Zotero/storage/XMP4RT3I/cogsci10cj.pdf:application/pdf}
}

@article{roelfsema-attention-gated-2005,
	title = {Attention-gated reinforcement learning of internal representations for classification},
	volume = {17},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/0899766054615699},
	number = {10},
	urldate = {2015-11-05},
	journal = {Neural computation},
	author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
	year = {2005},
	pages = {2176--2214},
	file = {2984.tex - 0899766054615699:/home/user/Zotero/storage/WAICDNDQ/0899766054615699.pdf:application/pdf}
}

@article{niv-reinforcement-2015,
	title = {Reinforcement {Learning} in {Multidimensional} {Environments} {Relies} on {Attention} {Mechanisms}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2978-14.2015},
	doi = {10.1523/JNEUROSCI.2978-14.2015},
	language = {en},
	number = {21},
	urldate = {2015-11-05},
	journal = {Journal of Neuroscience},
	author = {Niv, Y. and Daniel, R. and Geana, A. and Gershman, S. J. and Leong, Y. C. and Radulescu, A. and Wilson, R. C.},
	month = may,
	year = {2015},
	pages = {8145--8157},
	file = {Niv15.pdf:/home/user/Zotero/storage/KCB9V78T/Niv15.pdf:application/pdf}
}

@article{leong-human-nodate,
	title = {Human reinforcement learning processes act on learned attentionally-filtered representations of the world!},
	volume = {60},
	url = {http://web.stanford.edu/~ycleong/Publications/Leong\_RLDM.pdf},
	urldate = {2015-11-05},
	journal = {Learning},
	author = {Leong, Y. and Niv, Yael},
	pages = {70},
	file = {Leong_RLDM.pdf:/home/user/Zotero/storage/WXUM3W3S/Leong_RLDM.pdf:application/pdf}
}

@article{li-focus-2007,
	title = {Focus of {Attention} in {Reinforcement} {Learning}.},
	volume = {13},
	url = {http://www.jucs.org/jucs\_13\_9/focus\_of\_attention\_in/jucs\_13\_9\_1246\_1269\_li.pdf},
	number = {9},
	urldate = {2015-11-05},
	journal = {J. UCS},
	author = {Li, Lihong and Bulitko, Vadim and Greiner, Russell},
	year = {2007},
	pages = {1246--1269},
	file = {li06jucs.dvi - jucs_13_9_1246_1269_li.pdf:/home/user/Zotero/storage/HM9RQSWN/jucs_13_9_1246_1269_li.pdf:application/pdf}
}

@phdthesis{phillips-reinforcement-2004,
	title = {Reinforcement learning of dimensional attention for categorization},
	url = {http://www.cogsci.northwestern.edu/cogsci2004./papers/paper552.pdf},
	urldate = {2015-11-05},
	school = {Vanderbilt University},
	author = {Phillips, Joshua Lee and Noelle, David C.},
	year = {2004},
	file = {paper552.pdf:/home/user/Zotero/storage/ZUDJJG22/paper552.pdf:application/pdf}
}

@inproceedings{goodrich-reinforcement-2012,
	title = {Reinforcement learning based visual attention with application to face detection},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6239177},
	urldate = {2015-11-05},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW}), 2012 {IEEE} {Computer} {Society} {Conference} on},
	publisher = {IEEE},
	author = {Goodrich, Ben and Arel, Itamar},
	year = {2012},
	pages = {19--24},
	file = {Reinforcement Learning based Visual Attention with Application to Face Detection - CVPR_2012.pdf:/home/user/Zotero/storage/P8KMWRCA/CVPR_2012.pdf:application/pdf}
}

@incollection{rombouts-biologically-2012,
	title = {Biologically plausible multi-dimensional reinforcement learning in neural networks},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33269-2\_56},
	urldate = {2015-11-05},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning}–{ICANN} 2012},
	publisher = {Springer},
	author = {Rombouts, Jaldert O. and Van Ooyen, Arjen and Roelfsema, Pieter R. and Bohte, Sander M.},
	year = {2012},
	pages = {443--450},
	file = {rombouts_bohte_icann_2012.pdf:/home/user/Zotero/storage/TTMVWRU3/rombouts_bohte_icann_2012.pdf:application/pdf}
}

@article{roelfsema-perceptual-2010,
	title = {Perceptual learning rules based on reinforcers and attention},
	volume = {14},
	issn = {13646613},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661309002617},
	doi = {10.1016/j.tics.2009.11.005},
	language = {en},
	number = {2},
	urldate = {2015-11-05},
	journal = {Trends in Cognitive Sciences},
	author = {Roelfsema, Pieter R. and van Ooyen, Arjen and Watanabe, Takeo},
	month = feb,
	year = {2010},
	pages = {64--71},
	file = {doi\:10.1016/j.tics.2009.11.005 - roelfsema_watanabe_tics_2010.pdf:/home/user/Zotero/storage/Q38KMWDA/roelfsema_watanabe_tics_2010.pdf:application/pdf}
}

@article{roelfsema-attention-gated-2005-1,
	title = {Attention-gated reinforcement learning of internal representations for classification},
	volume = {17},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/0899766054615699},
	number = {10},
	urldate = {2015-11-05},
	journal = {Neural computation},
	author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
	year = {2005},
	pages = {2176--2214},
	file = {2984.tex - agrel2005.pdf:/home/user/Zotero/storage/B6RIQ33N/agrel2005.pdf:application/pdf}
}

@article{roelfsema-attention-gated-2005-2,
	title = {Attention-{Gated} {Reinforcement} {Learning} of {Internal} {Representations} for {Classification}},
	volume = {17},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/0899766054615699},
	doi = {10.1162/0899766054615699},
	abstract = {Animal learning is associated with changes in the efficacy of connections between neurons. The rules that govern this plasticity can be tested in neural networks. Rules that train neural networks to map stimuli onto outputs are given by supervised learning and reinforcement learning theories. Supervised learning is efficient but biologically implausible. In contrast, reinforcement learning is biologically plausible but comparatively inefficient. It lacks a mechanism that can identify units at early processing levels that play a decisive role in the stimulus-response mapping. Here we show that this so-called credit assignment problem can be solved by a new role for attention in learning. There are two factors in our new learning scheme that determine synaptic plasticity: (1) a reinforcement signal that is homogeneous across the network and depends on the amount of reward obtained after a trial, and (2) an attentional feedback signal from the output layer that limits plasticity to those units at earlier processing levels that are crucial for the stimulus-response mapping. The new scheme is called attention-gated reinforcement learning (AGREL). We show that it is as efficient as supervised learning in classification tasks. AGREL is biologically realistic and integrates the role of feedback connections, attention effects, synaptic plasticity, and reinforcement learning signals into a coherent framework.},
	number = {10},
	urldate = {2015-11-05},
	journal = {Neural Computation},
	author = {Roelfsema, Pieter R. and Ooyen, Arjen van},
	month = oct,
	year = {2005},
	pages = {2176--2214},
	file = {Neural Computation Snapshot:/home/user/Zotero/storage/2N5BIMWT/0899766054615699.html:text/html;Roelfsema_Ooyen_2005_Attention-Gated Reinforcement Learning of Internal Representations for.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Roelfsema_Ooyen_2005_Attention-Gated Reinforcement Learning of Internal Representations for.pdf:application/pdf}
}

@article{rabusseau-weighted-2015,
	title = {Weighted {Tree} {Automata} {Approximation} by {Singular} {Value} {Truncation}},
	url = {http://arxiv.org/abs/1511.01442},
	abstract = {We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.},
	urldate = {2015-11-05},
	journal = {arXiv:1511.01442 [cs]},
	author = {Rabusseau, Guillaume and Balle, Borja and Cohen, Shay B.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01442},
	keywords = {Computer Science - Learning, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv\:1511.01442 PDF:/home/user/Zotero/storage/UQBWBCS4/Rabusseau et al. - 2015 - Weighted Tree Automata Approximation by Singular V.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/DBEFZSPQ/1511.html:text/html}
}

@article{dai-semi-supervised-2015,
	title = {Semi-supervised {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.01432},
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	urldate = {2015-11-05},
	journal = {arXiv:1511.01432 [cs]},
	author = {Dai, Andrew M. and Le, Quoc V.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01432},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1511.01432 PDF:/home/user/Zotero/storage/X8ASHSR2/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6FPEXMQQ/1511.html:text/html}
}

@article{feng-distributed-2015,
	title = {Distributed {Deep} {Learning} for {Answer} {Selection}},
	url = {http://arxiv.org/abs/1511.01158},
	abstract = {This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.},
	urldate = {2015-11-05},
	journal = {arXiv:1511.01158 [cs]},
	author = {Feng, Minwei and Xiang, Bing and Zhou, Bowen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01158},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv\:1511.01158 PDF:/home/user/Zotero/storage/NWX9H3WJ/Feng et al. - 2015 - Distributed Deep Learning for Answer Selection.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/3CD74FGC/1511.html:text/html}
}

@book{international-speech-communication-association-11th-2011,
	address = {Red Hook, NY},
	title = {11th annual conference of the {International} {Speech} {Communication} {Association} 2010 ({INTERSPEECH} 2010): {Makuhari}, {Chiba}, {Japan}, 26 - 30 {September} 2010},
	isbn = {978-1-61782-123-3},
	shorttitle = {11th annual conference of the {International} {Speech} {Communication} {Association} 2010 ({INTERSPEECH} 2010)},
	language = {eng},
	publisher = {Curran},
	editor = {{International Speech Communication Association}},
	year = {2011},
	file = {nips2013.pdf:/home/user/Zotero/storage/2XCAPKB6/nips2013.pdf:application/pdf}
}

@article{dhillon-eigenwords:-2015,
	title = {Eigenwords: {Spectral} {Word} {Embeddings}},
	shorttitle = {Eigenwords},
	url = {http://www.pdhillon.com/dhillon15a.pdf},
	urldate = {2015-11-03},
	author = {Dhillon, Paramveer S. and Foster, Dean P. and Ungar, Lyle H.},
	year = {2015},
	file = {dhillon15a.pdf:/home/user/Zotero/storage/6UE9CPJV/dhillon15a.pdf:application/pdf}
}

@article{monroe-learning-2015,
	title = {Learning in the {Rational} {Speech} {Acts} {Model}},
	url = {http://arxiv.org/abs/1510.06807},
	urldate = {2015-11-03},
	journal = {arXiv preprint arXiv:1510.06807},
	author = {Monroe, Will and Potts, Christopher},
	year = {2015},
	file = {1510.06807v1.pdf:/home/user/Zotero/storage/48WMTF5M/1510.06807v1.pdf:application/pdf}
}

@article{luck-quirky-2015,
	title = {Quirky {Quantifiers}: {Optimal} {Models} and {Complexity} of {Computation} {Tree} {Logic}},
	shorttitle = {Quirky {Quantifiers}},
	url = {http://arxiv.org/abs/1510.08786},
	abstract = {The satisfiability problem of the branching time logic CTL is studied in terms of computational complexity. Tight upper and lower bounds are provided for each temporal operator fragment. In parallel the minimal model size is studied with a suitable notion of minimality. Thirdly, flat CTL is investigated, i.e., formulas with very low temporal operator nesting depth. A sharp dichotomy is shown in terms of complexity and optimal models: Temporal depth one has low expressive power, while temporal depth two is equivalent to full CTL.},
	urldate = {2015-11-03},
	journal = {arXiv:1510.08786 [cs]},
	author = {Lück, Martin},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.08786},
	keywords = {Computer Science - Logic in Computer Science, 68Q17, Computer Science - Computational Complexity, F.2.2},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/M2J65EPG/1510.html:text/html;Lück_2015_Quirky Quantifiers.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Lück_2015_Quirky Quantifiers.pdf:application/pdf}
}

@article{brosch-reinforcement-2015,
	title = {Reinforcement {Learning} of {Linking} and {Tracing} {Contours} in {Recurrent} {Neural} {Networks}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004489},
	doi = {10.1371/journal.pcbi.1004489},
	abstract = {Author Summary Our experience with the visual world allows us to group image elements that belong to the same perceptual object and to segregate them from other objects and the background. If subjects learn to group contour elements, this experience influences neuronal activity in early visual cortical areas, including the primary visual cortex (V1). Learning presumably depends on alterations in the pattern of connections within and between areas of the visual cortex. However, the processes that control changes in connectivity are not well understood. Here we present the first computational model that can train a neural network to integrate collinear contour elements into elongated curves and to trace a curve through the visual field. The new learning algorithm trains fully recurrent neural networks, provided the connectivity causes the networks to reach a stable state. The model reproduces the behavioral performance of monkeys trained in these tasks and explains the patterns of neuronal activity in the visual cortex that emerge during learning, which is remarkable because the only feedback for the model is a reward for successful trials. We discuss a number of the model predictions that can be tested in future neuroscientific work.},
	number = {10},
	urldate = {2015-11-03},
	journal = {PLoS Comput Biol},
	author = {Brosch, Tobias and Neumann, Heiko and Roelfsema, Pieter R.},
	month = oct,
	year = {2015},
	pages = {e1004489},
	file = {Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf:application/pdf}
}

@article{vavatzanidis-basis-2015,
	title = {The {Basis} for {Language} {Acquisition}: {Congenitally} {Deaf} {Infants} {Discriminate} {Vowel} {Length} in the {First} {Months} after {Cochlear} {Implantation}},
	volume = {27},
	issn = {0898-929X},
	shorttitle = {The {Basis} for {Language} {Acquisition}},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00868},
	doi = {10.1162/jocn\_a\_00868},
	abstract = {One main incentive for supplying hearing impaired children with a cochlear implant is the prospect of oral language acquisition. Only scarce knowledge exists, however, of what congenitally deaf children actually perceive when receiving their first auditory input, and specifically what speech-relevant features they are able to extract from the new modality. We therefore presented congenitally deaf infants and young children implanted before the age of 4 years with an oddball paradigm of long and short vowel variants of the syllable /ba/. We measured the EEG in regular intervals to study their discriminative ability starting with the first activation of the implant up to 8 months later. We were thus able to time-track the emerging ability to differentiate one of the most basic linguistic features that bears semantic differentiation and helps in word segmentation, namely, vowel length. Results show that already 2 months after the first auditory input, but not directly after implant activation, these early implanted children differentiate between long and short syllables. Surprisingly, after only 4 months of hearing experience, the ERPs have reached the same properties as those of the normal hearing control group, demonstrating the plasticity of the brain with respect to the new modality. We thus show that a simple but linguistically highly relevant feature such as vowel length reaches age-appropriate electrophysiological levels as fast as 4 months after the first acoustic stimulation, providing an important basis for further language acquisition.},
	number = {12},
	urldate = {2015-11-03},
	journal = {Journal of Cognitive Neuroscience},
	author = {Vavatzanidis, Niki Katerina and Mürbe, Dirk and Friederici, Angela and Hahne, Anja},
	month = sep,
	year = {2015},
	pages = {2427--2441},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/FTKZIXNX/jocn_a_00868.html:text/html}
}

@article{kauffmann-neural-2015,
	title = {The {Neural} {Bases} of the {Semantic} {Interference} of {Spatial} {Frequency}-based {Information} in {Scenes}},
	volume = {27},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00861},
	doi = {10.1162/jocn\_a\_00861},
	abstract = {Current models of visual perception suggest that during scene categorization, low spatial frequencies (LSF) are processed rapidly and activate plausible interpretations of visual input. This coarse analysis would then be used to guide subsequent processing of high spatial frequencies (HSF). The present fMRI study examined how processing of LSF may influence that of HSF by investigating the neural bases of the semantic interference effect. We used hybrid scenes as stimuli by combining LSF and HSF from two different scenes, and participants had to categorize the HSF scene. Categorization was impaired when LSF and HSF scenes were semantically dissimilar, suggesting that the LSF scene was processed automatically and interfered with categorization of the HSF scene. fMRI results revealed that this semantic interference effect was associated with increased activation in the inferior frontal gyrus, the superior parietal lobules, and the fusiform and parahippocampal gyri. Furthermore, a connectivity analysis (psychophysiological interaction) revealed that the semantic interference effect resulted in increasing connectivity between the right fusiform and the right inferior frontal gyri. Results support influential models suggesting that, during scene categorization, LSF information is processed rapidly in the pFC and activates plausible interpretations of the scene category. These coarse predictions would then initiate top–down influences on recognition-related areas of the inferotemporal cortex, and these could interfere with the categorization of HSF information in case of semantic dissimilarity to LSF.},
	number = {12},
	urldate = {2015-11-03},
	journal = {Journal of Cognitive Neuroscience},
	author = {Kauffmann, Louise and Bourgin, Jessica and Guyader, Nathalie and Peyrin, Carole},
	month = aug,
	year = {2015},
	pages = {2394--2405},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/J8M5NV8D/jocn_a_00861.html:text/html}
}

@article{fitzroy-musical-2015,
	title = {Musical {Meter} {Modulates} the {Allocation} of {Attention} across {Time}},
	volume = {27},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00862},
	doi = {10.1162/jocn\_a\_00862},
	abstract = {Dynamic attending theory predicts that attention is allocated hierarchically across time during processing of hierarchical rhythmic structures such as musical meter. ERP research demonstrates that attention to a moment in time modulates early auditory processing as evidenced by the amplitude of the first negative peak (N1) approximately 100 msec after sound onset. ERPs elicited by tones presented at times of high and low metric strength in short melodies were compared to test the hypothesis that hierarchically structured rhythms direct attention in a manner that modulates early perceptual processing. A more negative N1 was observed for metrically strong beats compared with metrically weak beats; this result provides electrophysiological evidence that hierarchical rhythms direct attention to metrically strong times during engaged listening. The N1 effect was observed only on fast tempo trials, suggesting that listeners more consistently invoke selective processing based on hierarchical rhythms when sounds are presented rapidly. The N1 effect was not modulated by musical expertise, indicating that the allocation of attention to metrically strong times is not dependent on extensive training. Additionally, changes in P2 amplitude and a late negativity were associated with metric strength under some conditions, indicating that multiple cognitive processes are associated with metric perception.},
	number = {12},
	urldate = {2015-11-03},
	journal = {Journal of Cognitive Neuroscience},
	author = {Fitzroy, Ahren B. and Sanders, Lisa D.},
	month = aug,
	year = {2015},
	pages = {2339--2351},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/ZUFGIMMQ/jocn_a_00862.html:text/html}
}

@article{boudewyn-sensitivity-2015,
	title = {Sensitivity to {Referential} {Ambiguity} in {Discourse}: {The} {Role} of {Attention}, {Working} {Memory}, and {Verbal} {Ability}},
	volume = {27},
	issn = {0898-929X},
	shorttitle = {Sensitivity to {Referential} {Ambiguity} in {Discourse}},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00837},
	doi = {10.1162/jocn\_a\_00837},
	abstract = {The establishment of reference is essential to language comprehension. The goal of this study was to examine listeners' sensitivity to referential ambiguity as a function of individual variation in attention, working memory capacity, and verbal ability. Participants listened to stories in which two entities were introduced that were either very similar (e.g., two oaks) or less similar (e.g., one oak and one elm). The manipulation rendered an anaphor in a subsequent sentence (e.g., oak) ambiguous or unambiguous. EEG was recorded as listeners comprehended the story, after which participants completed tasks to assess working memory, verbal ability, and the ability to use context in task performance. Power in the alpha and theta frequency bands when listeners received critical information about the discourse entities (e.g., oaks) was used to index attention and the involvement of the working memory system in processing the entities. These measures were then used to predict an ERP component that is sensitive to referential ambiguity, the Nref, which was recorded when listeners received the anaphor. Nref amplitude at the anaphor was predicted by alpha power during the earlier critical sentence: Individuals with increased alpha power in ambiguous compared with unambiguous stories were less sensitive to the anaphor's ambiguity. Verbal ability was also predictive of greater sensitivity to referential ambiguity. Finally, increased theta power in the ambiguous compared with unambiguous condition was associated with higher working-memory span. These results highlight the role of attention and working memory in referential processing during listening comprehension.},
	number = {12},
	urldate = {2015-11-03},
	journal = {Journal of Cognitive Neuroscience},
	author = {Boudewyn, Megan A. and Long, Debra L. and Traxler, Matthew J. and Lesh, Tyler A. and Dave, Shruti and Mangun, George R. and Carter, Cameron S. and Swaab, Tamara Y.},
	month = sep,
	year = {2015},
	pages = {2309--2323},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/M7QSH3TR/jocn_a_00837.html:text/html}
}

@misc{noauthor-plos-nodate,
	title = {{PLOS} {Computational} {Biology}: {Reinforcement} {Learning} of {Linking} and {Tracing} {Contours} in {Recurrent} {Neural} {Networks}},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004489},
	urldate = {2015-11-03},
	file = {PLOS Computational Biology\: Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural Networks:/home/user/Zotero/storage/35U2RDPT/article.html:text/html}
}

@article{markovic-modeling-2015,
	title = {Modeling the {Evolution} of {Beliefs} {Using} an {Attentional} {Focus} {Mechanism}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004558},
	doi = {10.1371/journal.pcbi.1004558},
	abstract = {Author Summary When making decisions in our everyday life (e.g. where to eat) we first have to identify a set of environmental features that are relevant for the decision (e.g. the distance to the place, current time or the price). Although we are able to make such inferences almost effortlessly, this type of problems is computationally challenging, as we live in a complex environment that constantly changes and contains an immense number of features. Here we investigated the question of how the human brain solves this computational challenge. In particular, we designed a new experimental paradigm and derived novel behavioral models to test the hypothesis that attention modulates the formation of beliefs about the relevance of several environmental features. As each behavioral model accounted for a different hypothesis about the underlying computational mechanism we compared them in their ability to explain the measured behavior of human subjects performing the experimental task. The model comparison indicates that an attentional-focus mechanism is a key feature of behavioral models that accurately replicate subjects’ behavior. These findings suggest that the evolution of beliefs is modulated by a competitive attractor dynamics that forms prior expectation about future outcomes. Hence, the findings provide interesting and novel insights into the computational mechanisms underlying human behavior when making decisions in complex environments.},
	number = {10},
	urldate = {2015-11-03},
	journal = {PLoS Comput Biol},
	author = {Marković, Dimitrije and Gläscher, Jan and Bossaerts, Peter and O’Doherty, John and Kiebel, Stefan J.},
	month = oct,
	year = {2015},
	pages = {e1004558},
	file = {Marković et al_2015_Modeling the Evolution of Beliefs Using an Attentional Focus Mechanism.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Marković et al_2015_Modeling the Evolution of Beliefs Using an Attentional Focus Mechanism.pdf:application/pdf}
}

@article{bichot-source-nodate,
	title = {A {Source} for {Feature}-{Based} {Attention} in the {Prefrontal} {Cortex}},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627315008673/abstract},
	doi = {10.1016/j.neuron.2015.10.001},
	abstract = {In cluttered scenes, we can use feature-based attention to quickly locate a target object. To understand how feature attention is used to find and select objects for action, we focused on the ventral prearcuate (VPA) region of prefrontal cortex. In a visual search task, VPA cells responded selectively to search cues, maintained their feature selectivity throughout the delay and subsequent saccades, and discriminated the search target in their receptive fields with a time course earlier than in FEF or IT cortex. Inactivation of VPA impaired the animals’ ability to find targets, and simultaneous recordings in FEF revealed that the effects of feature attention were eliminated while leaving the effects of spatial attention in FEF intact. Altogether, the results suggest that VPA neurons compute the locations of objects with the features sought and send this information to FEF to guide eye movements to those relevant stimuli.},
	language = {English},
	number = {0},
	urldate = {2015-11-03},
	journal = {Neuron},
	author = {Bichot, Narcisse P. and Heard, Matthew T. and DeGennaro, Ellen M. and Desimone, Robert},
	file = {Bichot et al_A Source for Feature-Based Attention in the Prefrontal Cortex.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bichot et al_A Source for Feature-Based Attention in the Prefrontal Cortex.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RSCHKG8Z/S0896-6273(15)00867-3.html:text/html}
}

@phdthesis{levy-probabilistic-2005,
	title = {Probabilistic models of word order and syntactic discontinuity},
	url = {http://ling.ucsd.edu/~rlevy/papers/thesis.pdf},
	urldate = {2015-11-03},
	school = {stanford university},
	author = {Levy, Roger},
	year = {2005},
	file = {levy-2005-thesis.pdf:/home/user/Zotero/storage/AN7VU84S/levy-2005-thesis.pdf:application/pdf}
}

@article{higgins-proof-1997,
	title = {A proof of {Simon}'s theorem on piecewise testable languages},
	volume = {178},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397596002307},
	number = {1},
	urldate = {2015-11-03},
	journal = {Theoretical computer science},
	author = {Higgins, Peter M.},
	year = {1997},
	pages = {257--264},
	file = {1-s2.0-S0304397596002307-main.pdf:/home/user/Zotero/storage/DEBRU6E4/1-s2.0-S0304397596002307-main.pdf:application/pdf}
}

@inproceedings{liu-predicting-2015,
	title = {Predicting {Eye} {Fixations} using {Convolutional} {Neural} {Networks}},
	url = {http://www.cv-foundation.org/openaccess/content\_cvpr\_2015/app/1A\_040.pdf},
	urldate = {2015-11-03},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Nian and Han, Junwei and Zhang, Dingwen and Wen, Shifeng and Liu, Tianming},
	year = {2015},
	pages = {362--370},
	file = {Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf:/home/user/Zotero/storage/A82742SA/Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf:application/pdf}
}

@article{bowman-learning-2014,
	title = {Learning {Distributed} {Word} {Representations} for {Natural} {Logic} {Reasoning}},
	url = {http://arxiv.org/abs/1410.4176},
	urldate = {2015-11-03},
	journal = {arXiv preprint arXiv:1410.4176},
	author = {Bowman, Samuel R. and Potts, Christopher and Manning, Christopher D.},
	year = {2014},
	file = {10221-45210-1-PB.pdf:/home/user/Zotero/storage/6VEXPHH6/10221-45210-1-PB.pdf:application/pdf}
}

@book{oberdiek-pdf-1999,
	title = {{PDF} information and navigation elements with hyperref, {pdfTEX}, and thumbpdf},
	url = {ftp://ftp.rrzn.uni-hannover.de/pub/mirror/tex-archive/macros/latex/contrib/hyperref/doc/slides.pdf},
	urldate = {2015-11-03},
	publisher = {EuroTEX},
	author = {Oberdiek, Heiko},
	year = {1999},
	file = {0111050v7.pdf:/home/user/Zotero/storage/XRU3BIUC/0111050v7.pdf:application/pdf}
}

@article{jeffreypennington-glove:-2014,
	title = {Glove: {Global} vectors for word representation},
	shorttitle = {Glove},
	url = {http://www.emnlp2014.org/papers/pdf/EMNLP2014162.pdf},
	urldate = {2015-11-03},
	author = {JeffreyPennington, RichardSocher and Manning, ChristopherD},
	year = {2014},
	file = {glove.pdf:/home/user/Zotero/storage/B2NBE9JM/glove.pdf:application/pdf}
}

@article{frermann-incremental-2014,
	title = {Incremental {Bayesian} {Learning} of {Semantic} {Categories}},
	url = {http://www.aclweb.org/website/old\_anthology/E/E14/E14-1.pdf#page=275},
	urldate = {2015-11-03},
	journal = {EACL 2014},
	author = {Frermann, Lea and Lapata, Mirella},
	year = {2014},
	pages = {249},
	file = {E14-1027.pdf:/home/user/Zotero/storage/B373B9DD/E14-1027.pdf:application/pdf}
}

@article{loukam-towards-2015,
	title = {Towards an open platform based on {HPSG} formalism for the standard {Arabic} language},
	issn = {1381-2416, 1572-8110},
	url = {http://link.springer.com/article/10.1007/s10772-015-9314-4},
	doi = {10.1007/s10772-015-9314-4},
	abstract = {The aim of this paper is to present an open software platform for analysing texts in standard Arabic language. The originality of this platform is that it is an integrated software environment which offers all the necessary resources and tools for parsing Arabic texts. For formalising the several elements of the language, the HPSG formalism has been adopted because of its effectiveness and its ability to be adapted to any natural language. Currently, the platform is operational with an appreciable coverage of many Arabic syntactic structures. In the medium-term, our objective is to use the platform for developing applications for the Arabic language such as interfaces, learning, information retrieval…etc.},
	language = {en},
	urldate = {2015-11-03},
	journal = {International Journal of Speech Technology},
	author = {Loukam, Mourad and Balla, Amar and Laskri, Mohamed Tayeb},
	month = oct,
	year = {2015},
	keywords = {Artificial Intelligence (incl. Robotics), HPSG, Interface in natural language, Linguistic resources, Natural language processing, Signal, Image and Speech Processing, Social Sciences, general, Software platform, Standard Arabic language, Text parsing},
	pages = {1--14},
	file = {Full Text PDF:/home/user/Zotero/storage/36R98Z85/Loukam et al. - 2015 - Towards an open platform based on HPSG formalism f.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZT3K78AB/s10772-015-9314-4.html:text/html}
}

@article{noauthor-34.pdf-nodate,
	title = {34.pdf},
	file = {34:/home/user/Zotero/storage/XQ2PJ26K/34.pdf:application/pdf}
}

@article{noauthor--2013,
	title = {{ON} {THE} ‘ {SPIRIT} {OF} {LFG} ’ {IN} {CURRENT} {COMPUTATIONAL} {LINGUISTICS} {Jonas} {Kuhn} {Universit} ¨ at {Stuttgart} {Proceedings} of the {LFG}13 {Conference} {Miriam} {Butt} and {Tracy} {Holloway} {King} ( {Editors} ) {CSLI} {Publications} 1 {Introduction}},
	year = {2013},
	file = {lfg13kuhn:/home/user/Zotero/storage/ZIP2R5AT/lfg13kuhn.pdf:application/pdf;Unknown - 2013 - ON THE ‘ SPIRIT OF LFG ’ IN CURRENT COMPUTATIONAL LINGUISTICS Jonas Kuhn Universit ¨ at Stuttgart Proceedings of t:/home/user/Zotero/storage/WH8FAWZC/Unknown - 2013 - ON THE ‘ SPIRIT OF LFG ’ IN CURRENT COMPUTATIONAL LINGUISTICS Jonas Kuhn Universit ¨ at Stuttgart Proceedings of t.pdf:application/pdf}
}

@inproceedings{bos-three-2006,
	title = {Three {Stories} on {Automated} {Reasoning} for {Natural} {Language} {Understanding}},
	booktitle = {Proceedings of {ESCoR} ({IJCAR} {Workshop}): {Empirically} {Successful} {Computerized} {Reasoning}},
	author = {Bos, Johan},
	year = {2006},
	pages = {81--91}
}

@inproceedings{bos-when-2006,
	title = {When logical inference helps determining textual entailment (and when it doesn't)},
	url = {http://www.let.rug.nl/bos/pubs/BosMarkert2006RTE2.pdf},
	abstract = {We compare and combine two methods to approach the second textual entailment challenge (RTE-2): a shallow method based mainly on word-overlap and a method based on logical inference, us- ing first-order theorem proving and model building techniques. We use a machine learning technique to combine features of both methods. We submitted two runs, one using only the shallow features, yield- ing an accuracy of 61.6\%, and one using features of both methods, performing with an accuracy score of 60.6\%. These figures suggest that logical inference didnt help much. Closer inspection of the results re- vealed that only for some of the subtasks logical inference played a significant role in performance. We try to explain the rea- son for these results.},
	booktitle = {The {Second} {PASCAL} {Recognising} {Textual} {Entailment} {Challenge}. {Proceedings} of the {Challenges} {Workshop}},
	author = {Bos, Johan and Markert, Katja},
	editor = {Magnini, Bernardo and Dagan, Ido},
	year = {2006},
	pages = {98--103},
	file = {Bos, Markert - 2006 - When logical inference helps determining textual entailment (and when it doesn't):/home/user/Zotero/storage/IJUD8IRF/Bos, Markert - 2006 - When logical inference helps determining textual entailment (and when it doesn't).pdf:application/pdf}
}

@article{boleda-distributional-nodate,
	title = {Distributional semantic features as semantic primitives - or not},
	author = {Boleda, Gemma and Pompeu, Universitat},
	pages = {1--1},
	file = {19:/home/user/Zotero/storage/5U8VSXE2/19.pdf:application/pdf}
}

@article{bojanczyk-decomposition-2014-1,
	title = {Decomposition {Theorems} and {Model}-{Checking} for the {Modal} \$\mu\$-{Calculus}},
	issn = {9781450328869},
	url = {http://arxiv.org/abs/1405.2234},
	doi = {10.1145/2603088.2603144},
	abstract = {We prove a general decomposition theorem for the modal \$\mu\$-calculus \$L\_\mu\$ in the spirit of Feferman and Vaught's theorem for disjoint unions. In particular, we show that if a structure (i.e., transition system) is composed of two substructures \$M\_1\$ and \$M\_2\$ plus edges from \$M\_1\$ to \$M\_2\$, then the formulas true at a node in \$M\$ only depend on the formulas true in the respective substructures in a sense made precise below. As a consequence we show that the model-checking problem for \$L\_\mu\$ is fixed-parameter tractable (fpt) on classes of structures of bounded Kelly-width or bounded DAG-width. As far as we are aware, these are the first fpt results for \$L\_\mu\$ which do not follow from embedding into monadic second-order logic.},
	journal = {arXiv:1405.2234 [cs, math]},
	author = {Bojanczyk, Mikolaj and Dittmann, Christoph and Kreutzer, Stephan},
	year = {2014},
	file = {Bojanczyk, Dittmann, Kreutzer - 2014 - Decomposition Theorems and Model-Checking for the Modal \$mu\$-Calculus:/home/user/Zotero/storage/TNE7KISP/Bojanczyk, Dittmann, Kreutzer - 2014 - Decomposition Theorems and Model-Checking for the Modal \$mu\$-Calculus.pdf:application/pdf}
}

@article{bojanczyk-recognisable-2015-1,
	title = {Recognisable languages over monads},
	volume = {abs/1502.0},
	url = {http://arxiv.org/abs/1502.04898},
	journal = {CoRR},
	author = {Bojanczyk, Mikolaj},
	year = {2015}
}

@book{blackburn-representation-2005,
	address = {Stanford},
	title = {Representation and {{I}}nference for {{N}}atural {{L}}anguage},
	publisher = {CSLI Publications},
	author = {Blackburn, Patrick and Bos, Johan},
	year = {2005}
}

@article{blackburn-representation-1999,
	title = {Representation and {IInference} for {Natural} {Language}. {A} {First} {Course} in {Computational} {Semantics}. {Volume} {II}: {Working} with {Discourse} {Representation} {Structures}},
	author = {Blackburn, Patrick and Bos, Johan},
	year = {1999}
}

@article{blackburn-computational-nodate,
	title = {Computational semantics},
	url = {http://www.ulb.tu-darmstadt.de/tocs/204406005.pdf},
	author = {Blackburn, Johan, Patrick {and} Bos},
	pages = {27--45},
	file = {Blackburn, Patrick and Bos - Unknown - Computational semantics:/home/user/Zotero/storage/SUASV7MG/Blackburn, Patrick and Bos - Unknown - Computational semantics.pdf:application/pdf}
}

@inproceedings{bhagat-ledir:-2007,
	title = {{LEDIR}: {An} unsupervised algorithm for learning directionality of inference rules},
	booktitle = {Proceedings of {EMNLP}-{CoNLL}},
	author = {Bhagat, R and Pantel, P and Hovy, E and Rey, M},
	year = {2007},
	pages = {161--170}
}

@article{bolanos-towards-2015,
	title = {Towards {Storytelling} from {Visual} {Lifelogging}: {An} {Overview}},
	url = {http://arxiv.org/abs/1507.06120},
	abstract = {Recent technology of visual lifelogging consists in acquiring images that capture our everyday experience by wearing a camera over long periods of time. The collected data offer a large potential of mining and inferring knowledge about how people live their lives and for analyzing the story behind these data. This opens new opportunities for a large amount of potential applications ranging from health-care, security or leisure, to quantified self. However, extracting and locating relevant content from a huge collection of personal data triggers major challenges that strongly limit their utility and usability in practice. Furthermore, due to the first-person view nature of lifelogging data and to the technical specifications of the imaging sensor, many available Computer Vision techniques become inadequate, demanding full or partial re-formulation. This review aims to provide a comprehensive summary of what is currently available and what would be needed from a Computer Vision perspective to complete the building blocks for automatic personal storytelling, being a suitable reference for all Computer Vision scientists interested in Egocentric vision in general, and in Storytelling through visual lifelogging.},
	number = {July},
	author = {Bolaños, Marc and Dimiccoli, Mariella and Radeva, Petia},
	year = {2015},
	pages = {1--14},
	file = {1507.06120v1:/home/user/Zotero/storage/A6SA26VP/1507.06120v1.pdf:application/pdf}
}

@article{blacoe-semantic-nodate,
	title = {Semantic {Composition} {Inspired} by {Quantum} {Measurement}},
	author = {Blacoe, William},
	pages = {1--14},
	file = {Blacoe - Unknown - Semantic Composition Inspired by Quantum Measurement:/home/user/Zotero/storage/7R667HF5/Blacoe - Unknown - Semantic Composition Inspired by Quantum Measurement.pdf:application/pdf}
}

@article{berant-modeling-2014-1,
	title = {Modeling {Biological} {Processes} for {Reading} {Comprehension}},
	journal = {EMNLP (Best Paper Award)},
	author = {Berant, Jonathan and Clark, Peter},
	year = {2014},
	file = {Berant, Clark - 2014 - Modeling Biological Processes for Reading Comprehension:/home/user/Zotero/storage/KPJ289HK/Berant, Clark - 2014 - Modeling Biological Processes for Reading Comprehension.pdf:application/pdf}
}

@article{beltagy-montague-2013,
	title = {Montague {Meets} {Markov}: {Deep} {Semantics} with {Probabilistic} {Logical} {Form}},
	abstract = {We combine logical and distributional rep- resentations of natural language meaning by transforming distributional similarity judg- ments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judg- ing sentence similarity and recognizing tex- tual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase simi- larity, used as textual inference rules created on the fly, improves its performance. 1},
	journal = {Joint Conference on Lexical and Computational Semantics (*SEM)},
	author = {Beltagy, Islam and Chau, Cuong and Boleda, Gemma and Garrette, Dan and Erk, Katrin},
	year = {2013},
	pages = {11--21},
	file = {beltagy.starsem13:/home/user/Zotero/storage/CA8CNKJH/beltagy.starsem13.pdf:application/pdf}
}

@article{belinkov-arabic-2009,
	title = {Arabic {Diacritization} with {Recurrent} {Neural} {Networks}},
	author = {Belinkov, Yonatan and Glass, James},
	year = {2009},
	pages = {2--3},
	file = {ISCOL2015_submission14_c_7:/home/user/Zotero/storage/CTA8UWBQ/ISCOL2015_submission14_c_7.pdf:application/pdf}
}

@article{baroni-wacky-2009,
	title = {The {WaCky} {Wide} {Web}: {A} collection of very large linguistically processed {Web}-crawled corpora},
	volume = {3},
	url = {http://wacky.sslmit.unibo.it/lib/exe/fetch.php?media=papers:wacky\_2008.pdf},
	number = {43},
	journal = {Journal of Language Resources and Evaluation},
	author = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
	year = {2009},
	keywords = {dewac, itwac, ukwac, web as corpus},
	pages = {209--226}
}

@inproceedings{bailey-diagnosing-2008,
	title = {Diagnosing meaning errors in short answers to reading comprehension questions},
	url = {http://aclweb.org/anthology/W08-0913},
	booktitle = {Proceedings of the 3rd {Workshop} on {Innovative} {Use} of {{NLP}} for {Building} {Educational} {Applications} ({BEA}-3) at {{ACL}}'08},
	author = {Bailey, Stacey and Meurers, Detmar},
	editor = {Tetreault, Joel and Burstein, Jill and Felice, Rachele De},
	year = {2008},
	pages = {107--115}
}

@inproceedings{baroni-nouns-2010,
	series = {{EMNLP} '10},
	title = {Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space},
	volume = {Semantics-},
	url = {http://portal.acm.org/citation.cfm?id=1870658.1870773},
	booktitle = {Proceedings of the 2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Baroni, Marco and Zamparelli, Roberto},
	year = {2010},
	pages = {1183--1193}
}

@article{baroni-don-2014,
	title = {Don ’ t count , predict ! {A} systematic comparison of context-counting vs . context-predicting semantic vectors},
	issn = {9781937284725},
	abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
	journal = {Acl},
	author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
	year = {2014},
	pages = {238--247},
	file = {Baroni, Dinu, Kruszewski - 2014 - Don ’ t count , predict ! A systematic comparison of context-counting vs . context-predicting semant:/home/user/Zotero/storage/ZF2KD98R/Baroni, Dinu, Kruszewski - 2014 - Don ’ t count , predict ! A systematic comparison of context-counting vs . context-predicting semant.pdf:application/pdf}
}

@article{balkir-distributional-2013,
	title = {Distributional {Sentence} {Entailment} {Using} {Density} {Matrices}},
	number = {2007},
	author = {Balkır, Esma and Sadrzadeh, Mehrnoosh and Coecke, Bob},
	year = {2013},
	file = {1506.06534v1:/home/user/Zotero/storage/CSDKNEXC/1506.06534v1.pdf:application/pdf}
}

@phdthesis{bailey-content-2088,
	title = {Content {Assessment} in {Intelligent} {Computer}-{Aided} {Language} {Learning}: {Meaning} {Error} {Diagnosis} for {English} as a {Second} {Language}},
	author = {Bailey, Stacey},
	year = {2088}
}

@article{baayen-experimental-2012,
	title = {Experimental and psycholinguistic approaches to studying derivation},
	author = {Baayen, R. Harald},
	year = {2012},
	pages = {1--32},
	file = {Baayen - 2012 - Experimental and psycholinguistic approaches to studying derivation:/home/user/Zotero/storage/F23WAC34/Baayen - 2012 - Experimental and psycholinguistic approaches to studying derivation.pdf:application/pdf}
}

@article{baayen-corpus-2011,
	title = {Corpus linguistics and naive discriminative learning},
	volume = {11},
	doi = {10.1590/S1984-63982011000200003},
	abstract = {Three classifiers from machine learning (the generalized linear mixed model, memory based learning, and support vector machines) are com- pared with a naive discriminative learning classifier, derived from ba- sic principles of error-driven learning characterizing animal and hu- man learning. Tested on the dative alternation in English, using the Switchboard data from (Bresnan, Cueni, Nikitina, \& Baayen, 2007), naive discriminative learning emerges with state-of-the-art predictive accuracy. Naive discriminative learning offers a unified framework for understanding the learning of probabilistic distributional patterns, for classification, and for a cognitive grounding of distinctive collexeme analysis.},
	journal = {Revista Brasileira de Linguística Aplicada},
	author = {Baayen, R. Harald},
	year = {2011},
	pages = {295--328},
	file = {Baayen - 2011 - Corpus linguistics and naive discriminative learning:/home/user/Zotero/storage/6Q8FNBC7/Baayen - 2011 - Corpus linguistics and naive discriminative learning.pdf:application/pdf}
}

@article{baayen-sidestepping-nodate,
	title = {sidestepping the combinatorial explosion},
	volume = {49},
	number = {0},
	author = {Baayen, R Harald and Hendrix, Peter},
	pages = {1--43},
	file = {Baayen, Hendrix - Unknown - sidestepping the combinatorial explosion:/home/user/Zotero/storage/IK48DK45/Baayen, Hendrix - Unknown - sidestepping the combinatorial explosion.pdf:application/pdf}
}

@article{arora-random-2015,
	title = {Random {Walks} on {Context} {Spaces} : {Towards} an {Explanation} of the {Mysteries} of {Semantic} {Word} {Embeddings}},
	author = {Arora, Sanjeev and Risteski, Andrej},
	year = {2015},
	file = {Arora, Risteski - 2015 - Random Walks on Context Spaces Towards an Explanation of the Mysteries of Semantic Word Embeddings:/home/user/Zotero/storage/6XMFFWTA/Arora, Risteski - 2015 - Random Walks on Context Spaces Towards an Explanation of the Mysteries of Semantic Word Embeddings.pdf:application/pdf}
}

@article{arias-castro-near-optimal-2005,
	title = {Near-optimal detection of geometric objects by fast multiscale methods},
	volume = {51},
	doi = {10.1109/TIT.2005.850056},
	abstract = {We construct detectors for "geometric" objects in noisy data. Examples include a detector for presence of a line segment of unknown length, position, and orientation in two-dimensional image data with additive white Gaussian noise. We focus on the following two issues. i) The optimal detection threshold-i.e., the signal strength below which no method of detection can be successful for large dataset size n. ii) The optimal computational complexity of a near-optimal detector, i.e., the complexity required to detect signals slightly exceeding the detection threshold. We describe a general approach to such problems which covers several classes of geometrically defined signals; for example, with one-dimensional data, signals having elevated mean on an interval, and, in d-dimensional data, signals with elevated mean on a rectangle, a ball, or an ellipsoid. In all these problems, we show that a naive or straightforward approach leads to detector thresholds and algorithms which are asymptotically far away from optimal. At the same time, a multiscale geometric analysis of these classes of objects allows us to derive asymptotically optimal detection thresholds and fast algorithms for near-optimal detectors.},
	journal = {IEEE Transactions on Information Theory},
	author = {Arias-Castro, Ery and Donoho, David L. and Huo, Xiaoming},
	year = {2005},
	keywords = {Beamlets, Detecting hot spots, Detecting line segments, Hough transform, Image processing, Maxima of Gaussian processes, Multiscale geometric analysis, Radon transform},
	pages = {2402--2425},
	file = {Arias-Castro, Donoho, Huo - 2005 - Near-optimal detection of geometric objects by fast multiscale methods:/home/user/Zotero/storage/ZBF95B7T/Arias-Castro, Donoho, Huo - 2005 - Near-optimal detection of geometric objects by fast multiscale methods.pdf:application/pdf}
}

@article{baayen-learning-2012,
	title = {Learning from the {Bible}: {Computational} modeling of the costs of letter transpositions and letter exchanges in reading {Classical} {Hebrew} and {Modern} {English}},
	volume = {2},
	abstract = {Letter transpositions are relatively harmless for reading English and other Indo-European languages with an alphabetic script, but severely disrupt comprehension in Hebrew. Furthermore, masked orthographic priming does not produce facilitation as in English (Frost, 2012). This simulation study compares the orthographic processing costs of letter transpositions and of letter exchanges for Modern English and Classical Hebrew, using the framework of naïve discriminative learning (Baayen et al., 2011). The greater disruption of transpositions for Hebrew as compared to English is correctly replicated by the model, as is the relative immunity of loanwords in Hebrew to letter transpositions. Furthermore, the absence of facilitation of form priming in Hebrew is correctly predicted. The results confirm the hypothesis that the distributional statistics of the orthographic cues in the two languages are the crucial factor determining the experimental hallmarks of orthographic processing, as argued by Frost (2012).},
	journal = {Lingue \& Linguaggio},
	author = {Baayen, R Harald},
	year = {2012},
	pages = {123--146},
	file = {Baayen - 2012 - Learning from the Bible Computational modeling of the costs of letter transpositions and letter exchanges in reading Cla:/home/user/Zotero/storage/MIWCFEKI/Baayen - 2012 - Learning from the Bible Computational modeling of the costs of letter transpositions and letter exchanges in reading Cla.pdf:application/pdf}
}

@article{angeli-philosophers-2013-1,
	title = {Philosophers are {Mortal}: {Inferring} the {Truth} of {Unseen} {Facts}},
	url = {http://www.aclweb.org/anthology/W13-3515},
	journal = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
	author = {Angeli, Gabor and Manning, Christopher},
	year = {2013},
	pages = {133--142},
	file = {Angeli, Manning - 2013 - Philosophers are Mortal Inferring the Truth of Unseen Facts:/home/user/Zotero/storage/GN7M36W8/Angeli, Manning - 2013 - Philosophers are Mortal Inferring the Truth of Unseen Facts.pdf:application/pdf}
}

@article{alayrac-learning-nodate,
	title = {Learning from narrated instruction videos},
	author = {Alayrac, Jean-baptiste and Laptev, Ivan},
	pages = {1--13},
	file = {1506.09215v1:/home/user/Zotero/storage/VR94RFZ8/1506.09215v1.pdf:application/pdf}
}

@article{noauthor-.pdf-nodate,
	title = {王力《中国现代语法》.pdf},
	file = {Unknown - Unknown - 王力《中国现代语法》.pdf:/home/user/Zotero/storage/FWQCCB84/Unknown - Unknown - 王力《中国现代语法》.pdf.pdf:application/pdf}
}

@article{noauthor-http://www.hexiaomeng.com-nodate,
	title = {更多书籍：http://www.hexiaomeng.com},
	file = {Unknown - Unknown - 更多书籍：httpwww.hexiaomeng.com:/home/user/Zotero/storage/FM5HJS64/Unknown - Unknown - 更多书籍：httpwww.hexiaomeng.com.pdf:application/pdf}
}

@article{noauthor-tense-nodate,
	title = {Tense and {Aspect} in {Early} {Chinese} {Buddhist} {Translations} : {A} comparative study on the {Liudu} jijing 六度集經},
	pages = {907--907},
	file = {EACL-9_submission_57:/home/user/Zotero/storage/65RZTT2K/EACL-9_submission_57.pdf:application/pdf}
}

@article{noauthor-chapter-2012,
	title = {Chapter 1: {Introduction}.},
	volume = {2},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24087863},
	doi = {10.1038/kisup.2012.51},
	number = {5},
	journal = {Kidney international supplements},
	month = dec,
	year = {2012},
	pages = {343--346},
	file = {Unknown - 2012 - Chapter 1 Introduction:/home/user/Zotero/storage/KDERBCN2/Unknown - 2012 - Chapter 1 Introduction.pdf:application/pdf}
}

@article{sukkarieh-mind-nodate,
	title = {Mind your {Language} ! {Controlled} {Language} for {Inference} {Purposes}},
	author = {Sukkarieh, Jana Z},
	file = {Sukkarieh - Unknown - Mind your Language ! Controlled Language for Inference Purposes:/home/user/Zotero/storage/FBGCVEX4/Sukkarieh - Unknown - Mind your Language ! Controlled Language for Inference Purposes.pdf:application/pdf}
}

@article{simpson-expressing-2007,
	title = {Expressing pragmatic constraints on word order in {Warlpiri}},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Expressing+pragmatic+constraints+on+word+order+in+Warlpiri#0},
	journal = {Architectures, rules, and preferences: Variations on …},
	author = {Simpson, J},
	year = {2007},
	pages = {2--27},
	file = {Simpson - 2007 - Expressing pragmatic constraints on word order in Warlpiri:/home/user/Zotero/storage/BM9B554F/Simpson - 2007 - Expressing pragmatic constraints on word order in Warlpiri.pdf:application/pdf}
}

@article{simpson-aspects-1983,
	title = {{ASPECTS} {OF} {WARLPlRl} {MORPHOLOGY} {AND} {SYNTAX}},
	author = {Simpson, J},
	year = {1983},
	file = {Keyser - 1983 - ASPECTS OF WARLPlRl MORPHOLOGY AND SYNTAX:/home/user/Zotero/storage/3RI6QKHJ/Keyser - 1983 - ASPECTS OF WARLPlRl MORPHOLOGY AND SYNTAX.pdf:application/pdf}
}

@article{morzycki-lexical-2014,
	title = {The {Lexical} {Semantics} of {Adjectives} : {More} {Than} {Just} {Scales}},
	number = {June},
	author = {Morzycki, Marcin},
	year = {2014},
	pages = {1--84},
	file = {Morzycki - 2014 - The Lexical Semantics of Adjectives More Than Just Scales:/home/user/Zotero/storage/ZSCX8B9J/Morzycki - 2014 - The Lexical Semantics of Adjectives More Than Just Scales.pdf:application/pdf}
}

@article{meisterernst-object-2010,
	title = {Object {Preposing} in {Classical} and {Pre}-{Medieval} {Chinese}},
	volume = {19},
	journal = {Journal of East Asian Linguistics},
	author = {Meisterernst, Barbara},
	year = {2010},
	pages = {75--102}
}

@inproceedings{erlewine-semantics-2005,
	title = {The semantics of the {Mandarin} focus marker sh ` ı},
	booktitle = {{EACL}},
	author = {Erlewine, Michael Yoshitaka},
	year = {2005},
	keywords = {discourse, Focus, 498, abstract word count, cleft, english, exhaustivity, i discuss the use, language of presentation, of sh, question under discussion},
	pages = {2005--2005},
	file = {EACL-9_submission_117:/home/user/Zotero/storage/9QMEM7RM/EACL-9_submission_117.pdf:application/pdf}
}

@article{barker-sensorimotor-2015,
	title = {Sensorimotor {Decision} {Making} in the {Zebrafish} {Tectum}},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S0960982215011665/abstract},
	doi = {10.1016/j.cub.2015.09.055},
	abstract = {An animal’s survival depends on its ability to correctly evaluate sensory stimuli and select appropriate behavioral responses. When confronted with ambiguous stimuli, the brain is faced with the task of selecting one action while suppressing others. Although conceptually simple, the site and substrate of this elementary form of decision making is still largely unknown. Zebrafish larvae respond to a moving dot stimulus in either of two ways: a small object (potential prey) evokes approach, whereas a large object (potential predator) is avoided. The classification of object size relies on processing in the optic tectum. We genetically identified a population of cells, largely comprised of glutamatergic tectal interneurons with non-stratified morphologies, that are specifically required for approach toward small objects. When these neurons are ablated, we found that the behavioral response is shifted; small objects now tend to elicit avoidance. Conversely, optogenetic facilitation of neuronal responses with channelrhodopsin (ChR2) enhances approaches to small objects. Calcium imaging in head-fixed larvae shows that a large proportion of these neurons are tuned to small sizes. Their receptive fields are shaped by input from retinal ganglion cells (RGCs) that are selective for prey identity. We propose a model in which valence-based decisions arise, at a fundamental level, from competition between dedicated sensorimotor pathways in the tectum.},
	language = {English},
	number = {21},
	urldate = {2015-11-02},
	journal = {Current Biology},
	author = {Barker, Alison J. and Baier, Herwig},
	month = feb,
	year = {2015},
	pages = {2804--2814},
	file = {Barker_Baier_2015_Sensorimotor Decision Making in the Zebrafish Tectum.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Barker_Baier_2015_Sensorimotor Decision Making in the Zebrafish Tectum.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EGU47X7X/S0960-9822(15)01166-5.html:text/html}
}

@article{melnattur-learning-2015,
	title = {Learning and {Memory}: {Do} {Bees} {Dream}?},
	volume = {25},
	issn = {0960-9822},
	shorttitle = {Learning and {Memory}},
	url = {http://www.cell.com/article/S0960982215010805/abstract},
	doi = {10.1016/j.cub.2015.09.001},
	abstract = {In mammals, evidence for memory reactivation during sleep highlighted the important role that sleep plays in memory consolidation. A new study reports that memory reactivation is evolutionarily conserved and can also be found in the honeybee.},
	language = {English},
	number = {21},
	urldate = {2015-11-02},
	journal = {Current Biology},
	author = {Melnattur, Krishna and Dissel, Stephane and Shaw, Paul J.},
	month = feb,
	year = {2015},
	pages = {R1040--R1041},
	file = {Melnattur et al_2015_Learning and Memory.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Melnattur et al_2015_Learning and Memory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7BC5QDFG/S0960-9822(15)01080-5.html:text/html}
}

@inproceedings{levy-dependencybased-2014,
	title = {Dependencybased word embeddings},
	volume = {2},
	url = {http://www.aclweb.org/anthology/P14-2050.pdf},
	urldate = {2015-11-02},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Levy, Omer and Goldberg, Yoav},
	year = {2014},
	pages = {302--308},
	file = {Dependency-Based Word Embeddings - P14-2050.pdf:/home/user/Zotero/storage/9NXTEFG3/P14-2050.pdf:application/pdf}
}

@inproceedings{levy-neural-2014-1,
	title = {Neural word embedding as implicit matrix factorization},
	url = {http://papers.nips.cc/paper/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions},
	urldate = {2015-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Levy, Omer and Goldberg, Yoav},
	year = {2014},
	pages = {2177--2185},
	file = {neural-word-embeddings-as-implicit-matrix-factorization.pdf:/home/user/Zotero/storage/WWE6KEXS/neural-word-embeddings-as-implicit-matrix-factorization.pdf:application/pdf}
}

@incollection{mikolov-distributed-2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	urldate = {2015-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {3111--3119},
	file = {Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf:application/pdf;NIPS Snapshort:/home/user/Zotero/storage/GACSGI48/5021-di.html:text/html}
}

@article{mikolov-efficient-2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2015-11-02},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZV6T295X/1301.html:text/html;Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf}
}

@article{clarke-understanding-2015,
	title = {Understanding {What} {We} {See}: {How} {We} {Derive} {Meaning} {From} {Vision}},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Understanding {What} {We} {See}},
	url = {http://www.cell.com/article/S1364661315001989/abstract},
	doi = {10.1016/j.tics.2015.08.008},
	abstract = {Recognising objects goes beyond vision, and requires models that incorporate different aspects of meaning. Most models focus on superordinate categories (e.g., animals, tools) which do not capture the richness of conceptual knowledge. We argue that object recognition must be seen as a dynamic process of transformation from low-level visual input through categorical organisation to specific conceptual representations. Cognitive models based on large normative datasets are well-suited to capture statistical regularities within and between concepts, providing both category structure and basic-level individuation. We highlight recent research showing how such models capture important properties of the ventral visual pathway. This research demonstrates that significant advances in understanding conceptual representations can be made by shifting the focus from studying superordinate categories to basic-level concepts., We view object recognition as a dynamic process of transformation from low-level visual analyses through superordinate category to basic-level conceptual representations., Understanding this process is facilitated by using semantic cognitive models that can capture feature-based statistical regularities between concepts, providing both superordinate category and basic-level information., We highlight research using fMRI, MEG, and neuropsychological and behavioural testing to show how feature-based cognitive models can relate to object semantic representations in the ventral visual pathway., The posterior fusiform and perirhinal cortex are shown to process complementary aspects of object semantics., The temporal coordination between these regions is also highlighted, while superordinate category information precedes basic-level semantic information in time.},
	language = {English},
	number = {11},
	urldate = {2015-10-30},
	journal = {Trends in Cognitive Sciences},
	author = {Clarke, Alex and Tyler, Lorraine K.},
	month = jan,
	year = {2015},
	keywords = {Semantics, category, Concepts, fusiform gyrus, perirhinal cortex, ventral visual pathway},
	pages = {677--687},
	file = {Clarke_Tyler_2015_Understanding What We See.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Clarke_Tyler_2015_Understanding What We See.pdf:application/pdf}
}

@article{stokes-decoding-2015,
	title = {Decoding {Rich} {Spatial} {Information} with {High} {Temporal} {Resolution}},
	volume = {19},
	issn = {1364-6613},
	url = {http://www.cell.com/article/S1364661315002077/abstract},
	doi = {10.1016/j.tics.2015.08.016},
	abstract = {New research suggests that magnetoencephalography (MEG) contains rich spatial information for decoding neural states. Even small differences in the angle of neighbouring dipoles generate subtle, but statistically separable field patterns. This implies MEG (and electroencephalography: EEG) is ideal for decoding neural states with high-temporal resolution in the human brain.},
	language = {English},
	number = {11},
	urldate = {2015-10-30},
	journal = {Trends in Cognitive Sciences},
	author = {Stokes, Mark G. and Wolff, Michael J. and Spaak, Eelke},
	month = jan,
	year = {2015},
	keywords = {Electroencephalography, Magnetoencephalography, multivariate pattern analysis, Neural decoding, orientation tuning, spatiotemporal information},
	pages = {636--638},
	file = {Snapshot:/home/user/Zotero/storage/AQ7XUT3U/S1364-6613(15)00207-7.html:text/html;Stokes et al_2015_Decoding Rich Spatial Information with High Temporal Resolution.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Stokes et al_2015_Decoding Rich Spatial Information with High Temporal Resolution.pdf:application/pdf}
}

@article{kim-prediction-2015,
	title = {Prediction during sentence comprehension is more than a sum of lexical associations: the role of event knowledge},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Prediction during sentence comprehension is more than a sum of lexical associations},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1102950},
	doi = {10.1080/23273798.2015.1102950},
	number = {0},
	urldate = {2015-10-30},
	journal = {Language, Cognition and Neuroscience},
	author = {Kim, Albert E. and Oines, Leif D. and Sikos, Les},
	month = oct,
	year = {2015},
	pages = {1--5},
	file = {Kim et al_2015_Prediction during sentence comprehension is more than a sum of lexical.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kim et al_2015_Prediction during sentence comprehension is more than a sum of lexical.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HA6G4KW8/23273798.2015.html:text/html}
}

@inproceedings{lu-study-2015,
	title = {A {Study} of the {Recurrent} {Neural} {Network} {Encoder}-{Decoder} for {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://www.research.ed.ac.uk/portal/files/19909910/Lu\_Zhang\_ET\_AL\_2015\_A\_Study\_of\_the\_Recurrent\_Neural\_Network\_Encoder\_Decoder.pdf},
	urldate = {2015-10-29},
	booktitle = {Sixteenth {Annual} {Conference} of the {International} {Speech} {Communication} {Association}},
	author = {Lu, Liang and Zhang, Xingxing and Cho, Kyunghyun and Renals, Steve},
	year = {2015},
	file = {liang_is15a.pdf:/home/user/Zotero/storage/R3SMQMEC/liang_is15a.pdf:application/pdf}
}

@article{yurovsky-integrative-2015,
	title = {An integrative account of constraints on cross-situational learning},
	volume = {145},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715300391},
	doi = {10.1016/j.cognition.2015.07.013},
	language = {en},
	urldate = {2015-10-29},
	journal = {Cognition},
	author = {Yurovsky, Daniel and Frank, Michael C.},
	month = dec,
	year = {2015},
	pages = {53--62},
	file = {An integrative account of constraints on cross-situational learning - 1-s2.0-S0010027715300391-main.pdf:/home/user/Zotero/storage/356I2UA3/1-s2.0-S0010027715300391-main.pdf:application/pdf}
}

@article{perner-mental-2015,
	title = {Mental files and belief: {A} cognitive theory of how children represent belief and its intensionality},
	volume = {145},
	issn = {00100277},
	shorttitle = {Mental files and belief},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715300500},
	doi = {10.1016/j.cognition.2015.08.006},
	language = {en},
	urldate = {2015-10-29},
	journal = {Cognition},
	author = {Perner, Josef and Huemer, Michael and Leahy, Brian},
	month = dec,
	year = {2015},
	pages = {77--88},
	file = {Mental files and belief\: A cognitive theory of how children represent belief and its intensionality - 1-s2.0-S0010027715300500-main.pdf:/home/user/Zotero/storage/GVAVA3BM/1-s2.0-S0010027715300500-main.pdf:application/pdf}
}

@article{han-working-2015,
	title = {Working memory contents revive the neglected, but suppress the inhibited},
	volume = {145},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715300561},
	doi = {10.1016/j.cognition.2015.08.012},
	language = {en},
	urldate = {2015-10-29},
	journal = {Cognition},
	author = {Han, Suk Won},
	month = dec,
	year = {2015},
	pages = {116--121},
	file = {Working memory contents revive the neglected, but suppress the inhibited - 1-s2.0-S0010027715300561-main.pdf:/home/user/Zotero/storage/MWQN5Q3G/1-s2.0-S0010027715300561-main.pdf:application/pdf}
}

@article{kazic-ten-2015,
	title = {Ten {Simple} {Rules} for {Experiments}’ {Provenance}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004384},
	doi = {10.1371/journal.pcbi.1004384},
	number = {10},
	urldate = {2015-10-29},
	journal = {PLoS Comput Biol},
	author = {Kazic, Toni},
	month = oct,
	year = {2015},
	pages = {e1004384},
	file = {Kazic_2015_Ten Simple Rules for Experiments’ Provenance.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kazic_2015_Ten Simple Rules for Experiments’ Provenance.pdf:application/pdf}
}

@article{schlegel-information-2015,
	title = {Information {Processing} in the {Mental} {Workspace} {Is} {Fundamentally} {Distributed}},
	issn = {0898-929X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00899},
	doi = {10.1162/jocn\_a\_00899},
	abstract = {The brain is a complex, interconnected information processing network. In humans, this network supports a mental workspace that enables high-level abilities such as scientific and artistic creativity. Do the component processes underlying these abilities occur in discrete anatomical modules, or are they distributed widely throughout the brain? How does the flow of information within this network support specific cognitive functions? Current approaches have limited ability to answer such questions. Here, we report novel multivariate methods to analyze information flow within the mental workspace during visual imagery manipulation. We find that mental imagery entails distributed information flow and shared representations throughout the cortex. These findings challenge existing, anatomically modular models of the neural basis of higher-order mental functions, suggesting that such processes may occur at least in part at a fundamentally distributed level of organization. The novel methods we report may be useful in studying other similarly complex, high-level informational processes.},
	urldate = {2015-10-29},
	journal = {Journal of Cognitive Neuroscience},
	author = {Schlegel, Alexander and Alexander, Prescott and Tse, Peter U.},
	month = oct,
	year = {2015},
	pages = {1--13},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/Q7EJ35M6/jocn_a_00899.html:text/html}
}

@article{balcarras-attentional-2015,
	title = {Attentional {Selection} {Can} {Be} {Predicted} by {Reinforcement} {Learning} of {Task}-relevant {Stimulus} {Features} {Weighted} by {Value}-independent {Stickiness}},
	issn = {0898-929X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00894},
	doi = {10.1162/jocn\_a\_00894},
	abstract = {Attention includes processes that evaluate stimuli relevance, select the most relevant stimulus against less relevant stimuli, and bias choice behavior toward the selected information. It is not clear how these processes interact. Here, we captured these processes in a reinforcement learning framework applied to a feature-based attention task that required macaques to learn and update the value of stimulus features while ignoring nonrelevant sensory features, locations, and action plans. We found that value-based reinforcement learning mechanisms could account for feature-based attentional selection and choice behavior but required a value-independent stickiness selection process to explain selection errors while at asymptotic behavior. By comparing different reinforcement learning schemes, we found that trial-by-trial selections were best predicted by a model that only represents expected values for the task-relevant feature dimension, with nonrelevant stimulus features and action plans having only a marginal influence on covert selections. These findings show that attentional control subprocesses can be described by (1) the reinforcement learning of feature values within a restricted feature space that excludes irrelevant feature dimensions, (2) a stochastic selection process on feature-specific value representations, and (3) value-independent stickiness toward previous feature selections akin to perseveration in the motor domain. We speculate that these three mechanisms are implemented by distinct but interacting brain circuits and that the proposed formal account of feature-based stimulus selection will be important to understand how attentional subprocesses are implemented in primate brain networks.},
	urldate = {2015-10-29},
	journal = {Journal of Cognitive Neuroscience},
	author = {Balcarras, Matthew and Ardid, Salva and Kaping, Daniel and Everling, Stefan and Womelsdorf, Thilo},
	month = oct,
	year = {2015},
	pages = {1--17},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/7Z5VVIR9/jocn_a_00894.html:text/html}
}

@article{wybo-sparse-2015,
	title = {A {Sparse} {Reformulation} of the {Green}’s {Function} {Formalism} {Allows} {Efficient} {Simulations} of {Morphological} {Neuron} {Models}},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO\_a\_00788},
	doi = {10.1162/NECO\_a\_00788},
	abstract = {We prove that when a class of partial differential equations, generalized from the cable equation, is defined on tree graphs and the inputs are restricted to a spatially discrete, well chosen set of points, the Green’s function (GF) formalism can be rewritten to scale as O (n) with the number n of inputs locations, contrary to the previously reported O (n2) scaling. We show that the linear scaling can be combined with an expansion of the remaining kernels as sums of exponentials to allow efficient simulations of equations from the aforementioned class. We furthermore validate this simulation paradigm on models of nerve cells and explore its relation with more traditional finite difference approaches. Situations in which a gain in computational performance is expected are discussed.},
	urldate = {2015-10-29},
	journal = {Neural Computation},
	author = {Wybo, Willem A. M. and Boccalini, Daniele and Torben-Nielsen, Benjamin and Gewaltig, Marc-Oliver},
	month = oct,
	year = {2015},
	pages = {1--36},
	file = {Neural Computation Snapshot:/home/user/Zotero/storage/XW3IWTMM/NECO_a_00788.html:text/html;Wybo et al_2015_A Sparse Reformulation of the Green’s Function Formalism Allows Efficient.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Wybo et al_2015_A Sparse Reformulation of the Green’s Function Formalism Allows Efficient.pdf:application/pdf}
}

@article{gardner-learning-2015,
	title = {Learning {Spatiotemporally} {Encoded} {Pattern} {Transformations} in {Structured} {Spiking} {Neural} {Networks}},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO\_a\_00790},
	doi = {10.1162/NECO\_a\_00790},
	abstract = {Information encoding in the nervous system is supported through the precise spike timings of neurons; however, an understanding of the underlying processes by which such representations are formed in the first place remains an open question. Here we examine how multilayered networks of spiking neurons can learn to encode for input patterns using a fully temporal coding scheme. To this end, we introduce a new supervised learning rule, MultilayerSpiker, that can train spiking networks containing hidden layer neurons to perform transformations between spatiotemporal input and output spike patterns. The performance of the proposed learning rule is demonstrated in terms of the number of pattern mappings it can learn, the complexity of network structures it can be used on, and its classification accuracy when using multispike-based encodings. In particular, the learning rule displays robustness against input noise and can generalize well on an example data set. Our approach contributes to both a systematic understanding of how computations might take place in the nervous system and a learning rule that displays strong technical capability.},
	urldate = {2015-10-29},
	journal = {Neural Computation},
	author = {Gardner, Brian and Sporea, Ioana and Grüning, André},
	month = oct,
	year = {2015},
	pages = {1--39},
	file = {Gardner et al_2015_Learning Spatiotemporally Encoded Pattern Transformations in Structured Spiking.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Gardner et al_2015_Learning Spatiotemporally Encoded Pattern Transformations in Structured Spiking.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/EN2R84NR/NECO_a_00790.html:text/html}
}

@article{smith-response-2015,
	title = {The response modulation hypothesis of psychopathy: {A} meta-analytic and narrative analysis},
	volume = {141},
	copyright = {(c) 2015 APA, all rights reserved},
	issn = {1939-1455(Electronic);0033-2909(Print)},
	shorttitle = {The response modulation hypothesis of psychopathy},
	doi = {10.1037/bul0000024},
	abstract = {The causes of psychopathy, a condition characterized by interpersonal (e.g., superficial charm), affective (e.g., lack of empathy), and behavioral (e.g., impulsive actions) features, remain contested. The present review examines 1 of the most influential etiological models of psychopathy, the response modulation hypothesis (RMH), which proposes that psychopathic individuals exhibit difficulties in adjusting their behavior in the presence of a dominant response set. We conduct a meta-analysis and narrative literature review to examine the RMH quantitatively and qualitatively, estimate the statistical effects of response modulation (RM) deficits in psychopathic individuals, and ascertain the boundary conditions of the RMH. Ninety-four samples from published and unpublished studies involving 7,340 participants were identified for inclusion. Overall results provided some support for the RMH, revealing a small to medium relationship between psychopathy and RM deficits (r = .20, p {\textless} .001, d = .41) that extended to both psychopathy dimensions. Moreover, as predicted by the RMH, RM deficits were observed for both affectively neutral and affectively laden tasks. A number of moderators, such as anxiety, laboratory task, dependent measure, psychopathy measure, and race, contributed to significant variability in effect sizes; we also found evidence for potential publication bias using 2 methods, raising questions concerning the robustness of RM findings. An ancillary narrative review revealed that the RMH is inconsistent with a number of replicated findings in the psychopathy literature, suggesting that the RMH, at least in its present form, is unlikely to provide a comprehensive etiological account of psychopathy. Nevertheless, more recent attentional versions of the RMH may hold promise with respect to intervention. Further fruitful directions for research on the RMH, including the use of multiple dependent measures of RM and latent variable approaches, are delineated.},
	number = {6},
	journal = {Psychological Bulletin},
	author = {Smith, Sarah Francis and Lilienfeld, Scott O.},
	year = {2015},
	keywords = {*Psychopathy, *Etiology, *Narratives, Responses, Startle Reflex},
	pages = {1145--1177}
}

@article{huang-visual-2015,
	title = {Visual features for perception, attention, and working memory: {Toward} a three-factor framework},
	volume = {145},
	issn = {0010-0277},
	shorttitle = {Visual features for perception, attention, and working memory},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027715300512},
	doi = {10.1016/j.cognition.2015.08.007},
	abstract = {Visual features are the general building blocks for attention, perception, and working memory. Here, I explore the factors which can quantitatively predict all the differences they make in various paradigms. I tried to combine the strengths of experimental and correlational approaches in a novel way by developing an individual-item differences analysis to extract the factors from 16 stimulus types on the basis of their roles in eight tasks. A large sample size (410) ensured that all eight tasks had a reliability (Cronbach’s α) of no less than 0.975, allowing the factors to be precisely determined. Three orthogonal factors were identified which correspond respectively to featural strength (i.e., how close a stimulus is to a basic feature), visual strength (i.e., visual quality of the stimulus), and spatial strength (i.e., how well a stimulus can be represented as a spatial structure). Featural strength helped substantially in all the tasks but moderately less so in perceptual discrimination; visual strength helped substantially in low-level tasks but not in high-level tasks; and spatial strength helped change detection but hindered ensemble matching and visual search. Jointly, these three factors explained 96.4\% of all the variances of the eight tasks, making it clear that they account for almost everything about the roles of these 16 stimulus types in these eight tasks.},
	urldate = {2015-10-29},
	journal = {Cognition},
	author = {Huang, Liqiang},
	month = dec,
	year = {2015},
	keywords = {visual perception, Visual attention, Visual features, Visual working memory},
	pages = {43--52},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/HDMRNZZC/S0010027715300512.html:text/html}
}

@article{coen-cagli-flexible-2015,
	title = {Flexible gating of contextual influences in natural vision},
	volume = {18},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v18/n11/full/nn.4128.html?WT.ec\_id=NEURO-201511&spMailingID=49875947&spUserID=MTU1MTI5MzYyMjk0S0&spJobID=783833084&spReportId=NzgzODMzMDg0S0},
	doi = {10.1038/nn.4128},
	abstract = {Identical sensory inputs can be perceived as markedly different when embedded in distinct contexts. Neural responses to simple stimuli are also modulated by context, but the contribution of this modulation to the processing of natural sensory input is unclear. We measured surround suppression, a quintessential contextual influence, in macaque primary visual cortex with natural images. We found that suppression strength varied substantially for different images. This variability was not well explained by existing descriptions of surround suppression, but it was predicted by Bayesian inference about statistical dependencies in images. In this framework, surround suppression was flexible: it was recruited when the image was inferred to contain redundancies and substantially reduced in strength otherwise. Thus, our results reveal a gating of a basic, widespread cortical computation by inference about the statistics of natural input.},
	language = {en},
	number = {11},
	urldate = {2015-10-29},
	journal = {Nature Neuroscience},
	author = {Coen-Cagli, Ruben and Kohn, Adam and Schwartz, Odelia},
	month = nov,
	year = {2015},
	pages = {1648--1655},
	file = {Coen-Cagli et al_2015_Flexible gating of contextual influences in natural vision.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Coen-Cagli et al_2015_Flexible gating of contextual influences in natural vision.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XDJ6Q22A/nn.4128.html:text/html}
}

@article{mrsic-flogel-editorial-nodate,
	title = {Editorial overview: {Circuit} plasticity and memory},
	issn = {0959-4388},
	shorttitle = {Editorial overview},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438815001531},
	doi = {10.1016/j.conb.2015.09.006},
	urldate = {2015-10-29},
	journal = {Current Opinion in Neurobiology},
	author = {Mrsic-Flogel, Thomas and Treves, Alessandro},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/2IZVCJSG/Mrsic-Flogel and Treves - Editorial overview Circuit plasticity and memory.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CN6VCJ4H/S0959438815001531.html:text/html}
}

@article{finlayson-representation-2015,
	title = {The representation and perception of 3D space: {Interactions} between 2D location and depth},
	volume = {0},
	issn = {1350-6285},
	shorttitle = {The representation and perception of 3D space},
	url = {http://dx.doi.org/10.1080/13506285.2015.1093239},
	doi = {10.1080/13506285.2015.1093239},
	abstract = {We live in a 3D world, and yet the majority of vision research is restricted to 2D phenomena, with depth research typically treated as a separate field. Here we ask whether 2D spatial information and depth information interact to form neural representations of 3D space, and if so, what are the perceptual implications? Using fMRI and behavioural methods, we reveal that human visual cortex gradually transitions from 2D to 3D spatial representations, with depth information emerging later along the visual hierarchy, and demonstrate that 2D location holds a fundamentally special place in early visual processing.},
	number = {0},
	urldate = {2015-10-29},
	journal = {Visual Cognition},
	author = {Finlayson, Nonie J. and Zhang, Xiaoli and Golomb, Julie D.},
	month = oct,
	year = {2015},
	pages = {1--5},
	file = {Full Text PDF:/home/user/Zotero/storage/KDEQXFZI/Finlayson et al. - 2015 - The representation and perception of 3D space Int.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/6IQENEPJ/13506285.2015.html:text/html}
}

@article{maneva-new-2007,
	title = {A new look at survey propagation and its generalizations},
	volume = {54},
	issn = {00045411},
	url = {http://portal.acm.org/citation.cfm?doid=1255443.1255445},
	doi = {10.1145/1255443.1255445},
	language = {en},
	number = {4},
	urldate = {2015-10-29},
	journal = {Journal of the ACM},
	author = {Maneva, Elitza and Mossel, Elchanan and Wainwright, Martin J.},
	month = jul,
	year = {2007},
	pages = {17--es},
	file = {ACMJ294-05.tex - ManMosWai_JACM07.pdf:/home/user/Zotero/storage/DXJ85ETC/ManMosWai_JACM07.pdf:application/pdf}
}

@book{halldorsson-icalp-2015,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ICALP} 2015},
	volume = {9135},
	isbn = {978-3-662-47665-9 978-3-662-47666-6},
	url = {http://link.springer.com/10.1007/978-3-662-47666-6},
	urldate = {2015-10-28},
	publisher = {Springer Berlin Heidelberg},
	editor = {Halldórsson, Magnús M. and Iwama, Kazuo and Kobayashi, Naoki and Speckmann, Bettina},
	year = {2015},
	file = {bok%3A978-3-662-47666-6.pdf:/home/user/Zotero/storage/GXER7Q52/bok%3A978-3-662-47666-6.pdf:application/pdf}
}

@book{halldorsson-icalp-2015-1,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ICALP} 2015},
	volume = {9134},
	isbn = {978-3-662-47671-0 978-3-662-47672-7},
	url = {http://link.springer.com/10.1007/978-3-662-47672-7},
	urldate = {2015-10-28},
	publisher = {Springer Berlin Heidelberg},
	editor = {Halldórsson, Magnús M. and Iwama, Kazuo and Kobayashi, Naoki and Speckmann, Bettina},
	year = {2015},
	file = {bok%3A978-3-662-47672-7.pdf:/home/user/Zotero/storage/NDTTME8B/bok%3A978-3-662-47672-7.pdf:application/pdf}
}

@article{lage-repetition-2015,
	title = {Repetition and variation in motor practice: {A} review of neural correlates},
	volume = {57},
	issn = {01497634},
	shorttitle = {Repetition and variation in motor practice},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415002304},
	doi = {10.1016/j.neubiorev.2015.08.012},
	language = {en},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Lage, Guilherme M. and Ugrinowitsch, Herbert and Apolinário-Souza, Tércio and Vieira, Márcio Mário and Albuquerque, Maicon R. and Benda, Rodolfo Novellino},
	month = oct,
	year = {2015},
	pages = {132--141},
	file = {Lage et al_2015_Repetition and variation in motor practice.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Lage et al_2015_Repetition and variation in motor practice.pdf:application/pdf}
}

@article{bolton-role-2015,
	title = {The role of the cerebral cortex in postural responses to externally induced perturbations},
	volume = {57},
	issn = {01497634},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415002328},
	doi = {10.1016/j.neubiorev.2015.08.014},
	language = {en},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Bolton, D.A.E.},
	month = oct,
	year = {2015},
	pages = {142--155},
	file = {Bolton_2015_The role of the cerebral cortex in postural responses to externally induced.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bolton_2015_The role of the cerebral cortex in postural responses to externally induced.pdf:application/pdf}
}

@article{winter-mental-2015,
	title = {Mental number space in three dimensions},
	volume = {57},
	issn = {01497634},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415002407},
	doi = {10.1016/j.neubiorev.2015.09.005},
	language = {en},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Winter, Bodo and Matlock, Teenie and Shaki, Samuel and Fischer, Martin H.},
	month = oct,
	year = {2015},
	pages = {209--219},
	file = {Winter et al_2015_Mental number space in three dimensions.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Winter et al_2015_Mental number space in three dimensions.pdf:application/pdf}
}

@article{van-der-weiden-selfother-2015,
	title = {Self–other integration and distinction in schizophrenia: {A} theoretical analysis and a review of the evidence},
	volume = {57},
	issn = {01497634},
	shorttitle = {Self–other integration and distinction in schizophrenia},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415002390},
	doi = {10.1016/j.neubiorev.2015.09.004},
	language = {en},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {van der Weiden, Anouk and Prikken, Merel and van Haren, Neeltje E.M.},
	month = oct,
	year = {2015},
	pages = {220--237},
	file = {van der Weiden et al_2015_Self–other integration and distinction in schizophrenia.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/van der Weiden et al_2015_Self–other integration and distinction in schizophrenia.pdf:application/pdf}
}

@article{tibboel-implicit-2015,
	title = {Implicit measures of “wanting” and “liking” in humans},
	volume = {57},
	issn = {01497634},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0149763415002481},
	doi = {10.1016/j.neubiorev.2015.09.015},
	language = {en},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Tibboel, Helen and De Houwer, Jan and Van Bockstaele, Bram},
	month = oct,
	year = {2015},
	pages = {350--364},
	file = {Tibboel et al_2015_Implicit measures of “wanting” and “liking” in humans.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Tibboel et al_2015_Implicit measures of “wanting” and “liking” in humans.pdf:application/pdf}
}

@article{hamacher-brain-2015,
	title = {Brain activity during walking: {A} systematic review},
	volume = {57},
	issn = {0149-7634},
	shorttitle = {Brain activity during walking},
	url = {http://www.sciencedirect.com/science/article/pii/S014976341500202X},
	doi = {10.1016/j.neubiorev.2015.08.002},
	abstract = {Background
This systematic review provides an overview of the literature deducing information about brain activation during (1) imagined walking using MRI/fMRI or (2) during real walking using measurement systems as fNIRS, EEG and PET.
Methods
Three independent reviewers undertook an electronic database research browsing six databases. The search request consisted of three search fields. The first field comprised common methods to evaluate brain activity. The second search field comprised synonyms for brain responses to movements. The third search field comprised synonyms for walking.
Results
48 of an initial yield of 1832 papers were reviewed. We found differences in cortical activity regarding young vs. old individuals, physically fit vs. physically unfit cohorts, healthy people vs. patients with neurological diseases, and between simple and complex walking tasks.
Conclusions
We summarize that the dimension of brain activity in different brain areas during walking is highly sensitive to task complexity, age and pathologies supporting previous assumptions underpinning the significance of cortical control. Many compensation mechanisms reflect the brain's plasticity which ensures stable walking.},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Hamacher, Dennis and Herold, Fabian and Wiegel, Patrick and Hamacher, Daniel and Schega, Lutz},
	month = oct,
	year = {2015},
	keywords = {fMRI, Cognition, Cortical activation, EEG, fNIRS, Gait, MRI, PET},
	pages = {310--327},
	file = {Hamacher et al_2015_Brain activity during walking.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hamacher et al_2015_Brain activity during walking.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/QVXBXS4R/S014976341500202X.html:text/html}
}

@article{yang-neural-2015,
	title = {The neural basis of hand gesture comprehension: {A} meta-analysis of functional magnetic resonance imaging studies},
	volume = {57},
	issn = {0149-7634},
	shorttitle = {The neural basis of hand gesture comprehension},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415002183},
	doi = {10.1016/j.neubiorev.2015.08.006},
	abstract = {Gestures play an important role in face-to-face communication and have been increasingly studied via functional magnetic resonance imaging. Although a large amount of data has been provided to describe the neural substrates of gesture comprehension, these findings have never been quantitatively summarized and the conclusion is still unclear. This activation likelihood estimation meta-analysis investigated the brain networks underpinning gesture comprehension while considering the impact of gesture type (co-speech gestures vs. speech-independent gestures) and task demand (implicit vs. explicit) on the brain activation of gesture comprehension. The meta-analysis of 31 papers showed that as hand actions, gestures involve a perceptual-motor network important for action recognition. As meaningful symbols, gestures involve a semantic network for conceptual processing. Finally, during face-to-face interactions, gestures involve a network for social emotive processes. Our finding also indicated that gesture type and task demand influence the involvement of the brain networks during gesture comprehension. The results highlight the complexity of gesture comprehension, and suggest that future research is necessary to clarify the dynamic interactions among these networks.},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Yang, Jie and Andric, Michael and Mathew, Mili M.},
	month = oct,
	year = {2015},
	keywords = {Meta-analysis, fMRI, Activation likelihood estimation, Co-speech gesture, Emblem},
	pages = {88--104},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/763JT583/S0149763415002183.html:text/html;Yang et al_2015_The neural basis of hand gesture comprehension.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Yang et al_2015_The neural basis of hand gesture comprehension.pdf:application/pdf}
}

@article{blanco-neurological-2015,
	title = {Neurological changes in brain structure and functions among individuals with a history of childhood sexual abuse: {A} review},
	volume = {57},
	issn = {0149-7634},
	shorttitle = {Neurological changes in brain structure and functions among individuals with a history of childhood sexual abuse},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415001980},
	doi = {10.1016/j.neubiorev.2015.07.013},
	abstract = {Objective
Review literature focused on neurological associations in brain structure among individuals with a history of childhood sexual abuse (CSA).
Methodology
A review of literature examining physiological irregularities in brain structures of individuals with a history of CSA was conducted.
Results
Results revealed that a history of CSA was associated with irregularities in the cortical and subcortical regions of the brain. These irregularities have been recognized to contribute to various cognitive, behavioral, and psychological health outcomes later in life. Age of CSA onset was associated with differential neurological brain structures.
Conclusion
Mental and behavioral health problems such as anxiety, depression, substance abuse, dissociative disorders, and sexual dysfunction are associated with CSA and may persist into adulthood. Research depicting the associations of CSA on neurological outcomes emphasizes the need to examine the biological and subsequent psychological outcomes associated with CSA. Early intervention is imperative for CSA survivors.},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Blanco, Lyzette and Nydegger, Liesl A. and Camarillo, Giselle and Trinidad, Dennis R. and Schramm, Emily and Ames, Susan L.},
	month = oct,
	year = {2015},
	keywords = {Childhood sexual abuse, Cortical, Human behavior, Subcortical},
	pages = {63--69},
	file = {Blanco et al_2015_Neurological changes in brain structure and functions among individuals with a.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Blanco et al_2015_Neurological changes in brain structure and functions among individuals with a.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/ZD7X3FPT/S0149763415001980.html:text/html}
}

@article{smith-neural-2015,
	title = {The neural basis of one's own conscious and unconscious emotional states},
	volume = {57},
	issn = {0149-7634},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415002031},
	doi = {10.1016/j.neubiorev.2015.08.003},
	abstract = {The study of emotional states has recently received considerable attention within the cognitive and neural sciences. However, limited work has been done to synthesize this growing body of literature within a coherent hierarchical, neuro-cognitive framework. In this article, we review evidence pertaining to three interacting hierarchical neural systems associated with the generation, perception and regulation of one's own emotional state. In the framework we propose, emotion generation proceeds through a series of appraisal mechanisms – some of which appear to require more cognitively sophisticated computational processing (and hence more time) than others – that ultimately trigger iterative adjustments to one's bodily state (as well as to the modes of processing in other cognitive systems). Perceiving one's own emotions then involves a multi-stage interoceptive/somatosensory process by which these body state patterns are detected and assigned conceptual emotional meaning. Finally, emotion regulation can be understood as a hierarchical control system that, at various levels, modulates autonomic reactions, appraisal mechanisms, attention, the contents of working memory, and goal-directed action selection. We highlight implications this integrative model may have for competing theories of emotion and emotional consciousness and for guiding future research.},
	urldate = {2015-10-28},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Smith, Ryan and Lane, Richard D.},
	month = oct,
	year = {2015},
	keywords = {working memory, emotion, Cognition, Affective theory of mind, Anterior cingulate cortex (ACC), Appraisal, Consciousness, Emotion regulation, Hierarchical processing, Insula, Interoception, Medial prefrontal cortex (MPFC)},
	pages = {1--29},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/MCRUQG56/S0149763415002031.html:text/html;Smith_Lane_2015_The neural basis of one's own conscious and unconscious emotional states.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Smith_Lane_2015_The neural basis of one's own conscious and unconscious emotional states.pdf:application/pdf}
}

@inproceedings{tang-learning-2014-1,
	title = {Learning generative models with visual attention},
	url = {http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention},
	urldate = {2015-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tang, Yichuan and Srivastava, Nitish and Salakhutdinov, Ruslan R.},
	year = {2014},
	pages = {1808--1816},
	file = {atten_nips14.pdf:/home/user/Zotero/storage/C48XHENE/atten_nips14.pdf:application/pdf}
}

@inproceedings{mnih-recurrent-2014,
	title = {Recurrent models of visual attention},
	url = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention},
	urldate = {2015-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and {others}},
	year = {2014},
	pages = {2204--2212},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C5QP4CN7/1406.html:text/html;Mnih et al_2014_Recurrent Models of Visual Attention.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mnih et al_2014_Recurrent Models of Visual Attention.pdf:application/pdf;Recurrent Models of Visual Attention - 5542-recurrent-models-of-visual-attention.pdf:/home/user/Zotero/storage/CN58UG87/5542-recurrent-models-of-visual-attention.pdf:application/pdf}
}

@book{zhong-attention-2012,
	title = {Attention modeling for face recognition via deep learning},
	url = {https://mindmodeling.org/cogsci2012/papers/0453/paper0453.pdf},
	urldate = {2015-10-27},
	author = {Zhong, Sheng-hua and Liu, Yan and Zhang, Yao and Chung, Fu-lai},
	year = {2012},
	file = {paper0453.pdf:/home/user/Zotero/storage/WH6ZGR65/paper0453.pdf:application/pdf}
}

@article{ba-multiple-2014,
	title = {Multiple object recognition with visual attention},
	url = {http://arxiv.org/abs/1412.7755},
	urldate = {2015-10-27},
	journal = {arXiv preprint arXiv:1412.7755},
	author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
	year = {2014},
	file = {1412.7755v2.pdf:/home/user/Zotero/storage/4KUERJRI/1412.7755v2.pdf:application/pdf}
}

@incollection{walles-neural-2013,
	title = {A neural network model of visual attention and group classification, and its performance in a visual search task},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-03680-9\_11},
	urldate = {2015-10-27},
	booktitle = {{AI} 2013: {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Walles, Hayden and Robins, Anthony and Knott, Alistair},
	year = {2013},
	pages = {98--103},
	file = {ai2013-visual-search.pdf:/home/user/Zotero/storage/B7GMRXGD/ai2013-visual-search.pdf:application/pdf}
}

@inproceedings{schaul-universal-2015,
	title = {Universal {Value} {Function} {Approximators}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/icml2015\_schaul15.pdf},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning} ({ICML}-15)},
	author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
	year = {2015},
	pages = {1312--1320},
	file = {schaul15.pdf:/home/user/Zotero/storage/9PQ2EUT3/schaul15.pdf:application/pdf}
}

@article{gregor-draw:-2015-1,
	title = {{DRAW}: {A} recurrent neural network for image generation},
	shorttitle = {{DRAW}},
	url = {http://arxiv.org/abs/1502.04623},
	urldate = {2015-10-27},
	journal = {arXiv preprint arXiv:1502.04623},
	author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
	year = {2015},
	file = {DRAW\: A Recurrent Neural Network For Image Generation - gregor15.pdf:/home/user/Zotero/storage/CMC6FC7I/gregor15.pdf:application/pdf}
}

@article{bowman-large-2015,
	title = {A large annotated corpus for learning natural language inference},
	url = {http://arxiv.org/abs/1508.05326},
	urldate = {2015-10-27},
	journal = {arXiv preprint arXiv:1508.05326},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	year = {2015},
	file = {snli_paper.pdf:/home/user/Zotero/storage/9RDN8VEV/snli_paper.pdf:application/pdf}
}

@article{bowman-recursive-2015,
	title = {Recursive neural networks can learn logical semantics},
	url = {http://www.aclweb.org/anthology/W/W15/W15-40.pdf#page=22},
	urldate = {2015-10-27},
	journal = {ACL-IJCNLP 2015},
	author = {Bowman, Samuel R. and Potts, Christopher and Manning, Christopher D.},
	year = {2015},
	pages = {12},
	file = {Recursive Neural Networks Can Learn Logical Semantics - W15-4002.pdf:/home/user/Zotero/storage/UKIXZKBG/W15-4002.pdf:application/pdf}
}

@article{dabaghian-reconceiving-2014,
	title = {Reconceiving the hippocampal map as a topological template},
	volume = {3},
	copyright = {© 2014, Dabaghian et al.           This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/3/e03476},
	doi = {10.7554/eLife.03476},
	abstract = {The role of the hippocampus in spatial cognition is incontrovertible yet controversial. Place cells, initially thought to be location-specifiers, turn out to respond promiscuously to a wide range of stimuli. Here we test the idea, which we have recently demonstrated in a computational model, that the hippocampal place cells may ultimately be interested in a space's topological qualities (its connectivity) more than its geometry (distances and angles); such higher-order functioning would be more consistent with other known hippocampal functions. We recorded place cell activity in rats exploring morphing linear tracks that allowed us to dissociate the geometry of the track from its topology. The resulting place fields preserved the relative sequence of places visited along the track but did not vary with the metrical features of the track or the direction of the rat's movement. These results suggest a reinterpretation of previous studies and new directions for future experiments.DOI: http://dx.doi.org/10.7554/eLife.03476.001View Full TextTo Top
The role of the hippocampus in spatial cognition is incontrovertible yet controversial. Place cells, initially thought to be location-specifiers, turn out to respond promiscuously to a wide range of stimuli. Here we test the idea, which we have recently demonstrated in a computational model, that the hippocampal place cells may ultimately be interested in a space's topological qualities (its connectivity) more than its geometry (distances and angles); such higher-order functioning would be more consistent with other known hippocampal functions. We recorded place cell activity in rats exploring morphing linear tracks that allowed us to dissociate the geometry of the track from its topology. The resulting place fields preserved the relative sequence of places visited along the track but did not vary with the metrical features of the track or the direction of the rat's movement. These results suggest a reinterpretation of previous studies and new directions for future experiments.
DOI: http://dx.doi.org/10.7554/eLife.03476.001
The hippocampus is one of the most easily recognizable structures in the brain owing to its characteristic seahorse-like shape. Brain imaging studies in the 1990s famously showed the hippocampus to be larger in London taxi drivers than in other people, suggesting that it plays a role in spatial navigation. This was consistent with previous findings in rodents, which had shown that the hippocampus is active when animals find their way through mazes.
Electrode recordings have revealed that whenever an animal is in a specific location of a particular environment (for example, in the back left-hand corner of a small room with white walls) one or a small number of cells within the hippocampus will fire to encode that location. When the animal moves to a new location within the same environment, other cells will fire to encode the new location. In this way, the population of cells—which are known as place cells—can together construct a virtual ‘map’ of the environment.
It is generally assumed that this hippocampal map represents space in terms of the absolute distances and angles between locations, rather like a street map. However, this type of geometric map appears inconsistent with the results of certain experiments. Dabaghian et al. proposed instead that the hippocampal map is based on topology, or the relative order of locations and the connections between them, rather like a subway map. Subsequently, computer models demonstrated that virtual simulations of place cells could effectively ‘learn’ the topological features of different environments.
Now, Dabaghian et al. provide their own empirical data to support the existence of a hippocampal ‘subway-style’ map by recording the electrical impulses from place cells in the rat hippocampus as the animals ran through a U-shaped maze. The maze was constructed so that its arms could either be straight or folded into zigzags. Changing the maze in this way does not alter its topology because the relative order of its various components—such as the positions of food wells in the arms—is unchanged, but it does alter the maze's geometry. Notably, as rats ran through different conformations of the maze, the activity of the place cells in their hippocampi remained largely unchanged, consistent with a map based on topology rather than geometry.
By providing evidence that hippocampal maps have more in common with subway maps than street maps, the work of Dabaghian et al. offers an explanation for previously challenging results and provides a framework for further experiments into hippocampal memory function.
DOI: http://dx.doi.org/10.7554/eLife.03476.002},
	language = {en},
	urldate = {2015-10-27},
	journal = {eLife},
	author = {Dabaghian, Yuri and Brandt, Vicky L. and Frank, Loren M.},
	month = sep,
	year = {2014},
	pmid = {25141375},
	pages = {e03476},
	file = {Dabaghian et al_2014_Reconceiving the hippocampal map as a topological template.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Dabaghian et al_2014_Reconceiving the hippocampal map as a topological template.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/D66Z742V/e03476.html:text/html}
}

@article{strutz-decoding-2014,
	title = {Decoding odor quality and intensity in the {Drosophila} brain},
	volume = {3},
	copyright = {© 2014, Strutz et al.           This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/3/e04147},
	doi = {10.7554/eLife.04147},
	abstract = {To internally reflect the sensory environment, animals create neural maps encoding the external stimulus space. From that primary neural code relevant information has to be extracted for accurate navigation. We analyzed how different odor features such as hedonic valence and intensity are functionally integrated in the lateral horn (LH) of the vinegar fly, Drosophila melanogaster. We characterized an olfactory-processing pathway, comprised of inhibitory projection neurons (iPNs) that target the LH exclusively, at morphological, functional and behavioral levels. We demonstrate that iPNs are subdivided into two morphological groups encoding positive hedonic valence or intensity information and conveying these features into separate domains in the LH. Silencing iPNs severely diminished flies' attraction behavior. Moreover, functional imaging disclosed a LH region tuned to repulsive odors comprised exclusively of third-order neurons. We provide evidence for a feature-based map in the LH, and elucidate its role as the center for integrating behaviorally relevant olfactory information.DOI: http://dx.doi.org/10.7554/eLife.04147.001View Full TextTo Top
To internally reflect the sensory environment, animals create neural maps encoding the external stimulus space. From that primary neural code relevant information has to be extracted for accurate navigation. We analyzed how different odor features such as hedonic valence and intensity are functionally integrated in the lateral horn (LH) of the vinegar fly, Drosophila melanogaster. We characterized an olfactory-processing pathway, comprised of inhibitory projection neurons (iPNs) that target the LH exclusively, at morphological, functional and behavioral levels. We demonstrate that iPNs are subdivided into two morphological groups encoding positive hedonic valence or intensity information and conveying these features into separate domains in the LH. Silencing iPNs severely diminished flies' attraction behavior. Moreover, functional imaging disclosed a LH region tuned to repulsive odors comprised exclusively of third-order neurons. We provide evidence for a feature-based map in the LH, and elucidate its role as the center for integrating behaviorally relevant olfactory information.
DOI: http://dx.doi.org/10.7554/eLife.04147.001
Organisms need to sense and adapt to their environment in order to survive. Senses such as vision and smell allow an organism to absorb information about the external environment and translate it into a meaningful internal image. This internal image helps the organism to remember incidents and act accordingly when they encounter similar situations again. A typical example is when organisms are repeatedly attracted to odors that are essential for survival, such as food and pheromones, and are repulsed by odors that threaten survival.
Strutz et al. addressed how attractiveness or repulsiveness of a smell, and also the strength of a smell, are processed by a part of the olfactory system called the lateral horn in fruit flies. This involved mapping the neuronal patterns that were generated in the lateral horn when a fly was exposed to particular odors.
Strutz et al. found that a subset of neurons called inhibitory projection neurons processes information about whether the odor is attractive or repulsive, and that a second subset of these neurons process information about the intensity of the odor. Other insects, such as honey bees and hawk moths, have olfactory systems with a similar architecture and might also employ a similar spatial approach to encode information regarding the intensity and identity of odors. Locusts, on the other hand, employ a temporal approach to encoding information about odors.
The work of Strutz et al. shows that certain qualities of odors are contained in a spatial map in a specific brain region of the fly. This opens up the question of how the information in this spatial map influences decisions made by the fly.
DOI: http://dx.doi.org/10.7554/eLife.04147.002},
	language = {en},
	urldate = {2015-10-27},
	journal = {eLife},
	author = {Strutz, Antonia and Soelter, Jan and Baschwitz, Amelie and Farhan, Abu and Grabe, Veit and Rybak, Jürgen and Knaden, Markus and Schmuker, Michael and Hansson, Bill S. and Sachse, Silke},
	month = dec,
	year = {2014},
	pages = {e04147},
	file = {Snapshot:/home/user/Zotero/storage/AZRTQIJC/e04147.html:text/html;Strutz et al_2014_Decoding odor quality and intensity in the Drosophila brain.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Strutz et al_2014_Decoding odor quality and intensity in the Drosophila brain.pdf:application/pdf}
}

@article{chaudhuri-diversity-2014,
	title = {A diversity of localized timescales in network activity},
	volume = {3},
	copyright = {© 2013, Chaudhuri et al.           This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/3/e01239},
	doi = {10.7554/eLife.01239},
	abstract = {Neurons show diverse timescales, so that different parts of a network respond with disparate temporal dynamics. Such diversity is observed both when comparing timescales across brain areas and among cells within local populations; the underlying circuit mechanism remains unknown. We examine conditions under which spatially local connectivity can produce such diverse temporal behavior.In a linear network, timescales are segregated if the eigenvectors of the connectivity matrix are localized to different parts of the network. We develop a framework to predict the shapes of localized eigenvectors. Notably, local connectivity alone is insufficient for separate timescales. However, localization of timescales can be realized by heterogeneity in the connectivity profile, and we demonstrate two classes of network architecture that allow such localization. Our results suggest a framework to relate structural heterogeneity to functional diversity and, beyond neural dynamics, are generally applicable to the relationship between structure and dynamics in biological networks.DOI: http://dx.doi.org/10.7554/eLife.01239.001View Full TextTo Top
Neurons show diverse timescales, so that different parts of a network respond with disparate temporal dynamics. Such diversity is observed both when comparing timescales across brain areas and among cells within local populations; the underlying circuit mechanism remains unknown. We examine conditions under which spatially local connectivity can produce such diverse temporal behavior.
In a linear network, timescales are segregated if the eigenvectors of the connectivity matrix are localized to different parts of the network. We develop a framework to predict the shapes of localized eigenvectors. Notably, local connectivity alone is insufficient for separate timescales. However, localization of timescales can be realized by heterogeneity in the connectivity profile, and we demonstrate two classes of network architecture that allow such localization. Our results suggest a framework to relate structural heterogeneity to functional diversity and, beyond neural dynamics, are generally applicable to the relationship between structure and dynamics in biological networks.
DOI: http://dx.doi.org/10.7554/eLife.01239.001
Many biological systems can be thought of as networks in which a large number of elements, called ‘nodes’, are connected to each other. The brain, for example, is a network of interconnected neurons, and the changing activity patterns of this network underlie our experience of the world around us. Within the brain, different parts can process information at different speeds: sensory areas of the brain respond rapidly to the current environment, while the cognitive areas of the brain, involved in complex thought processes, are able to gather information over longer periods of time. However, it has been largely unknown what properties of a network allow different regions to process information over different timescales, and how variations in structural properties translate into differences in the timescales over which parts of a network can operate.
Now Chaudhuri et al. have addressed these issues using a simple but ubiquitous class of networks called linear networks. The activity of a linear network can be broken down into simpler patterns called eigenvectors that can be combined to predict the responses of the whole network. If these eigenvectors ‘map’ to different parts of the network, this could explain how distinct regions process information on different timescales.
Chaudhuri et al. developed a mathematical theory to predict what properties would cause such eigenvectors to be separated from each other and applied it to networks with architectures that resemble the wiring of the brain. This revealed that gradients in the connectivity across the network, such that nodes share more properties with neighboring nodes than distant nodes, combined with random differences in the strength of inter-node connections, are general motifs that give rise to such separated activity patterns. Intriguingly, such gradients and randomness are both common features of biological systems.
DOI: http://dx.doi.org/10.7554/eLife.01239.002},
	language = {en},
	urldate = {2015-10-27},
	journal = {eLife},
	author = {Chaudhuri, Rishidev and Bernacchia, Alberto and Wang, Xiao-Jing},
	month = jan,
	year = {2014},
	pages = {e01239},
	file = {Chaudhuri et al_2014_A diversity of localized timescales in network activity.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Chaudhuri et al_2014_A diversity of localized timescales in network activity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/2SDBU8TX/e01239.html:text/html}
}

@article{wei-principle-2015,
	title = {A principle of economy predicts the functional architecture of grid cells},
	volume = {4},
	copyright = {© 2015, Wei et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/4/e08362},
	doi = {10.7554/eLife.08362},
	abstract = {Grid cells in the brain respond when an animal occupies a periodic lattice of ‘grid fields’ during navigation. Grids are organized in modules with different periodicity. We propose that the grid system implements a hierarchical code for space that economizes the number of neurons required to encode location with a given resolution across a range equal to the largest period. This theory predicts that (i) grid fields should lie on a triangular lattice, (ii) grid scales should follow a geometric progression, (iii) the ratio between adjacent grid scales should be √e for idealized neurons, and lie between 1.4 and 1.7 for realistic neurons, (iv) the scale ratio should vary modestly within and between animals. These results explain the measured grid structure in rodents. We also predict optimal organization in one and three dimensions, the number of modules, and, with added assumptions, the ratio between grid periods and field widths.DOI: http://dx.doi.org/10.7554/eLife.08362.001View Full TextTo Top
Grid cells in the brain respond when an animal occupies a periodic lattice of ‘grid fields’ during navigation. Grids are organized in modules with different periodicity. We propose that the grid system implements a hierarchical code for space that economizes the number of neurons required to encode location with a given resolution across a range equal to the largest period. This theory predicts that (i) grid fields should lie on a triangular lattice, (ii) grid scales should follow a geometric progression, (iii) the ratio between adjacent grid scales should be √e for idealized neurons, and lie between 1.4 and 1.7 for realistic neurons, (iv) the scale ratio should vary modestly within and between animals. These results explain the measured grid structure in rodents. We also predict optimal organization in one and three dimensions, the number of modules, and, with added assumptions, the ratio between grid periods and field widths.
DOI: http://dx.doi.org/10.7554/eLife.08362.001
In the 1930s, neuroscientists studying how rodents find their way through a maze proposed that the animals could construct an internal map of the maze inside their heads. The map was thought to enable the animals to navigate between familiar locations and also to identify shortcuts and alternative routes whenever familiar ones were blocked.
In the 1960s, recordings of electrical activity in the rat brain provided the first clues as to which nerve cells form this spatial map. In a region of the brain called the hippocampus, nerve cells called ‘place cells’ are active whenever the rat finds itself in a specific location. However, place cells alone are not able to support all types of navigation. Some spatial tasks also require cells in a region of the brain called the medial entorhinal cortex (MEC), which supplies most of the information that the hippocampus receives.
Cells in the MEC called ‘grid cells’ represent two-dimensional space as a repeating grid of triangles. A given grid cell is activated if the animal is located at a particular distance and angle away from the center of any of these triangles. The size of the triangles in these grids varies systematically throughout the MEC. Individual grid cells at one end of the structure encode space in finer detail than grid cells at the opposite end.
Wei et al. have now used mathematical modeling to explore how grid cells are organized. The model assumes that the brain seeks to encode space at whatever resolution an animal requires using as few nerve cells as possible. The model successfully reproduces several known features of grid cells, including the triangular shape of the grid, and the fact that the size of the triangles increases in steps of a specific size across the MEC.
In addition to providing a mathematical basis for the way that grid cells are organized in the brain, the model makes a number of testable predictions. These include predictions of the number of grid cells in the rat brain, as well as the pattern that grid cells adopt in three-dimensions: a question that is currently being studied in bats. Wei et al.'s findings suggest that the code used by the grid to represent space is an analog of a decimal number system—except that space is not subdivided by factors of 10 to form decimal ‘digits’, but by a quantity related to a famous constant in the field of mathematics called Euler's number.
DOI: http://dx.doi.org/10.7554/eLife.08362.002},
	language = {en},
	urldate = {2015-10-27},
	journal = {eLife},
	author = {Wei, Xue-Xin and Prentice, Jason and Balasubramanian, Vijay},
	month = oct,
	year = {2015},
	pages = {e08362},
	file = {Snapshot:/home/user/Zotero/storage/WUA7CX8A/e08362.html:text/html;Wei et al_2015_A principle of economy predicts the functional architecture of grid cells.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Wei et al_2015_A principle of economy predicts the functional architecture of grid cells.pdf:application/pdf}
}

@article{gal-dropout-2015,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	abstract = {Deep learning has gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in neural networks (NNs) can be cast as a Bayesian approximation. As a direct result we obtain tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational complexity or test accuracy. We perform an extensive study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods. We finish by using dropout uncertainty in a Bayesian pipeline, with deep reinforcement learning as a practical task.},
	urldate = {2015-10-27},
	journal = {arXiv:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02142},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3KWEXFMU/1506.html:text/html;Gal_Ghahramani_2015_Dropout as a Bayesian Approximation.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Gal_Ghahramani_2015_Dropout as a Bayesian Approximation.pdf:application/pdf}
}

@article{kheradpisheh-deep-2015,
	title = {Deep {Networks} {Resemble} {Human} {Feed}-forward {Vision} in {Invariant} {Object} {Recognition}},
	url = {http://arxiv.org/abs/1508.03929},
	abstract = {Deep convolutional neural networks (DCNN) have attracted much attention recently, and have been shown able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Thus it seems natural to compare their performance to that of humans. In particular, it is well known that humans excel at recognizing objects despite huge variations in viewpoints. It is not clear to what extent DCNNs also have this ability.To investigate this issue, we benchmarked 8 state-of-the-art DCNNs, the HMAX model, and a baseline model and compared the results to humans with backward masking. By carefully controlling the magnitude of the viewpoint variations, we show that using a few layers is sufficient to match human performance with small variations, but larger variations require more layers, that is deep, not shallow, nets. A very deep net with 19 layers even outperformed humans at the maximum variation level. Our results suggest that one important benefit of having more layers is to tolerate larger viewpoint variations. The main cost is that more training examples are needed.},
	urldate = {2015-10-27},
	journal = {arXiv:1508.03929 [cs, q-bio]},
	author = {Kheradpisheh, Saeed Reza and Ghodrati, Masoud and Ganjtabesh, Mohammad and Masquelier, Timothée},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.03929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AN8QWA9F/1508.html:text/html;Kheradpisheh et al_2015_Deep Networks Resemble Human Feed-forward Vision in Invariant Object Recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kheradpisheh et al_2015_Deep Networks Resemble Human Feed-forward Vision in Invariant Object Recognition.pdf:application/pdf}
}

@article{chan-listen-2015,
	title = {Listen, {Attend} and {Spell}},
	url = {http://arxiv.org/abs/1508.01211},
	abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
	urldate = {2015-10-27},
	journal = {arXiv:1508.01211 [cs, stat]},
	author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.01211},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NPZIWCZH/1508.html:text/html;Chan et al_2015_Listen, Attend and Spell.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Chan et al_2015_Listen, Attend and Spell.pdf:application/pdf}
}

@article{luong-effective-2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	urldate = {2015-10-27},
	journal = {arXiv:1508.04025 [cs]},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.04025},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4V4I9B2A/1508.html:text/html;Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf:application/pdf}
}

@article{rush-neural-2015,
	title = {A {Neural} {Attention} {Model} for {Abstractive} {Sentence} {Summarization}},
	url = {http://arxiv.org/abs/1509.00685},
	abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
	urldate = {2015-10-27},
	journal = {arXiv:1509.00685 [cs]},
	author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.00685},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/HX3GJP9E/1509.html:text/html;Rush et al_2015_A Neural Attention Model for Abstractive Sentence Summarization.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rush et al_2015_A Neural Attention Model for Abstractive Sentence Summarization.pdf:application/pdf}
}

@article{ardakani-vlsi-2015,
	title = {{VLSI} {Implementation} of {Deep} {Neural} {Network} {Using} {Integral} {Stochastic} {Computing}},
	url = {http://arxiv.org/abs/1509.08972},
	abstract = {The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral stochastic computing. The proposed architecture uses integer stochastic streams and a modified Finite State Machine-based tanh function to perform computations and even reduce the latency compared to conventional stochastic computation. The proposed architecture has been implemented on a Virtex7 FPGA, resulting in 44.96\% and 62.36\% average reductions in area and latency compared to the best reported architecture in literature. We also synthesize the circuits in a 65 nm CMOS technology and show that they can tolerate a fault rate of up to 20\% on some computations when timing violations are allowed to occur, resulting in power savings. The fault-tolerance property of the proposed architectures make them suitable for inherently unreliable advanced process technologies such as memristor technology.},
	urldate = {2015-10-27},
	journal = {arXiv:1509.08972 [cs]},
	author = {Ardakani, Arash and Leduc-Primeau, François and Onizawa, Naoya and Hanyu, Takahiro and Gross, Warren J.},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.08972},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture},
	file = {Ardakani et al_2015_VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ardakani et al_2015_VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/4QMHTX7E/1509.html:text/html}
}

@article{he-text-attentional-2015,
	title = {Text-{Attentional} {Convolutional} {Neural} {Networks} for {Scene} {Text} {Detection}},
	url = {http://arxiv.org/abs/1510.03283},
	abstract = {Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate the true text features in the deep representation, leading to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Networks (Text-CNN) that particularly focus on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/non-text information. The informative supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where the low-level supervised information greatly facilitates the main task of text/non-text classification. In addition, a powerful low-level detector called the Contrast-Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing the intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2011 and ICDAR 2013 datasets, with F-measure of 0.82 and 0.84, improving the state-of-the-art results substantially.},
	urldate = {2015-10-27},
	journal = {arXiv:1510.03283 [cs]},
	author = {He, Tong and Huang, Weilin and Qiao, Yu and Yao, Jian},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.03283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BW9PVBDP/1510.html:text/html;He et al_2015_Text-Attentional Convolutional Neural Networks for Scene Text Detection.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/He et al_2015_Text-Attentional Convolutional Neural Networks for Scene Text Detection.pdf:application/pdf}
}

@article{kruthiventi-deepfix:-2015,
	title = {{DeepFix}: {A} {Fully} {Convolutional} {Neural} {Network} for predicting {Human} {Eye} {Fixations}},
	shorttitle = {{DeepFix}},
	url = {http://arxiv.org/abs/1510.02927},
	abstract = {Understanding and predicting the human visual attentional mechanism is an active area of research in the fields of neuroscience and computer vision. In this work, we propose DeepFix, a first-of-its-kind fully convolutional neural network for accurate saliency prediction. Unlike classical works which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant which prevents them from modeling location dependent patterns (e.g. centre-bias). Our network overcomes this limitation by incorporating a novel Location Biased Convolutional layer. We evaluate our model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that it outperforms other recent approaches by a significant margin.},
	urldate = {2015-10-27},
	journal = {arXiv:1510.02927 [cs]},
	author = {Kruthiventi, Srinivas S. S. and Ayush, Kumar and Babu, R. Venkatesh},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.02927},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/DKDV64IH/1510.html:text/html;Kruthiventi et al_2015_DeepFix.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kruthiventi et al_2015_DeepFix.pdf:application/pdf}
}

@article{yu-empirical-2015,
	title = {Empirical {Study} on {Deep} {Learning} {Models} for {QA}},
	url = {http://arxiv.org/abs/1510.07526},
	abstract = {In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.},
	urldate = {2015-10-27},
	journal = {arXiv:1510.07526 [cs]},
	author = {Yu, Yang and Zhang, Wei and Hang, Chung-Wei and Zhou, Bowen},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.07526},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WG2EZCTQ/1510.html:text/html;Yu et al_2015_Empirical Study on Deep Learning Models for QA.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Yu et al_2015_Empirical Study on Deep Learning Models for QA.pdf:application/pdf}
}

@article{bowman-tree-structured-2015-1,
	title = {Tree-structured composition in neural networks without tree-structured architectures},
	url = {http://arxiv.org/abs/1506.04834},
	abstract = {Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find that the sequence model can learn the underlying patterning. The sequence model is better in that it learns the value of tree structure from the data in an emergent way, while the tree-structured model is better in being able to learn with greater statistical efficiency due to its informative prior model structure.},
	urldate = {2015-10-27},
	journal = {arXiv:1506.04834 [cs]},
	author = {Bowman, Samuel R. and Manning, Christopher D. and Potts, Christopher},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04834},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6SUHGZDQ/1506.html:text/html;Bowman et al_2015_Tree-structured composition in neural networks without tree-structured.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bowman et al_2015_Tree-structured composition in neural networks without tree-structured.pdf:application/pdf}
}

@article{bowman-learning-2014-1,
	title = {Learning {Distributed} {Word} {Representations} for {Natural} {Logic} {Reasoning}},
	url = {http://arxiv.org/abs/1410.4176},
	urldate = {2015-10-27},
	journal = {arXiv preprint arXiv:1410.4176},
	author = {Bowman, Samuel R. and Potts, Christopher and Manning, Christopher D.},
	year = {2014},
	file = {10027.pdf:/home/user/Zotero/storage/RNQPVDAT/10027.pdf:application/pdf}
}

@article{rocktaschel-reasoning-2015,
	title = {Reasoning about {Entailment} with {Neural} {Attention}},
	url = {http://arxiv.org/abs/1509.06664},
	abstract = {Automatically recognizing entailment relations between pairs of natural language sentences has so far been the dominion of classifiers employing hand engineered features derived from natural language processing pipelines. End-to-end differentiable neural architectures have failed to approach state-of-the-art performance until very recently. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.},
	urldate = {2015-10-27},
	journal = {arXiv:1509.06664 [cs]},
	author = {Rocktäschel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Kočiský, Tomáš and Blunsom, Phil},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.06664},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.6, I.2.7, 68T50},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RWXR4IJD/1509.html:text/html;Rocktäschel et al_2015_Reasoning about Entailment with Neural Attention.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rocktäschel et al_2015_Reasoning about Entailment with Neural Attention.pdf:application/pdf}
}

@article{olafsdottir-hippocampal-2015,
	title = {Hippocampal place cells construct reward related sequences through unexplored space},
	volume = {4},
	copyright = {© 2015, Ólafsdóttir et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/4/e06063},
	doi = {10.7554/eLife.06063},
	abstract = {Dominant theories of hippocampal function propose that place cell representations are formed during an animal's first encounter with a novel environment and are subsequently replayed during off-line states to support consolidation and future behaviour. Here we report that viewing the delivery of food to an unvisited portion of an environment leads to off-line pre-activation of place cells sequences corresponding to that space. Such ‘preplay’ was not observed for an unrewarded but otherwise similar portion of the environment. These results suggest that a hippocampal representation of a visible, yet unexplored environment can be formed if the environment is of motivational relevance to the animal. We hypothesise such goal-biased preplay may support preparation for future experiences in novel environments.DOI: http://dx.doi.org/10.7554/eLife.06063.001View Full TextTo Top
Dominant theories of hippocampal function propose that place cell representations are formed during an animal's first encounter with a novel environment and are subsequently replayed during off-line states to support consolidation and future behaviour. Here we report that viewing the delivery of food to an unvisited portion of an environment leads to off-line pre-activation of place cells sequences corresponding to that space. Such ‘preplay’ was not observed for an unrewarded but otherwise similar portion of the environment. These results suggest that a hippocampal representation of a visible, yet unexplored environment can be formed if the environment is of motivational relevance to the animal. We hypothesise such goal-biased preplay may support preparation for future experiences in novel environments.
DOI: http://dx.doi.org/10.7554/eLife.06063.001
As an animal explores an area, part of the brain called the hippocampus creates a mental map of the space. When the animal is in one location, a few neurons called ‘place cells’ will fire. If the animal moves to a new spot, other place cells fire instead. Each time the animal returns to that spot, the same place cells will fire. Thus, as the animal moves, a place-specific pattern of firing emerges that scientists can view by recording the cells' activity and which can be used to reconstruct the animal's position.
After exploring a space, the hippocampus may replay the new place-specific pattern of activity during sleep. By doing so, the brain consolidates the memory of the space for return visits. Recent evidence now suggests that these mental rehearsals—or internal simulations of the space—may begin even before a new space has been explored.
Now, Ólafsdóttir, Barry et al. report that whether an animal's brain simulates a first visit to a new space depends on whether the animal anticipates a reward. In the experiments, rats were allowed to run up to the junction in a T-shaped track. The animals could see into each of the arms, but not enter them. Food was then placed in one of the inaccessible arms. Ólafsdóttir, Barry et al. recorded the firing of place cells in the brain of the animals when they were on the track and during a rest period afterwards. The rats were then allowed onto the inaccessible arms, and again their brain activity was recorded.
In the rest period after the rats first viewed the inaccessible arms, the place cell pattern that would later form the mental map of a journey to and from the food-containing arm was pre-activated. However, the place cell pattern that would become the mental map of the other inaccessible arm was not activated before the rat explored that area. Therefore, Ólafsdóttir, Barry et al. suggest that the perception of reward influences which place cell pattern is simulated during rest. An implication of these findings is that the brain preferentially simulates past or future experiences that are deemed to be functionally significant, such as those associated with reward. A future challenge will be to determine whether this goal-related simulation of unvisited spaces predicts and is needed for behaviour such as successful navigation to a goal.
DOI: http://dx.doi.org/10.7554/eLife.06063.002},
	language = {en},
	urldate = {2015-10-27},
	journal = {eLife},
	author = {Ólafsdóttir, H. Freyja and Barry, Caswell and Saleem, Aman B. and Hassabis, Demis and Spiers, Hugo J.},
	month = jun,
	year = {2015},
	pages = {e06063},
	file = {Ólafsdóttir et al_2015_Hippocampal place cells construct reward related sequences through unexplored.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Ólafsdóttir et al_2015_Hippocampal place cells construct reward related sequences through unexplored.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/WWTBIIN8/e06063.html:text/html}
}

@article{henaff-local-2014,
	title = {The local low-dimensionality of natural images},
	url = {http://arxiv.org/abs/1412.6626},
	abstract = {We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norms of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and bandpass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariances alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.},
	urldate = {2015-10-27},
	journal = {arXiv:1412.6626 [cs]},
	author = {Hénaff, Olivier J. and Ballé, Johannes and Rabinowitz, Neil C. and Simoncelli, Eero P.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IH2Z37XA/1412.html:text/html;Hénaff et al_2014_The local low-dimensionality of natural images.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Hénaff et al_2014_The local low-dimensionality of natural images.pdf:application/pdf}
}

@article{raiko-techniques-2014,
	title = {Techniques for {Learning} {Binary} {Stochastic} {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1406.2989},
	abstract = {Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.},
	urldate = {2015-11-26},
	journal = {arXiv:1406.2989 [cs, stat]},
	author = {Raiko, Tapani and Berglund, Mathias and Alain, Guillaume and Dinh, Laurent},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2989},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XV33QDIK/1406.html:text/html;Raiko et al_2014_Techniques for Learning Binary Stochastic Feedforward Neural Networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Raiko et al_2014_Techniques for Learning Binary Stochastic Feedforward Neural Networks.pdf:application/pdf}
}

@article{srivastava-training-2015-1,
	title = {Training {Very} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1507.06228},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
	urldate = {2015-11-26},
	journal = {arXiv:1507.06228 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.06228},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, 68T01, G.1.6},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/FRMIUHWZ/1507.html:text/html;Srivastava et al_2015_Training Very Deep Networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Srivastava et al_2015_Training Very Deep Networks.pdf:application/pdf}
}

@article{dai-variational-2015,
	title = {Variational {Auto}-encoded {Deep} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1511.06455},
	abstract = {We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.},
	urldate = {2015-11-26},
	journal = {arXiv:1511.06455 [cs, stat]},
	author = {Dai, Zhenwen and Damianou, Andreas and González, Javier and Lawrence, Neil},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06455},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {1510.06807v1.pdf:/home/user/Zotero/storage/I2GN23DC/1510.06807v1.pdf:application/pdf;annurev-linguist-030514-125312.pdf:/home/user/Zotero/storage/E62RKQFU/annurev-linguist-030514-125312.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/G75CJQJG/1511.html:text/html;cogacll15_surprisal.pdf:/home/user/Zotero/storage/36WB8AHV/cogacll15_surprisal.pdf:application/pdf;Dai et al_2015_Variational Auto-encoded Deep Gaussian Processes.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Dai et al_2015_Variational Auto-encoded Deep Gaussian Processes.pdf:application/pdf;paper527.pdf:/home/user/Zotero/storage/HCIU96IU/paper527.pdf:application/pdf}
}

@article{serban-building-2015,
	title = {Building {End}-{To}-{End} {Dialogue} {Systems} {Using} {Generative} {Hierarchical} {Neural} {Network} {Models}},
	url = {http://arxiv.org/abs/1507.04808},
	abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
	urldate = {2015-11-26},
	journal = {arXiv:1507.04808 [cs]},
	author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.04808},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.7, I.5.1},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZEQAV6G6/1507.html:text/html;Serban et al_2015_Building End-To-End Dialogue Systems Using Generative Hierarchical Neural.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Serban et al_2015_Building End-To-End Dialogue Systems Using Generative Hierarchical Neural.pdf:application/pdf}
}

@article{tresp-learning-2015,
	title = {Learning with {Memory} {Embeddings}},
	url = {http://arxiv.org/abs/1511.07972},
	abstract = {Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. In recent publications the embedding models were extended to also consider temporal evolutions, temporal patterns and subsymbolic representations. These extended models were used successfully to predict clinical events like procedures, lab measurements, and diagnoses. In this paper, we attempt to map these embedding models, which were developed purely as solutions to technical problems, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory and sensory memory. We also make an analogy between a predictive model, which uses entity representations derived in memory models, to working memory. Cognitive memory functions are typically classified as long-term or short-term memory, where long-term memory has the subcategories declarative memory and non-declarative memory and the short term memory has the subcategories sensory memory and working memory. There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage. However, there is also evidence indicating that the different memory functions are not mutually independent. A hypothesis that arises out off this work is that mutual information exchange can be achieved by sharing or coupling of distributed latent representations of entities across different memory functions.},
	urldate = {2015-11-26},
	journal = {arXiv:1511.07972 [cs]},
	author = {Tresp, Volker and Esteban, Cristóbal and Yang, Yinchong and Baier, Stephan and Krompaß, Denis},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07972},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XG4ZUAIQ/1511.html:text/html;Tresp et al_2015_Learning with Memory Embeddings.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Tresp et al_2015_Learning with Memory Embeddings.pdf:application/pdf}
}

@article{wieting-towards-2015,
	title = {Towards {Universal} {Paraphrastic} {Sentence} {Embeddings}},
	url = {http://arxiv.org/abs/1511.08198},
	abstract = {In this paper, we show how to create paraphrastic sentence embeddings using the Paraphrase Database (Ganitkevitch et al., 2013), an extensive semantic resource with millions of phrase pairs. We consider several compositional architectures and evaluate them on 24 textual similarity datasets encompassing domains such as news, tweets, web forums, news headlines, machine translation output, glosses, and image and video captions. We present the interesting result that simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks and that these simpler architectures allow us to learn models with superior generalization. Our models are efficient, very easy to use, and competitive with task-tuned systems. We make them available to the research community1 with the hope that they can serve as the new baseline for further work on universal paraphrastic sentence embeddings.},
	urldate = {2015-11-26},
	journal = {arXiv:1511.08198 [cs]},
	author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08198},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZURZPZDQ/1511.html:text/html;Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:application/pdf}
}

@article{cho-natural-2015,
	title = {Natural {Language} {Understanding} with {Distributed} {Representation}},
	url = {http://arxiv.org/abs/1511.07916},
	abstract = {This is a lecture note for the course DS-GA 3001 {\textless}Natural Language Understanding with Distributed Representation{\textgreater} at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.},
	urldate = {2015-11-26},
	journal = {arXiv:1511.07916 [cs, stat]},
	author = {Cho, Kyunghyun},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07916},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/N8XFJUS7/1511.html:text/html;Cho_2015_Natural Language Understanding with Distributed Representation.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Cho_2015_Natural Language Understanding with Distributed Representation.pdf:application/pdf}
}

@misc{noauthor-non-uniform-nodate,
	title = {Non-{Uniform} {Random} {Variate} {Generation}},
	url = {http://luc.devroye.org/rnbookindex.html},
	urldate = {2015-11-26},
	file = {Non-Uniform Random Variate Generation:/home/user/Zotero/storage/7BBA99MG/rnbookindex.html:text/html}
}

@misc{noauthor-the-vowel-system-of-jibbali.pdf-nodate,
	title = {The\_Vowel\_System\_of\_Jibbali.pdf},
	file = {The_Vowel_System_of_Jibbali.pdf:/home/user/Zotero/storage/XWECDMH5/The_Vowel_System_of_Jibbali.pdf:application/pdf}
}

@inproceedings{gal-systematic-2013,
	title = {A {Systematic} {Bayesian} {Treatment} of the {IBM} {Alignment} {Models}.},
	url = {http://www.anthology.aclweb.org/N/N13/N13-1.pdf#page=1007},
	urldate = {2015-11-25},
	booktitle = {{HLT}-{NAACL}},
	author = {Gal, Yarin and Blunsom, Phil},
	year = {2013},
	pages = {969--977},
	file = {GalBlu13.pdf:/home/user/Zotero/storage/PHUARU5C/GalBlu13.pdf:application/pdf}
}

@book{rasmussen-gaussian-2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {On_modern_deep_learning_and_VI.pdf:/home/user/Zotero/storage/XZ6KSPBX/On_modern_deep_learning_and_VI.pdf:application/pdf}
}

@book{rasmussen-gaussian-2006-1,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {damianou13a.pdf:/home/user/Zotero/storage/GGKTMJ9P/damianou13a.pdf:application/pdf}
}

@article{gal-bayesian-2015,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Bernoulli} {Approximate} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1506.02158},
	abstract = {Convolutional neural networks (convnets) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use convnets with small data -- as convnets overfit quickly. We present an efficient Bayesian convnet, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the convnet's kernels. To make this possible we present new theoretical results casting dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in the field with no increase in time complexity. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. We show a considerable improvement in classification accuracy compared to standard techniques with state-of-the-art results on CIFAR-10.},
	urldate = {2015-11-25},
	journal = {arXiv:1506.02158 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02158},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1506.02158 PDF:/home/user/Zotero/storage/CSFAZRXG/Gal and Ghahramani - 2015 - Bayesian Convolutional Neural Networks with Bernou.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/VI2XFFGU/1506.html:text/html}
}

@article{gal-dropout-2015-1,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2015-11-25},
	journal = {arXiv:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02142},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1506.02142 PDF:/home/user/Zotero/storage/GHNDS4QA/Gal and Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/K2U6S4PW/1506.html:text/html}
}

@article{gal-variational-2014,
	title = {Variational {Inference} in {Sparse} {Gaussian} {Process} {Regression} and {Latent} {Variable} {Models} - a {Gentle} {Tutorial}},
	url = {http://arxiv.org/abs/1402.1412},
	abstract = {In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \& Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.},
	urldate = {2015-11-25},
	journal = {arXiv:1402.1412 [stat]},
	author = {Gal, Yarin and van der Wilk, Mark},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.1412},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1402.1412 PDF:/home/user/Zotero/storage/UQ96BFC3/Gal and van der Wilk - 2014 - Variational Inference in Sparse Gaussian Process R.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/DBTFNHHB/1402.html:text/html}
}

@article{way-dependency-nodate,
	title = {Dependency {Graph}-to-{String} {Translation}},
	url = {https://www.cs.cmu.edu/~ark/EMNLP-2015/proceedings/EMNLP/pdf/EMNLP004.pdf},
	urldate = {2015-11-24},
	author = {Way, Liangyou Li Andy and Liu, Qun},
	file = {Dependency Graph-to-String Translation - D15-1004.pdf:/home/user/Zotero/storage/FRJ84WRN/D15-1004.pdf:application/pdf}
}

@article{rezende-variational-2015,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	urldate = {2015-11-24},
	journal = {arXiv preprint arXiv:1505.05770},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	year = {2015},
	file = {Variational Inference with Normalizing Flows - rezende15.pdf:/home/user/Zotero/storage/6XPZTQ6D/rezende15.pdf:application/pdf}
}

@inproceedings{lauly-autoencoder-2014,
	title = {An autoencoder approach to learning bilingual word representations},
	url = {http://papers.nips.cc/paper/5270-an-auto},
	urldate = {2015-11-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lauly, Stanislas and Larochelle, Hugo and Khapra, Mitesh and Ravindran, Balaraman and Raykar, Vikas C. and Saha, Amrita},
	year = {2014},
	pages = {1853--1861},
	file = {An Autoencoder Approach to Learning Bilingual Word Representations - 5270-an-autoencoder-approach-to-learning-bilingual-word-representations.pdf:/home/user/Zotero/storage/C2TWE98U/5270-an-autoencoder-approach-to-learning-bilingual-word-representations.pdf:application/pdf}
}

@inproceedings{mirowski-dynamic-2010,
	title = {Dynamic auto-encoders for semantic indexing},
	url = {http://www.academia.edu/download/31000666/mirowski-nipsdl-10.pdf},
	urldate = {2015-11-24},
	booktitle = {Proceedings of the {NIPS} 2010 {Workshop} on {Deep} {Learning}},
	author = {Mirowski, Piotr and Ranzato, M. and LeCun, Yann},
	year = {2010},
	file = {mirowski-nipsdl-10.pdf:/home/user/Zotero/storage/HFTRVGNS/mirowski-nipsdl-10.pdf:application/pdf}
}

@inproceedings{socher-semi-supervised-2011,
	title = {Semi-supervised recursive autoencoders for predicting sentiment distributions},
	url = {http://dl.acm.org/citation.cfm?id=2145450},
	urldate = {2015-11-24},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
	year = {2011},
	pages = {151--161},
	file = {SocherPenningtonHuangNgManning_EMNLP2011.pdf:/home/user/Zotero/storage/73E3NN8F/SocherPenningtonHuangNgManning_EMNLP2011.pdf:application/pdf}
}

@inproceedings{salakhutdinov-deep-2009,
	title = {Deep boltzmann machines},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS09\_SalakhutdinovH.pdf},
	urldate = {2015-11-24},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey E.},
	year = {2009},
	pages = {448--455},
	file = {salakhutdinov09a.pdf:/home/user/Zotero/storage/EHKQPPB6/salakhutdinov09a.pdf:application/pdf}
}

@article{jordan-role-2015,
	title = {The role of complex systems theory in cognitive science},
	volume = {16},
	issn = {1612-4782, 1612-4790},
	url = {http://link.springer.com/10.1007/s10339-015-0739-0},
	doi = {10.1007/s10339-015-0739-0},
	language = {en},
	number = {4},
	urldate = {2015-11-24},
	journal = {Cognitive Processing},
	author = {Jordan, J. Scott and Srinivasan, Narayanan and van Leeuwen, Cees},
	month = nov,
	year = {2015},
	pages = {315--317},
	file = {The role of complex systems theory in cognitive science - art%3A10.1007%2Fs10339-015-0739-0.pdf:/home/user/Zotero/storage/CVK3HSI3/art%3A10.1007%2Fs10339-015-0739-0.pdf:application/pdf}
}

@article{schiebener-self-reported-2015,
	title = {Self-reported strategies in decisions under risk: role of feedback, reasoning abilities, executive functions, short-term-memory, and working memory},
	volume = {16},
	issn = {1612-4782, 1612-4790},
	shorttitle = {Self-reported strategies in decisions under risk},
	url = {http://link.springer.com/10.1007/s10339-015-0665-1},
	doi = {10.1007/s10339-015-0665-1},
	language = {en},
	number = {4},
	urldate = {2015-11-24},
	journal = {Cognitive Processing},
	author = {Schiebener, Johannes and Brand, Matthias},
	month = nov,
	year = {2015},
	pages = {401--416},
	file = {Self-reported strategies in decisions under risk\: role of feedback, reasoning abilities, executive functions, short-term-memory, and working memory - art%3A10.1007%2Fs10339-015-0665-1.pdf:/home/user/Zotero/storage/92VNADR2/art%3A10.1007%2Fs10339-015-0665-1.pdf:application/pdf}
}

@article{yamaoka-spatial-2015,
	title = {Spatial distribution of attention and inter-hemispheric competition},
	volume = {16},
	issn = {1612-4782, 1612-4790},
	url = {http://link.springer.com/10.1007/s10339-015-0734-5},
	doi = {10.1007/s10339-015-0734-5},
	language = {en},
	number = {4},
	urldate = {2015-11-24},
	journal = {Cognitive Processing},
	author = {Yamaoka, Kao and Michimata, Chikashi},
	month = nov,
	year = {2015},
	pages = {417--425},
	file = {Spatial distribution of attention and inter-hemispheric competition - art%3A10.1007%2Fs10339-015-0734-5.pdf:/home/user/Zotero/storage/43IZHXNW/art%3A10.1007%2Fs10339-015-0734-5.pdf:application/pdf}
}

@article{buz-dependence-2015,
	title = {The (in)dependence of articulation and lexical planning during isolated word production},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1105984},
	doi = {10.1080/23273798.2015.1105984},
	abstract = {The number of phonological neighbours to a word (PND) can affect its lexical planning and pronunciation. Similar parallel effects on planning and articulation have been observed for other lexical variables, such as a word's contextual predictability. Such parallelism is frequently taken to indicate that effects on articulation are mediated by effects on the time course of lexical planning. We test this mediation assumption for PND and find it unsupported. In a picture naming experiment, we measure speech onset latencies (planning), word durations, and vowel dispersion (articulation). We find that PND predicts both latencies and durations. Further, latencies predict durations. However, the effects of PND and latency on duration are independent: parallel effects do not imply mediation. We discuss the consequences for accounts of lexical planning, articulation, and the link between them. In particular, our results suggest that ease of planning does not explain effects of PND on articulation.},
	number = {0},
	urldate = {2015-11-24},
	journal = {Language, Cognition and Neuroscience},
	author = {Buz, Esteban and Jaeger, T. Florian},
	month = nov,
	year = {2015},
	pages = {1--21},
	file = {Full Text PDF:/home/user/Zotero/storage/Z9WAG9QE/Buz and Jaeger - 2015 - The (in)dependence of articulation and lexical pla.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IVP4ZEJX/23273798.2015.html:text/html}
}

@article{andreas-deep-2015-1,
	title = {Deep {Compositional} {Question} {Answering} with {Neural} {Module} {Networks}},
	url = {http://arxiv.org/abs/1511.02799},
	abstract = {Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
	urldate = {2015-11-24},
	journal = {arXiv:1511.02799 [cs]},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02799},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.02799 PDF:/home/user/Zotero/storage/DSCS45SH/Andreas et al. - 2015 - Deep Compositional Question Answering with Neural .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/S8ERC6IK/1511.html:text/html}
}

@article{lazaridou-unveiling-2015,
	title = {Unveiling the {Dreams} of {Word} {Embeddings}: {Towards} {Language}-{Driven} {Image} {Generation}},
	shorttitle = {Unveiling the {Dreams} of {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1506.03500},
	abstract = {We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.},
	urldate = {2015-11-24},
	journal = {arXiv:1506.03500 [cs]},
	author = {Lazaridou, Angeliki and Nguyen, Dat Tien and Bernardi, Raffaella and Baroni, Marco},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03500},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.03500 PDF:/home/user/Zotero/storage/UBIIHPDU/Lazaridou et al. - 2015 - Unveiling the Dreams of Word Embeddings Towards L.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/F5A662K4/1506.html:text/html}
}

@article{kottur-visual-2015,
	title = {Visual {Word}2Vec (vis-w2v): {Learning} {Visually} {Grounded} {Word} {Embeddings} {Using} {Abstract} {Scenes}},
	shorttitle = {Visual {Word}2Vec (vis-w2v)},
	url = {http://arxiv.org/abs/1511.07067},
	abstract = {We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.},
	urldate = {2015-11-24},
	journal = {arXiv:1511.07067 [cs]},
	author = {Kottur, Satwik and Vedantam, Ramakrishna and Moura, José M. F. and Parikh, Devi},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07067},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.07067 PDF:/home/user/Zotero/storage/FT9VJV7T/Kottur et al. - 2015 - Visual Word2Vec (vis-w2v) Learning Visually Groun.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/ZPT8KPU9/1511.html:text/html}
}

@article{lee-linear-2015,
	title = {On the {Linear} {Algebraic} {Structure} of {Distributed} {Word} {Representations}},
	url = {http://arxiv.org/abs/1511.06961},
	abstract = {In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for.},
	urldate = {2015-11-24},
	journal = {arXiv:1511.06961 [cs]},
	author = {Lee, Lisa Seung-Yeon},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06961},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1511.06961 PDF:/home/user/Zotero/storage/HUGG7N8J/Lee - 2015 - On the Linear Algebraic Structure of Distributed W.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/46RQ3UCC/1511.html:text/html}
}

@article{senju-early-nodate,
	title = {Early {Social} {Experience} {Affects} the {Development} of {Eye} {Gaze} {Processing}},
	volume = {0},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S0960982215012476/abstract},
	doi = {10.1016/j.cub.2015.10.019},
	abstract = {Eye gaze is a key channel of non-verbal communication in humans [ 1–3 ]. Eye contact with others is present from birth [ 4 ], and eye gaze processing is crucial for social learning and adult-infant communication [ 5–7 ]. However, little is known about the effect of selectively different experience of eye contact and gaze communication on early social and communicative development. To directly address this question, we assessed 14 sighted infants of blind parents (SIBPs) longitudinally at 6–10 and 12–16 months. Face scanning [ 8 ] and gaze following [ 7, 9 ] were assessed using eye tracking. In addition, naturalistic observations were made when the infants were interacting with their blind parent and with an unfamiliar sighted adult. Established measures of emergent autistic-like behaviors [ 10 ] and standardized tests of cognitive, motor, and linguistic development [ 11 ] were also collected. These data were then compared with those obtained from a group of infants of sighted parents. Despite showing typical social skills development overall, infants of blind parents allocated less attention to adult eye movements and gaze direction, an effect that increased between 6–10 and 12–16 months of age. The results suggest that infants adjust their use of adults’ eye gaze depending on gaze communication experience from early in life. The results highlight that human functional brain development shows selective experience-dependent plasticity adaptive to the individual’s specific social environment.},
	language = {English},
	number = {0},
	urldate = {2015-11-24},
	journal = {Current Biology},
	author = {Senju, Atsushi and Vernetti, Angélina and Ganea, Natasa and Hudry, Kristelle and Tucker, Leslie and Charman, Tony and Johnson, Mark H.},
	file = {Full Text PDF:/home/user/Zotero/storage/ND26PZ27/Senju et al. - Early Social Experience Affects the Development of.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/T4GSQD5U/S0960-9822(15)01247-6.html:text/html}
}

@article{lauly-learning-2014,
	title = {Learning {Multilingual} {Word} {Representations} using a {Bag}-of-{Words} {Autoencoder}},
	url = {http://arxiv.org/abs/1401.1803},
	abstract = {Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g. infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages. In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements. The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation. We evaluate our approach on a multilingual document classification task, where labeled data is available only for one language (e.g. English) while classification must be performed in a different language (e.g. French). In our experiments, we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations.},
	urldate = {2015-11-21},
	journal = {arXiv:1401.1803 [cs, stat]},
	author = {Lauly, Stanislas and Boulanger, Alex and Larochelle, Hugo},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.1803},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2I7VWKCZ/1401.html:text/html;Lauly et al_2014_Learning Multilingual Word Representations using a Bag-of-Words Autoencoder.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Lauly et al_2014_Learning Multilingual Word Representations using a Bag-of-Words Autoencoder.pdf:application/pdf}
}

@article{kingma-adam:-2014-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2015-11-20},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QK3STGJF/1412.html:text/html;Kingma_Ba_2014_Adam.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kingma_Ba_2014_Adam.pdf:application/pdf}
}

@article{kiros-skip-thought-2015,
	title = {Skip-{Thought} {Vectors}},
	url = {http://arxiv.org/abs/1506.06726},
	abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
	urldate = {2015-11-20},
	journal = {arXiv:1506.06726 [cs]},
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.06726},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/JT7FG8CN/1506.html:text/html;Kiros et al_2015_Skip-Thought Vectors.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kiros et al_2015_Skip-Thought Vectors.pdf:application/pdf}
}

@article{ellis-information-2015,
	title = {Information theoretic complexity affects multisensory perception},
	volume = {23},
	issn = {1350-6285, 1464-0716},
	url = {http://www.tandfonline.com/doi/full/10.1080/13506285.2015.1093237},
	doi = {10.1080/13506285.2015.1093237},
	language = {en},
	number = {7},
	urldate = {2015-11-20},
	journal = {Visual Cognition},
	author = {Ellis, Cameron T. and Turk-Browne, Nicholas B.},
	month = aug,
	year = {2015},
	pages = {825--829},
	file = {untitled - 13506285.2015.1093237:/home/user/Zotero/storage/ABA4P388/13506285.2015.pdf:application/pdf}
}

@article{luong-multi-task-2015,
	title = {Multi-task {Sequence} to {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.06114},
	abstract = {Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three settings to multi-task sequence to sequence learning: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on parsing and image caption generation improves translation accuracy and vice versa. We also present novel findings on the benefit of the different unsupervised learning objectives: we found that the skip-thought objective is beneficial to translation while the sequence autoencoder objective is not.},
	urldate = {2015-11-20},
	journal = {arXiv:1511.06114 [cs, stat]},
	author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06114},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BTIM6INI/1511.html:text/html;Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:application/pdf}
}

@article{cogswell-reducing-2015,
	title = {Reducing {Overfitting} in {Deep} {Networks} by {Decorrelating} {Representations}},
	url = {http://arxiv.org/abs/1511.06068},
	abstract = {One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.},
	urldate = {2015-11-20},
	journal = {arXiv:1511.06068 [cs, stat]},
	author = {Cogswell, Michael and Ahmed, Faruk and Girshick, Ross and Zitnick, Larry and Batra, Dhruv},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06068},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/36K9WFJ5/1511.html:text/html;Cogswell et al_2015_Reducing Overfitting in Deep Networks by Decorrelating Representations.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Cogswell et al_2015_Reducing Overfitting in Deep Networks by Decorrelating Representations.pdf:application/pdf}
}

@inproceedings{sutskever-importance-2013-1,
	title = {On the importance of initialization and momentum in deep learning},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013\_sutskever13},
	urldate = {2015-11-19},
	booktitle = {Proceedings of the 30th international conference on machine learning ({ICML}-13)},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	year = {2013},
	pages = {1139--1147},
	file = {1051_2 - sutskever13.pdf:/home/user/Zotero/storage/7T3BCSKW/sutskever13.pdf:application/pdf}
}

@misc{noauthor-generative-nodate,
	title = {generative process sampling auto-encoders},
	url = {http://icml.cc/2012/papers/910.pdf},
	urldate = {2015-11-18},
	file = {910.pdf:/home/user/Zotero/storage/JMZFA73P/910.pdf:application/pdf}
}

@article{stanford-complexity-2014,
	title = {Complexity and shock wave geometries},
	volume = {90},
	issn = {1550-7998, 1550-2368},
	url = {http://link.aps.org/doi/10.1103/PhysRevD.90.126007},
	doi = {10.1103/PhysRevD.90.126007},
	language = {en},
	number = {12},
	urldate = {2015-11-19},
	journal = {Physical Review D},
	author = {Stanford, Douglas and Susskind, Leonard},
	month = dec,
	year = {2014},
	file = {untitled - PhysRevD.90.126007:/home/user/Zotero/storage/I4F8BJUD/PhysRevD.90.pdf:application/pdf}
}

@misc{sander-dieleman-3x-nodate,
	title = {3x faster convolutions in {Theano}},
	url = {http://benanne.github.io/2014/04/03/faster-convolutions-in-theano.html},
	abstract = {How to use the cuda-convnet convolution implementation from Theano},
	urldate = {2015-11-19},
	journal = {Sander Dieleman},
	author = {Sander Dieleman},
	file = {Snapshot:/home/user/Zotero/storage/675GQVIQ/faster-convolutions-in-theano.html:text/html}
}

@article{gross-how-2015,
	title = {How nature copes with climate change},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215013524},
	doi = {10.1016/j.cub.2015.10.058},
	abstract = {Summary
As the world is about to find out whether or not our civilisation is up to the challenge of dealing with climate change, research shows a wide range of responses from other species, which may benefit or suffer from the change, and mitigate it or make it worse. Michael Gross reports.},
	number = {22},
	urldate = {2015-11-19},
	journal = {Current Biology},
	author = {Gross, Michael},
	month = nov,
	year = {2015},
	pages = {R1057--R1059},
	file = {How nature copes with climate change - 1-s2.0-S0960982215013524-main.pdf:/home/user/Zotero/storage/RP3NFACM/1-s2.0-S0960982215013524-main.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/G3N8G86A/S0960982215013524.html:text/html}
}

@article{tan-lstm-based-2015,
	title = {{LSTM}-based {Deep} {Learning} {Models} for non-factoid answer selection},
	url = {http://arxiv.org/abs/1511.04108},
	abstract = {In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework, the other is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. Experimental results on a public insurance-domain dataset demonstrate that the extended models substantially outperform two state-of-the-art non-DL baselines and a strong DL baseline.},
	urldate = {2015-11-19},
	journal = {arXiv:1511.04108 [cs]},
	author = {Tan, Ming and Xiang, Bing and Zhou, Bowen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04108},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1511.04108 PDF:/home/user/Zotero/storage/JDB66PIT/Tan et al. - 2015 - LSTM-based Deep Learning Models for non-factoid an.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/IPJ3ABA7/1511.html:text/html}
}

@article{meng-deep-2015,
	title = {A {Deep} {Memory}-based {Architecture} for {Sequence}-to-{Sequence} {Learning}},
	url = {http://arxiv.org/abs/1506.06442},
	abstract = {We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequence-to-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improves upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.},
	urldate = {2015-11-19},
	journal = {arXiv:1506.06442 [cs]},
	author = {Meng, Fandong and Lu, Zhengdong and Tu, Zhaopeng and Li, Hang and Liu, Qun},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.06442},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv\:1506.06442 PDF:/home/user/Zotero/storage/BQNN7ZSP/Meng et al. - 2015 - A Deep Memory-based Architecture for Sequence-to-S.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/JKH5X68Z/1506.html:text/html}
}

@article{ropovik-cautionary-2015,
	title = {A cautionary note on testing latent variable models},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01715/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w47-2015},
	doi = {10.3389/fpsyg.2015.01715},
	abstract = {The article tackles the practice of testing latent variable models. The analysis covered recently published studies from 11 psychology journals varying in orientation and impact. Seventy-five studies that matched the criterion of applying some of the latent modeling techniques were reviewed. Results indicate the presence of a general tendency to ignore the model test (χ2) followed by the acceptance of approximate fit hypothesis without detailed model examination yielding relevant empirical evidence. Due to reduced sensitivity of such a procedure to confront theory with data, there is an almost invariable tendency to accept the theoretical model. This absence of model test consequences, manifested in frequently unsubstantiated neglect of evidence speaking against the model, thus implies the perilous question of whether such empirical testing of latent structures (the way it is widely applied) makes sense at all.},
	urldate = {2015-11-19},
	journal = {Quantitative Psychology and Measurement},
	author = {Ropovik, Ivan},
	year = {2015},
	keywords = {approximate fit indices, chi square test, confirmatory factor analysis, model fit, structural equation modeling},
	pages = {1715},
	file = {Full Text PDF:/home/user/Zotero/storage/2JEU6GXQ/Ropovik - 2015 - A cautionary note on testing latent variable model.pdf:application/pdf}
}

@article{leiken-filling-2015,
	title = {Filling {Predictable} and {Unpredictable} {Gaps}, with and without {Similarity}-{Based} {Interference}: {Evidence} for {LIFG} {Effects} of {Dependency} {Processing}},
	shorttitle = {Filling {Predictable} and {Unpredictable} {Gaps}, with and without {Similarity}-{Based} {Interference}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01739/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w47-2015},
	doi = {10.3389/fpsyg.2015.01739},
	abstract = {One of the most replicated findings in neurolinguistic literature on syntax is the increase of hemodynamic activity in the left inferior frontal gyrus (LIFG) in response to object relative (OR) clauses compared to subject relative clauses. However, behavioral studies have shown that ORs are primarily only costly when similarity-based interference is involved and recently, Leiken and Pylkkänen (2014) showed with magnetoencephalography (MEG) that an LIFG increase at an OR gap is also dependent on such interference. However, since ORs always involve a cue indicating an upcoming dependency formation, OR dependencies could be processed already prior to the gap-site and thus show no sheer dependency effects at the gap itself. To investigate the role of gap predictability in LIFG dependency effects, this MEG study compared ORs to verb phrase ellipsis (VPE), which was used as an example of a non-predictable dependency. Additionally, we explored LIFG sensitivity to filler-gap order by including right node raising structures, in which the order of filler and gap is reverse to that of ORs and VPE. Half of the stimuli invoked similarity-based interference and half did not. Our results demonstrate that LIFG effects of dependency can be elicited regardless of whether the dependency is predictable, the stimulus materials evoke similarity-based interference, or the filler precedes the gap. Thus, contrary to our own prior data, the current findings suggest a highly general role for the LIFG in dependency interpretation that is not limited to environments involving similarity-based interference. Additionally, the millisecond time-resolution of MEG allowed for a detailed characterization of the temporal profiles of LIFG dependency effects across our three constructions, revealing that the timing of these effects is somewhat construction-specific.},
	urldate = {2015-11-19},
	journal = {Language Sciences},
	author = {Leiken, Kimberly and McElree, Brian and Pylkkänen, Liina},
	year = {2015},
	keywords = {Magnetoencephalography, Filler-gap dependency, left inferior frontal gyrus, neurolinguistics, object relative clause, right node raising, similarity-based interference, verb phrase ellipsis},
	pages = {1739},
	file = {Full Text PDF:/home/user/Zotero/storage/BWRTA3AH/Leiken et al. - 2015 - Filling Predictable and Unpredictable Gaps, with a.pdf:application/pdf}
}

@article{garcia-commentary:-2015,
	title = {Commentary: {Attentional} control and the self: {The} {Self}-{Attention} {Network} ({SAN})},
	shorttitle = {Commentary},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01726/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w47-2015},
	doi = {10.3389/fpsyg.2015.01726},
	urldate = {2015-11-19},
	journal = {Language Sciences},
	author = {García, Adolfo M. and Huepe, David and Martinez, David and Morales, Juan P. and Huepe, Daniela and Hurtado, Esteban and Calvo, Noelia and Ibáñez, Agustín},
	year = {2015},
	keywords = {familiarity, own-face effect, own-name effect, Self-Attention Network model, ventromedial prefrontal cortex (vmPFC)},
	pages = {1726},
	file = {Full Text PDF:/home/user/Zotero/storage/3NC6D75E/García et al. - 2015 - Commentary Attentional control and the self The .pdf:application/pdf}
}

@article{ma-reactive-2016,
	title = {Reactive and proactive control in bilingual word production: {An} investigation of influential factors},
	volume = {86},
	issn = {0749-596X},
	shorttitle = {Reactive and proactive control in bilingual word production},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X1500100X},
	doi = {10.1016/j.jml.2015.08.004},
	abstract = {The present study examined how reactive control (indexed by switching costs) and proactive control (indexed by mixing costs) during bilingual language production was modulated by three factors reflected by different time-courses of stimulus presentation. In three experiments, unbalanced Chinese–English bilinguals named digits in Chinese or English according to a naming cue. In Experiment 1, switching costs reduced when participants had longer preparation time to select the target language to name digits (during the Cue-Stimulus interval, CSI), indicating that longer preparation time helps overcome reactive inhibition. In addition, mixing costs declined drastically at a longer preparation time, indicating that a tiny amount of preparation time allows bilinguals to overcome costs associated with proactively preparing two languages. In Experiment 2, the stimuli were presented prior to the cues, so that participants were given different amounts of time to activate the target lexical nodes in both languages before they were informed of the naming language (during the Stimulus-Cue interval, SCI). Symmetrical switching and mixing costs were observed, indicating that bilinguals can strategically boost activation of the target lexical item in the second language (L2) and attempt to equalize it with its translation equivalent in the native language (L1), when they know previously the specific lexical items to be prepared in two languages. In Experiment 3, different Response-Cue intervals (RCIs) were provided after participants named a digit. It was found that the switching cost asymmetry was more prominent when the time to resolve competition was shorter, while the mixing cost asymmetry emerged only with the longest waiting time. These findings provide the first piece of evidence for the dissipation of the reactive inhibition over time, and suggest that longer preparation would allow the proactive control mechanism to be sensitive the relative proficiency levels of the two languages, leading to stronger proactive control on the dominant language. Taken together, the findings in the present study suggest the dynamic nature of reactive and proactive control in unbalanced bilinguals and have important implications for the current models of bilingual language production, which do not explicitly distinguish the two types of control or address how they adapt to the fine-grained time course of the situation.},
	urldate = {2015-11-19},
	journal = {Journal of Memory and Language},
	author = {Ma, Fengyang and Li, Shengcao and Guo, Taomei},
	month = jan,
	year = {2016},
	keywords = {Bilingual language production, Inhibitory control, Proactive control, Reactive control},
	pages = {35--59},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/E3C45U8V/S0749596X1500100X.html:text/html}
}

@article{wagner-why-2015,
	title = {Why is number word learning hard? {Evidence} from bilingual learners},
	volume = {83},
	issn = {0010-0285},
	shorttitle = {Why is number word learning hard?},
	url = {http://www.sciencedirect.com/science/article/pii/S001002851500064X},
	doi = {10.1016/j.cogpsych.2015.08.006},
	abstract = {Young children typically take between 18 months and 2 years to learn the meanings of number words. In the present study, we investigated this developmental trajectory in bilingual preschoolers to examine the relative contributions of two factors in number word learning: (1) the construction of numerical concepts, and (2) the mapping of language specific words onto these concepts. We found that children learn the meanings of small number words (i.e., one, two, and three) independently in each language, indicating that observed delays in learning these words are attributable to difficulties in mapping words to concepts. In contrast, children generally learned to accurately count larger sets (i.e., five or greater) simultaneously in their two languages, suggesting that the difficulty in learning to count is not tied to a specific language. We also replicated previous studies that found that children learn the counting procedure before they learn its logic – i.e., that for any natural number, n, the successor of n in the count list denotes the cardinality n + 1. Consistent with past studies, we found that children’s knowledge of successors is first acquired incrementally. In bilinguals, we found that this knowledge exhibits item-specific transfer between languages, suggesting that the logic of the positive integers may not be stored in a language-specific format. We conclude that delays in learning the meanings of small number words are mainly due to language-specific processes of mapping words to concepts, whereas the logic and procedures of counting appear to be learned in a format that is independent of a particular language and thus transfers rapidly from one language to the other in development.},
	urldate = {2015-11-19},
	journal = {Cognitive Psychology},
	author = {Wagner, Katie and Kimura, Katherine and Cheung, Pierina and Barner, David},
	month = dec,
	year = {2015},
	keywords = {language acquisition, Cognitive development, bilingualism, Counting, Number word learning},
	pages = {1--21},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/MAH7G5CT/S001002851500064X.html:text/html}
}

@article{corgnet-cognitive-2015,
	title = {The cognitive basis of social behavior: cognitive reflection overrides antisocial but not always prosocial motives},
	shorttitle = {The cognitive basis of social behavior},
	url = {http://journal.frontiersin.org/article/10.3389/fnbeh.2015.00287/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Neuroscience-w47-2015},
	doi = {10.3389/fnbeh.2015.00287},
	abstract = {Even though human social behavior has received considerable scientific attention in the last decades, its cognitive underpinnings are still poorly understood. Applying a dual-process framework to the study of social preferences, we show in two studies that individuals with a more reflective/deliberative cognitive style, as measured by scores on the Cognitive Reflection Test (CRT), are more likely to make choices consistent with “mild” altruism in simple non-strategic decisions. Such choices increase social welfare by increasing the other person's payoff at very low or no cost for the individual. The choices of less reflective individuals (i.e., those who rely more heavily on intuition), on the other hand, are more likely to be associated with either egalitarian or spiteful motives. We also identify a negative link between reflection and choices characterized by “strong” altruism, but this result holds only in Study 2. Moreover, we provide evidence that the relationship between social preferences and CRT scores is not driven by general intelligence. We discuss how our results can reconcile some previous conflicting findings on the cognitive basis of social behavior.},
	urldate = {2015-11-19},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Corgnet, Brice and Espín, Antonio M. and Hernán-González, Roberto},
	year = {2015},
	keywords = {altruism, antisocial behavior, dual-process, inequality aversion, intuition, prosocial behavior, social preferences, spitefulness},
	pages = {287},
	file = {Full Text PDF:/home/user/Zotero/storage/WGDTDK3F/Corgnet et al. - 2015 - The cognitive basis of social behavior cognitive .pdf:application/pdf}
}

@article{struyf-perceptual-nodate,
	title = {Perceptual discrimination in fear generalization: {Mechanistic} and clinical implications},
	issn = {0149-7634},
	shorttitle = {Perceptual discrimination in fear generalization},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415002742},
	doi = {10.1016/j.neubiorev.2015.11.004},
	abstract = {For almost a century, Pavlovian conditioning is the imperative experimental paradigm to investigate the development and generalization of fear. However, despite the rich research tradition, the conceptualization of fear generalization has remained somewhat ambiguous. In this selective review, we focus explicitly on some challenges with the current operationalization of fear generalization and their impact on the ability to make inferences on its clinical potential and underlying processes. The main conclusion is that, despite the strong evidence that learning influences perception, current research has largely neglected the role of perceptual discriminability and its plasticity in fear generalization. We propose an alternative operationalization of generalization, where the essence is that Pavlovian conditioning itself influences the breadth of fear generalization via learning-related changes in perceptual discriminability. Hence a conceptualization of fear generalization is incomplete without an in-depth analysis of processes of perceptual discriminability. Furthermore, this highlights perceptual learning and discriminability as important future targets for pre-clinical and clinical research.},
	urldate = {2015-11-19},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Struyf, Dieter and Zaman, Jonas and Vervliet, Bram and Van Diest, Ilse},
	keywords = {conditioning, fear generalization, perceptual discrimination},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/N8JJHHV6/Struyf et al. - Perceptual discrimination in fear generalization .pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DRNFUQ54/S0149763415002742.html:text/html}
}

@article{womelsdorf-long-range-2015,
	title = {Long-{Range} {Attention} {Networks}: {Circuit} {Motifs} {Underlying} {Endogenously} {Controlled} {Stimulus} {Selection}},
	volume = {38},
	issn = {0166-2236},
	shorttitle = {Long-{Range} {Attention} {Networks}},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223615001915},
	doi = {10.1016/j.tins.2015.08.009},
	abstract = {Attention networks comprise brain areas whose coordinated activity implements stimulus selection. This selection is reflected in spatially referenced priority maps across frontal-parietal-collicular areas and is controlled through interactions with circuits representing behavioral goals, including prefrontal, cingulate, and striatal circuits, among others. We review how these goal-providing structures control stimulus selection through long-range dynamic projection motifs. These motifs (i) combine feature-tuned subnetworks to a distributed priority map, (ii) establish endogenously controlled, long-range coherent activity at 4–10 Hz theta and 12–30 Hz beta-band frequencies, and (iii) are composed of unique cell types implementing long-range networks through disynaptic disinhibition, dendritic gating, and feedforward inhibitory gain control. This evidence reveals common circuit motifs used to coordinate attentionally selected information across multi-node brain networks during goal-directed behavior.},
	number = {11},
	urldate = {2015-11-19},
	journal = {Trends in Neurosciences},
	author = {Womelsdorf, Thilo and Everling, Stefan},
	month = nov,
	year = {2015},
	pages = {682--700},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/2NUZXDT6/Womelsdorf and Everling - 2015 - Long-Range Attention Networks Circuit Motifs Unde.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/BGBCEIG6/S0166223615001915.html:text/html}
}

@article{joseph-drosophila-nodate,
	title = {Drosophila {Chemoreceptors}: {A} {Molecular} {Interface} {Between} the {Chemical} {World} and the {Brain}},
	issn = {0168-9525},
	shorttitle = {Drosophila {Chemoreceptors}},
	url = {http://www.sciencedirect.com/science/article/pii/S0168952515001651},
	doi = {10.1016/j.tig.2015.09.005},
	abstract = {Chemoreception is essential for survival. Feeding, mating, and avoidance of predators depend on detection of sensory cues. Drosophila contains diverse families of chemoreceptors that detect odors, tastants, pheromones, and noxious stimuli, including receptors of the odor receptor (Or), gustatory receptor (Gr), ionotropic receptor (IR), Pickpocket (Ppk), and Trp families. We consider recent progress in understanding chemoreception in the fly, including the identification of new receptors, the discovery of novel biological functions for receptors, and the localization of receptors in unexpected places. We discuss major unsolved problems and suggest areas that may be particularly ripe for future discoveries, including the roles of these receptors in driving the circuits and behaviors that are essential to the survival and reproduction of the animal.},
	urldate = {2015-11-19},
	journal = {Trends in Genetics},
	author = {Joseph, Ryan M. and Carlson, John R.},
	keywords = {olfaction, gustatory receptor, ionotropic glutamate receptor, odor receptor, olfactory receptor neuron, taste},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/BMNGH84W/Joseph and Carlson - Drosophila Chemoreceptors A Molecular Interface B.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CCABKD74/S0168952515001651.html:text/html}
}

@article{ryu-holographic-2006,
	title = {Holographic {Derivation} of {Entanglement} {Entropy} from {AdS}/{CFT}},
	volume = {96},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/hep-th/0603001},
	doi = {10.1103/PhysRevLett.96.181602},
	abstract = {A holographic derivation of the entanglement entropy in quantum (conformal) field theories is proposed from AdS/CFT correspondence. We argue that the entanglement entropy in d+1 dimensional conformal field theories can be obtained from the area of d dimensional minimal surfaces in AdS\_{d+2}, analogous to the Bekenstein-Hawking formula for black hole entropy. We show that our proposal perfectly reproduces the correct entanglement entropy in 2D CFT when applied to AdS\_3. We also compare the entropy computed in AdS\_5 \times S{\textasciicircum}5 with that of the free N=4 super Yang-Mills.},
	number = {18},
	urldate = {2015-11-19},
	journal = {Physical Review Letters},
	author = {Ryu, Shinsei and Takayanagi, Tadashi},
	month = may,
	year = {2006},
	note = {arXiv: hep-th/0603001},
	keywords = {Condensed Matter - Strongly Correlated Electrons, General Relativity and Quantum Cosmology, High Energy Physics - Theory, Quantum Physics},
	file = {arXiv\:hep-th/0603001 PDF:/home/user/Zotero/storage/MZ6A9U7B/Ryu and Takayanagi - 2006 - Holographic Derivation of Entanglement Entropy fro.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6UFB5ND5/0603001.html:text/html}
}

@article{maldacena-large-1999,
	title = {The {Large} {N} {Limit} of {Superconformal} {Field} {Theories} and {Supergravity}},
	volume = {38},
	issn = {00207748},
	url = {http://arxiv.org/abs/hep-th/9711200},
	doi = {10.1023/A:1026654312961},
	abstract = {We show that the large \$N\$ limit of certain conformal field theories in various dimensions include in their Hilbert space a sector describing supergravity on the product of Anti-deSitter spacetimes, spheres and other compact manifolds. This is shown by taking some branes in the full M/string theory and then taking a low energy limit where the field theory on the brane decouples from the bulk. We observe that, in this limit, we can still trust the near horizon geometry for large \$N\$. The enhanced supersymmetries of the near horizon geometry correspond to the extra supersymmetry generators present in the superconformal group (as opposed to just the super-Poincare group). The 't Hooft limit of 4-d \${\cal N} =4\$ super-Yang-Mills at the conformal point is shown to contain strings: they are IIB strings. We conjecture that compactifications of M/string theory on various Anti-deSitter spacetimes are dual to various conformal field theories. This leads to a new proposal for a definition of M-theory which could be extended to include five non-compact dimensions.},
	number = {4},
	urldate = {2015-11-19},
	journal = {International Journal of Theoretical Physics},
	author = {Maldacena, Juan M.},
	year = {1999},
	note = {arXiv: hep-th/9711200},
	keywords = {High Energy Physics - Theory},
	pages = {1113--1133},
	file = {arXiv\:hep-th/9711200 PDF:/home/user/Zotero/storage/42P4S6IR/Maldacena - 1999 - The Large N Limit of Superconformal Field Theories.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/98ASIE6R/9711200.html:text/html}
}

@article{eldar-mood-nodate,
	title = {Mood as {Representation} of {Momentum}},
	volume = {0},
	issn = {1364-6613},
	url = {http://www.cell.com/article/S1364661315001746/abstract},
	doi = {10.1016/j.tics.2015.07.010},
	abstract = {Experiences affect mood, which in turn affects subsequent experiences. Recent studies suggest two specific principles. First, mood depends on how recent reward outcomes differ from expectations. Second, mood biases the way we perceive outcomes (e.g., rewards), and this bias affects learning about those outcomes. We propose that this two-way interaction serves to mitigate inefficiencies in the application of reinforcement learning to real-world problems. Specifically, we propose that mood represents the overall momentum of recent outcomes, and its biasing influence on the perception of outcomes ‘corrects’ learning to account for environmental dependencies. We describe potential dysfunctions of this adaptive mechanism that might contribute to the symptoms of mood disorders., With increasing use of computational models to understand human behavior, scientists have begun to model the dynamics of subjective states such as mood., Recent data suggest that mood reflects the cumulative impact of differences between reward outcomes and expectations., Behavioral and neural findings suggest that mood biases the perception of reward outcomes such that outcomes are perceived as better when one is in a good mood relative to when one is in a bad mood., These two lines of research establish a bidirectional interaction between mood and reinforcement learning, which may play an important adaptive role in healthy behavior, and whose dysfunction might contribute to psychiatric disorders.},
	language = {English},
	number = {0},
	urldate = {2015-11-19},
	journal = {Trends in Cognitive Sciences},
	author = {Eldar, Eran and Rutledge, Robb B. and Dolan, Raymond J. and Niv, Yael},
	keywords = {decision making, Mood, reinforcement learning},
	file = {Full Text PDF:/home/user/Zotero/storage/XK9WUXDG/Eldar et al. - Mood as Representation of Momentum.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/U7KAE736/S1364-6613(15)00174-6.html:text/html}
}

@misc{noauthor-what-nodate-1,
	title = {The {What} and {Where} of {Visual} {Attention}},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627315009848},
	urldate = {2015-11-19},
	file = {The What and Where of Visual Attention:/home/user/Zotero/storage/XX7NWJ6D/S0896627315009848.html:text/html}
}

@article{stanford-complexity-2014-1,
	title = {Complexity and shock wave geometries},
	volume = {90},
	url = {http://link.aps.org/doi/10.1103/PhysRevD.90.126007},
	doi = {10.1103/PhysRevD.90.126007},
	abstract = {In this paper we refine a conjecture relating the time-dependent size of an Einstein-Rosen bridge (ERB) to the computational complexity of the dual quantum state. Our refinement states that the complexity is proportional to the spatial volume of the ERB. More precisely, up to an ambiguous numerical coefficient, we propose that the complexity is the regularized volume of the largest codimension one surface crossing the bridge, divided by GNlAdS. We test this conjecture against a wide variety of spherically symmetric shock wave geometries in different dimensions. We find detailed agreement.},
	number = {12},
	urldate = {2015-11-19},
	journal = {Physical Review D},
	author = {Stanford, Douglas and Susskind, Leonard},
	month = dec,
	year = {2014},
	pages = {126007},
	file = {APS Snapshot:/home/user/Zotero/storage/EJX7RKW7/PhysRevD.90.html:text/html}
}

@article{vincent-stacked-2010,
	title = {Stacked denoising autoencoders: {Learning} useful representations in a deep network with a local denoising criterion},
	volume = {11},
	shorttitle = {Stacked denoising autoencoders},
	url = {http://dl.acm.org/citation.cfm?id=1953039},
	urldate = {2015-11-18},
	journal = {The Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2010},
	pages = {3371--3408},
	file = {vincent10a.dvi - vincent10a.pdf:/home/user/Zotero/storage/MZ542CIC/vincent10a.pdf:application/pdf}
}

@inproceedings{bengio-generalized-2013,
	title = {Generalized denoising auto-encoders as generative models},
	url = {http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models},
	urldate = {2015-11-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
	year = {2013},
	pages = {899--907},
	file = {Generalized Denoising Auto-Encoders as Generative Models - 5023-generalized-denoising-auto-encoders-as-generative-models.pdf:/home/user/Zotero/storage/N3BVKCMA/5023-generalized-denoising-auto-encoders-as-generative-models.pdf:application/pdf}
}

@inproceedings{vincent-extracting-2008,
	title = {Extracting and composing robust features with denoising autoencoders},
	url = {http://dl.acm.org/citation.cfm?id=1390294},
	urldate = {2015-11-18},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	publisher = {ACM},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2008},
	pages = {1096--1103},
	file = {592.pdf:/home/user/Zotero/storage/J4DA9BRQ/592.pdf:application/pdf}
}

@article{chen-marginalized-2012,
	title = {Marginalized denoising autoencoders for domain adaptation},
	url = {http://arxiv.org/abs/1206.4683},
	urldate = {2015-11-18},
	journal = {arXiv preprint arXiv:1206.4683},
	author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
	year = {2012},
	file = {Marginalized Denoising Autoencoders for Domain Adaptation - msdadomain.pdf:/home/user/Zotero/storage/35UPCM2A/msdadomain.pdf:application/pdf}
}

@inproceedings{chen-marginalized-2014,
	title = {Marginalized denoising auto-encoders for nonlinear representations},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2\_cheng14},
	urldate = {2015-11-18},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Chen, Minmin and Weinberger, Kilian Q. and Sha, Fei and Bengio, Yoshua},
	year = {2014},
	pages = {1476--1484},
	file = {deepmsda.pdf:/home/user/Zotero/storage/T3VHS3U6/deepmsda.pdf:application/pdf}
}

@article{liang-training-nodate,
	title = {Training {Stacked} {Denoising} {Autoencoders} for {Representation} {Learning}},
	url = {http://users.ices.utexas.edu/~keith/files/autoencoder/final\_report/autoencoder.pdf},
	urldate = {2015-11-18},
	author = {Liang, Jason and Kelly, Keith},
	file = {autoencoder.pdf:/home/user/Zotero/storage/PU7JR96I/autoencoder.pdf:application/pdf}
}

@misc{noauthor-very-nodate,
	title = {A very fast denoising autoencoder - {FastML}},
	url = {http://fastml.com/very-fast-denoising-autoencoder-with-a-robot-arm/},
	urldate = {2015-11-18},
	file = {A very fast denoising autoencoder - FastML:/home/user/Zotero/storage/QTE4AB26/very-fast-denoising-autoencoder-with-a-robot-arm.html:text/html}
}

@inproceedings{lange-deep-2010,
	title = {Deep auto-encoder neural networks in reinforcement learning},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5596468},
	urldate = {2015-11-18},
	booktitle = {Neural {Networks} ({IJCNN}), {The} 2010 {International} {Joint} {Conference} on},
	publisher = {IEEE},
	author = {Lange, Sascha and Riedmiller, Martin},
	year = {2010},
	pages = {1--8},
	file = {download:/home/user/Zotero/storage/ZAB2JCGT/download.pdf:application/pdf}
}

@article{karpathy-visualizing-2015,
	title = {Visualizing and {Understanding} {Recurrent} {Networks}},
	url = {http://arxiv.org/abs/1506.02078},
	abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	urldate = {2015-11-18},
	journal = {arXiv:1506.02078 [cs]},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02078},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv\:1506.02078 PDF:/home/user/Zotero/storage/K23CNSNM/Karpathy et al. - 2015 - Visualizing and Understanding Recurrent Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/XDZDXF93/1506.html:text/html}
}

@article{nalisnick-infinite-2015,
	title = {Infinite {Dimensional} {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1511.05392},
	abstract = {We describe a method for learning word embeddings with stochastic dimensionality. Our Infinite Skip-Gram (iSG) model specifies an energy-based joint distribution over a word vector, a context vector, and their dimensionality, which can be defined over a countably infinite domain by employing the same techniques used to make the Infinite Restricted Boltzmann Machine (Cote \& Larochelle, 2015) tractable. We find that the distribution over embedding dimensionality for a given word is highly interpretable and leads to an elegant probabilistic mechanism for word sense induction. We show qualitatively and quantitatively that the iSG produces parameter-efficient representations that are robust to language's inherent ambiguity.},
	urldate = {2015-11-18},
	journal = {arXiv:1511.05392 [cs, stat]},
	author = {Nalisnick, Eric and Ravi, Sachin},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05392},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1511.05392 PDF:/home/user/Zotero/storage/TAAEMSWX/Nalisnick and Ravi - 2015 - Infinite Dimensional Word Embeddings.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/JUEAPMN2/1511.html:text/html}
}

@article{hendricks-deep-2015,
	title = {Deep {Compositional} {Captioning}: {Describing} {Novel} {Object} {Categories} without {Paired} {Training} {Data}},
	shorttitle = {Deep {Compositional} {Captioning}},
	url = {http://arxiv.org/abs/1511.05284},
	abstract = {While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.},
	urldate = {2015-11-18},
	journal = {arXiv:1511.05284 [cs]},
	author = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05284},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.05284 PDF:/home/user/Zotero/storage/E6MQ7WF9/Hendricks et al. - 2015 - Deep Compositional Captioning Describing Novel Ob.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/ZDSJZG5W/1511.html:text/html}
}

@article{xu-ask-2015,
	title = {Ask, {Attend} and {Answer}: {Exploring} {Question}-{Guided} {Spatial} {Attention} for {Visual} {Question} {Answering}},
	shorttitle = {Ask, {Attend} and {Answer}},
	url = {http://arxiv.org/abs/1511.05234},
	abstract = {The problem of Visual Question Answering (VQA) requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on recurrent LSTM networks to this problem, but have failed to model spatial inference. In this paper, we propose a memory network with spatial attention for the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. We store neuron activations from different spatial receptive fields in the memory, and use the question to choose relevant regions for computing the answer. We experiment with spatial attention architectures that use different question representations to choose regions, and also show that two attention steps (hops) obtain improved results compared to a single step. To understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR and VQA, and obtain promising results.},
	urldate = {2015-11-18},
	journal = {arXiv:1511.05234 [cs]},
	author = {Xu, Huijuan and Saenko, Kate},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05234},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv\:1511.05234 PDF:/home/user/Zotero/storage/W9485KAX/Xu and Saenko - 2015 - Ask, Attend and Answer Exploring Question-Guided .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6R4UNXS6/1511.html:text/html}
}

@article{sheikh-learning-2015,
	title = {Learning to retrieve out-of-vocabulary words in speech recognition},
	url = {http://arxiv.org/abs/1511.05389},
	abstract = {Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process diachronic audio data. To help recovery of the PNs missed by the system, relevant OOV PNs can be retrieved out of the many OOVs by exploiting semantic context of the spoken content. In this paper, we propose two neural network models targeted to retrieve OOV PNs relevant to an audio document: (a) Document level Continuous Bag of Words (D-CBOW), (b) Document level Continuous Bag of Weighted Words (D-CBOW2). Both these models take document words as input and learn with an objective to maximise the retrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new approach in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) key-words in a bag-of-word neural network model. With experiments on French broadcast news videos we show that these two models outperform the baseline methods based on raw embeddings from LDA, Skip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives faster convergence during training.},
	urldate = {2015-11-18},
	journal = {arXiv:1511.05389 [cs]},
	author = {Sheikh, Imran A. and Illina, Irina and Fohr, Dominique and Linarès, Georges},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05389},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1511.05389 PDF:/home/user/Zotero/storage/4ZT5NWWQ/Sheikh et al. - 2015 - Learning to retrieve out-of-vocabulary words in sp.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/JI4TG56C/1511.html:text/html}
}

@article{mnih-playing-2013,
	title = {Playing atari with deep reinforcement learning},
	url = {http://arxiv.org/abs/1312.5602},
	urldate = {2015-11-17},
	journal = {arXiv preprint arXiv:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year = {2013},
	file = {dqn.pdf:/home/user/Zotero/storage/U4EPBA6A/dqn.pdf:application/pdf}
}

@article{tanner-cues-2015,
	title = {Cues, quantification, and agreement in language comprehension},
	volume = {22},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-015-0850-3},
	doi = {10.3758/s13423-015-0850-3},
	language = {en},
	number = {6},
	urldate = {2015-11-17},
	journal = {Psychonomic Bulletin \& Review},
	author = {Tanner, Darren and Bulkes, Nyssa Z.},
	month = dec,
	year = {2015},
	pages = {1753--1763},
	file = {13423_2015_850_Article 1753..1763 - art%3A10.3758%2Fs13423-015-0850-3.pdf:/home/user/Zotero/storage/I8X6GNXU/art%3A10.3758%2Fs13423-015-0850-3.pdf:application/pdf}
}

@inproceedings{artzi-broad-coverage-2009-1,
	title = {Broad-coverage {CCG} {Semantic} {Parsing} with {AMR}},
	url = {http://anthology.aclweb.org/D/D15/D15-1198.pdf},
	urldate = {2015-11-17},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}. {Màrquez}, {Adam} {Meyers}, {Joakim} {Nivre}, {Sebastian} {Padó}, {Jan} Štepánek, {Pavel} {Stranák}, {Mihai} {Surdeanu}, {Nianwen} {Xue}, and {Yi} {Zhang}},
	author = {Artzi, Yoav and Zettlemoyer, Kenton Lee Luke},
	year = {2009},
	file = {Broad-coverage CCG Semantic Parsing with AMR - D15-1198.pdf:/home/user/Zotero/storage/HQPXWZF6/D15-1198.pdf:application/pdf}
}

@article{bubeck-convex-2014,
	title = {Convex {Optimization}: {Algorithms} and {Complexity}},
	shorttitle = {Convex {Optimization}},
	url = {http://arxiv.org/abs/1405.4980},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	urldate = {2015-11-17},
	journal = {arXiv:1405.4980 [cs, math, stat]},
	author = {Bubeck, Sébastien},
	month = may,
	year = {2014},
	note = {arXiv: 1405.4980},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Numerical Analysis, Computer Science - Computational Complexity},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6VKBG3MB/1405.html:text/html;Bubeck_2014_Convex Optimization.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bubeck_2014_Convex Optimization.pdf:application/pdf}
}

@article{jaitly-online-2015,
	title = {An {Online} {Sequence}-to-{Sequence} {Model} {Using} {Partial} {Conditioning}},
	url = {http://arxiv.org/abs/1511.04868},
	abstract = {Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online - at the end of each block, the best candidates from the previous block are extended through the local sequence-to-sequence model. On TIMIT, our online method achieves 19.8\% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7\% PER. This compares favorably to the best reported sequence-to-sequence model which achieves 17.6\%. Importantly, unlike sequence-to-sequence models our model is minimally impacted by the length of the input. On 10-times replicated utterances, it achieves 20.9\% with a unidirectional model, compared to 20\% from the best bidirectional sequence-to-sequence models.},
	urldate = {2015-11-17},
	journal = {arXiv:1511.04868 [cs]},
	author = {Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol and Sutskeyver, Ilya and Bengio, Samy},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04868},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv\:1511.04868 PDF:/home/user/Zotero/storage/TWE3CMMH/Jaitly et al. - 2015 - An Online Sequence-to-Sequence Model Using Partial.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/N4XJNRKI/1511.html:text/html}
}

@article{neelakantan-neural-2015,
	title = {Neural {Programmer}: {Inducing} {Latent} {Programs} with {Gradient} {Descent}},
	shorttitle = {Neural {Programmer}},
	url = {http://arxiv.org/abs/1511.04834},
	abstract = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
	urldate = {2015-11-17},
	journal = {arXiv:1511.04834 [cs, stat]},
	author = {Neelakantan, Arvind and Le, Quoc V. and Sutskever, Ilya},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04834},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1511.04834 PDF:/home/user/Zotero/storage/HSCCQ25A/Neelakantan et al. - 2015 - Neural Programmer Inducing Latent Programs with G.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GC6DFZCR/1511.html:text/html}
}

@article{he-deep-2015,
	title = {Deep {Reinforcement} {Learning} with an {Unbounded} {Action} {Space}},
	url = {http://arxiv.org/abs/1511.04636},
	abstract = {In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN). To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over the DQN.},
	urldate = {2015-11-17},
	journal = {arXiv:1511.04636 [cs]},
	author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04636},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1511.04636 PDF:/home/user/Zotero/storage/G4DNDBJN/He et al. - 2015 - Deep Reinforcement Learning with an Unbounded Acti.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/3W5VZSKC/1511.html:text/html}
}

@article{yao-trainable-2015,
	title = {Trainable performance upper bounds for image and video captioning},
	url = {http://arxiv.org/abs/1511.04590},
	abstract = {The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvement. This work takes a step back, and begs two questions: "what is the best performance one could possibly achieve on a specific dataset?" and "How many visual elements does a given model capture?". To answer the first question, we first utilize existing natural language parsers to extract key concepts from ground truth captions. Then a conditional language model is trained to reproduce the original captions given various amount of extracted conceptual hints. By adjusting the amount of visual hints to the language model, we establish empirically dataset-dependent upper bounds on various automatic evaluation metrics commonly used to compare models. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). As an upper bound, it suggests the best possible performance one could achieve on a particular dataset. To answer the second question, the current state-of-the-art results are compared against the proposed upper bounds. Based on such a comparison, we experimentally quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy,and the intrinsic difficulty and blessing of different datasets.},
	urldate = {2015-11-17},
	journal = {arXiv:1511.04590 [cs, stat]},
	author = {Yao, Li and Ballas, Nicolas and Cho, Kyunghyun and Smith, John R. and Bengio, Yoshua},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04590},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv\:1511.04590 PDF:/home/user/Zotero/storage/MD8TV9TT/Yao et al. - 2015 - Trainable performance upper bounds for image and v.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/CBVJQ2BM/1511.html:text/html}
}

@article{ling-character-based-2015,
	title = {Character-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1511.04586},
	abstract = {We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.},
	urldate = {2015-11-17},
	journal = {arXiv:1511.04586 [cs]},
	author = {Ling, Wang and Trancoso, Isabel and Dyer, Chris and Black, Alan W.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04586},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1511.04586 PDF:/home/user/Zotero/storage/M494ECGD/Ling et al. - 2015 - Character-based Neural Machine Translation.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/WXUQPDME/1511.html:text/html}
}

@article{strother-visual-2015,
	title = {Visual {Cortical} {Representation} of {Whole} {Words} and {Hemifield}-split {Word} {Parts}},
	issn = {0898-929X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_00900},
	doi = {10.1162/jocn\_a\_00900},
	abstract = {Reading requires the neural integration of visual word form information that is split between our retinal hemifields. We examined multiple visual cortical areas involved in this process by measuring fMRI responses while observers viewed words that changed or repeated in one or both hemifields. We were specifically interested in identifying brain areas that exhibit decreased fMRI responses as a result of repeated versus changing visual word form information in each visual hemifield. Our method yielded highly significant effects of word repetition in a previously reported visual word form area (VWFA) in occipitotemporal cortex, which represents hemifield-split words as whole units. We also identified a more posterior occipital word form area (OWFA), which represents word form information in the right and left hemifields independently and is thus both functionally and anatomically distinct from the VWFA. Both the VWFA and the OWFA were left-lateralized in our study and strikingly symmetric in anatomical location relative to known face-selective visual cortical areas in the right hemisphere. Our findings are consistent with the observation that category-selective visual areas come in pairs and support the view that neural mechanisms in left visual cortex—especially those that evolved to support the visual processing of faces—are developmentally malleable and become incorporated into a left-lateralized visual word form network that supports rapid word recognition and reading.},
	urldate = {2015-11-16},
	journal = {Journal of Cognitive Neuroscience},
	author = {Strother, Lars and Coros, Alexandra M. and Vilis, Tutis},
	month = nov,
	year = {2015},
	pages = {1--9},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/9BDCXEF5/jocn_a_00900.html:text/html}
}

@article{brehm-empirical-2015,
	title = {Empirical and conceptual challenges for neurocognitive theories of language production},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1110604},
	doi = {10.1080/23273798.2015.1110604},
	number = {0},
	urldate = {2015-11-16},
	journal = {Language, Cognition and Neuroscience},
	author = {Brehm, Laurel and Goldrick, Matthew},
	month = nov,
	year = {2015},
	pages = {1--4},
	file = {Brehm_Goldrick_2015_Empirical and conceptual challenges for neurocognitive theories of language.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Brehm_Goldrick_2015_Empirical and conceptual challenges for neurocognitive theories of language.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IAITNTZX/23273798.2015.html:text/html}
}

@article{kuperberg-what-2015,
	title = {What do we mean by prediction in language comprehension?},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1102299},
	doi = {10.1080/23273798.2015.1102299},
	abstract = {We consider several key aspects of prediction in language comprehension: its computational nature, the representational level(s) at which we predict, whether we use higher-level representations to predictively pre-activate lower level representations, and whether we “commit” in any way to our predictions, beyond pre-activation. We argue that the bulk of behavioural and neural evidence suggests that we predict probabilistically and at multiple levels and grains of representation. We also argue that we can, in principle, use higher-level inferences to predictively pre-activate information at multiple lower representational levels. We suggest that the degree and level of predictive pre-activation might be a function of its expected utility, which, in turn, may depend on comprehenders’ goals and their estimates of the relative reliability of their prior knowledge and the bottom-up input. Finally, we argue that all these properties of language understanding can be naturally explained and productively explored within a multi-representational hierarchical actively generative architecture whose goal is to infer the message intended by the producer, and in which predictions play a crucial role in explaining the bottom-up input.},
	number = {0},
	urldate = {2015-11-16},
	journal = {Language, Cognition and Neuroscience},
	author = {Kuperberg, Gina R. and Jaeger, T. Florian},
	month = nov,
	year = {2015},
	pages = {1--28},
	file = {Kuperberg_Jaeger_2015_What do we mean by prediction in language comprehension.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kuperberg_Jaeger_2015_What do we mean by prediction in language comprehension.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9QNDT7GJ/23273798.2015.html:text/html}
}

@article{manning-computational-2015,
	title = {Computational {Linguistics} and {Deep} {Learning}},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00239},
	doi = {10.1162/COLI\_a\_00239},
	urldate = {2015-11-16},
	journal = {Computational Linguistics},
	author = {Manning, Christopher D.},
	month = sep,
	year = {2015},
	pages = {1--7},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/GX9XK96U/COLI_a_00239.html:text/html;Manning_2015_Computational Linguistics and Deep Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Manning_2015_Computational Linguistics and Deep Learning.pdf:application/pdf}
}

@article{everaert-structures-nodate-1,
	title = {Structures, {Not} {Strings}: {Linguistics} as {Part} of the {Cognitive} {Sciences}},
	issn = {1364-6613},
	shorttitle = {Structures, {Not} {Strings}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002326},
	doi = {10.1016/j.tics.2015.09.008},
	abstract = {There are many questions one can ask about human language: its distinctive properties, neural representation, characteristic uses including use in communicative contexts, variation, growth in the individual, and origin. Every such inquiry is guided by some concept of what ‘language’ is. Sharpening the core question – what is language? – and paying close attention to the basic property of the language faculty and its biological foundations makes it clear how linguistics is firmly positioned within the cognitive sciences. Here we will show how recent developments in generative grammar, taking language as a computational cognitive mechanism seriously, allow us to address issues left unexplained in the increasingly popular surface-oriented approaches to language.},
	urldate = {2015-11-16},
	journal = {Trends in Cognitive Sciences},
	author = {Everaert, Martin B. H. and Huybregts, Marinus A. C. and Chomsky, Noam and Berwick, Robert C. and Bolhuis, Johan J.},
	file = {Everaert et al_Structures, Not Strings.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Everaert et al_Structures, Not Strings.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/GPIZ3F8G/S1364661315002326.html:text/html}
}

@book{syed-reinforcement-2010,
	title = {Reinforcement learning without rewards},
	url = {ftp://ftp.cs.princeton.edu/reports/2010/883.pdf},
	urldate = {2015-11-16},
	publisher = {Princeton University},
	author = {Syed, Umar Ali},
	year = {2010},
	file = {thesis.dvi - UmarSyedThesis.pdf:/home/user/Zotero/storage/PC9QF9UA/UmarSyedThesis.pdf:application/pdf}
}

@book{noauthor-machine-2013,
	address = {New York},
	edition = {1st edition},
	series = {Lecture notes in artificial intelligence},
	title = {Machine learning and knowledge discovery in databases: {European} {Conference}, {ECML} {PKDD} 2013, {Prague}, {Czech} {Republic}, {September} 23-27, 2013, {Proceedings}, {Part} {I}},
	isbn = {978-3-642-40987-5},
	shorttitle = {Machine learning and knowledge discovery in databases},
	number = {8188},
	publisher = {Springer},
	year = {2013},
	file = {daswani14.pdf:/home/user/Zotero/storage/S89VUDIG/daswani14.pdf:application/pdf}
}

@inproceedings{wiewiora-principled-2003,
	title = {Principled methods for advising reinforcement learning agents},
	url = {http://www.aaai.org/Papers/ICML/2003/ICML03-103.pdf},
	urldate = {2015-11-16},
	booktitle = {{ICML}},
	author = {Wiewiora, Eric and Cottrell, Garrison and Elkan, Charles},
	year = {2003},
	pages = {792--799},
	file = {03principled.dvi - 03principled.pdf:/home/user/Zotero/storage/C5SP58DP/03principled.pdf:application/pdf}
}

@incollection{torrey-using-2005,
	title = {Using advice to transfer knowledge acquired in one reinforcement learning task to another},
	url = {http://link.springer.com/chapter/10.1007/11564096\_40},
	urldate = {2015-11-16},
	booktitle = {Machine {Learning}: {ECML} 2005},
	publisher = {Springer},
	author = {Torrey, Lisa and Walker, Trevor and Shavlik, Jude and Maclin, Richard},
	year = {2005},
	pages = {412--424},
	file = {torrey-ecml2005.pdf:/home/user/Zotero/storage/WWW8TTGC/torrey-ecml2005.pdf:application/pdf}
}

@inproceedings{torrey-advice-2008,
	title = {Advice {Taking} and {Transfer} {Learning}: {Naturally} {Inspired} {Extensions} to {Reinforcement} {Learning}.},
	shorttitle = {Advice {Taking} and {Transfer} {Learning}},
	url = {http://www.aaai.org/Papers/Symposia/Fall/2008/FS-08-06/FS08-06-019.pdf},
	urldate = {2015-11-16},
	booktitle = {{AAAI} {Fall} {Symposium}: {Naturally}-{Inspired} {Artificial} {Intelligence}},
	author = {Torrey, Lisa and Walker, Trevor and Maclin, Richard and Shavlik, Jude W.},
	year = {2008},
	pages = {103--110},
	file = {torrey_symp08.pdf:/home/user/Zotero/storage/ZSNQTKFX/torrey_symp08.pdf:application/pdf}
}

@inproceedings{poon-grounded-2013,
	title = {Grounded {Unsupervised} {Semantic} {Parsing}.},
	url = {http://www.msr-waypoint.net/en-us/um/people/hoifung/papers/gusp13.pdf},
	urldate = {2015-11-16},
	booktitle = {{ACL} (1)},
	author = {Poon, Hoifung},
	year = {2013},
	pages = {933--943},
	file = {gusp13.pdf:/home/user/Zotero/storage/B4ZSWAHE/gusp13.pdf:application/pdf}
}

@article{venugopalan-translating-2014,
	title = {Translating videos to natural language using deep recurrent neural networks},
	url = {http://arxiv.org/abs/1412.4729},
	urldate = {2015-11-16},
	journal = {arXiv preprint arXiv:1412.4729},
	author = {Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
	year = {2014},
	file = {1412.4729v3.pdf:/home/user/Zotero/storage/EG55RXAX/1412.4729v3.pdf:application/pdf}
}

@article{hill-goldilocks-2015-1,
	title = {The {Goldilocks} {Principle}: {Reading} {Children}'s {Books} with {Explicit} {Memory} {Representations}},
	shorttitle = {The {Goldilocks} {Principle}},
	url = {http://arxiv.org/abs/1511.02301},
	urldate = {2015-11-16},
	journal = {arXiv preprint arXiv:1511.02301},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	year = {2015},
	file = {1511.02301v1.pdf:/home/user/Zotero/storage/5CB37EU2/1511.02301v1.pdf:application/pdf}
}

@article{zeiler-adadelta:-2012,
	title = {{ADADELTA}: {An} adaptive learning rate method},
	shorttitle = {{ADADELTA}},
	url = {http://arxiv.org/abs/1212.5701},
	urldate = {2015-11-16},
	journal = {arXiv preprint arXiv:1212.5701},
	author = {Zeiler, Matthew D.},
	year = {2012},
	file = {googleTR2012.pdf:/home/user/Zotero/storage/CBJ9W7VC/googleTR2012.pdf:application/pdf}
}

@book{noauthor-conll-2015,
	title = {{CoNLL} 2015 {The} 19th {Conference} on {Computational} {Natural} {Language} {Learning} {Proceedings} of the {Conference} {Beijing} , {China}},
	isbn = {978-1-941643-77-8},
	year = {2015},
	file = {K15-1:/home/user/Zotero/storage/NXQJ5Z5X/K15-1.pdf:application/pdf}
}

@article{zitnick-adopting-nodate,
	title = {Adopting {Abstract} {Images} for {Semantic} {Scene} {Understanding}},
	author = {Zitnick, C Lawrence and Vedantam, Ramakrishna and Parikh, Devi},
	pages = {1--12},
	file = {Zitnick, Vedantam, Parikh - Unknown - Adopting Abstract Images for Semantic Scene Understanding:/home/user/Zotero/storage/ZAPQC24N/Zitnick, Vedantam, Parikh - Unknown - Adopting Abstract Images for Semantic Scene Understanding.pdf:application/pdf}
}

@article{zitnick-learning-2013,
	title = {Learning the {Visual} {Interpretation} of {Sentences}},
	issn = {978-1-4799-2840-8},
	doi = {10.1109/ICCV.2013.211},
	abstract = {Sentences that describe visual scenes contain a wide variety of \ninformation pertaining to the presence of objects, their attributes and their \nspatial relations. In this paper we learn the visual features that correspond to \nsemantic phrases derived from sentences. Specifically, we extract predicate \ntuples that contain two nouns and a relation. The relation may take several \nforms, such as a verb, preposition, adjective or their combination. We model a \nscene using a Conditional Random Field (CRF) formulation where each node \ncorresponds to an object, and the edges to their relations. We determine the \npotentials of the CRF using the tuples extracted from the sentences. We generate \nnovel scenes depicting the sentences' visual meaning by sampling from the CRF. \nThe CRF is also used to score a set of scenes for a text-based image retrieval \ntask. Our results show we can generate (retrieve) scenes that convey the desired \nsemantic meaning, even when scenes (queries) are described by multiple \nsentences. Significant improvement is found over several baseline approaches.},
	journal = {Computer Vision (ICCV), 2013 IEEE International Conference on},
	author = {Zitnick, C L and Parikh, D and Vanderwende, L},
	year = {2013},
	keywords = {CRF, Image processing, image retrieval, text analysis},
	pages = {1681--1688},
	file = {Zitnick, Parikh, Vanderwende - 2013 - Learning the Visual Interpretation of Sentences:/home/user/Zotero/storage/I3PDCGCM/Zitnick, Parikh, Vanderwende - 2013 - Learning the Visual Interpretation of Sentences.pdf:application/pdf}
}

@article{yang-learning-2014,
	title = {Learning {Multi}-{Relational} {Semantics} {Using} {Neural}-{Embedding} {Models}},
	url = {http://arxiv.org/abs/1411.4072},
	abstract = {In this paper we present a unified framework for modeling multi-relational representations, scoring, and learning, and conduct an empirical study of several recent multi-relational embedding models under the framework. We investigate the different choices of relation operators based on linear and bilinear transformations, and also the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources. Our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on Freebase.},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	year = {2014},
	pages = {7--7},
	file = {1411.4072v1:/home/user/Zotero/storage/DIXQW9CD/1411.4072v1.pdf:application/pdf}
}

@article{yu-grounded-2013,
	title = {Grounded {Language} {Learning} from {Video} {Described} with {Sentences}},
	abstract = {We present a method that learns repre- sentations for word meanings from short video clips paired with sentences. Un- like prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sen- tences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The cor- respondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)},
	author = {Yu, Haonan and Siskind, Jeffrey Mark},
	year = {2013},
	pages = {53--63},
	file = {acl2013:/home/user/Zotero/storage/VRUSPH34/acl2013.pdf:application/pdf}
}

@inproceedings{yannakoudakis-evaluating-2015,
	title = {Evaluating the performance of {Automated} {Text} {Scoring} systems},
	booktitle = {{BEA}},
	author = {Yannakoudakis, Helen},
	year = {2015},
	pages = {213--223},
	file = {W15-0625:/home/user/Zotero/storage/PGZU4D3W/W15-0625.pdf:application/pdf}
}

@article{xu-improving-nodate,
	title = {Improving {Word} {Representations} via {Global} {Visual} {Context}},
	author = {Xu, J, R; Lu},
	pages = {6--10},
	file = {35:/home/user/Zotero/storage/6ESH92BF/35.pdf:application/pdf}
}

@article{wu-image-2015,
	title = {Image {Captioning} with an {Intermediate} {Attributes} {Layer}},
	author = {Wu, Qi and Shen, Chunhua and Hengel, Anton Van Den and Liu, Lingqiao and Dick, Anthony},
	year = {2015},
	pages = {1--12},
	file = {Wu et al. - 2015 - Image Captioning with an Intermediate Attributes Layer:/home/user/Zotero/storage/MFEB5K8V/Wu et al. - 2015 - Image Captioning with an Intermediate Attributes Layer.pdf:application/pdf}
}

@inproceedings{widdows-orthogonal-2003,
	series = {{ACL} '03},
	title = {Orthogonal negation in vector spaces for modelling word-meanings and document retrieval},
	url = {http://dx.doi.org/10.3115/1075096.1075114},
	doi = {10.3115/1075096.1075114},
	booktitle = {Proceedings of the 41st {Annual} {Meeting} on {Association} for {Computational} {Linguistics} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Widdows, Dominic},
	year = {2003},
	keywords = {Semantics-Distributional-Compositional, Information-retrieval},
	pages = {136--143}
}

@article{widdows-semantic-2008,
	title = {Semantic vectors: a scalable open source package and online technology management application},
	journal = {Proceedings of the Sixth International Language Resources and Evaluation (LREC???08)},
	author = {Widdows, D and Ferraro, K},
	year = {2008},
	pages = {1183--1190}
}

@article{wang-building-nodate,
	title = {Building a {Semantic} {Parser} {Overnight}},
	author = {Wang, Yushi and Berant, Jonathan and Liang, Percy},
	file = {wang-berant-liang-acl2015:/home/user/Zotero/storage/SJ2J29PF/wang-berant-liang-acl2015.pdf:application/pdf}
}

@article{wood-1-1995-1,
	title = {From 1},
	volume = {2541},
	number = {1037},
	journal = {Shock},
	author = {Wood, Wassennan Bernard and Davies, Alastair},
	year = {1995},
	file = {Wood, Davies - 1995 - From 1:/home/user/Zotero/storage/NUW36GSV/Wood, Davies - 1995 - From 1.pdf:application/pdf}
}

@inproceedings{widdows-word-2003,
	title = {Word {Vectors} and {Quantum} {Logic}: {Experiments} with negation and disjunction},
	booktitle = {Proceedings of {{M}}athematics of {{L}}anguage 8},
	author = {Widdows, Dominic and Peters, Stanley},
	year = {2003},
	keywords = {Semantics-Distributional-Compositional, Information-retrieval},
	pages = {141--154}
}

@book{widdows-geometry-2004,
	address = {Stanford},
	series = {{CSLI} lecture notes},
	title = {Geometry and {Meaning}},
	publisher = {CSLI Publications},
	author = {Widdows, Dominic},
	year = {2004}
}

@article{weston-memory-2015-1,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	journal = {Iclr 2015},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
	pages = {1--14},
	file = {1410.3916v10:/home/user/Zotero/storage/APAVQGBR/1410.3916v10.pdf:application/pdf}
}

@inproceedings{versley-yannick-and-zinsmeister-surface-2006,
	title = {From {{S}}urface {{D}}ependencies towards {{D}}eeper {{S}}emantic {{R}}epresentations},
	booktitle = {Fifth {{W}}orkshop on {{T}}reebanks and {{L}}inguistic {{T}}heories ({{TLT}} 2006)},
	author = {Versley Yannick {and} Zinsmeister, Heike},
	year = {2006}
}

@book{van-eijck-computational-2010,
	title = {Computational {Semantics} with {Functional} {Programming}},
	publisher = {Cambridge University Press},
	author = {van Eijck, Jan and Unger, Christina},
	year = {2010}
}

@article{ullman-theory-2012,
	title = {Theory learning as stochastic search in the language of thought},
	volume = {27},
	url = {citeulike-article-id:11044312\nhttp://dx.doi.org/10.1016/j.cogdev.2012.07.005},
	doi = {10.1016/j.cogdev.2012.07.005},
	abstract = {We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. We contrast our approach with connectionist and other emergentist approaches to modeling cognitive development. While their subsymbolic representations provide a smooth error surface that supports efficient gradient-based learning, our symbolic representations are better suited to capturing children's intuitive theories but give rise to a harder learning problem, which can only be solved by exploratory search. Our algorithm attempts to discover the theory that best explains a set of observed data by performing stochastic search at two levels of abstraction: an outer loop in the space of theories and an inner loop in the space of explanations or models generated by each theory given a particular dataset. We show that this stochastic search is capable of learning appropriate theories in several everyday domains and discuss its dynamics in the context of empirical studies of children's learning. âº We give an algorithmic level account of Hierarchical Bayesian learning. âº We contrast this approach with connectionism and emergentism. âº A Markov Chain Monte Carlo explores theories constructed by horn clause logic. âº The algorithm constructs theories of 2 domains, taxonomy and simplified magnetism. âº Predictions resulting from the learning dynamics are considered.},
	journal = {Cognitive Development},
	author = {Ullman, Tomer and Goodman, Noah and Tenenbaum, Joshua},
	year = {2012},
	keywords = {induction, bayesian, cognition, MCMC, cognitive\_development, computational\_cognitive\_science, hierarchical\_bayesian\_models, inductive\_learning, sampling, search},
	pages = {455--480},
	file = {Ullman, Goodman, Tenenbaum - 2012 - Theory learning as stochastic search in the language of thought:/home/user/Zotero/storage/3SGB5G5G/Ullman, Goodman, Tenenbaum - 2012 - Theory learning as stochastic search in the language of thought.pdf:application/pdf}
}

@article{turney-similarity-2006,
	title = {Similarity of {Semantic} {Relations}},
	volume = {32},
	url = {http://dx.doi.org/10.1162/coli.2006.32.3.379},
	doi = {10.1162/coli.2006.32.3.379},
	number = {3},
	journal = {Comput. Linguist.},
	author = {Turney, Peter D},
	month = sep,
	year = {2006},
	pages = {379--416}
}

@inproceedings{turney-mining-2001,
	title = {Mining the {Web} for {Synonyms}: {{PMI}-{IR}} {Versus} {{LSA}} on {{TOEFL}}},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Machine} {Learning} ({ECML}-2001)},
	author = {Turney, Peter},
	year = {2001},
	pages = {491--502}
}

@article{titov-bayesian-2012-1,
	title = {A {Bayesian} approach to unsupervised semantic role induction},
	issn = {978-1-937284-19-0},
	url = {http://dl.acm.org/citation.cfm?id=2380821},
	number = {c},
	journal = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics},
	author = {Titov, Ivan and Klementiev, a},
	year = {2012},
	pages = {12--22},
	file = {eacl12:/home/user/Zotero/storage/BIJB7KDK/eacl12.pdf:application/pdf}
}

@article{titov-unsupervised-nodate,
	title = {Unsupervised {Induction} of {Semantic} {Roles} within a {Reconstruction}-{Error} {Minimization} {Framework}},
	author = {Titov, Ivan and Khoddam, Ehsan},
	pages = {1--12},
	file = {1412.2812v1:/home/user/Zotero/storage/FRG9XZXQ/1412.2812v1.pdf:application/pdf}
}

@inproceedings{thater-word-2011,
	title = {Word meaning in context: {A} simple and effective vector model},
	url = {http://www.aclweb.org/anthology-new/I/I11/I11-1127.pdf},
	booktitle = {Proceedings of {IJCNLP}},
	author = {Thater, Stefan and F??rstenau, Hagen and Pinkal, Manfred},
	year = {2011}
}

@inproceedings{szpektor-scaling-2004,
	title = {Scaling web-based acquisition of entailment relations},
	volume = {4},
	booktitle = {Proceedings of {EMNLP}},
	author = {Szpektor, I and Tanev, H and Dagan, I and Coppola, B and {others}},
	year = {2004},
	pages = {41--48}
}

@article{titov--nodate,
	title = {( within a reconstruction-error minimization framework )},
	author = {Titov, Ivan},
	file = {IvanTitov:/home/user/Zotero/storage/KQZJNK59/IvanTitov.pdf:application/pdf}
}

@article{tietze-syntactic-2008,
	title = {Syntactic complexity induces explicit grounding in the {MapTask} corpus},
	abstract = {This paper provides evidence for theories of grounding and dialogue management in human conversation. For each utterance in a corpus of task-oriented dialogues, we calculated integration costs, which are based on syntactic sentence complexity. We compared the integration costs and grounding behavior under two conditions, namely face-to-face and a no-eye-contact condition. The results show that integration costs were significantlyhigher for explicitly grounded utterances in the no-eye-contact condition, but not in the face-to-face condition.},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	author = {Tietze, Martin and Demberg, Vera and Moore, Johanna D.},
	year = {2008},
	keywords = {syntactic complexity, Dialogue, Grounding},
	pages = {542--542},
	file = {Tietze, Demberg, Moore - 2008 - Syntactic complexity induces explicit grounding in the MapTask corpus:/home/user/Zotero/storage/3CXPQD82/Tietze, Demberg, Moore - 2008 - Syntactic complexity induces explicit grounding in the MapTask corpus.pdf:application/pdf}
}

@inproceedings{thater-contextualizing-2010,
	title = {Contextualizing semantic representations using syntactically enriched vector models},
	url = {http://eprints.pascal-network.org/archive/00008090/01/acl10\_contextualizing.pdf},
	booktitle = {Proceedings of the 48th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Thater, Stefan and Fürstenau, Hagen and Pinkal, Manfred},
	year = {2010},
	pages = {948--957}
}

@inproceedings{thater-ranking-2009,
	title = {Ranking {Paraphrases} in {Context}},
	booktitle = {Proceedings of the 2009 {Workshop} on {Applied} {Textual} {Inference} ({TextInfer})},
	author = {Thater, Stefan and Dinu, Georgiana and Pinkal, Manfred},
	year = {2009}
}

@inproceedings{szpektor-augmenting-2009,
	title = {Augmenting {WordNet}-based {Inference} with {Argument} {Mapping}},
	url = {http://eprints.pascal-network.org/archive/00006567/01/W09-2504.pdf},
	booktitle = {Proceedings of the 2009 {Workshop} on {Applied} {Textual} {Inference} ({TextInfer})},
	author = {Szpektor, Idan and Dagan, Ido},
	year = {2009}
}

@article{symonds-gender-2006-1,
	title = {Gender differences in publication output: {Towards} an unbiased metric of research performance},
	volume = {1},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0000127},
	abstract = {We examined the publication records of a cohort of 168 life scientists in the field of ecology and evolutionary biology to assess gender differences in research performance. Clear discrepancies in publication rate between men and women appear very early in their careers and this has consequences for the subsequent citation of their work. We show that a recently proposed index designed to rank scientists fairly is in fact strongly biased against female researchers, and advocate a modified index to assess men and women on a more equitable basis.},
	number = {1},
	journal = {PLoS ONE},
	author = {Symonds, M. R E and Gemmell, Neil J. and Braisher, Tamsin L. and Gorringe, Kylie L. and Elgar, Mark a.},
	year = {2006},
	pages = {1--5},
	file = {Symonds et al. - 2006 - Gender differences in publication output Towards an unbiased metric of research performance:/home/user/Zotero/storage/HHPWN676/Symonds et al. - 2006 - Gender differences in publication output Towards an unbiased metric of research performance.pdf:application/pdf}
}

@article{sutskever-modelling-2009-1,
	title = {Modelling {Relational} {Data} using {Bayesian} {Clustered} {Tensor} {Factorization}},
	volume = {22},
	url = {http://www.ece.duke.edu/~lcarin/pmfcrp.pdf},
	abstract = {We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance thatmatches or beats previous probabilisticmodels for relational data.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sutskever, Ilya},
	year = {2009},
	pages = {1--8},
	file = {pmfcrp:/home/user/Zotero/storage/PQZJABGE/pmfcrp.pdf:application/pdf}
}

@article{supervisor-automated-nodate,
	title = {{AUTOMATED} {MULTILINGUAL} {ALIGNMENT} {OF} {DISCONTINUOUS} {SEQUENCES} {First} {Supervisor} : {Prof} . {Dr} . {Erhard} {Hinrichs} {Hiermit} versichere ich , dass ich die vorgelegte {Arbeit} selbstst ¨ andig und nur mit den angegebenen {Quellen} und {Hilfsmitteln} einschließlich des www},
	number = {July 2011},
	author = {Supervisor, Second and Gerdemann, Dale},
	file = {Supervisor, Gerdemann - Unknown - AUTOMATED MULTILINGUAL ALIGNMENT OF DISCONTINUOUS SEQUENCES First Supervisor Prof . Dr . Erhard Hin:/home/user/Zotero/storage/DJJF7M39/Supervisor, Gerdemann - Unknown - AUTOMATED MULTILINGUAL ALIGNMENT OF DISCONTINUOUS SEQUENCES First Supervisor Prof . Dr . Erhard Hin.pdf:application/pdf}
}

@article{studies-tokenization-2013,
	title = {On {Tokenization}},
	author = {Studies, International and Linguistics, Computational},
	year = {2013},
	file = {Studies, Linguistics - 2013 - On Tokenization:/home/user/Zotero/storage/CVNXIA4P/Studies, Linguistics - 2013 - On Tokenization.pdf:application/pdf}
}

@book{stuart-inverse-2010,
	title = {Inverse problems : a {Bayesian} perspective},
	isbn = {0962492910000},
	url = {http://webcat.warwick.ac.uk:80/record=b2341209},
	abstract = {The subject of inverse problems in differential equations is of enormous practical importance, and has also generated substantial mathematical and computational innovation. Typically some form of regularization is required to ameliorate ill-posed behaviour. In this article we review the Bayesian approach to regularization, developing a function space viewpoint on the subject. This approach allows for a full characterization of all possible solutions, and their relative probabilities, whilst simultaneously forcing significant modelling issues to be addressed in a clear and precise fashion. Although expensive to implement, this approach is starting to lie within the range of the available computational resources in many application areas. It also allows for the quantification of uncertainty and risk, something which is increasingly demanded by these applications. Furthermore, the approach is conceptually important for the understanding of simpler, computationally expedient approaches to inverse problems.},
	author = {Stuart, a. M.},
	year = {2010},
	keywords = {QA Mathematics},
	file = {Stuart - 2010 - Inverse problems a Bayesian perspective:/home/user/Zotero/storage/B3S84DSA/Stuart - 2010 - Inverse problems a Bayesian perspective.pdf:application/pdf}
}

@article{steinberg-theory-2010,
	title = {A theory of transformation monoids: {Combinatorics} and representation theory},
	volume = {17},
	abstract = {The aim of this paper is to develop a theory of finite transformation monoids and in particular to study primitive transformation monoids. We introduce the notion of orbitals and orbital digraphs for transformation monoids and prove a monoid version of D. Higman's celebrated theorem characterizing primitivity in terms of connectedness of orbital digraphs. A thorough study of the module (or representation) associated to a transformation monoid is initiated. In particular, we compute the projective cover of the transformation module over a field of characteristic zero in the case of a transitive transformation or partial transformation monoid. Applications of probability theory and Markov chains to transformation monoids are also considered and an ergodic theorem is proved in this context. In particular, we obtain a generalization of a lemma of P. Neumann, from the theory of synchronizing groups, concerning the partition associated to a transformation of minimal rank.},
	journal = {Electronic Journal of Combinatorics},
	author = {Steinberg, Benjamin},
	year = {2010},
	pages = {1--55},
	file = {Steinberg - 2010 - A theory of transformation monoids Combinatorics and representation theory:/home/user/Zotero/storage/DC6MUBCX/Steinberg - 2010 - A theory of transformation monoids Combinatorics and representation theory.pdf:application/pdf}
}

@article{socher-semi-supervised-2011-1,
	title = {Semi-supervised recursive autoencoders for predicting sentiment distributions},
	url = {http://dl.acm.org/citation.cfm?id=2145450},
	number = {i},
	journal = {Conference on Empirical Methods in Natural Language Processing, EMNLP},
	author = {Socher, Richard and Pennington, Jeffrey and Huang, Eh},
	year = {2011},
	pages = {151--161},
	file = {Socher, Pennington, Huang - 2011 - Semi-supervised recursive autoencoders for predicting sentiment distributions:/home/user/Zotero/storage/E6IXWB59/Socher, Pennington, Huang - 2011 - Semi-supervised recursive autoencoders for predicting sentiment distributions.pdf:application/pdf}
}

@article{socher-semantic-2012,
	title = {Semantic {Compositionality} through {Recursive} {Matrix}-{Vector} {Spaces}},
	issn = {9781937284435},
	abstract = {Single-word vector space models have been very successful at learning lexical informa- tion. However, they cannot capture the com- positional meaning of longer phrases, prevent- ing them from a deeper understanding of lan- guage. We introduce a recursive neural net- work (RNN) model that learns compositional vector representations for phrases and sen- tences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to ev- ery node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the mean- ing of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying senti- ment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syn- tactic path between them.},
	number = {July},
	journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	author = {Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
	year = {2012},
	pages = {1201--1211},
	file = {Socher-MVRNN-D12-1110:/home/user/Zotero/storage/P5M92CMR/Socher-MVRNN-D12-1110.pdf:application/pdf}
}

@article{socher-dynamic-2011,
	title = {Dynamic {Pooling} and {Unfolding} {Recursive} {Autoencoders} for {Paraphrase} {Detection}.},
	issn = {9781618395993},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2011\_0538.pdf\nhttps://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf},
	abstract = {Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Socher, Richard and Huang, Eh and Pennington, Jeffrey},
	year = {2011},
	pages = {801--809},
	file = {Socher, Huang, Pennington - 2011 - Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection:/home/user/Zotero/storage/C3T5TPNH/Socher, Huang, Pennington - 2011 - Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.pdf:application/pdf}
}

@article{socher-zero-shot-2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	journal = {Advances in Neural Information Processing Systems},
	author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
	year = {2013},
	pages = {935--943},
	file = {Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer:/home/user/Zotero/storage/3CI74DXN/Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer.pdf:application/pdf}
}

@article{skrzypczak-equational-2011-1,
	title = {Equational theories of profinite structures},
	volume = {abs/1111.0},
	url = {http://arxiv.org/abs/1111.0476},
	journal = {CoRR},
	author = {Skrzypczak, Michal},
	year = {2011}
}

@article{shashua-non-negative-2005,
	title = {Non-negative tensor factorization with applications to statistics and computer vision},
	issn = {1595931805},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102451},
	doi = {10.1145/1102351.1102451},
	abstract = {We derive algorithms for finding a non- negative n-dimensional tensor factorization (n-NTF) which includes the non-negative matrix factorization (NMF) as a particular case when n = 2. We motivate the use of n-NTF in three areas of data analysis: (i) connection to latent class models in statis- tics, (ii) sparse image coding in computer vi- sion, and (iii) model selection problems. We derive a ”direct” positive-preserving gradient descent algorithm and an alternating scheme based on repeated multiple rank-1 problems.},
	journal = {International Conference on Machine Learning},
	author = {Shashua, Amnon and Hazan, Tamir},
	year = {2005},
	pages = {792--799},
	file = {100_NonNegative_ShashuaHazan:/home/user/Zotero/storage/RHS7QKBK/100_NonNegative_ShashuaHazan.pdf:application/pdf}
}

@book{seeger-gaussian-2004-1,
	title = {Gaussian processes for machine learning.},
	volume = {14},
	isbn = {0-262-18253-X},
	abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
	author = {Seeger, Matthias},
	year = {2004},
	file = {Seeger - 2004 - Gaussian processes for machine learning:/home/user/Zotero/storage/AHDIGUFW/Seeger - 2004 - Gaussian processes for machine learning.pdf:application/pdf}
}

@article{socher-recursive-2014-1,
	title = {Recursive {Deep} {Learning} for {Natural} {Language} {Processing} and {Computer} {Vision}},
	number = {August},
	author = {Socher, Richard},
	year = {2014},
	file = {thesis(1):/home/user/Zotero/storage/NXSG8NR7/thesis(1).pdf:application/pdf}
}

@article{sirts-pos-2014-1,
	title = {{POS} induction with distributional and morphological information using a distance-dependent {Chinese} restaurant process},
	issn = {9781937284732},
	url = {http://www.aclweb.org/anthology/P/P14/P14-2044},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	author = {Sirts, Kairit and Eisenstein, Jacob and Elsner, Micha and Goldwater, Sharon},
	year = {2014},
	pages = {265--271},
	file = {Sirts et al. - 2014 - POS induction with distributional and morphological information using a distance-dependent Chinese restaurant proc:/home/user/Zotero/storage/5B7DGPHI/Sirts et al. - 2014 - POS induction with distributional and morphological information using a distance-dependent Chinese restaurant proc.pdf:application/pdf}
}

@inproceedings{schutze-distributional-1995,
	title = {Distributional {Part}-of-{Speech} {Tagging}},
	booktitle = {Proceedings of the 7th {EACL} conference},
	author = {Schütze, Hinrich},
	year = {1995},
	keywords = {POS Tagging}
}

@article{schober-probabilistic-2014-1,
	title = {Probabilistic {ODE} {Solvers} with {Runge}-{Kutta} {Means}},
	url = {http://arxiv.org/abs/1406.2582},
	abstract = {Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state-of-the-art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.},
	author = {Schober, Michael and Duvenaud, David and Hennig, Philipp},
	year = {2014},
	pages = {18--18},
	file = {Schober, Duvenaud, Hennig - 2014 - Probabilistic ODE Solvers with Runge-Kutta Means:/home/user/Zotero/storage/EUD6UN27/Schober, Duvenaud, Hennig - 2014 - Probabilistic ODE Solvers with Runge-Kutta Means.pdf:application/pdf}
}

@inproceedings{schmid-estimation-2008,
	title = {Estimation of conditional probabilities with decision trees and an application to fine-grained {POS} tagging},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Computational} {Linguistics}-{Volume} 1},
	author = {Schmid, H and Laws, F},
	year = {2008},
	pages = {777--784}
}

@article{schutze-automatic-1998,
	title = {Automatic word sense discrimination},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=972719.972724},
	number = {1},
	journal = {Comput. Linguist.},
	author = {Schütze, Hinrich},
	month = mar,
	year = {1998},
	keywords = {Semantics-Distributional-Compositional},
	pages = {97--123}
}

@inproceedings{schnattinger-quality-based-1998,
	title = {Quality-{Based} {Learning}},
	booktitle = {In {Proc}. of the {ECAI}'98},
	publisher = {John Wiley},
	author = {Schnattinger, Klemens and Hahn, Udo},
	year = {1998},
	keywords = {AI-Reasoning},
	pages = {160--164}
}

@inproceedings{schmid-probabilistic-1994,
	title = {Probabilistic part-of-speech tagging using decision trees},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Methods} in {Language} {Processing}},
	author = {Schmid, Helmut},
	year = {1994},
	pages = {44--49}
}

@inproceedings{schafer-building-2012,
	title = {Building large corpora from the web using a new effcient tool chain},
	booktitle = {Proceedings of the {Eight} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'12), {Istanbul}, {ELRA}},
	author = {Schäfer, Roland and Bildhauer, Felix},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Dogan, Mehmet Ugur and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios},
	year = {2012},
	pages = {486???493--486???493}
}

@article{sayeed-semantic-2013,
	title = {The semantic augmentation of a psycholinguistically-motivated syntactic formalism},
	url = {http://www.aclweb.org/anthology/W13-2607},
	number = {3},
	journal = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
	author = {Sayeed, Asad and Demberg, Vera},
	year = {2013},
	pages = {57--65},
	file = {Sayeed, Demberg - 2013 - The semantic augmentation of a psycholinguistically-motivated syntactic formalism:/home/user/Zotero/storage/683DES2V/Sayeed, Demberg - 2013 - The semantic augmentation of a psycholinguistically-motivated syntactic formalism.pdf:application/pdf}
}

@article{sammons-relation-2009,
	title = {Relation alignment for textual entailment recognition},
	journal = {Proceedings of Recognizing Textual Entailment 2009},
	author = {Sammons, M and Vydiswaran, V G V and Vieira, T and Johri, N and Chang, M W and Goldwasser, D and Srikumar, V and Kundu, G and Tu, Y and Small, K and {others}},
	year = {2009}
}

@inproceedings{sahlgren-introduction-2005,
	title = {An introduction to random indexing},
	volume = {5},
	url = {http://www.sics.se/??mange/papers/RI\_intro.pdf},
	booktitle = {Proceedings of the {Methods} and {Applications} of {Semantic} {Indexing} {Workshop} at the 7th {International} {Conference} on {Terminology} and {Knowledge} {Engineering} ({TKE}), {Copenhagen}, {Denmark}, 2005},
	author = {Sahlgren, M},
	year = {2005}
}

@article{sadrzadeh-compositional-2011,
	title = {A {Compositional} {Distributional} {Semantics}, {Two} {Concrete} {Constructions}, and some {Experimental} {Evaluations}},
	volume = {abs/1105.1},
	journal = {CoRR},
	author = {Sadrzadeh, Mehrnoosh and Grefenstette, Edward},
	year = {2011}
}

@article{rohrbach-long-short-nodate,
	title = {The {Long}-{Short} {Story} of {Movie} {Description}},
	author = {Rohrbach, Anna and Rohrbach, Marcus and Schiele, Bernt},
	file = {1506.01698v1:/home/user/Zotero/storage/RHP49MZ8/1506.01698v1.pdf:application/pdf}
}

@article{roy-reasoning-2015-1,
	title = {Reasoning about {Quantities} in {Natural} {Language}},
	volume = {3},
	journal = {TACL},
	author = {Roy, Subhro and Vieira, Tim and Roth, Dan},
	year = {2015},
	pages = {1--13},
	file = {452-1484-1-PB:/home/user/Zotero/storage/U4BGP5GK/452-1484-1-PB.pdf:application/pdf}
}

@article{roller-representing-2005,
	title = {Representing {Meaning} with a {Combination} of {Logical} {Form} and {Vectors}},
	author = {Roller, Stephen and Mooney, Raymond J},
	year = {2005},
	pages = {1--44},
	file = {1505.06816v1:/home/user/Zotero/storage/NNBU9VSM/1505.06816v1.pdf:application/pdf}
}

@article{rockt-injecting-2014,
	title = {Injecting {Logical} {Background} {Knowledge} into {Embeddings} for {Relation} {Extraction}},
	author = {Rockt, Tim and Riedel, Sebastian},
	year = {2014},
	file = {Rockt, Riedel - 2014 - Injecting Logical Background Knowledge into Embeddings for Relation Extraction:/home/user/Zotero/storage/TBB3AZK7/Rockt, Riedel - 2014 - Injecting Logical Background Knowledge into Embeddings for Relation Extraction.pdf:application/pdf}
}

@article{ritter-leveraging-nodate,
	title = {Leveraging {Preposition} {Ambiguity} to {Assess} {Representation} of {Semantic} {Interaction} in {CDSM}},
	author = {Ritter, Samuel and Baroni, Marco and Botvinick, Matthew and Paperno, Denis and Goldberg, Adele},
	pages = {1--6},
	file = {30:/home/user/Zotero/storage/64BA37XM/30.pdf:application/pdf}
}

@article{richard-socher-and-john-bauer-and-christopher-d.-manning-and-andrew-y.-ng-parsing-2013-1,
	title = {Parsing {With} {Compositional} {Vector} {Grammars}},
	abstract = {Natural language parsing has typically been done with small sets of discrete categories such as {NP} and {VP}, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar ({CVG)}, which combines {PCFGs} with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The {CVG} improves the {PCFG} of the Stanford Parser by 3.8 \% to obtain an F1 score of 90.4\%. It is fast to train and implemented approximately as an efficient reranker it is about 20 \% faster than the current Stanford factored parser. The {CVG} learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as {PP} attachments. 1},
	journal = {In Proceedings of the {ACL} conference},
	author = {{Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng} and Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
	year = {2013},
	pages = {455--465},
	file = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng et al. - 2013 - Parsing With Compositional Vector Grammars:/home/user/Zotero/storage/NCDTMI6B/Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng et al. - 2013 - Parsing With Compositional Vector Grammars.pdf:application/pdf}
}

@inproceedings{rapp-automatic-1999,
	series = {{ACL} '99},
	title = {Automatic identification of word translations from unrelated {English} and {German} corpora},
	isbn = {1-55860-609-3},
	url = {http://dx.doi.org/10.3115/1034678.1034756},
	doi = {10.3115/1034678.1034756},
	booktitle = {Proceedings of the 37th annual meeting of the {Association} for {Computational} {Linguistics} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rapp, Reinhard},
	year = {1999},
	pages = {519--526}
}

@article{rapp-automatic-2008,
	title = {The automatic generation of thesauri of related words for {English}, {French}, {German}, and {Russian}},
	volume = {11},
	url = {http://dx.doi.org/10.1007/s10772-009-9043-7},
	number = {3},
	journal = {International Journal of Speech Technology},
	author = {Rapp, Reinhard},
	year = {2008},
	keywords = {Semantics-Distributional-Compositional},
	pages = {147--156}
}

@article{ramscar-role-2002,
	title = {The role of meaning in in ection: {Why} the past tense does not require a rule},
	volume = {45},
	journal = {Cognitive Psychology},
	author = {Ramscar, Michael and Ramscar, Michael},
	year = {2002},
	pages = {45--94},
	file = {Ramscar, Ramscar - 2002 - The role of meaning in in ection Why the past tense does not require a rule:/home/user/Zotero/storage/9F836JHA/Ramscar, Ramscar - 2002 - The role of meaning in in ection Why the past tense does not require a rule.pdf:application/pdf}
}

@article{ramscar-running-2010,
	title = {Running down the clock: {The} role of expectation in our understanding of time and motion},
	volume = {25},
	issn = {0169-0965},
	doi = {10.1080/01690960903381166},
	abstract = {Time is often talked about in terms of motion. People talk about themselves ‘moving’ through time, or about time ‘moving’ relative to them. Previous research has shown that attending to actual motion can influence judgements about time. Further, fictive motion language  figurative attributions of motion to static objects in space  has been shown to have much the same effect, suggesting that thought about space influences thought about time. However, evidence to date on fictive motion comes from experiments that included some degree of actual motion, such as drawing. In a series of four experiments, we tease apart the influence of actual motion and fictive motion language on people’s understanding of time. The results suggest that the similar ways in which people talk about motion through space and motion through time play an important part in their common underlying conceptualisation. This has important implications for our understanding of what comprises literal and metaphorical uses of language, and for the relationship between language, language use, and thought.},
	number = {00},
	journal = {Language and Cognitive Processes},
	author = {Ramscar, Michael and Matlock, Teenie and Dye, Melody},
	year = {2010},
	pages = {589--615},
	file = {Ramscar, Matlock, Dye - 2010 - Running down the clock The role of expectation in our understanding of time and motion:/home/user/Zotero/storage/3WTAZIV5/Ramscar, Matlock, Dye - 2010 - Running down the clock The role of expectation in our understanding of time and motion.pdf:application/pdf}
}

@article{ramscar-children-2013,
	title = {Children value informativity over logic in word learning.},
	volume = {24},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23610135},
	doi = {10.1177/0956797612460691},
	abstract = {The question of how children learn the meanings of words has long puzzled philosophers and psychologists. As Quine famously pointed out, simply hearing a word in context reveals next to nothing about its meaning. How then do children learn to understand and use words correctly? Here, we show how learning theory can offer an elegant solution to this seemingly intractable puzzle in language acquisition. From it, we derived formal predictions about word learning in situations of Quinean ambiguity, and subsequently tested our predictions on toddlers, undergraduates, and developmental psychologists. The toddlers' performance was consistent both with our predictions and with the workings of implicit mechanisms that can facilitate the learning of meaningful lexical systems. Adults adopted a markedly different and likely suboptimal strategy. These results suggest one explanation for why early word learning can appear baffling: Adult intuitions may be a poor source of insight into how children learn.},
	journal = {Psychological science},
	author = {Ramscar, Michael and Dye, Melody and Klein, Joseph},
	year = {2013},
	keywords = {language, Learning, prediction, 12, 14, 27, cognitive development, how do children figure, language development, out the meanings of, received 2, revision accepted 8, the words},
	pages = {1017--23},
	file = {Ramscar, Dye, Klein - 2013 - Children value informativity over logic in word learning:/home/user/Zotero/storage/KB4JGDQN/Ramscar, Dye, Klein - 2013 - Children value informativity over logic in word learning.pdf:application/pdf}
}

@article{ramscar-when-2012,
	title = {When the fly flied and when the fly flew: {How} semantics affect the processing of inflected verbs},
	doi = {10.1080/01690965.2011.649041},
	journal = {Language and Cognitive Processes},
	author = {Ramscar, Michael and Dye, Melody and Hübner, Malte},
	year = {2012},
	keywords = {Semantics, prediction, at first blush an, discrimination, has long been at, how does the cognitive, innocuous question, key debate between conflicting, obscure and recondite aspect, of language, past-tense inflection, processing, system mark the past, tense of verbs, the forefront of a, theories, this seemingly},
	pages = {1--30},
	file = {Ramscar, Dye, Hübner - 2012 - When the fly flied and when the fly flew How semantics affect the processing of inflected verbs:/home/user/Zotero/storage/BNE2BVHE/Ramscar, Dye, Hübner - 2012 - When the fly flied and when the fly flew How semantics affect the processing of inflected verbs.pdf:application/pdf}
}

@inproceedings{rachidi-effect-2007,
	title = {The {Effect} of {Full} and {Partial} {Diacritization} on {Arabic} {Root} {Extraction}},
	booktitle = {Proceeding of the {Second} {International} {Conference} on {Arabic} {Language} {Processing}},
	author = {Rachidi, T and {A. Chekayri} and {M. Mhamdi} and {O. Chhoul} and {A. Fala}},
	year = {2007},
	pages = {189--200}
}

@article{polajnar-using-nodate,
	title = {Using {Sentence} {Plausibility} to {Learn} the {Semantics} of {Transitive} {Verbs}},
	author = {Polajnar, Tamara and Rimell, Laura and Clark, Stephen},
	pages = {1--6},
	file = {1411.7942v2:/home/user/Zotero/storage/9MV2IAFQ/1411.7942v2.pdf:application/pdf}
}

@article{peters-identifying-2009-1,
	title = {Identifying {Cause} and {Effect} on {Discrete} {Data} using {Additive} {Noise} {Models}},
	number = {2008},
	journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {Peters, Jonas and Janzing, Dominik and Sch, Bernhard},
	year = {2009},
	file = {Peters, Janzing, Sch - 2009 - Identifying Cause and Effect on Discrete Data using Additive Noise Models:/home/user/Zotero/storage/APV2W79T/Peters, Janzing, Sch - 2009 - Identifying Cause and Effect on Discrete Data using Additive Noise Models.pdf:application/pdf}
}

@article{ramscar-learning-2011,
	title = {Learning language from the input: {Why} innate constraints can't explain noun compounding},
	volume = {62},
	issn = {0010-0285},
	url = {http://dx.doi.org/10.1016/j.cogpsych.2010.10.001},
	doi = {10.1016/j.cogpsych.2010.10.001},
	abstract = {Do the production and interpretation of patterns of plural forms in noun-noun compounds reveal the workings of innate constraints that govern morphological processing? The results of previous studies on compounding have been taken to support a number of important theoretical claims: first, that there are fundamental differences in the way that children and adults learn and process regular and irregular plurals, second, that these differences reflect formal constraints that govern the way the way regular and irregular plurals are processed in language, and third, that these constraints are unlikely to be the product of learning. In a series of seven experiments, we critically assess the evidence that is cited in support of these arguments. The results of our experiments provide little support for the idea that substantively different factors govern the patterns of acquisition, production and interpretation patterns of regular and irregular plural forms in compounds. Once frequency differences between regular and irregular plurals are accounted for, we find no evidence of any qualitative difference in the patterns of interpretation and production of regular and irregular plural nouns in compounds, in either adults or children. Accordingly, we suggest that the pattern of acquisition of both regular and irregular plurals in compounds is consistent with a simple account, in which children learn the conventions that govern plural compounding using evidence that is readily available in the distribution patterns of adult speech. ?? 2010 Elsevier Inc.},
	number = {1},
	journal = {Cognitive Psychology},
	author = {Ramscar, Michael and Dye, Melody},
	year = {2011},
	keywords = {language, Semantics, Learning, morphology, Compounding, Conventions, Distributional information, Inflection, Informativity, Phonology},
	pages = {1--40},
	file = {Ramscar, Dye - 2011 - Learning language from the input Why innate constraints can't explain noun compounding:/home/user/Zotero/storage/VWVG3XTF/Ramscar, Dye - 2011 - Learning language from the input Why innate constraints can't explain noun compounding.pdf:application/pdf}
}

@article{rabinovich-efficient-nodate-1,
	title = {Efficient {Inference} for {Unsupervised} {Semantic} {Parsing}},
	author = {Rabinovich, Maxim},
	pages = {1--5},
	file = {24:/home/user/Zotero/storage/9UB22PZ5/24.pdf:application/pdf}
}

@article{potts-recursive-2014-1,
	title = {Recursive {Neural} {Networks} for {Learning} {Logical} {Semantics}},
	author = {Potts, Christopher},
	year = {2014},
	pages = {1--10},
	file = {Potts - 2014 - Recursive Neural Networks for Learning Logical Semantics:/home/user/Zotero/storage/ZGE7M2DV/Potts - 2014 - Recursive Neural Networks for Learning Logical Semantics.pdf:application/pdf}
}

@article{perera-semantic-nodate,
	title = {Semantic {Parsing}},
	author = {Perera, Ian},
	file = {PedroDomingos:/home/user/Zotero/storage/WFG6KP27/PedroDomingos.pdf:application/pdf}
}

@article{pearl-causal-2009,
	title = {Causal inference in statistics: {An} overview},
	volume = {3},
	issn = {1935-7516},
	doi = {10.1214/09-SS057},
	abstract = {This reviewpresents empirical researcherswith recent advances in causal inference, and stresses the paradigmatic shifts that must be un- dertaken in moving fromtraditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that un- derly all causal inferences, the languages used in formulating those assump- tions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coher- entmathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interven- tions, (also called “causal effects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attri- bution” or “causes of effects”) and (3) queries about direct and indirect effects (also known as “mediation”). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
	number = {September},
	journal = {Statistics Surveys},
	author = {Pearl, Judea},
	year = {2009},
	keywords = {and phrases, counterfactuals, policy evaluation, causal effects, causes of effects, confounding, graph-, ical methods, mediation, potential-outcome, received september 2009, structural equation models, Structural equation models, confounding, graphical},
	pages = {96--146},
	file = {Pearl - 2009 - Causal inference in statistics An overview:/home/user/Zotero/storage/HDHVG9K9/Pearl - 2009 - Causal inference in statistics An overview.pdf:application/pdf}
}

@article{odic-young-2012,
	title = {Young {Children} ’ s {Understanding} of “ {More} ” and {Discrimination} of {Number}},
	volume = {39},
	doi = {10.1037/a0028874},
	journal = {Journal of experimental psychology. Learning, memory, and cognition},
	author = {Odic, Darko and Pietroski, Paul and Hunter, Tim and Lidz, Jeffrey and Halberda, Justin},
	year = {2012},
	keywords = {comparatives, count, mass-nouns, quantifier acquisition, quantity representation},
	pages = {451--461},
	file = {Odic et al. - 2012 - Young Children ’ s Understanding of “ More ” and Discrimination of Number:/home/user/Zotero/storage/VJJATVSU/Odic et al. - 2012 - Young Children ’ s Understanding of “ More ” and Discrimination of Number.pdf:application/pdf}
}

@inproceedings{pantel-unsupervised-2000,
	series = {{ACL} '00},
	title = {An unsupervised approach to prepositional phrase attachment using contextually similar words},
	url = {http://dx.doi.org/10.3115/1075218.1075232},
	doi = {10.3115/1075218.1075232},
	booktitle = {Proceedings of the 38th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Pantel, Patrick and Lin, Dekang},
	year = {2000},
	pages = {101--108}
}

@inproceedings{ott-comet:-2013,
	title = {{CoMeT}: {Integrating} different levels of linguistic modeling for meaning assessment},
	url = {http://aclweb.org/anthology/S13-2102.pdf},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval})},
	author = {Ott, Niels and Ziai, Ramon and Hahn, Michael and Meurers, Detmar},
	year = {2013},
	pages = {608--616}
}

@inproceedings{ott-evaluating-2010,
	series = {{NEALT} {Proceeding} {Series}},
	title = {Evaluating {Dependency} {Parsing} {Performance} on {{G}}erman {Learner} {Language}},
	url = {http://drni.de/zap/ott-ziai-10},
	booktitle = {Proceedings of the {Ninth} {International} {Workshop} on {Treebanks} and {Linguistic} {Theories} ({TLT}9)},
	author = {Ott, Niels and Ziai, Ramon},
	year = {2010}
}

@inproceedings{och-discriminative-2002,
	title = {Discriminative {Training} and {Maximum} {Entropy} {Models} for {Statistical} {Machine} {Translation}},
	booktitle = {{ACL} 2002},
	author = {Och, Franz Josef and Ney, Hermann},
	year = {2002},
	keywords = {SMT},
	pages = {295--302}
}

@inproceedings{och-minimum-2003,
	title = {Minimum {Error} {Rate} {Training} in {Statistical} {Machine} {Translation}},
	url = {http://acl.ldc.upenn.edu/acl2003/main/pdfs/Och.pdf},
	booktitle = {Proceedings of the 41st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL}'03)},
	author = {Och, Franz Josef},
	year = {2003}
}

@inproceedings{nivre-maltparser:-2005,
	title = {Maltparser: {A} language-independent system for data-driven dependency parsing},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Treebanks} and {Linguistic} {Theories}},
	author = {Nivre, Joakim and Hall, Johan},
	year = {2005},
	keywords = {Parsing},
	pages = {13--95}
}

@article{nielsen-recognizing-2009,
	title = {Recognizing entailment in intelligent tutoring systems},
	volume = {15},
	number = {4},
	journal = {Natural Language Engineering},
	author = {Nielsen, R D and Ward, W and Martin, J H},
	year = {2009},
	pages = {479--501}
}

@article{mooij-probabilistic-2010,
	title = {Probabilistic latent variable models for distinguishing between cause and effect},
	url = {http://eprints.pascal-network.org/archive/00007863/},
	abstract = {We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.},
	number = {26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Mooij, Joris and Stegle, Oliver and Janzing, Dominik and Zhang, Kun and Schölkopf, Bernhard},
	year = {2010},
	pages = {1--9},
	file = {Mooij et al. - 2010 - Probabilistic latent variable models for distinguishing between cause and effect:/home/user/Zotero/storage/G7ERWCKC/Mooij et al. - 2010 - Probabilistic latent variable models for distinguishing between cause and effect.pdf:application/pdf}
}

@inproceedings{mohler-learning-2011,
	title = {Learning to {Grade} {Short} {Answer} {Questions} using {Semantic} {Similarity} {Measures} and {Dependency} {Graph} {Alignments}},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Comnputational} {Linguistics}},
	author = {Mohler, Michael and Bunescu, Razvan and Mihalcea, Rada},
	year = {2011},
	pages = {752--762}
}

@article{nguyen-improving-2015,
	title = {Improving {Topic} {Models} with {Latent} {Feature} {Word} {Representations}},
	volume = {3},
	journal = {TACL},
	author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
	year = {2015},
	pages = {299--313},
	file = {582-1696-1-PB:/home/user/Zotero/storage/JKG9JCGF/582-1696-1-PB.pdf:application/pdf}
}

@article{muskens-natural-2009,
	title = {Natural {Language} {Semantics}},
	number = {June},
	author = {Muskens, Reinhard},
	year = {2009},
	pages = {1--49},
	file = {Muskens - 2010 - Natural Language Semantics:/home/user/Zotero/storage/CI966JPA/Muskens - 2010 - Natural Language Semantics.pdf:application/pdf;Unknown - 2009 - Natural Language Semantics:/home/user/Zotero/storage/T7GCHMNK/Unknown - 2009 - Natural Language Semantics.pdf:application/pdf}
}

@article{mitchell-vector-based-2008,
	title = {Vector-based models of semantic composition},
	journal = {Proceedings of ACL-08: HLT},
	author = {Mitchell, Jeff and Lapata, Mirella},
	year = {2008},
	pages = {236--244}
}

@book{misc-naacl-2015,
	title = {{NAACL} {HLT} 2015 {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	isbn = {978-1-932432-41-1},
	url = {papers2://publication/uuid/9BED23AB-DE6A-4216-AAD1-599980A286D1},
	author = {{Misc}},
	year = {2015},
	file = {N15-1:/home/user/Zotero/storage/8JA39GKQ/N15-1.pdf:application/pdf}
}

@article{mikolov-challenges-2014,
	title = {Challenges in {Development} of {Machine} {Intelligence}},
	author = {Mikolov, Tomas},
	year = {2014}
}

@inproceedings{meurers-web-based-2002,
	title = {A {Web}-based {Instructional} {Platform} for {Constraint}-{Based} {Grammar} {Formalisms} and {Parsing}},
	url = {http://ling.osu.edu/~dm/papers/acl02.html},
	booktitle = {Effective {Tools} and {Methodologies} for {Teaching} {{NLP}} and {{CL}}},
	publisher = {The Association for Computational Linguistics},
	author = {Meurers, Walt Detmar and Penn, Gerald and Richter, Frank},
	editor = {Radev, Dragomir and Brew, Chris},
	year = {2002},
	pages = {18--25}
}

@inproceedings{meurers-evaluating-2011,
	title = {Evaluating {Answers} to {Reading} {Comprehension} {Questions} in {Context}: {Results} for {German} and the {Role} of {Information} {Structure}},
	url = {http://www.aclweb.org/anthology/W11-2401},
	booktitle = {Proceedings of the {TextInfer} 2011 {Workshop} on {Textual} {Entailment}},
	publisher = {Association for Computational Linguistics},
	author = {Meurers, Detmar and Ziai, Ramon and Ott, Niels and Kopp, Janina},
	month = jul,
	year = {2011},
	pages = {1--9}
}

@article{meurers-integrating-2011,
	title = {Integrating {Parallel} {Analysis} {Modules} to {Evaluate} the {Meaning} of {Answers} to {Reading} {Comprehension} {Questions}},
	volume = {21},
	url = {http://purl.org/dm/papers/meurers-ziai-ott-bailey-11.html},
	number = {4},
	journal = {Special Issue on Free-text Automatic Evaluation. International Journal of Continuing Engineering Education and Life-Long Learning (IJCEELL)},
	author = {Meurers, Detmar and Ziai, Ramon and Ott, Niels and Bailey, Stacey},
	year = {2011},
	pages = {355--369}
}

@article{may-tensor-2015-1,
	title = {Tensor {Factorization} via {Matrix} {Factorization}},
	volume = {38},
	author = {May, L G},
	year = {2015},
	file = {1501.07320v2:/home/user/Zotero/storage/NX43249W/1501.07320v2.pdf:application/pdf}
}

@article{matlock-even-2011,
	title = {Even {Abstract} {Motion} {Influences} the {Understanding} of {Time}},
	volume = {26},
	doi = {10.1080/10926488.2011.609065},
	abstract = {Many metaphor theorists argue that our mental experience of time is grounded in our understanding of space, including motion through space. Results from recent experiments – in which people think about motion, which in turn influences their thinking about time – support this position. Still, many questions remain about the nature of the metaphorical connection between time and space. Can the mere suggestion of motion influence how people reason about time, and if so, when and how? Three experiments investigated how thinking about “abstract” motion through sequences of numbers or let- ters would influence reasoning about time. Our results extend earlier psychological work on the link between time and space by showing that even motion in non-physical domains can influence tempo- ral reasoning. The results provide further evidence that metaphorical understanding is grounded in our everyday physical and conceptual experience.},
	journal = {Metaphor and Symbol},
	author = {Matlock, Teenie and Holmes, Kevin J. and Srinivasan, Mahesh and Ramscar, Michael},
	year = {2011},
	pages = {260--271},
	file = {Matlock et al. - 2011 - Even Abstract Motion Influences the Understanding of Time:/home/user/Zotero/storage/CTGZ93C2/Matlock et al. - 2011 - Even Abstract Motion Influences the Understanding of Time.pdf:application/pdf}
}

@inproceedings{meurers-compiling-2010,
	title = {Compiling a {Task}-{Based} {Corpus} for the {Analysis} of {Learner} {Language} in {Context}},
	url = {http://purl.org/dm/papers/meurers-ott-ziai-10.html},
	abstract = {Corpora in linguistics and computational linguistics have traditionally
been assembled from data sources such as newspaper texts, books and,
more recently, the web. While these sources provide large quantities
of language data, typically very little or nothing is known about
the context under which the text has been produced. The only information
an analysis can refer to is the text itself, e.g., when a sentence
is analyzed using the preceding sentences for disambiguation. However,
language is always produced in a concrete extra-linguistic context.
This contextual setting includes world knowledge and situational
knowledge, i.e., the aspects of world knowledge which are relevant
to interpret the given text and the concrete task and situation that
the language was produced for. The notion of a task and the evaluation
of language in context plays a particularly important role in foreign
language teaching and learning (cf., e.g., Ellis 2003) and a representation
of the learner's ability to use language in context and to perform
tasks using appropriate task strategies has been argued to be crucial
for learner modeling (Amaral and Meurers 2008). However, the so-called
learner corpora created to document the language produced by language
learners typically consist only of learner essays. In this paper,
we present our efforts at collecting a longitudinal learner corpus
consisting of the answers to reading comprehension questions, including
an explicit representation of the task context and learner information.
After introducing the data sources and characteristics of the corpus
we are collecting, we discuss the development of the open-source
WELCOME tool, which we have created to facilitate the interdisciplinary
exchange of the contextualized learner corpus between the language
programs providing the data and the computational linguists working
on its encoding and automatic analysis.},
	booktitle = {Proceedings of {Linguistic} {Evidence}},
	author = {Meurers, Detmar and Ott, Niels and Ziai, Ramon},
	year = {2010},
	pages = {214--217}
}

@article{martins-turbo-2010,
	title = {Turbo {Parsers}: {Dependency} {Parsing} by {Approximate} {Variational} {Inference}},
	issn = {1932432868},
	url = {http://www.aclweb.org/anthology/D10-1004\nhttp://dl.acm.org/citation.cfm?id=1870662},
	number = {October},
	journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	author = {Martins, Andre and Smith, Noah and Xing, Eric and Aguiar, Pedro and Figueiredo, Mario},
	year = {2010},
	pages = {34--44},
	file = {D10-1004:/home/user/Zotero/storage/EEFUF7DG/D10-1004.pdf:application/pdf}
}

@article{martins-structured-2011,
	title = {Structured {Sparsity} in {Structured} {Prediction}},
	issn = {978-1-937284-11-4},
	url = {http://aclweb.org/anthology/D/D11/D11-1139.pdf\nhttp://aclweb.org/supplementals/D/D11/D11-1022.Attachment.pdf},
	abstract = {Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient train- ing with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc filtering or L1- regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algo- rithms to handle them. Experiments on three tasks (chunking, entity recognition, and de- pendency parsing) showgains in performance, compactness, and model interpretability.},
	journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	author = {Martins, André F. T. and Smith, Noah a. and Aguiar, Pedro M. Q. and Figueiredo, Mário a. T.},
	year = {2011},
	pages = {1500--1511},
	file = {martins+smith+aguiar+figueiredo.emnlp11b:/home/user/Zotero/storage/H2TVMXW8/martins+smith+aguiar+figueiredo.emnlp11b.pdf:application/pdf}
}

@article{lowe-ubuntu-2015,
	title = {The {Ubuntu} {Dialogue} {Corpus}: {A} {Large} {Dataset} for {Research} in {Unstructured} {Multi}-{Turn} {Dialogue} {Systems}},
	url = {http://arxiv.org/abs/1506.08909},
	abstract = {This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.},
	author = {Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
	year = {2015},
	file = {1506.08909v2:/home/user/Zotero/storage/JKFE2EAX/1506.08909v2.pdf:application/pdf}
}

@article{louis-which-2015,
	title = {Which {Step} {Do} {I} {Take} {First} ? {Troubleshooting} with {Bayesian} {Models}},
	volume = {3},
	author = {Louis, Annie and Lapata, Mirella},
	year = {2015},
	pages = {73--85},
	file = {440-1513-1-PB:/home/user/Zotero/storage/RSPTHA83/440-1513-1-PB.pdf:application/pdf}
}

@incollection{guo-deep-2014-1,
	title = {Deep {Learning} for {Real}-{Time} {Atari} {Game} {Play} {Using} {Offline} {Monte}-{Carlo} {Tree} {Search} {Planning}},
	url = {http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf},
	urldate = {2015-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard L and Wang, Xiaoshi},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3338--3346},
	file = {Guo et al_2014_Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree.pdf:/home/user/Zotero/storage/E4FPQR6A/Guo et al_2014_Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree.pdf:application/pdf;NIPS Snapshort:/home/user/Zotero/storage/JMT8VIGW/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging.html:text/html}
}

@article{ross-reinforcement-2014,
	title = {Reinforcement and {Imitation} {Learning} via {Interactive} {No}-{Regret} {Learning}},
	url = {http://arxiv.org/abs/1406.5979},
	abstract = {Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning.},
	urldate = {2015-12-13},
	journal = {arXiv:1406.5979 [cs, stat]},
	author = {Ross, Stephane and Bagnell, J. Andrew},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.5979},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/PGW783WN/1406.html:text/html;Ross_Bagnell_2014_Reinforcement and Imitation Learning via Interactive No-Regret Learning.pdf:/home/user/Zotero/storage/CJIH8DZP/Ross_Bagnell_2014_Reinforcement and Imitation Learning via Interactive No-Regret Learning.pdf:application/pdf}
}

@article{ross-reduction-2010,
	title = {A {Reduction} of {Imitation} {Learning} and {Structured} {Prediction} to {No}-{Regret} {Online} {Learning}},
	url = {http://arxiv.org/abs/1011.0686},
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
	urldate = {2015-12-13},
	journal = {arXiv:1011.0686 [cs, stat]},
	author = {Ross, Stephane and Gordon, Geoffrey J. and Bagnell, J. Andrew},
	month = nov,
	year = {2010},
	note = {arXiv: 1011.0686},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2WMXIJ74/1011.html:text/html;Ross et al_2010_A Reduction of Imitation Learning and Structured Prediction to No-Regret Online.pdf:/home/user/Zotero/storage/7ZNKKJQD/Ross et al_2010_A Reduction of Imitation Learning and Structured Prediction to No-Regret Online.pdf:application/pdf}
}

@incollection{gabillon-approximate-2013,
	title = {Approximate {Dynamic} {Programming} {Finally} {Performs} {Well} in the {Game} of {Tetris}},
	url = {http://papers.nips.cc/paper/5190-approximate-dynamic-programming-finally-performs-well-in-the-game-of-tetris.pdf},
	urldate = {2015-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Scherrer, Bruno},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {1754--1762},
	file = {Gabillon et al_2013_Approximate Dynamic Programming Finally Performs Well in the Game of Tetris.pdf:/home/user/Zotero/storage/DXGNITT4/Gabillon et al_2013_Approximate Dynamic Programming Finally Performs Well in the Game of Tetris.pdf:application/pdf;NIPS Snapshort:/home/user/Zotero/storage/X5DPD9FD/5190-approximate-dynamic-programming-finally-performs-well-in-the-game-of-tetris.html:text/html}
}

@article{scherrer-approximate-2012,
	title = {Approximate {Modified} {Policy} {Iteration}},
	url = {http://arxiv.org/abs/1205.3054},
	abstract = {Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI's main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.},
	urldate = {2015-12-13},
	journal = {arXiv:1205.3054 [cs]},
	author = {Scherrer, Bruno and Gabillon, Victor and Ghavamzadeh, Mohammad and Geist, Matthieu},
	month = may,
	year = {2012},
	note = {arXiv: 1205.3054},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CX7AQGKN/1205.html:text/html;Scherrer et al_2012_Approximate Modified Policy Iteration.pdf:/home/user/Zotero/storage/UF23KAHE/Scherrer et al_2012_Approximate Modified Policy Iteration.pdf:application/pdf}
}

@article{jaakkola-convergence-1994,
	title = {On the {Convergence} of {Stochastic} {Iterative} {Dynamic} {Programming} {Algorithms}},
	volume = {6},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1994.6.6.1185},
	doi = {10.1162/neco.1994.6.6.1185},
	abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.},
	number = {6},
	urldate = {2015-12-13},
	journal = {Neural Computation},
	author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
	month = nov,
	year = {1994},
	pages = {1185--1201},
	file = {Jaakkola et al_1994_On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf:/home/user/Zotero/storage/PZEQZMPR/Jaakkola et al_1994_On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/4BS25F4I/neco.1994.6.6.html:text/html}
}

@article{schulman-trust-2015,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2015-12-13},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GH38A3UK/1502.html:text/html;Schulman et al_2015_Trust Region Policy Optimization.pdf:/home/user/Zotero/storage/J9VSKFTN/Schulman et al_2015_Trust Region Policy Optimization.pdf:application/pdf}
}

@article{greensmith-variance-2004,
	title = {Variance {Reduction} {Techniques} for {Gradient} {Estimates} in {Reinforcement} {Learning}},
	volume = {5},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1005332.1044710},
	abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
	urldate = {2015-12-13},
	journal = {J. Mach. Learn. Res.},
	author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
	month = dec,
	year = {2004},
	pages = {1471--1530},
	file = {Greensmith et al_2004_Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.pdf:/home/user/Zotero/storage/GPU9N7H4/Greensmith et al_2004_Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.pdf:application/pdf}
}

@article{bahl-neural-nodate,
	title = {Neural {Mechanisms} for {Drosophila} {Contrast} {Vision}},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627315009836},
	doi = {10.1016/j.neuron.2015.11.004},
	abstract = {Summary
Spatial contrast, the difference in adjacent luminance values, provides information about objects, textures, and motion and supports diverse visual behaviors. Contrast computation is therefore an essential element of visual processing. The underlying mechanisms, however, are poorly understood. In human psychophysics, contrast illusions are means to explore such computations, but humans offer limited experimental access. Via behavioral experiments in Drosophila, we find that flies are also susceptible to contrast illusions. Using genetic silencing techniques, electrophysiology, and modeling, we systematically dissect the mechanisms and neuronal correlates underlying the behavior. Our results indicate that spatial contrast computation involves lateral inhibition within the same pathway that computes motion of luminance increments (ON pathway). Yet motion-blind flies, in which we silenced downstream motion-sensitive neurons needed for optomotor behavior, have fully intact contrast responses. In conclusion, spatial contrast and motion cues are first computed by overlapping neuronal circuits which subsequently feed into parallel visual processing streams.},
	urldate = {2015-12-13},
	journal = {Neuron},
	author = {Bahl, Armin and Serbe, Etienne and Meier, Matthias and Ammer, Georg and Borst, Alexander},
	file = {Bahl et al_Neural Mechanisms for Drosophila Contrast Vision.pdf:/home/user/Zotero/storage/J7A252B8/Bahl et al_Neural Mechanisms for Drosophila Contrast Vision.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/P29WJNEG/S0896627315009836.html:text/html}
}

@book{folland-harmonicanalysiscourse.pdf-nodate,
	title = {{HarmonicAnalysisCourse}.pdf},
	url = {https://homepages.warwick.ac.uk/~masmbh/files/HarmonicAnalysisCourse.pdf},
	urldate = {2015-12-12},
	author = {Folland, G. B.},
	file = {HarmonicAnalysisCourse.pdf:/home/user/Zotero/storage/F2X47BH6/HarmonicAnalysisCourse.pdf:application/pdf}
}

@book{folland-real-1999,
	address = {New York},
	edition = {2nd ed},
	series = {Pure and applied mathematics},
	title = {Real analysis: modern techniques and their applications},
	isbn = {978-0-471-31716-6},
	shorttitle = {Real analysis},
	publisher = {Wiley},
	author = {Folland, G. B.},
	year = {1999},
	keywords = {Functions of real variables, Mathematical analysis},
	file = {0modern techniques and their applications (2ed., PAM, Wiley, 1999)(ISBN 0471317160)(600dpi)(T)(402s)_MCat_ - Real Analysis folland.pdf:/home/user/Zotero/storage/5ESAKN5I/Real Analysis folland.pdf:application/pdf}
}

@techreport{pineda-investigation-1989,
	title = {Investigation of {Neural} {Network} {Dynamics}},
	url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA216791},
	urldate = {2015-12-12},
	institution = {DTIC Document},
	author = {Pineda, Fernando J.},
	year = {1989},
	file = {Generalization of back-propagation to recurrent neural networks - PhysRevLett.59.2229:/home/user/Zotero/storage/GDWWDB48/PhysRevLett.59.pdf:application/pdf}
}

@incollection{almeida-backpropagation-1989,
	series = {Springer {Study} {Edition}},
	title = {Backpropagation in {Perceptrons} with {Feedback}},
	copyright = {©1989 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-50892-2 978-3-642-83740-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-83740-1\_22},
	abstract = {Backpropagation has shown to be an efficient learning rule for graded perceptrons. However, as initially introduced, it was limited to feedforward structures. Extension of backpropagation to systems with feedback was done by this author, in [4]. In this paper, this extension is presented, and the error propagation circuit is interpreted as the transpose of the linearized perceptron network. The error propagation network is shown to always be stable during training, and a sufficient condition for the stability of the perceptron network is derived. Finally, potentially useful relationships with Hopfield networks and Boltzmann machines are discussed.},
	language = {en},
	number = {41},
	urldate = {2015-12-12},
	booktitle = {Neural {Computers}},
	publisher = {Springer Berlin Heidelberg},
	author = {Almeida, Luís B.},
	editor = {Eckmiller, Prof Dr-Ing Rolf and Malsburg, Dr Christoph v d},
	year = {1989},
	doi = {10.1007/978-3-642-83740-1\_22},
	keywords = {Artificial Intelligence (incl. Robotics), Computation by Abstract Devices, Processor Architectures},
	pages = {199--208},
	file = {Snapshot:/home/user/Zotero/storage/CS7MRMFK/978-3-642-83740-1_22.html:text/html}
}

@article{fitch-evolving-2015,
	title = {Evolving pragmatics},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215012415},
	doi = {10.1016/j.cub.2015.10.013},
	number = {23},
	urldate = {2015-12-12},
	journal = {Current Biology},
	author = {Fitch, W. Tecumseh},
	month = dec,
	year = {2015},
	pages = {R1110--R1112},
	file = {Fitch_2015_Evolving pragmatics.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fitch_2015_Evolving pragmatics2.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/W3DXR25H/S0960982215012415.html:text/html}
}

@article{brosch-reinforcement-2015-1,
	title = {Reinforcement {Learning} of {Linking} and {Tracing} {Contours} in {Recurrent} {Neural} {Networks}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004489},
	doi = {10.1371/journal.pcbi.1004489},
	abstract = {Author Summary Our experience with the visual world allows us to group image elements that belong to the same perceptual object and to segregate them from other objects and the background. If subjects learn to group contour elements, this experience influences neuronal activity in early visual cortical areas, including the primary visual cortex (V1). Learning presumably depends on alterations in the pattern of connections within and between areas of the visual cortex. However, the processes that control changes in connectivity are not well understood. Here we present the first computational model that can train a neural network to integrate collinear contour elements into elongated curves and to trace a curve through the visual field. The new learning algorithm trains fully recurrent neural networks, provided the connectivity causes the networks to reach a stable state. The model reproduces the behavioral performance of monkeys trained in these tasks and explains the patterns of neuronal activity in the visual cortex that emerge during learning, which is remarkable because the only feedback for the model is a reward for successful trials. We discuss a number of the model predictions that can be tested in future neuroscientific work.},
	number = {10},
	urldate = {2015-12-12},
	journal = {PLoS Comput Biol},
	author = {Brosch, Tobias and Neumann, Heiko and Roelfsema, Pieter R.},
	month = oct,
	year = {2015},
	pages = {e1004489},
	file = {Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Brosch et al_2015_Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural.pdf:application/pdf}
}

@misc{noauthor-plos-nodate-1,
	title = {{PLOS} {Computational} {Biology}: {Network} {Plasticity} as {Bayesian} {Inference}},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004485},
	urldate = {2015-12-12},
	file = {PLOS Computational Biology\: Network Plasticity as Bayesian Inference:/home/user/Zotero/storage/GKEBEDDW/article.html:text/html}
}

@article{herbert-read-collective-2015,
	title = {Collective {Behaviour}: {Leadership} and {Learning} in {Flocks}},
	volume = {25},
	issn = {0960-9822},
	shorttitle = {Collective {Behaviour}},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215012932},
	doi = {10.1016/j.cub.2015.10.031},
	abstract = {Summary
A new study has decoded which birds become leaders in homing pigeon flocks, finding an unexpected benefit of leadership: faster birds emerge as leaders, and these leaders learn more about their environment than their followers.},
	number = {23},
	urldate = {2015-12-12},
	journal = {Current Biology},
	author = {Herbert-Read, James},
	month = dec,
	year = {2015},
	pages = {R1127--R1129},
	file = {Herbert-Read_2015_Collective Behaviour.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Herbert-Read_2015_Collective Behaviour.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/IN9MT3E8/S0960982215012932.html:text/html}
}

@article{peever-human-2015,
	title = {Human {Behavior}: {Sleep} in {Hunter}–{Gatherer} {Societies}},
	volume = {25},
	issn = {0960-9822},
	shorttitle = {Human {Behavior}},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215012403},
	doi = {10.1016/j.cub.2015.10.012},
	abstract = {Summary
How much would we sleep if we lived without the pressures and distractions associated with industrialized lifestyles? New research shows that hunter–gatherer societies sleep for 6–7 hours a night — a level similar to industrialized societies.},
	number = {23},
	urldate = {2015-12-12},
	journal = {Current Biology},
	author = {Peever, John and Horner, Richard L.},
	month = dec,
	year = {2015},
	pages = {R1133--R1135},
	file = {Peever_Horner_2015_Human Behavior.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Peever_Horner_2015_Human Behavior.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CGRA4G59/S0960982215012403.html:text/html}
}

@article{fitch-evolving-2015-1,
	title = {Evolving pragmatics},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215012415},
	doi = {10.1016/j.cub.2015.10.013},
	number = {23},
	urldate = {2015-12-12},
	journal = {Current Biology},
	author = {Fitch, W. Tecumseh},
	month = dec,
	year = {2015},
	pages = {R1110--R1112},
	file = {Fitch_2015_Evolving pragmatics.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Fitch_2015_Evolving pragmatics.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/KJK9MQB2/S0960982215012415.html:text/html}
}

@article{bartholomew-analysis-2015,
	title = {Analysis of {Genetic} and {Non}-{Genetic} {Factors} {Influencing} {Timing} and {Time} {Perception}},
	volume = {10},
	url = {http://dx.doi.org/10.1371/journal.pone.0143873},
	doi = {10.1371/journal.pone.0143873},
	abstract = {Performance on different psychophysical tasks measuring the sense of time indicates a large amount of individual variation in the accuracy and precision of timing in the hundredths of milliseconds-to-minutes range. Quantifying factors with an influence on timing is essential to isolating a biological (genetic) contribution to the perception and estimation of time. In the largest timing study to date, 647 participants completed a duration-discrimination task in the sub-second range and a time-production task in the supra-second range. We confirm the stability of a participant’s time sense across multiple sessions and substantiate a modest sex difference on time production. Moreover, we demonstrate a strong correlation between performance on a standardized cognitive battery and performance in both duration-discrimination and time-production tasks; we further show that performance is uncorrelated with age after controlling for general intelligence. Additionally, we find an effect of ethnicity on time sense, with African Americans and possibly Hispanics in our cohort differing in accuracy and precision from other ethnic groups. Finally, a preliminary genome-wide association and exome chip study was performed on 148 of the participants, ruling out the possibility for a single common variant or groups of low-frequency coding variants within a single gene to explain more than {\textasciitilde}18\% of the variation in the sense of time.},
	number = {12},
	urldate = {2015-12-12},
	journal = {PLoS ONE},
	author = {Bartholomew, Alex J. and Meck, Warren H. and Cirulli, Elizabeth T.},
	month = dec,
	year = {2015},
	pages = {e0143873},
	file = {Bartholomew et al_2015_Analysis of Genetic and Non-Genetic Factors Influencing Timing and Time.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bartholomew et al_2015_Analysis of Genetic and Non-Genetic Factors Influencing Timing and Time2.pdf:application/pdf}
}

@article{konigs-impaired-2015,
	title = {Impaired {Visual} {Integration} in {Children} with {Traumatic} {Brain} {Injury}: {An} {Observational} {Study}},
	volume = {10},
	shorttitle = {Impaired {Visual} {Integration} in {Children} with {Traumatic} {Brain} {Injury}},
	url = {http://dx.doi.org/10.1371/journal.pone.0144395},
	doi = {10.1371/journal.pone.0144395},
	abstract = {Background Axonal injury after traumatic brain injury (TBI) may cause impaired sensory integration. We aim to determine the effects of childhood TBI on visual integration in relation to general neurocognitive functioning. Methods We compared children aged 6–13 diagnosed with TBI (n = 103; M = 1.7 years post-injury) to children with traumatic control (TC) injury (n = 44). Three TBI severity groups were distinguished: mild TBI without risk factors for complicated TBI (mildRF- TBI, n = 22), mild TBI with ≥1 risk factor (mildRF+ TBI, n = 46) or moderate/severe TBI (n = 35). An experimental paradigm measured speed and accuracy of goal-directed behavior depending on: (1) visual identification; (2) visual localization; or (3) both, measuring visual integration. Group-differences on reaction time (RT) or accuracy were tracked down to task strategy, visual processing efficiency and extra-decisional processes (e.g. response execution) using diffusion model analysis. General neurocognitive functioning was measured by a Wechsler Intelligence Scale short form. Results The TBI group had poorer accuracy of visual identification and visual integration than the TC group (Ps ≤ .03; ds ≤ -0.40). Analyses differentiating TBI severity revealed that visual identification accuracy was impaired in the moderate/severe TBI group (P = .05, d = -0.50) and that visual integration accuracy was impaired in the mildRF+ TBI group and moderate/severe TBI group (Ps {\textless} .02, ds ≤ -0.56). Diffusion model analyses tracked impaired visual integration accuracy down to lower visual integration efficiency in the mildRF+ TBI group and moderate/severe TBI group (Ps {\textless} .001, ds ≤ -0.73). Importantly, intelligence impairments observed in the TBI group (P = .009, d = -0.48) were statistically explained by visual integration efficiency (P = .002). Conclusions Children with mildRF+ TBI or moderate/severe TBI have impaired visual integration efficiency, which may contribute to poorer general neurocognitive functioning.},
	number = {12},
	urldate = {2015-12-12},
	journal = {PLoS ONE},
	author = {Königs, Marsh and Weeda, Wouter D. and van Heurn, L. W. Ernest and Vermeulen, R. Jeroen and Goslings, J. Carel and Luitse, Jan S. K. and Poll-Thé, Bwee Tien and Beelen, Anita and van der Wees, Marleen and Kemps, Rachèl J. J. K. and Catsman-Berrevoets, Coriene E. and Oosterlaan, Jaap},
	month = dec,
	year = {2015},
	pages = {e0144395},
	file = {Königs et al_2015_Impaired Visual Integration in Children with Traumatic Brain Injury.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Königs et al_2015_Impaired Visual Integration in Children with Traumatic Brain Injury2.pdf:application/pdf}
}

@article{mattar-functional-2015,
	title = {A {Functional} {Cartography} of {Cognitive} {Systems}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004533},
	doi = {10.1371/journal.pcbi.1004533},
	abstract = {Author Summary As we go about the day, our brains must quickly adapt in accordance with our internal goals. These modifications generally occur within the constraints of a fixed structural architecture, thus manifesting as rapid changes in the recruitment of and integration between brain regions conforming to task demands. For example, regions of the visual system are preferentially recruited as we perform a visual task, motor regions are recruited in the execution of actions, frontal networks bias signals to other networks by differentially integrating across them, and the default mode network is activated during introspection as opposed to task execution. However, there is as yet no common framework under which the role of each cognitive system in a task can be defined in a normative way with respect to both other systems and the rest of the brain. Here, we address this issue by formalizing a network-based theoretical framework in which the role of each cognitive system is defined based on its level of recruitment and integration defined dynamically across a battery of cognitive tasks.},
	number = {12},
	urldate = {2015-12-12},
	journal = {PLoS Comput Biol},
	author = {Mattar, Marcelo G. and Cole, Michael W. and Thompson-Schill, Sharon L. and Bassett, Danielle S.},
	month = dec,
	year = {2015},
	pages = {e1004533},
	file = {Mattar et al_2015_A Functional Cartography of Cognitive Systems.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Mattar et al_2015_A Functional Cartography of Cognitive Systems.pdf:application/pdf}
}

@article{senju-early-2015,
	title = {Early {Social} {Experience} {Affects} the {Development} of {Eye} {Gaze} {Processing}},
	volume = {25},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S0960982215012476/abstract},
	doi = {10.1016/j.cub.2015.10.019},
	abstract = {Eye gaze is a key channel of non-verbal communication in humans [ 1–3 ]. Eye contact with others is present from birth [ 4 ], and eye gaze processing is crucial for social learning and adult-infant communication [ 5–7 ]. However, little is known about the effect of selectively different experience of eye contact and gaze communication on early social and communicative development. To directly address this question, we assessed 14 sighted infants of blind parents (SIBPs) longitudinally at 6–10 and 12–16 months. Face scanning [ 8 ] and gaze following [ 7, 9 ] were assessed using eye tracking. In addition, naturalistic observations were made when the infants were interacting with their blind parent and with an unfamiliar sighted adult. Established measures of emergent autistic-like behaviors [ 10 ] and standardized tests of cognitive, motor, and linguistic development [ 11 ] were also collected. These data were then compared with those obtained from a group of infants of sighted parents. Despite showing typical social skills development overall, infants of blind parents allocated less attention to adult eye movements and gaze direction, an effect that increased between 6–10 and 12–16 months of age. The results suggest that infants adjust their use of adults’ eye gaze depending on gaze communication experience from early in life. The results highlight that human functional brain development shows selective experience-dependent plasticity adaptive to the individual’s specific social environment.},
	language = {English},
	number = {23},
	urldate = {2015-12-12},
	journal = {Current Biology},
	author = {Senju, Atsushi and Vernetti, Angélina and Ganea, Natasa and Hudry, Kristelle and Tucker, Leslie and Charman, Tony and Johnson, Mark H.},
	month = jul,
	year = {2015},
	pages = {3086--3091},
	file = {Senju et al_2015_Early Social Experience Affects the Development of Eye Gaze Processing.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Senju et al_2015_Early Social Experience Affects the Development of Eye Gaze Processing.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XZ76R7ST/S0960-9822(15)01247-6.html:text/html}
}

@article{demberg-winterfors-broad-coverage-2010,
	title = {Broad-coverage model of prediction in human sentence processing},
	url = {https://www.era.lib.ed.ac.uk/handle/1842/4785},
	urldate = {2015-12-11},
	author = {Demberg-Winterfors, Vera},
	year = {2010},
	file = {() - PhDThesis.pdf:/home/user/Zotero/storage/9AWF6NVM/PhDThesis.pdf:application/pdf}
}

@incollection{peters-natural-2005,
	title = {Natural actor-critic},
	url = {http://link.springer.com/chapter/10.1007/11564096\_29},
	urldate = {2015-12-11},
	booktitle = {Machine {Learning}: {ECML} 2005},
	publisher = {Springer},
	author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
	year = {2005},
	pages = {280--291},
	file = {NaturalActorCritic.pdf:/home/user/Zotero/storage/MNNTQS8W/NaturalActorCritic.pdf:application/pdf}
}

@inproceedings{sutton-policy-1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}.},
	volume = {99},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.696&rep=rep1&type=pdf},
	urldate = {2015-12-11},
	booktitle = {{NIPS}},
	publisher = {Citeseer},
	author = {Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay and {others}},
	year = {1999},
	pages = {1057--1063},
	file = {nips99.tex - SMSM-NIPS99.pdf:/home/user/Zotero/storage/QNBFDCKE/SMSM-NIPS99.pdf:application/pdf}
}

@article{peters-reinforcement-2008,
	title = {Reinforcement learning of motor skills with policy gradients},
	volume = {21},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608008000701},
	doi = {10.1016/j.neunet.2008.02.003},
	language = {en},
	number = {4},
	urldate = {2015-12-11},
	journal = {Neural Networks},
	author = {Peters, Jan and Schaal, Stefan},
	month = may,
	year = {2008},
	pages = {682--697},
	file = {1-s2.0-S0893608008000701-main.pdf:/home/user/Zotero/storage/ZH4PR54R/1-s2.0-S0893608008000701-main.pdf:application/pdf}
}

@article{braziunas-pomdp-2003,
	title = {Pomdp solution methods},
	url = {http://www.cs.toronto.edu/~darius/papers/POMDP\_survey.pdf},
	urldate = {2015-12-11},
	journal = {University of Toronto, Tech. Rep},
	author = {Braziunas, Darius},
	year = {2003},
	file = {survey.dvi - POMDP_solution.pdf:/home/user/Zotero/storage/JDUH4WVG/POMDP_solution.pdf:application/pdf}
}

@inproceedings{kim-autonomous-2003,
	title = {Autonomous helicopter flight via reinforcement learning},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2003\_CN07.pdf},
	urldate = {2015-12-11},
	booktitle = {Advances in neural information processing systems},
	author = {Kim, H. J. and Jordan, Michael I. and Sastry, Shankar and Ng, Andrew Y.},
	year = {2003},
	pages = {None},
	file = {ng-etal03.pdf:/home/user/Zotero/storage/I43N3M75/ng-etal03.pdf:application/pdf}
}

@article{peters-reinforcement-2008-1,
	title = {Reinforcement learning of motor skills with policy gradients},
	volume = {21},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608008000701},
	doi = {10.1016/j.neunet.2008.02.003},
	language = {en},
	number = {4},
	urldate = {2015-12-11},
	journal = {Neural Networks},
	author = {Peters, Jan and Schaal, Stefan},
	month = may,
	year = {2008},
	pages = {682--697},
	file = {Peters_Schaal_2008_Reinforcement learning of motor skills with policy gradients.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Peters_Schaal_2008_Reinforcement learning of motor skills with policy gradients.pdf:application/pdf}
}

@article{zhang-tree-2015,
	title = {Tree {Recurrent} {Neural} {Networks} with {Application} to {Language} {Modeling}},
	url = {http://arxiv.org/abs/1511.00060},
	abstract = {In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.},
	urldate = {2015-11-03},
	journal = {arXiv:1511.00060 [cs]},
	author = {Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
	month = oct,
	year = {2015},
	note = {arXiv: 1511.00060},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WT4CU2N8/1511.html:text/html;Snapshot:/home/user/Zotero/storage/TJVFQQD2/3738.html:text/html;Zhang et al_2015_Tree Recurrent Neural Networks with Application to Language Modeling.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Zhang et al_2015_Tree Recurrent Neural Networks with Application to Language Modeling.pdf:application/pdf}
}

@article{gesmundo-undirected-2014,
	title = {Undirected machine translation with discriminative reinforcement learning},
	url = {http://www.aclweb.org/website/old\_anthology/E/E14/E14-1.pdf#page=36},
	urldate = {2015-12-10},
	journal = {EACL 2014},
	author = {Gesmundo, Andrea and Henderson, James},
	year = {2014},
	pages = {10},
	file = {Undirected Machine Translation with Discriminative Reinforcement Learning - E14-1002.pdf:/home/user/Zotero/storage/T6BPZ3GB/E14-1002.pdf:application/pdf}
}

@article{kennedy-parafoveal--foveal-2005,
	title = {Parafoveal-on-foveal effects in normal reading},
	volume = {45},
	issn = {00426989},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698904003979},
	doi = {10.1016/j.visres.2004.07.037},
	language = {en},
	number = {2},
	urldate = {2015-12-10},
	journal = {Vision Research},
	author = {Kennedy, Alan and Pynte, Joël},
	month = jan,
	year = {2005},
	pages = {153--168},
	file = {doi\:10.1016/j.visres.2004.07.037 - 1-s2.0-S0042698904003979-main.pdf:/home/user/Zotero/storage/AEAQDEFG/1-s2.0-S0042698904003979-main.pdf:application/pdf}
}

@article{hermann-teaching-2015,
	title = {Teaching machines to read and comprehend},
	url = {http://arxiv.org/abs/1506.03340},
	urldate = {2015-10-27},
	journal = {arXiv preprint arXiv:1506.03340},
	author = {Hermann, Karl Moritz and Ko{\v c}isk{\` y}, Tom{\' a}{\v s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	year = {2015},
	file = {1506.03340v1.pdf:/home/user/Zotero/storage/XVCRDQHB/1506.03340v1.pdf:application/pdf}
}

@inproceedings{xu-show-2015,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	shorttitle = {Show, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	urldate = {2015-10-27},
	booktitle = {International {Conference} for {Machine} {Learning}},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	year = {2015},
	note = {arXiv: 1502.03044},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6VVNBXIB/1502.html:text/html;Xu et al_2015_Show, Attend and Tell.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Xu et al_2015_Show, Attend and Tell.pdf:application/pdf}
}

@inproceedings{sutskever-sequence-2014,
	title = {Sequence to sequence learning with neural networks},
	url = {http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces},
	urldate = {2015-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc VV},
	year = {2014},
	pages = {3104--3112},
	file = {Sutskever et al_2014_Sequence to sequence learning with neural networks.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Sutskever et al_2014_Sequence to sequence learning with neural networks.pdf:application/pdf}
}

@article{frank-cross-linguistic-2015,
	title = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}: {Working}-{Memory} {Constraints} or {Language} {Statistics}?},
	shorttitle = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12247/full},
	urldate = {2015-10-26},
	journal = {Cognitive Science},
	author = {Frank, Stefan L. and Trompenaars, Thijs and Vasishth, Shravan},
	year = {2015},
	file = {Frank et al_2015_Cross-Linguistic Differences in Processing Double-Embedded Relative Clauses.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank et al_2015_Cross-Linguistic Differences in Processing Double-Embedded Relative Clauses.pdf:application/pdf}
}

@inproceedings{frank-speaking-2008,
	title = {Speaking rationally: {Uniform} information density as an optimal strategy for language production},
	shorttitle = {Speaking rationally},
	url = {https://www.hlp.rochester.edu/resources/workshop\_materials/EVELIN12/FrankJaeger08cogsci.pdf},
	urldate = {2015-12-09},
	booktitle = {Proceedings of the 30th {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	publisher = {Cognitive Science Society Washington, DC},
	author = {Frank, Austin and Jaeger, T. Florian},
	year = {2008},
	pages = {933--938},
	file = {frankjaeger08cogsci.pdf:/home/user/Zotero/storage/NQSFRTP8/frankjaeger08cogsci.pdf:application/pdf}
}

@inproceedings{bahdanau-neural-2015,
	title = {Neural machine translation by jointly learning to align and translate},
	url = {http://arxiv.org/abs/1409.0473},
	urldate = {2015-12-08},
	booktitle = {International {Conference} on {Learning} {Representations}  2015},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2015},
	file = {1409.0473v6.pdf:/home/user/Zotero/storage/MNTPWNS7/1409.0473v6.pdf:application/pdf}
}

@article{rayner-eye-2009-1,
	title = {Eye movements and non-canonical reading: {Comments} on {Kennedy} and {Pynte} (2008)},
	volume = {49},
	issn = {0042-6989},
	shorttitle = {Eye movements and non-canonical reading},
	url = {http://www.sciencedirect.com/science/article/pii/S004269890800535X},
	doi = {10.1016/j.visres.2008.10.013},
	abstract = {Kennedy and Pynte [Kennedy, A., \&amp; Pynte, J. (2008). The consequences of violations to reading order: An eye movement analysis. Vision Research, 48, 2309–2320] presented data that they suggested pose problems for models of eye movement control in reading in which words are encoded serially. They focus on situations in which pairs of words are fixated out of order (i.e., the first word is skipped and the second fixated prior to a regression back to the first word). We strongly disagree with their claims and contest their arguments. We argue that their data set was obtained selectively and the events they believe are problematic do not occur frequently during reading. Furthermore, we do not consider that Kennedy and Pynte’s arguments pose serious difficulties for serial models of reading such as E-Z Reader.},
	number = {17},
	urldate = {2015-12-10},
	journal = {Vision Research},
	author = {Rayner, Keith and Pollatsek, Alexander and Liversedge, Simon P. and Reichle, Erik D.},
	month = aug,
	year = {2009},
	keywords = {Reading, Eye movement control, Models of eye movement control in reading},
	pages = {2232--2236},
	file = {Rayner et al_2009_Eye movements and non-canonical reading.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Rayner et al_2009_Eye movements and non-canonical reading.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/AKEIUHGK/S004269890800535X.html:text/html}
}

@article{pickering-prediction-2015,
	title = {Prediction and learning in the dynamics of speaking},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1117645},
	doi = {10.1080/23273798.2015.1117645},
	number = {0},
	urldate = {2015-12-10},
	journal = {Language, Cognition and Neuroscience},
	author = {Pickering, Martin J. and Gambi, Chiara},
	month = dec,
	year = {2015},
	pages = {1--3},
	file = {Pickering_Gambi_2015_Prediction and learning in the dynamics of speaking.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Pickering_Gambi_2015_Prediction and learning in the dynamics of speaking.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HP7HVKH9/23273798.2015.html:text/html}
}

@article{weaver-optimal-2013,
	title = {The {Optimal} {Reward} {Baseline} for {Gradient}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1301.2315},
	abstract = {There exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward. Their long-runconvergence has been proved, even in partially observableenvironments with non-deterministic actions, and without the need fora system model. However, the variance of the gradient estimator hasbeen found to be a significant practical problem. Recent approacheshave discounted future rewards, introducing a bias-variance trade-offinto the gradient estimate. We incorporate a reward baseline into thelearning system, and show that it affects variance without introducingfurther bias. In particular, as we approach the zero-bias,high-variance parameterization, the optimal (or variance minimizing)constant reward baseline is equal to the long-term average expectedreward. Modified policy-gradient algorithms are presented, and anumber of experiments demonstrate their improvement over previous work.},
	urldate = {2015-12-10},
	journal = {arXiv:1301.2315 [cs, stat]},
	author = {Weaver, Lex and Tao, Nigel},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.2315},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4IWMUHHB/1301.html:text/html;Weaver_Tao_2013_The Optimal Reward Baseline for Gradient-Based Reinforcement Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Weaver_Tao_2013_The Optimal Reward Baseline for Gradient-Based Reinforcement Learning.pdf:application/pdf}
}

@article{chien-priming-2016,
	title = {Priming the representation of {Mandarin} tone 3 sandhi words},
	volume = {31},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1064976},
	doi = {10.1080/23273798.2015.1064976},
	abstract = {Phonological alternation poses problems for spoken word recognition. In Mandarin Tone 3 sandhi, a Tone 3 syllable changes to a Tone 2 syllable when followed by another Tone 3 syllable. A traditional phonological account assumes that the initial syllable of Mandarin disyllabic sandhi words is Tone 3 (T3) underlyingly, but becomes Tone 2 (T2) on the surface. In an auditory–auditory priming lexical decision experiment, each disyllabic tone sandhi target word (e.g., chu3-li3) was preceded by one of three monosyllabic primes: a T2 prime (Surface-Tone overlap) (chu2), a T3 prime (Underlying-Tone overlap) (chu3), or a control prime (Baseline condition) (chu1). Results showed that Tone 3 primes (Underlying-Tone) elicited significantly stronger facilitation effects for the sandhi targets than Tone 2 primes (Surface-Tone), with little effect of target frequency. The data are examined in terms of the contribution of underlying representations for models of spoken word recognition.},
	number = {2},
	urldate = {2015-12-09},
	journal = {Language, Cognition and Neuroscience},
	author = {Chien, Yu-Fu and Sereno, Joan A. and Zhang, Jie},
	month = feb,
	year = {2016},
	pages = {179--189},
	file = {Chien et al_2016_Priming the representation of Mandarin tone 3 sandhi words.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Chien et al_2016_Priming the representation of Mandarin tone 3 sandhi words.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7SQZQH6I/23273798.2015.html:text/html}
}

@article{smith-transsexual-2015,
	title = {The transsexual brain – {A} review of findings on the neural basis of transsexualism},
	volume = {59},
	issn = {0149-7634},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415002432},
	doi = {10.1016/j.neubiorev.2015.09.008},
	abstract = {Transsexualism describes the condition when a person's psychological gender differs from his or her biological sex and is commonly thought to arise from a discrepant cerebral and genital sexual differentiation. This review intends to give an extensive overview of structural and functional neurobiological correlates of transsexualism and their course under cross-sex hormonal treatment. Research in this field enables insight into the stability or variability of gender differences and their relation to hormonal status. For a number of sexually dimorphic brain structures or processes, signs of feminisation or masculinisation are observable in transsexual individuals, which, during hormonal treatment, partly seem to further adjust to characteristics of the desired sex. Still, it appears the data are quite inhomogeneous, mostly not replicated and in many cases available for male-to-female transsexuals only. As the prevalence of homosexuality is markedly higher among transsexuals than among the general population, disentangling correlates of sexual orientation and gender identity is a major problem. To resolve such deficiencies, the implementation of specific research standards is proposed.},
	urldate = {2015-12-09},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Smith, Elke Stefanie and Junger, Jessica and Derntl, Birgit and Habel, Ute},
	month = dec,
	year = {2015},
	keywords = {MRI, Brain, Gender dysphoria, Gender identity disorder, Transsexualism},
	pages = {251--266},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/69MG9EMK/S0149763415002432.html:text/html;Smith et al_2015_The transsexual brain – A review of findings on the neural basis of.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Smith et al_2015_The transsexual brain – A review of findings on the neural basis of.pdf:application/pdf}
}

@article{ouyang-prosody-2015,
	title = {Prosody marks different kinds of informativity: {Interactions} between frequency, probability and focus},
	volume = {21},
	shorttitle = {Prosody marks different kinds of informativity},
	url = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1853&context=pwpl},
	number = {1},
	urldate = {2015-12-09},
	journal = {University of Pennsylvania Working Papers in Linguistics},
	author = {Ouyang, Iris and Kaiser, Elsi},
	year = {2015},
	pages = {24},
	file = {Prosody Marks Different Kinds of Informativity\: Interactions Between Frequency, Probability and Focus - viewcontent.cgi:/home/user/Zotero/storage/P7HGXHHF/viewcontent.pdf:application/pdf}
}

@inproceedings{ouyang-prosodic-2014,
	title = {Prosodic {Encoding} of {Informativity}: {Word} {Frequency} and {Contextual} {Probability} {Interact} with {Information} {Structure}},
	shorttitle = {Prosodic {Encoding} of {Informativity}},
	url = {https://mindmodeling.org/cogsci2014/papers/199/paper199.pdf},
	urldate = {2015-12-09},
	booktitle = {Proceedings of the 36th {Annual} {Meeting} of the {Cognitive} {Science} {Society} 2014 ({CogSci} 2014)},
	author = {Ouyang, Iris Chuoying and Kaiser, Elsi},
	year = {2014},
	pages = {1120--1125},
	file = {paper199.pdf:/home/user/Zotero/storage/H7GQAB2Q/paper199.pdf:application/pdf}
}

@phdthesis{jaeger-redundancy-2006,
	title = {Redundancy and syntactic reduction in spontaneous speech},
	url = {http://www.researchgate.net/profile/Thomas\_Wasow/publication/35415378\_Redundancy\_and\_syntactic\_reduction\_in\_spontaneous\_speech\_/links/540dc0e50cf2f2b29a3a03f7.pdf},
	urldate = {2015-12-09},
	school = {Stanford University},
	author = {Jaeger, Tim Florian},
	year = {2006},
	file = {my thesis.dvi - thanks.pdf:/home/user/Zotero/storage/EC6P562C/thanks.pdf:application/pdf}
}

@article{frank-predicting-2012-1,
	title = {Predicting {Pragmatic} {Reasoning} in {Language} {Games}},
	volume = {336},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/336/6084/998},
	doi = {10.1126/science.1218633},
	abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers’ intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.},
	language = {en},
	number = {6084},
	urldate = {2015-12-09},
	journal = {Science},
	author = {Frank, Michael C. and Goodman, Noah D.},
	month = may,
	year = {2012},
	pmid = {22628647},
	pages = {998--998},
	file = {Frank_Goodman_2012_Predicting Pragmatic Reasoning in Language Games.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Frank_Goodman_2012_Predicting Pragmatic Reasoning in Language Games.pdf:application/pdf;FrankGoodman-Science2012.pdf:/home/user/Zotero/storage/UQEG4TBF/FrankGoodman-Science2012.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CD9IRSIX/998.html:text/html}
}

@article{labai-hankel-2015,
	title = {Hankel {Matrices} for {Weighted} {Visibly} {Pushdown} {Automata}},
	url = {http://arxiv.org/abs/1512.02430},
	abstract = {Hankel matrices (aka connection matrices) of word functions and graph parameters have wide applications in automata theory, graph theory, and machine learning. We give a characterization of real-valued functions on nested words recognized by weighted visibly pushdown automata in terms of Hankel matrices on nested words. This complements C. Mathissen's characterization in terms of weighted monadic second order logic.},
	urldate = {2015-12-09},
	journal = {arXiv:1512.02430 [cs]},
	author = {Labai, Nadia and Makowsky, Johann A.},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.02430},
	keywords = {Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IPZZ354G/1512.html:text/html;Labai_Makowsky_2015_Hankel Matrices for Weighted Visibly Pushdown Automata.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Labai_Makowsky_2015_Hankel Matrices for Weighted Visibly Pushdown Automata.pdf:application/pdf}
}

@phdthesis{jaeger-redundancy-2006-1,
	title = {Redundancy and syntactic reduction in spontaneous speech},
	url = {http://www.researchgate.net/profile/Thomas\_Wasow/publication/35415378\_Redundancy\_and\_syntactic\_reduction\_in\_spontaneous\_speech\_/links/540dc0e50cf2f2b29a3a03f7.pdf},
	urldate = {2015-12-09},
	school = {Stanford University},
	author = {Jaeger, Tim Florian},
	year = {2006},
	file = {my thesis.dvi - thanks.pdf:/home/user/Zotero/storage/AHISVJM4/thanks.pdf:application/pdf}
}

@inproceedings{jaeger-speakers-2006,
	title = {Speakers optimize information density through syntactic reduction},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2006\_515.pdf},
	urldate = {2015-12-09},
	booktitle = {Advances in neural information processing systems},
	author = {Jaeger, T. F. and Levy, Roger P.},
	year = {2006},
	pages = {849--856},
	file = {Speakers optimize information density through syntactic reduction - 3129-speakers-optimize-information-density-through-syntactic-reduction.pdf:/home/user/Zotero/storage/MJ9CG9R5/3129-speakers-optimize-information-density-through-syntactic-reduction.pdf:application/pdf}
}

@incollection{giry-categorical-1982,
	series = {Lecture {Notes} in {Mathematics}},
	title = {A categorical approach to probability theory},
	copyright = {©1982 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-11211-2 978-3-540-39041-1},
	url = {http://link.springer.com/chapter/10.1007/BFb0092872},
	language = {en},
	number = {915},
	urldate = {2015-12-09},
	booktitle = {Categorical {Aspects} of {Topology} and {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Giry, Michèle},
	editor = {Banaschewski, B.},
	year = {1982},
	doi = {10.1007/BFb0092872},
	keywords = {Algebraic Topology, Analysis},
	pages = {68--85},
	file = {Full Text PDF:/home/user/Zotero/storage/M58IVD4G/Giry - 1982 - A categorical approach to probability theory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TC492ICQ/10.html:text/html}
}

@article{thompson-reading-time-nodate,
	title = {Reading-time data for evaluating broad-coverage models of {English} sentence processing},
	volume = {45},
	url = {http://www.academia.edu/download/30392149/BRM.pdf},
	urldate = {2015-12-08},
	journal = {Behavior Research Methods},
	author = {Thompson, Robin L. and Vigliocco, Gabriella},
	pages = {1182--1190},
	file = {C\:/Users/Stefan/Onderzoek/Papers/Journals/BRM/BRM.dvi - BRM.pdf:/home/user/Zotero/storage/H9RAQQZJ/BRM.pdf:application/pdf}
}

@article{denil-learning-2012,
	title = {Learning where to attend with deep architectures for image tracking},
	volume = {24},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO\_a\_00312},
	number = {8},
	urldate = {2015-12-08},
	journal = {Neural computation},
	author = {Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
	year = {2012},
	pages = {2151--2184},
	file = {1109.3737.pdf:/home/user/Zotero/storage/8R7FG26K/1109.3737.pdf:application/pdf}
}

@inproceedings{butko-i-pomdp:-2008,
	title = {I-{POMDP}: {An} infomax model of eye movement},
	shorttitle = {I-{POMDP}},
	doi = {10.1109/DEVLRN.2008.4640819},
	abstract = {Modeling eye-movements during search is important for building intelligent robotic vision systems, and for understanding how humans select relevant information and structure behavior in real time. Previous models of visual search (VS) rely on the idea of ldquosaliency mapsrdquo which indicate likely locations for targets of interest. In these models the eyes move to locations with maximum saliency. This approach has several drawbacks: (1) It assumes that oculomotor control is a greedy process, i.e., every eye movement is planned as if no further eye movements would be possible after it. (2) It does not account for temporal dynamics and how information is integrated as over time. (3) It does not provide a formal basis to understand how optimal search should vary as a function of the operating characteristics of the visual system. To address these limitations, we reformulate the problem of VS as an Information-gathering Partially Observable Markov Decision Process (I-POMDP). We find that the optimal control law depends heavily on the Foveal-Peripheral Operating Characteristic (FPOC) of the visual system.},
	booktitle = {7th {IEEE} {International} {Conference} on {Development} and {Learning}, 2008. {ICDL} 2008},
	author = {Butko, N.J. and Movellan, Javier R.},
	month = aug,
	year = {2008},
	keywords = {Humans, vision, artificial intelligence, bioinformatics, Buildings, decision theory, eye movement infomax model, Eyes, foveal peripheral operating characteristic, I-POMDP, information gathering partially observable Markov decision process, intelligent robotic vision systems, Intelligent robots, Intelligent structures, Intelligent systems, Machine vision, Markov processes, neural nets, oculomotor control, optimal control law, optimal search variation, real time information selection, real time structure selection, Real time systems, robot vision, Robot vision systems, saliency maps, search problems, temporal dynamics, temporal information integration, visual search models, Visual system, visual system FPOC, visual system operating characteristics},
	pages = {139--144},
	file = {Butko_Movellan_2008_I-POMDP.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Butko_Movellan_2008_I-POMDP.pdf:application/pdf;IEEE Xplore Abstract Record:/home/user/Zotero/storage/BG62KHFT/abs_all.html:text/html}
}

@book{noauthor-combinatorial-2008,
	address = {Berlin, Heidelberg},
	series = {Algorithms and {Combinatorics}},
	title = {Combinatorial {Optimization}},
	volume = {21},
	isbn = {978-3-540-71843-7},
	url = {http://link.springer.com/10.1007/978-3-540-71844-4},
	language = {en},
	urldate = {2015-11-29},
	publisher = {Springer Berlin Heidelberg},
	year = {2008},
	file = {bok%3A978-3-540-71844-4.pdf:/home/user/Zotero/storage/UUAJCJEZ/bok%3A978-3-540-71844-4.pdf:application/pdf}
}

@misc{noauthor-fisher-nodate,
	title = {fisher information},
	url = {http://ejwagenmakers.com/submitted/LyEtAlTutorial.pdf},
	urldate = {2015-11-29},
	file = {LyEtAlTutorial.pdf:/home/user/Zotero/storage/7SZMR4I3/LyEtAlTutorial.pdf:application/pdf}
}

@inproceedings{wang-two-2014,
	title = {Two knives cut better than one: {Chinese} word segmentation with dual decomposition},
	shorttitle = {Two knives cut better than one},
	url = {http://www.aclweb.org/website/old\_anthology/P/P14/P14-2032.pdf},
	urldate = {2015-11-29},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL} 2014), {Baltimore}, {MD}},
	author = {Wang, Mengqiu and Voigt, Rob and Manning, Christopher D.},
	year = {2014},
	file = {cws_wang_acl14.pdf:/home/user/Zotero/storage/DW6TZ5T4/cws_wang_acl14.pdf:application/pdf}
}

@misc{noauthor-linguistic-nodate,
	title = {Linguistic priors shape categorical perception - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1072638#.VlrmkpTjD-4},
	urldate = {2015-11-29},
	file = {Linguistic priors shape categorical perception - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/BP7J87FJ/23273798.2015.html:text/html}
}

@misc{noauthor-is-nodate,
	title = {Is there a common oscillatory brain mechanism for producing and predicting language? - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1077978},
	urldate = {2015-11-29},
	file = {Is there a common oscillatory brain mechanism for producing and predicting language? - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/SPXZVFHS/23273798.2015.html:text/html}
}

@misc{noauthor-neural-nodate,
	title = {Neural systems involved in processing novel linguistic constructions and their visual referents - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1055280#.Vlrmc5TjD-4},
	urldate = {2015-11-29},
	file = {Neural systems involved in processing novel linguistic constructions and their visual referents - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/DR6A86GK/23273798.2015.html:text/html}
}

@misc{noauthor-comprehension-nodate,
	title = {Comprehension without segmentation: a proof of concept with naive discriminative learning - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1065336#.VlrmaZTjD-4},
	urldate = {2015-11-29},
	file = {Comprehension without segmentation\: a proof of concept with naive discriminative learning - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/DQ25KM82/23273798.2015.html:text/html}
}

@misc{noauthor-learning-nodate-1,
	title = {Learning to predict or predicting to learn? - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1077979#.VlrmYZTjD-5},
	urldate = {2015-11-29},
	file = {Learning to predict or predicting to learn? - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/95F8SZ9G/23273798.2015.html:text/html}
}

@misc{noauthor-individual-nodate,
	title = {Individual differences in working memory and processing speed predict anticipatory spoken language processing in the visual world - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1047459#.VlrmWJTjD-5},
	urldate = {2015-11-29},
	file = {Individual differences in working memory and processing speed predict anticipatory spoken language processing in the visual world - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/BW89G7ID/23273798.2015.html:text/html}
}

@misc{noauthor-prediction-nodate,
	title = {Prediction in the processing of repair disfluencies - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1036089#.VlrmTpTjD-4},
	urldate = {2015-11-29},
	file = {Prediction in the processing of repair disfluencies - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/FQ2XA7PN/23273798.2015.html:text/html}
}

@misc{noauthor-predicting-nodate,
	title = {Predicting and imagining language - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1049188#.VlrmPZTjD-4},
	urldate = {2015-11-29},
	file = {Predicting and imagining language - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/AXWR9277/23273798.2015.html:text/html}
}

@misc{noauthor-what-nodate-2,
	title = {What do we mean by prediction in language comprehension? - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 1},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1102299#.VlrmK5TjD-4},
	urldate = {2015-11-29},
	file = {What do we mean by prediction in language comprehension? - Language, Cognition and Neuroscience - Volume 31, Issue 1:/home/user/Zotero/storage/5VHH2GKU/23273798.2015.html:text/html}
}

@article{huettig-is-2016,
	title = {Is prediction necessary to understand language? {Probably} not},
	volume = {31},
	issn = {2327-3798},
	shorttitle = {Is prediction necessary to understand language?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1072223},
	doi = {10.1080/23273798.2015.1072223},
	abstract = {Some recent theoretical accounts in the cognitive sciences suggest that prediction is necessary to understand language. Here we evaluate this proposal. We consider arguments that prediction provides a unified theoretical principle of the human mind and that it pervades cortical function. We discuss whether evidence of human abilities to detect statistical regularities is necessarily evidence for predictive processing and evaluate suggestions that prediction is necessary for language learning. We point out that not all language users appear to predict language and that suboptimal input makes prediction often very challenging. Prediction, moreover, is strongly context-dependent and impeded by resource limitations. We also argue that it may be problematic that most experimental evidence for predictive language processing comes from “prediction-encouraging” experimental set-ups. We conclude that languages can be learned and understood in the absence of prediction. Claims that all language processing is predictive in nature are premature.},
	number = {1},
	urldate = {2015-11-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Huettig, Falk and Mani, Nivedita},
	month = jan,
	year = {2016},
	pages = {19--31},
	file = {Full Text PDF:/home/user/Zotero/storage/4P8Q64D5/Huettig and Mani - 2016 - Is prediction necessary to understand language Pr.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/NWU95KP6/23273798.2015.html:text/html}
}

@article{kuperberg-what-2016,
	title = {What do we mean by prediction in language comprehension?},
	volume = {31},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1102299},
	doi = {10.1080/23273798.2015.1102299},
	abstract = {We consider several key aspects of prediction in language comprehension: its computational nature, the representational level(s) at which we predict, whether we use higher-level representations to predictively pre-activate lower level representations, and whether we “commit” in any way to our predictions, beyond pre-activation. We argue that the bulk of behavioural and neural evidence suggests that we predict probabilistically and at multiple levels and grains of representation. We also argue that we can, in principle, use higher-level inferences to predictively pre-activate information at multiple lower representational levels. We suggest that the degree and level of predictive pre-activation might be a function of its expected utility, which, in turn, may depend on comprehenders’ goals and their estimates of the relative reliability of their prior knowledge and the bottom-up input. Finally, we argue that all these properties of language understanding can be naturally explained and productively explored within a multi-representational hierarchical actively generative architecture whose goal is to infer the message intended by the producer, and in which predictions play a crucial role in explaining the bottom-up input.},
	number = {1},
	urldate = {2015-11-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Kuperberg, Gina R. and Jaeger, T. Florian},
	month = jan,
	year = {2016},
	pages = {32--59},
	file = {Snapshot:/home/user/Zotero/storage/WSIRBGE4/23273798.2015.html:text/html}
}

@article{huettig-is-2016-1,
	title = {Is prediction necessary to understand language? {Probably} not},
	volume = {31},
	issn = {2327-3798},
	shorttitle = {Is prediction necessary to understand language?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1072223},
	doi = {10.1080/23273798.2015.1072223},
	abstract = {Some recent theoretical accounts in the cognitive sciences suggest that prediction is necessary to understand language. Here we evaluate this proposal. We consider arguments that prediction provides a unified theoretical principle of the human mind and that it pervades cortical function. We discuss whether evidence of human abilities to detect statistical regularities is necessarily evidence for predictive processing and evaluate suggestions that prediction is necessary for language learning. We point out that not all language users appear to predict language and that suboptimal input makes prediction often very challenging. Prediction, moreover, is strongly context-dependent and impeded by resource limitations. We also argue that it may be problematic that most experimental evidence for predictive language processing comes from “prediction-encouraging” experimental set-ups. We conclude that languages can be learned and understood in the absence of prediction. Claims that all language processing is predictive in nature are premature.},
	number = {1},
	urldate = {2015-11-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Huettig, Falk and Mani, Nivedita},
	month = jan,
	year = {2016},
	pages = {19--31},
	file = {Huettig_Mani_2016_Is prediction necessary to understand language.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Huettig_Mani_2016_Is prediction necessary to understand language.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/R7PRU887/23273798.2015.html:text/html}
}

@article{norris-prediction-2016,
	title = {Prediction, {Bayesian} inference and feedback in speech recognition},
	volume = {31},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1081703},
	doi = {10.1080/23273798.2015.1081703},
	abstract = {Speech perception involves prediction, but how is that prediction implemented? In cognitive models prediction has often been taken to imply that there is feedback of activation from lexical to pre-lexical processes as implemented in interactive-activation models (IAMs). We show that simple activation feedback does not actually improve speech recognition. However, other forms of feedback can be beneficial. In particular, feedback can enable the listener to adapt to changing input, and can potentially help the listener to recognise unusual input, or recognise speech in the presence of competing sounds. The common feature of these helpful forms of feedback is that they are all ways of optimising the performance of speech recognition using Bayesian inference. That is, listeners make predictions about speech because speech recognition is optimal in the sense captured in Bayesian models.},
	number = {1},
	urldate = {2015-11-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
	month = jan,
	year = {2016},
	pages = {4--18},
	file = {Norris et al_2016_Prediction, Bayesian inference and feedback in speech recognition.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Norris et al_2016_Prediction, Bayesian inference and feedback in speech recognition.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XT7CENMA/23273798.2015.html:text/html}
}

@article{tackstrom-efficient-2015,
	title = {Efficient {Inference} and {Structured} {Learning} for {Semantic} {Role} {Labeling}},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/465},
	urldate = {2015-11-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Täckström, Oscar and Ganchev, Kuzman and Das, Dipanjan},
	year = {2015},
	pages = {29--41},
	file = {43251.pdf:/home/user/Zotero/storage/83XQVJC6/43251.pdf:application/pdf}
}

@inproceedings{fitzgerald-semantic-2015,
	title = {Semantic {Role} {Labeling} with {Neural} {Network} {Factors}},
	url = {http://www.nfitz.net/papers/pdf/ftgd.emnlp.2015.pdf},
	urldate = {2015-11-29},
	booktitle = {Proc. of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {FitzGerald, Nicholas and Täckström, Oscar and Ganchev, Kuzman and Das, Dipanjan},
	year = {2015},
	pages = {960--970},
	file = {43892.pdf:/home/user/Zotero/storage/3IQDJJX9/43892.pdf:application/pdf}
}

@article{liang-bringing-2015,
	title = {Bringing {Machine} {Learning} and {Compositional} {Semantics} {Together}},
	volume = {1},
	issn = {2333-9683, 2333-9691},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-linguist-030514-125312},
	doi = {10.1146/annurev-linguist-030514-125312},
	language = {en},
	number = {1},
	urldate = {2015-11-29},
	journal = {Annual Review of Linguistics},
	author = {Liang, Percy and Potts, Christopher},
	month = jan,
	year = {2015},
	pages = {355--376},
	file = {annurev-linguist-030514-125312.pdf:/home/user/Zotero/storage/AGX6D6SS/annurev-linguist-030514-125312.pdf:application/pdf}
}

@inproceedings{vogel-learning-2014,
	title = {Learning to reason pragmatically with cognitive limitations},
	url = {http://web.stanford.edu/~jurafsky/pubs/ann-ibr.pdf},
	urldate = {2015-11-29},
	booktitle = {Proceedings of the 36th annual meeting of the cognitive science society},
	publisher = {Cognitive Science Society Austin, TX},
	author = {Vogel, Adam and Emilsson, Andrés Goméz and Frank, Michael C. and Jurafsky, Dan and Potts, Christopher},
	year = {2014},
	pages = {3055--3060},
	file = {paper527.pdf:/home/user/Zotero/storage/DVQXX8U3/paper527.pdf:application/pdf}
}

@inproceedings{luong-evaluating-2015,
	title = {Evaluating {Models} of {Computation} and {Storage} in {Human} {Sentence} {Processing}},
	url = {http://www.aclweb.org/anthology/W/W15/W15-24.pdf#page=26},
	urldate = {2015-11-29},
	booktitle = {{CONFERENCE} {ON} {EMPIRICAL} {METHODS} {IN} {NATURAL} {LANGUAGE} {PROCESSING}},
	author = {Luong, Minh-Thang and O’Donnell, Timothy J. and Goodman, Noah D.},
	year = {2015},
	pages = {14},
	file = {cogacll15_surprisal.pdf:/home/user/Zotero/storage/B3QGT7T5/cogacll15_surprisal.pdf:application/pdf}
}

@article{fengler-brain-2015,
	title = {Brain structural correlates of complex sentence comprehension in children},
	volume = {15},
	issn = {18789293},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1878929315000900},
	doi = {10.1016/j.dcn.2015.09.004},
	language = {en},
	urldate = {2015-11-29},
	journal = {Developmental Cognitive Neuroscience},
	author = {Fengler, Anja and Meyer, Lars and Friederici, Angela D.},
	month = oct,
	year = {2015},
	pages = {48--57},
	file = {1-s2.0-S1878929315000900-main.pdf:/home/user/Zotero/storage/DHB23REA/1-s2.0-S1878929315000900-main.pdf:application/pdf}
}

@article{neri-elementary-2015,
	title = {The {Elementary} {Operations} of {Human} {Vision} {Are} {Not} {Reducible} to {Template}-{Matching}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004499},
	doi = {10.1371/journal.pcbi.1004499},
	language = {en},
	number = {11},
	urldate = {2015-11-29},
	journal = {PLOS Computational Biology},
	author = {Neri, Peter},
	editor = {Baker, Daniel Hart},
	month = nov,
	year = {2015},
	pages = {e1004499},
	file = {journal.pcbi.1004499.pdf:/home/user/Zotero/storage/7KHQMIIN/journal.pcbi.1004499.pdf:application/pdf}
}

@article{korattikara-sequential-2015,
	title = {Sequential {Tests} for {Large}-{Scale} {Learning}},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO\_a\_00796},
	doi = {10.1162/NECO\_a\_00796},
	abstract = {We argue that when faced with big data sets, learning and inference algorithms should compute updates using only subsets of data items. We introduce algorithms that use sequential hypothesis tests to adaptively select such a subset of data points. The statistical properties of this subsampling process can be used to control the efficiency and accuracy of learning or inference. In the context of learning by optimization, we test for the probability that the update direction is no more than 90 degrees in the wrong direction. In the context of posterior inference using Markov chain Monte Carlo, we test for the probability that our decision to accept or reject a sample is wrong. We experimentally evaluate our algorithms on a number of models and data sets.},
	urldate = {2015-11-29},
	journal = {Neural Computation},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	month = nov,
	year = {2015},
	pages = {1--26},
	file = {Korattikara et al_2015_Sequential Tests for Large-Scale Learning.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Korattikara et al_2015_Sequential Tests for Large-Scale Learning.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/9U5FM898/NECO_a_00796.html:text/html}
}

@inproceedings{hahn-deriving-2014,
	series = {Frontiers in {AI} and {Applications} {Series}},
	title = {On deriving semantic representations from dependencies: {A} practical approach for evaluating meaning in learner corpora},
	booktitle = {Dependency {Theory}},
	publisher = {IOS Press},
	author = {Hahn, Michael and Meurers, Detmar},
	editor = {Gerdes, Kim and Hajicov{\'a}, Eva and Wanner, Leo},
	year = {2014}
}

@inproceedings{monroe-learning-2015-1,
	address = {Amsterdam},
	title = {Learning in the {Rational} {Speech} {Acts} {Model}},
	booktitle = {Proceedings of 20th {Amsterdam} {Colloquium}},
	publisher = {ILLC},
	author = {Monroe, Will and Potts, Christopher},
	month = dec,
	year = {2015},
	file = {1510.06807v1.pdf:/home/user/Zotero/storage/NJXQHHN9/1510.06807v1.pdf:application/pdf}
}

@article{bengio-estimating-2013-1,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2015-11-26},
	journal = {arXiv:1308.3432 [cs]},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3432},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QNNGXK8S/1308.html:text/html;Bengio et al_2013_Estimating or Propagating Gradients Through Stochastic Neurons for Conditional.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bengio et al_2013_Estimating or Propagating Gradients Through Stochastic Neurons for Conditional.pdf:application/pdf}
}

@book{noauthor-language-2011,
	address = {Cambridge ; New York},
	title = {The {Language} of {Stories}: {A} {Cognitive} {Approach}},
	isbn = {978-1-107-00582-2},
	shorttitle = {The {Language} of {Stories}},
	abstract = {How do we read stories? How do they engage our minds and create meaning? Are they a mental construct, a linguistic one or a cultural one? What is the difference between real stories and fictional ones? This book addresses such questions by describing the conceptual and linguistic underpinnings of narrative interpretation. Barbara Dancygier discusses literary texts as linguistic artifacts, describing the processes which drive the emergence of literary meaning. If a text means something to someone, she argues, there have to be linguistic phenomena that make it possible. Drawing on blending theory and construction grammar, the book focuses its linguistic lens on the concepts of the narrator and the story, and defines narrative viewpoint in a new way. The examples come from a wide spectrum of texts, primarily novels and drama, by authors such as William Shakespeare, Margaret Atwood, Philip Roth, Dave Eggers, Jan Potocki and Mikhail Bulgakov.},
	language = {English},
	publisher = {Cambridge University Press},
	month = nov,
	year = {2011}
}

@article{coulmance-trans-gram-2016,
	title = {Trans-gram, {Fast} {Cross}-lingual {Word}-embeddings},
	url = {http://arxiv.org/abs/1601.02502},
	abstract = {We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.},
	urldate = {2016-01-12},
	journal = {arXiv:1601.02502 [cs]},
	author = {Coulmance, Jocelyn and Marty, Jean-Marc and Wenzek, Guillaume and Benhalloum, Amine},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.02502},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZNCZXE4I/1601.html:text/html;Coulmance et al_2016_Trans-gram, Fast Cross-lingual Word-embeddings.pdf:/home/user/Zotero/storage/QKXGDPH2/Coulmance et al_2016_Trans-gram, Fast Cross-lingual Word-embeddings.pdf:application/pdf}
}

@article{decker-freeze-2015,
	title = {On {Freeze} {LTL} with {Ordered} {Attributes}},
	url = {http://arxiv.org/abs/1504.06355},
	abstract = {This paper is concerned with Freeze LTL, a temporal logic on data words with registers. In a (multi-attributed) data word each position carries a letter from a finite alphabet and assigns a data value to a fixed, finite set of attributes. The satisfiability problem of Freeze LTL is undecidable if more than one register is available or tuples of data values can be stored and compared arbitrarily. Starting from the decidable one-register fragment we propose an extension that allows for specifying a dependency relation on attributes. This restricts in a flexible way how collections of attribute values can be stored and compared. This conceptual dimension is orthogonal to the number of registers or the available temporal operators. The extension is strict. Admitting arbitrary dependency relations satisfiability becomes undecidable. Tree-like relations, however, induce a family of decidable fragments escalating the ordinal-indexed hierarchy of fast-growing complexity classes, a recently introduced framework for non-primitive recursive complexities. This results in completeness for the class \${\bf F}\_{\epsilon\_0}\$. We employ nested counter systems and show that they relate to the hierarchy in terms of the nesting depth.},
	urldate = {2016-01-12},
	journal = {arXiv:1504.06355 [cs]},
	author = {Decker, Normann and Thoma, Daniel},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.06355},
	keywords = {Computer Science - Logic in Computer Science, F.1.1, F.4.1},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3VICMX2R/1504.html:text/html;Decker_Thoma_2015_On Freeze LTL with Ordered Attributes.pdf:/home/user/Zotero/storage/56Q5KX8P/Decker_Thoma_2015_On Freeze LTL with Ordered Attributes.pdf:application/pdf}
}

@article{mei-what-2015,
	title = {What to talk about and how? {Selective} {Generation} using {LSTMs} with {Coarse}-to-{Fine} {Alignment}},
	shorttitle = {What to talk about and how?},
	url = {http://arxiv.org/abs/1509.00838},
	abstract = {We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59\% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.},
	urldate = {2016-01-12},
	journal = {arXiv:1509.00838 [cs]},
	author = {Mei, Hongyuan and Bansal, Mohit and Walter, Matthew R.},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.00838},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XNW2NBBM/1509.html:text/html;Mei et al_2015_What to talk about and how.pdf:/home/user/Zotero/storage/3X2Z383M/Mei et al_2015_What to talk about and how.pdf:application/pdf}
}

@article{coulmance-trans-gram-2016-1,
	title = {Trans-gram, {Fast} {Cross}-lingual {Word}-embeddings},
	url = {http://arxiv.org/abs/1601.02502},
	abstract = {We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.},
	urldate = {2016-01-12},
	journal = {arXiv:1601.02502 [cs]},
	author = {Coulmance, Jocelyn and Marty, Jean-Marc and Wenzek, Guillaume and Benhalloum, Amine},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.02502},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NHV57NJR/1601.html:text/html;Coulmance et al_2016_Trans-gram, Fast Cross-lingual Word-embeddings.pdf:/home/user/Zotero/storage/WDS76X3K/Coulmance et al_2016_Trans-gram, Fast Cross-lingual Word-embeddings.pdf:application/pdf}
}

@article{mendrek-sex/gender-nodate,
	title = {Sex/gender differences in the brain and cognition in schizophrenia},
	issn = {0149-7634},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763415301111},
	doi = {10.1016/j.neubiorev.2015.10.013},
	abstract = {The early conceptualizations of schizophrenia have noted some sex/gender differences in epidemiology and clinical expression of the disorder. Over the past few decades, the interest in differences between male and female patients has expanded to encompass brain morphology and neurocognitive function. Despite some variability and methodological shortcomings, a few patterns emerge from the available literature. Most studies of gross neuroanatomy show more enlarged ventricles and smaller frontal lobes in men than in women with schizophrenia; finding reflecting normal sexual dimorphism. In comparison, studies of brain asymmetry and specific corticolimbic structures, suggest a disturbance in normal sexual dimorphism. The neurocognitive findings are somewhat consistent with this picture. Studies of cognitive functions mediated by the lateral frontal network tend to show sex differences in patients which are in the same direction as those observed in the general population, whereas studies of processes mediated by the corticolimbic system more frequently reveal reversal of normal sexual dimorphisms. These trends are faint and future research would need to delineate neurocognitive differences between men and women with various subtypes of schizophrenia (e.g., early versus late onset), while taking into consideration hormonal status and gender of tested participants.},
	urldate = {2016-01-11},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Mendrek, Adrianna and Mancini-Marïe, Adham},
	keywords = {Brain morphology, Gender, Neurocognitive function, Schizophrenia, Sex differences, Sex steroid hormones},
	file = {Mendrek_Mancini-Marïe_Sex-gender differences in the brain and cognition in schizophrenia.pdf:/home/user/Zotero/storage/SG8ZE3R8/Mendrek_Mancini-Marïe_Sex-gender differences in the brain and cognition in schizophrenia.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/A2PKJU29/S0149763415301111.html:text/html}
}

@article{miwa-end--end-2016,
	title = {End-to-end {Relation} {Extraction} using {LSTMs} on {Sequences} and {Tree} {Structures}},
	url = {http://arxiv.org/abs/1601.00770},
	abstract = {We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model stacks bidirectional sequential LSTM-RNNs and bidirectional tree-structured LSTM-RNNs to capture both word sequence and dependency tree substructure information. This allows our model to jointly represent both entities and relations with shared parameters. We further encourage detection of entities during training and use of entity information in relation extraction via curriculum learning and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 3.5\% and 4.8\% relative error reductions in F-score on ACE2004 and ACE2005, respectively. We also show improvements over the state-of-the-art convolutional neural network based model on nominal relation classification (SemEval-2010 Task 8), with 2.5\% relative error reduction in F-score.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.00770 [cs]},
	author = {Miwa, Makoto and Bansal, Mohit},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00770},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TK2UQT3I/1601.html:text/html;Miwa_Bansal_2016_End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures.pdf:/home/user/Zotero/storage/6PNADNXZ/Miwa_Bansal_2016_End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures.pdf:application/pdf}
}

@article{drewes-recurrent-2016,
	title = {Recurrent {Processing} in the {Formation} of {Shape} {Percepts}},
	volume = {36},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/36/1/185},
	doi = {10.1523/JNEUROSCI.2347-15.2016},
	abstract = {The human visual system must extract reliable object information from cluttered visual scenes several times per second, and this temporal constraint has been taken as evidence that the underlying cortical processing must be strictly feedforward. Here we use a novel rapid reinforcement paradigm to probe the temporal dynamics of the neural circuit underlying rapid object shape perception and thus test this feedforward assumption. Our results show that two shape stimuli are optimally reinforcing when separated in time by ∼60 ms, suggesting an underlying recurrent circuit with a time constant (feedforward + feedback) of 60 ms. A control experiment demonstrates that this is not an attentional cueing effect. Instead, it appears to reflect the time course of feedback processing underlying the rapid perceptual organization of shape.
SIGNIFICANCE STATEMENT Human and nonhuman primates can spot an animal shape in complex natural scenes with striking speed, and this has been taken as evidence that the underlying cortical mechanisms are strictly feedforward. Using a novel paradigm to probe the dynamics of shape perception, we find that two shape stimuli are optimally reinforcing when separated in time by 60 ms, suggesting a fast but recurrent neural circuit. This work (1) introduces a novel method for probing the temporal dynamics of cortical circuits underlying perception, (2) provides direct evidence against the feedforward assumption for rapid shape perception, and (3) yields insight into the role of feedback connections in the object pathway.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {The Journal of Neuroscience},
	author = {Drewes, Jan and Goren, Galina and Zhu, Weina and Elder, James H.},
	month = jan,
	year = {2016},
	pmid = {26740660},
	keywords = {contour processing, feedback, grouping, perceptual organization, recurrent processing, shape perception, temporal integration},
	pages = {185--192},
	file = {Drewes et al_2016_Recurrent Processing in the Formation of Shape Percepts.pdf:/home/user/Zotero/storage/F68NZPH7/Drewes et al_2016_Recurrent Processing in the Formation of Shape Percepts.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Z8I4ZSGH/185.html:text/html}
}

@article{drewes-recurrent-2016-1,
	title = {Recurrent {Processing} in the {Formation} of {Shape} {Percepts}},
	volume = {36},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/36/1/185},
	doi = {10.1523/JNEUROSCI.2347-15.2016},
	abstract = {The human visual system must extract reliable object information from cluttered visual scenes several times per second, and this temporal constraint has been taken as evidence that the underlying cortical processing must be strictly feedforward. Here we use a novel rapid reinforcement paradigm to probe the temporal dynamics of the neural circuit underlying rapid object shape perception and thus test this feedforward assumption. Our results show that two shape stimuli are optimally reinforcing when separated in time by ∼60 ms, suggesting an underlying recurrent circuit with a time constant (feedforward + feedback) of 60 ms. A control experiment demonstrates that this is not an attentional cueing effect. Instead, it appears to reflect the time course of feedback processing underlying the rapid perceptual organization of shape.
SIGNIFICANCE STATEMENT Human and nonhuman primates can spot an animal shape in complex natural scenes with striking speed, and this has been taken as evidence that the underlying cortical mechanisms are strictly feedforward. Using a novel paradigm to probe the dynamics of shape perception, we find that two shape stimuli are optimally reinforcing when separated in time by 60 ms, suggesting a fast but recurrent neural circuit. This work (1) introduces a novel method for probing the temporal dynamics of cortical circuits underlying perception, (2) provides direct evidence against the feedforward assumption for rapid shape perception, and (3) yields insight into the role of feedback connections in the object pathway.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {The Journal of Neuroscience},
	author = {Drewes, Jan and Goren, Galina and Zhu, Weina and Elder, James H.},
	month = jan,
	year = {2016},
	pmid = {26740660},
	keywords = {contour processing, feedback, grouping, perceptual organization, recurrent processing, shape perception, temporal integration},
	pages = {185--192},
	file = {Drewes et al_2016_Recurrent Processing in the Formation of Shape Percepts.pdf:/home/user/Zotero/storage/QAQP65CQ/Drewes et al_2016_Recurrent Processing in the Formation of Shape Percepts.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/N8WMHM2V/185.html:text/html}
}

@article{schaechtle-probabilistic-2015,
	title = {Probabilistic {Programming} with {Gaussian} {Process} {Memoization}},
	url = {http://arxiv.org/abs/1512.05665},
	abstract = {Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.},
	urldate = {2016-01-11},
	journal = {arXiv:1512.05665 [cs, stat]},
	author = {Schaechtle, Ulrich and Zinberg, Ben and Radul, Alexey and Stathis, Kostas and Mansinghka, Vikash K.},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.05665},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CB6EBUC6/1512.html:text/html;Schaechtle et al_2015_Probabilistic Programming with Gaussian Process Memoization.pdf:/home/user/Zotero/storage/QBNS5CCJ/Schaechtle et al_2015_Probabilistic Programming with Gaussian Process Memoization.pdf:application/pdf}
}

@article{louizos-variational-2015,
	title = {The {Variational} {Fair} {Autoencoder}},
	url = {http://arxiv.org/abs/1511.00830},
	abstract = {We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational autoencoding architecture with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the "Maximum Mean Discrepancy" (MMD) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.00830 [cs, stat]},
	author = {Louizos, Christos and Swersky, Kevin and Li, Yujia and Zemel, Richard and Welling, Max},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.00830},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WJ858C56/1511.html:text/html;Louizos et al_2015_The Variational Fair Autoencoder.pdf:/home/user/Zotero/storage/8WNTDSS7/Louizos et al_2015_The Variational Fair Autoencoder.pdf:application/pdf}
}

@article{blei-variational-2016,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.00670 [cs, stat]},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ME5ZZS6C/1601.html:text/html;Blei et al_2016_Variational Inference.pdf:/home/user/Zotero/storage/UCQVPGZ6/Blei et al_2016_Variational Inference.pdf:application/pdf}
}

@article{nathanson-hiv/aids-2016,
	title = {{HIV}/{AIDS} epidemic: {The} whole truth},
	volume = {351},
	issn = {0036-8075, 1095-9203},
	shorttitle = {{HIV}/{AIDS} epidemic},
	url = {http://www.sciencemag.org/content/351/6269/133},
	doi = {10.1126/science.351.6269.133},
	language = {en},
	number = {6269},
	urldate = {2016-01-11},
	journal = {Science},
	author = {Nathanson, Neal},
	month = jan,
	year = {2016},
	pmid = {26744400},
	pages = {133--133},
	file = {Nathanson_2016_HIV-AIDS epidemic.pdf:/home/user/Zotero/storage/UEQTE2W2/Nathanson_2016_HIV-AIDS epidemic.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/P7C92JDI/133.html:text/html}
}

@article{firat-multi-way-2016,
	title = {Multi-{Way}, {Multilingual} {Neural} {Machine} {Translation} with a {Shared} {Attention} {Mechanism}},
	url = {http://arxiv.org/abs/1601.01073},
	abstract = {We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.01073 [cs, stat]},
	author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01073},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6QQ2X4DP/1601.html:text/html;Firat et al_2016_Multi-Way, Multilingual Neural Machine Translation with a Shared Attention.pdf:/home/user/Zotero/storage/8EP7FIZC/Firat et al_2016_Multi-Way, Multilingual Neural Machine Translation with a Shared Attention.pdf:application/pdf}
}

@article{de-podesta-rethinking-2016,
	title = {Rethinking the kelvin},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3617.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3617},
	abstract = {Michael de Podesta discusses the current definition of the kelvin — and why it is worth changing one last time.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {de Podesta, Michael},
	month = jan,
	year = {2016},
	pages = {104--104},
	file = {de Podesta_2016_Rethinking the kelvin.pdf:/home/user/Zotero/storage/9NJF3EXW/de Podesta_2016_Rethinking the kelvin.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/E9FQI4W9/nphys3617.html:text/html}
}

@article{yuan-replicating-2015,
	title = {Replicating the benefits of {Deutschian} closed timelike curves without breaking causality},
	volume = {1},
	issn = {2056-6387},
	url = {http://www.nature.com/articles/npjqi20157},
	doi = {10.1038/npjqi.2015.7},
	urldate = {2016-01-11},
	journal = {npj Quantum Information},
	author = {Yuan, Xiao and Assad, Syed M and Thompson, Jayne and Haw, Jing Yan and Vedral, Vlatko and Ralph, Timothy C and Lam, Ping Koy and Weedbrook, Christian and Gu, Mile},
	month = nov,
	year = {2015},
	pages = {15007}
}

@article{klopper-quantum-2016,
	title = {Quantum mechanics: {The} lone traveller},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	shorttitle = {Quantum mechanics},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3635.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3635},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Klopper, Abigail},
	month = jan,
	year = {2016},
	keywords = {Quantum mechanics, Quantum information},
	pages = {20--20},
	file = {Klopper_2016_Quantum mechanics.pdf:/home/user/Zotero/storage/5RKI9RM4/Klopper_2016_Quantum mechanics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Z9RJIEBX/nphys3635.html:text/html}
}

@article{fleet-topological-2016,
	title = {Topological phases: {Call} the tune},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	shorttitle = {Topological phases},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3637.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3637},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Fleet, Luke},
	month = jan,
	year = {2016},
	keywords = {Topological insulators},
	pages = {20--20},
	file = {Fleet_2016_Topological phases.pdf:/home/user/Zotero/storage/UWMV4Z2C/Fleet_2016_Topological phases.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/D6IMI3I4/nphys3637.html:text/html}
}

@article{buchanan-transition-2016,
	title = {Transition to turbulence},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3630.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3630},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Buchanan, Mark},
	month = jan,
	year = {2016},
	keywords = {Fluid dynamics, Nonlinear phenomena},
	pages = {18--18},
	file = {Buchanan_2016_Transition to turbulence.pdf:/home/user/Zotero/storage/RQNRAHHN/Buchanan_2016_Transition to turbulence.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9NT7KUVK/nphys3630.html:text/html}
}

@article{fischer-new-2016,
	title = {The new system of units},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3612.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3612},
	abstract = {The redefinition of several physical base units planned for 2018 requires precise knowledge of the values of certain fundamental physical constants. Scientists are working hard to meet the deadlines for realizing the ultimate International System of Units.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Fischer, Joachim and Ullrich, Joachim},
	month = jan,
	year = {2016},
	keywords = {Characterization and analytical techniques, Quantum metrology, Scientific community and society},
	pages = {4--7},
	file = {Fischer_Ullrich_2016_The new system of units.pdf:/home/user/Zotero/storage/A9ZQ7HNT/Fischer_Ullrich_2016_The new system of units.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CDPAVS5D/nphys3612.html:text/html}
}

@article{bonder-questioning-2016,
	title = {Questioning universal decoherence due to gravitational time dilation},
	volume = {12},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3573.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3573},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Bonder, Yuri and Okon, Elias and Sudarsky, Daniel},
	month = jan,
	year = {2016},
	keywords = {Quantum mechanics, Theoretical physics},
	pages = {2--2},
	file = {Bonder et al_2016_Questioning universal decoherence due to gravitational time dilation.pdf:/home/user/Zotero/storage/TD2HTHJD/Bonder et al_2016_Questioning universal decoherence due to gravitational time dilation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/668ZZH2G/nphys3573.html:text/html}
}

@article{pikovski-reply-2016,
	title = {Reply to '{Questioning} universal decoherence due to gravitational time dilation'},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3650.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3650},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	author = {Pikovski, Igor and Zych, Magdalena and Costa, Fabio and Brukner, Časlav},
	month = jan,
	year = {2016},
	keywords = {Quantum mechanics, Theoretical physics},
	pages = {2--3},
	file = {Pikovski et al_2016_Reply to 'Questioning universal decoherence due to gravitational time dilation'.pdf:/home/user/Zotero/storage/58KX5XDK/Pikovski et al_2016_Reply to 'Questioning universal decoherence due to gravitational time dilation'.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/E6QZI8DR/nphys3650.html:text/html}
}

@article{noauthor-art-2016,
	title = {The art of measurement},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3640.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3640},
	abstract = {With a dedicated monthly column, Nature Physics draws attention to metrology. And a set of Commentaries in this issue focuses on various aspects of thermometry.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	month = jan,
	year = {2016},
	keywords = {Characterization and analytical techniques, Scientific community and society, Magnetically confined plasmas, Quantum fluids and solids},
	pages = {1--1},
	file = {2016_The art of measurement.pdf:/home/user/Zotero/storage/8S3768CC/2016_The art of measurement.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/M492DR5K/nphys3640.html:text/html}
}

@article{noauthor-art-2016-1,
	title = {The art of measurement},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n1/full/nphys3640.html?WT.ec\_id=NPHYS-201601&spMailingID=50411318&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=840949370&spReportId=ODQwOTQ5MzcwS0},
	doi = {10.1038/nphys3640},
	abstract = {With a dedicated monthly column, Nature Physics draws attention to metrology. And a set of Commentaries in this issue focuses on various aspects of thermometry.},
	language = {en},
	number = {1},
	urldate = {2016-01-11},
	journal = {Nature Physics},
	month = jan,
	year = {2016},
	keywords = {Characterization and analytical techniques, Scientific community and society, Magnetically confined plasmas, Quantum fluids and solids},
	pages = {1--1},
	file = {2016_The art of measurement.pdf:/home/user/Zotero/storage/XNXCI4QI/2016_The art of measurement.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/PJWPDQGV/nphys3640.html:text/html}
}

@article{manning-computational-2015-1,
	title = {Computational {Linguistics} and {Deep} {Learning}},
	volume = {41},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/COLI\_a\_00239},
	doi = {10.1162/COLI\_a\_00239},
	number = {4},
	urldate = {2016-01-11},
	journal = {Computational Linguistics},
	author = {Manning, Christopher D.},
	month = sep,
	year = {2015},
	pages = {701--707},
	file = {Snapshot:/home/user/Zotero/storage/FJVUKK7K/COLI_a_00239.html:text/html}
}

@article{shutova-design-2015,
	title = {Design and {Evaluation} of {Metaphor} {Processing} {Systems}},
	volume = {41},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/COLI\_a\_00233},
	doi = {10.1162/COLI\_a\_00233},
	abstract = {System design and evaluation methodologies receive significant attention in natural language processing (NLP), with the systems typically being evaluated on a common task and against shared data sets. This enables direct system comparison and facilitates progress in the field. However, computational work on metaphor is considerably more fragmented than similar research efforts in other areas of NLP and semantics. Recent years have seen a growing interest in computational modeling of metaphor, with many new statistical techniques opening routes for improving system accuracy and robustness. However, the lack of a common task definition, shared data set, and evaluation strategy makes the methods hard to compare, and thus hampers our progress as a community in this area. The goal of this article is to review the system features and evaluation strategies that have been proposed for the metaphor processing task, and to analyze their benefits and downsides, with the aim of identifying the desired properties of metaphor processing systems and a set of requirements for their evaluation.},
	number = {4},
	urldate = {2016-01-11},
	journal = {Computational Linguistics},
	author = {Shutova, Ekaterina},
	month = sep,
	year = {2015},
	pages = {579--623},
	file = {Snapshot:/home/user/Zotero/storage/ZWP2DMFH/COLI_a_00233.html:text/html}
}

@article{vendrov-order-embeddings-2015,
	title = {Order-{Embeddings} of {Images} and {Language}},
	url = {http://arxiv.org/abs/1511.06361},
	abstract = {Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.06361 [cs]},
	author = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06361},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/F53M6RVS/1511.html:text/html;Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:/home/user/Zotero/storage/PXHE56BP/Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:application/pdf}
}

@article{kalchbrenner-grid-2015,
	title = {Grid {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1507.01526},
	abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.},
	urldate = {2016-01-11},
	journal = {arXiv:1507.01526 [cs]},
	author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.01526},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/EPGFQKM9/1507.html:text/html;Kalchbrenner et al_2015_Grid Long Short-Term Memory.pdf:/home/user/Zotero/storage/CFHS632F/Kalchbrenner et al_2015_Grid Long Short-Term Memory.pdf:application/pdf}
}

@article{zhang-top-down-2015,
	title = {Top-down {Tree} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1511.00060},
	abstract = {Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.00060 [cs]},
	author = {Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
	month = oct,
	year = {2015},
	note = {arXiv: 1511.00060},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/85855DIF/1511.html:text/html;Zhang et al_2015_Top-down Tree Long Short-Term Memory Networks.pdf:/home/user/Zotero/storage/GUUAHKBE/Zhang et al_2015_Top-down Tree Long Short-Term Memory Networks.pdf:application/pdf}
}

@article{meng-deep-2015-1,
	title = {A {Deep} {Memory}-based {Architecture} for {Sequence}-to-{Sequence} {Learning}},
	url = {http://arxiv.org/abs/1506.06442},
	abstract = {We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.},
	urldate = {2016-01-11},
	journal = {arXiv:1506.06442 [cs]},
	author = {Meng, Fandong and Lu, Zhengdong and Tu, Zhaopeng and Li, Hang and Liu, Qun},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.06442},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/T6J7PQ8T/1506.html:text/html;Meng et al_2015_A Deep Memory-based Architecture for Sequence-to-Sequence Learning.pdf:/home/user/Zotero/storage/4353JT6C/Meng et al_2015_A Deep Memory-based Architecture for Sequence-to-Sequence Learning.pdf:application/pdf}
}

@article{jacquemard-fo2<+1-2016,
	title = {{FO}2({\textless},+1,{\textasciitilde}) on data trees, data tree automata and branching vector addition systems},
	url = {http://arxiv.org/abs/1601.01579},
	abstract = {A data tree is an unranked ordered tree where each node carries a label from a finite alphabet and a datum from some infinite domain. We consider the two variable first order logic FO2({\textless},+1,{\textasciitilde}) over data trees. Here +1 refers to the child and the next sibling relations while {\textless} refers to the descendant and following sibling relations. Moreover, {\textasciitilde} is a binary predicate testing data equality. We exhibit an automata model, denoted DAD\# that is more expressive than FO2({\textless},+1,{\textasciitilde}) but such that emptiness of DAD\# and satisfiability of FO2({\textless},+1,{\textasciitilde}) are inter-reducible. This is proved via a model of counter tree automata, denoted EBVASS, that extends Branching Vector Addition Systems with States (BVASS) with extra features for merging counters. We show that, as decision problems, reachability for EBVASS, satisfiability of FO2({\textless},+1,{\textasciitilde}) and emptiness of DAD\# are equivalent.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.01579 [cs]},
	author = {Jacquemard, Florent and Segoufin, Luc and Dimino, Jerémie},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01579},
	keywords = {Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2HDPMS8S/1601.html:text/html;Jacquemard et al_2016_FO2(,+1,~) on data trees, data tree automata and branching vector addition.pdf:/home/user/Zotero/storage/VZMJFIAF/Jacquemard et al_2016_FO2(,+1,~) on data trees, data tree automata and branching vector addition.pdf:application/pdf}
}

@article{kurata-leveraging-2016,
	title = {Leveraging {Sentence}-level {Information} with {Encoder} {LSTM} for {Natural} {Language} {Understanding}},
	url = {http://arxiv.org/abs/1601.01530},
	abstract = {Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66\%.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.01530 [cs]},
	author = {Kurata, Gakuto and Xiang, Bing and Zhou, Bowen and Yu, Mo},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01530},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2SCNBWES/1601.html:text/html;Kurata et al_2016_Leveraging Sentence-level Information with Encoder LSTM for Natural Language.pdf:/home/user/Zotero/storage/AEER9D35/Kurata et al_2016_Leveraging Sentence-level Information with Encoder LSTM for Natural Language.pdf:application/pdf}
}

@article{ji-blackout:-2015,
	title = {{BlackOut}: {Speeding} up {Recurrent} {Neural} {Network} {Language} {Models} {With} {Very} {Large} {Vocabularies}},
	shorttitle = {{BlackOut}},
	url = {http://arxiv.org/abs/1511.06909},
	abstract = {We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.06909 [cs, stat]},
	author = {Ji, Shihao and Vishwanathan, S. V. N. and Satish, Nadathur and Anderson, Michael J. and Dubey, Pradeep},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06909},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/PFJPRBZJ/1511.html:text/html;Ji et al_2015_BlackOut.pdf:/home/user/Zotero/storage/I4WPIGIX/Ji et al_2015_BlackOut.pdf:application/pdf}
}

@article{yang-simultaneous-nodate,
	title = {Simultaneous {Multi}-plane {Imaging} of {Neural} {Circuits}},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627315010855/abstract},
	doi = {10.1016/j.neuron.2015.12.012},
	abstract = {Recording the activity of large populations of neurons is an important step toward understanding the emergent function of neural circuits. Here we present a simple holographic method to simultaneously perform two-photon calcium imaging of neuronal populations across multiple areas and layers of mouse cortex in vivo. We use prior knowledge of neuronal locations, activity sparsity, and a constrained nonnegative matrix factorization algorithm to extract signals from neurons imaged simultaneously and located in different focal planes or fields of view. Our laser multiplexing approach is simple and fast, and could be used as a general method to image the activity of neural circuits in three dimensions across multiple areas in the brain.},
	language = {English},
	number = {0},
	urldate = {2016-01-11},
	journal = {Neuron},
	author = {Yang, Weijian and Miller, Jae-eun Kang and Carrillo-Reid, Luis and Pnevmatikakis, Eftychios and Paninski, Liam and Yuste, Rafael and Peterka, Darcy S.},
	file = {Snapshot:/home/user/Zotero/storage/UB8DRPQE/S0896-6273(15)01085-5.html:text/html;Yang et al_Simultaneous Multi-plane Imaging of Neural Circuits.pdf:/home/user/Zotero/storage/QP2U3CBC/Yang et al_Simultaneous Multi-plane Imaging of Neural Circuits.pdf:application/pdf}
}

@article{keskar-adaqn:-2015,
	title = {{adaQN}: {An} {Adaptive} {Quasi}-{Newton} {Algorithm} for {Training} {RNNs}},
	shorttitle = {{adaQN}},
	url = {http://arxiv.org/abs/1511.01169},
	abstract = {Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional performance on several pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known "vanishing/exploding" gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as ADAGRAD and ADAM, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.01169 [cs, math, stat]},
	author = {Keskar, Nitish Shirish and Berahas, Albert S.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01169},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/76Q75QES/1511.html:text/html}
}

@article{lillicrap-continuous-2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2016-01-11},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/FTI4XN3T/1509.html:text/html;Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf:/home/user/Zotero/storage/XR9RJAGF/Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf:application/pdf}
}

@article{alquier-bayesian-2016,
	title = {Bayesian {Non}-{Negative} {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/1601.01345},
	abstract = {The aim of this paper is to provide some theoretical understanding of Bayesian non-negative matrix factorization methods, along with practical implementations. We provide a sharp oracle inequality for a quasi-Bayesian estimator, also known as the exponentially weighted aggregate (Dalalyan and Tsybakov, 2008). This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence. We then discuss possible algorithms. A natural choice in Bayesian statistics is the Gibbs sampler, used for example in Salakhutdinov and Mnih (2008). This algorithm is asymptotically exact, yet it suffers from the fact that the convergence might be very slow on large datasets. When faced with massive datasets, a more efficient path is to use approximate methods based on optimisation algorithms: we here describe a blockwise gradient descent which is a Bayesian version of the algorithm in Xu et al. (2012). Here again, the general form of the algorithm helps to understand the role of the prior, and some priors will clearly lead to more efficient (i.e., faster) implementations. We end the paper with a short simulation study and an application to finance. These numerical studies support our claim that the reconstruction of the matrix is usually not very sensitive to the choice of the hyperparameters whereas rank identification is.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.01345 [math, stat]},
	author = {Alquier, Pierre and Guedj, Benjamin},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01345},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {Alquier_Guedj_2016_Bayesian Non-Negative Matrix Factorization.pdf:/home/user/Zotero/storage/94NJ2DZZ/Alquier_Guedj_2016_Bayesian Non-Negative Matrix Factorization.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/T6DPRCN2/1601.html:text/html}
}

@article{lee-reasoning-2015,
	title = {Reasoning in {Vector} {Space}: {An} {Exploratory} {Study} of {Question} {Answering}},
	shorttitle = {Reasoning in {Vector} {Space}},
	url = {http://arxiv.org/abs/1511.06426},
	abstract = {Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms.},
	urldate = {2016-01-11},
	journal = {arXiv:1511.06426 [cs]},
	author = {Lee, Moontae and He, Xiaodong and Yih, Wen-tau and Gao, Jianfeng and Deng, Li and Smolensky, Paul},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06426},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3DFX2FPH/1511.html:text/html;Lee et al_2015_Reasoning in Vector Space.pdf:/home/user/Zotero/storage/7U9T9U5Q/Lee et al_2015_Reasoning in Vector Space.pdf:application/pdf}
}

@article{kamper-deep-2015,
	title = {Deep convolutional acoustic word embeddings using word-pair side information},
	url = {http://arxiv.org/abs/1510.01032},
	abstract = {Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.},
	urldate = {2016-01-11},
	journal = {arXiv:1510.01032 [cs]},
	author = {Kamper, Herman and Wang, Weiran and Livescu, Karen},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.01032},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4V689FUE/1510.html:text/html;Kamper et al_2015_Deep convolutional acoustic word embeddings using word-pair side information.pdf:/home/user/Zotero/storage/IH3FTSZS/Kamper et al_2015_Deep convolutional acoustic word embeddings using word-pair side information.pdf:application/pdf}
}

@article{asor-spectral-2016,
	title = {Spectral and {Modular} {Analysis} of \#{P} {Problems}},
	url = {http://arxiv.org/abs/1601.00691},
	abstract = {We present various analytic and number theoretic results concerning the \#SAT problem as reflected when reduced into a \#PART problem. As an application we propose a heuristic to probabilistically estimate the solution of \#SAT problems.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.00691 [cs, math]},
	author = {Asor, Ohad},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00691},
	keywords = {Computer Science - Computational Complexity, Mathematics - Combinatorics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2WE795N6/1601.html:text/html;Asor_2016_Spectral and Modular Analysis of #P Problems.pdf:/home/user/Zotero/storage/MUIPK4F2/Asor_2016_Spectral and Modular Analysis of #P Problems.pdf:application/pdf}
}

@article{li-visualizing-2015,
	title = {Visualizing and {Understanding} {Neural} {Models} in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	urldate = {2016-01-11},
	journal = {arXiv:1506.01066 [cs]},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.01066},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NI9FCAAT/1506.html:text/html;Li et al_2015_Visualizing and Understanding Neural Models in NLP.pdf:/home/user/Zotero/storage/RKZKUT72/Li et al_2015_Visualizing and Understanding Neural Models in NLP.pdf:application/pdf}
}

@article{andreas-learning-2016,
	title = {Learning to {Compose} {Neural} {Networks} for {Question} {Answering}},
	url = {http://arxiv.org/abs/1601.01705},
	abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
	urldate = {2016-01-11},
	journal = {arXiv:1601.01705 [cs]},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01705},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Andreas et al_2016_Learning to Compose Neural Networks for Question Answering.pdf:/home/user/Zotero/storage/F8D7TZC8/Andreas et al_2016_Learning to Compose Neural Networks for Question Answering.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/XQ2B2XJ9/1601.html:text/html}
}

@article{ma-online-2016,
	title = {An {Online} {Policy} {Gradient} {Algorithm} for {Markov} {Decision} {Processes} with {Continuous} {States} and {Actions}},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/NECO\_a\_00808},
	doi = {10.1162/NECO\_a\_00808},
	abstract = {We consider the learning problem under an online Markov decision process (MDP) aimed at learning the time-dependent decision-making policy of an agent that minimizes the regret—the difference from the best fixed policy. The difficulty of online MDP learning is that the reward function changes over time. In this letter, we show that a simple online policy gradient algorithm achieves regret for T steps under a certain concavity assumption and under a strong concavity assumption. To the best of our knowledge, this is the first work to present an online MDP algorithm that can handle continuous state, action, and parameter spaces with guarantee. We also illustrate the behavior of the proposed online policy gradient method through experiments.},
	urldate = {2016-01-11},
	journal = {Neural Computation},
	author = {Ma, Yao and Zhao, Tingting and Hatano, Kohei and Sugiyama, Masashi},
	month = jan,
	year = {2016},
	pages = {1--31},
	file = {Ma et al_2016_An Online Policy Gradient Algorithm for Markov Decision Processes with.pdf:/home/user/Zotero/storage/FXG6WQET/Ma et al_2016_An Online Policy Gradient Algorithm for Markov Decision Processes with.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/HJS4QUBN/NECO_a_00808.html:text/html}
}

@article{bouthillier-dropout-2015,
	title = {Dropout as data augmentation},
	url = {http://arxiv.org/abs/1506.08700},
	abstract = {Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cost.},
	urldate = {2016-01-11},
	journal = {arXiv:1506.08700 [cs, stat]},
	author = {Bouthillier, Xavier and Konda, Kishore and Vincent, Pascal and Memisevic, Roland},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08700},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/MR57JVSZ/1506.html:text/html;Bouthillier et al_2015_Dropout as data augmentation.pdf:/home/user/Zotero/storage/BS6BNUXC/Bouthillier et al_2015_Dropout as data augmentation.pdf:application/pdf}
}

@misc{noauthor-puzzle-nodate,
	title = {The puzzle of monogamous marriage {\textbar} {Philosophical} {Transactions} of the {Royal} {Society} {B}: {Biological} {Sciences}},
	url = {http://rstb.royalsocietypublishing.org/content/367/1589/657},
	urldate = {2016-01-11},
	file = {The puzzle of monogamous marriage | Philosophical Transactions of the Royal Society B\: Biological Sciences:/home/user/Zotero/storage/EV4GZIE6/657.html:text/html}
}

@article{noh-learning-2015,
	title = {Learning {Deconvolution} {Network} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1505.04366},
	abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
	urldate = {2016-01-11},
	journal = {arXiv:1505.04366 [cs]},
	author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04366},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZEINAGME/1505.html:text/html;Noh et al_2015_Learning Deconvolution Network for Semantic Segmentation.pdf:/home/user/Zotero/storage/E8HFF299/Noh et al_2015_Learning Deconvolution Network for Semantic Segmentation.pdf:application/pdf}
}

@article{ba-multiple-2014-1,
	title = {Multiple {Object} {Recognition} with {Visual} {Attention}},
	url = {http://arxiv.org/abs/1412.7755},
	abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
	urldate = {2016-01-11},
	journal = {arXiv:1412.7755 [cs]},
	author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7755},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/74WMAK5S/1412.html:text/html;Ba et al_2014_Multiple Object Recognition with Visual Attention.pdf:/home/user/Zotero/storage/MMD5HJE3/Ba et al_2014_Multiple Object Recognition with Visual Attention.pdf:application/pdf}
}

@article{ranzato-sequence-2015,
	title = {Sequence {Level} {Training} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06732},
	abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
	urldate = {2015-12-16},
	journal = {arXiv:1511.06732 [cs]},
	author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06732},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/T3D86VXB/1511.html:text/html;Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:/home/user/Zotero/storage/M7HCEJDP/Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:application/pdf}
}

@article{ranzato-sequence-2015-1,
	title = {Sequence {Level} {Training} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06732},
	abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
	urldate = {2015-12-16},
	journal = {arXiv:1511.06732 [cs]},
	author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06732},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9NCXWBJM/1511.html:text/html;Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:/home/user/Zotero/storage/MKEVGXUQ/Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:application/pdf}
}

@article{chen-strategies-2015,
	title = {Strategies for {Training} {Large} {Vocabulary} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/1512.04906},
	abstract = {Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.},
	urldate = {2015-12-16},
	journal = {arXiv:1512.04906 [cs]},
	author = {Chen, Welin and Grangier, David and Auli, Michael},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.04906},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KXQGQHDQ/1512.html:text/html;Chen et al_2015_Strategies for Training Large Vocabulary Neural Language Models.pdf:/home/user/Zotero/storage/FABC7AIA/Chen et al_2015_Strategies for Training Large Vocabulary Neural Language Models.pdf:application/pdf}
}

@article{cheng-agreement-based-2015,
	title = {Agreement-based {Joint} {Training} for {Bidirectional} {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1512.04650},
	abstract = {The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that joint training significantly improves both alignment and translation quality over independent training.},
	urldate = {2015-12-16},
	journal = {arXiv:1512.04650 [cs]},
	author = {Cheng, Yong and Shen, Shiqi and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.04650},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/PH2AFQZ6/1512.html:text/html;Cheng et al_2015_Agreement-based Joint Training for Bidirectional Attention-based Neural Machine.pdf:/home/user/Zotero/storage/PKD2U6JV/Cheng et al_2015_Agreement-based Joint Training for Bidirectional Attention-based Neural Machine.pdf:application/pdf}
}

@book{beal-variational-2003,
	title = {Variational algorithms for approximate {Bayesian} inference},
	url = {http://www.cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf},
	urldate = {2015-12-16},
	publisher = {University of London},
	author = {Beal, Matthew James},
	year = {2003},
	file = {Variational Algorithms for Approximate Bayesian Inference - beal03.pdf:/home/user/Zotero/storage/SXQ94S67/beal03.pdf:application/pdf}
}

@article{andrews-compressing-2015,
	title = {Compressing {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1511.06397},
	urldate = {2015-12-16},
	journal = {arXiv preprint arXiv:1511.06397},
	author = {Andrews, Martin},
	year = {2015},
	file = {() - 1511.06397v1.pdf:/home/user/Zotero/storage/S7KGJDUS/1511.06397v1.pdf:application/pdf}
}

@inproceedings{pham-dropout-2014,
	title = {Dropout improves recurrent neural networks for handwriting recognition},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6981034},
	urldate = {2015-12-16},
	booktitle = {Frontiers in {Handwriting} {Recognition} ({ICFHR}), 2014 14th {International} {Conference} on},
	publisher = {IEEE},
	author = {Pham, Vu and Bluche, Théodore and Kermorvant, Christopher and Louradour, Jérôme},
	year = {2014},
	pages = {285--290},
	file = {1312.4569.pdf:/home/user/Zotero/storage/CHIQX3GT/1312.4569.pdf:application/pdf}
}

@article{okimoto-differences-2015,
	title = {Differences in {Sensitivity} to {Deviance} {Partly} {Explain} {Ideological} {Divides} in {Social} {Policy} {Support}},
	issn = {1939-1315},
	doi = {10.1037/pspp0000080},
	abstract = {We propose that political differences in social policy support may be partly driven by the tendency for conservatives to show greater sensitivity to deviance than liberals, even among targets lacking social or functional relevance. In 3 studies, participants were shown geometric figures and were asked to identify the extent to which they were "triangles" (or circles, squares, etc.). More conservative participants reported greater differentiation between perfect and imperfect shapes than more liberal participants, indicating greater sensitivity to deviance. Moreover, shape differentiation partly accounted for the relationship between political ideology and social policy, partially mediating the link between conservatism and harsher punishment of wrongdoers (Studies 1 and 4), less support for public aid for disadvantaged groups (Study 2), and less financial backing for policies that benefit marginalized groups in society (Study 3). This effect was specific to policies that targeted deviant groups (Study 3) and who were not too highly deviant (Study 4). Results suggest that, in addition to commonly cited affective and motivational reactions to deviant actors, political differences in social policy may also be driven by conservatives' greater cognitive propensity to distinguish deviance. (PsycINFO Database Record},
	language = {ENG},
	journal = {Journal of Personality and Social Psychology},
	author = {Okimoto, Tyler G. and Gromet, Dena M.},
	month = nov,
	year = {2015},
	pmid = {26571208}
}

@article{chapman-rejection-2015,
	title = {Rejection of rejection: a novel approach to overcoming barriers to publication},
	issn = {1756-1833},
	shorttitle = {Rejection of rejection},
	url = {http://www.bmj.com/lookup/doi/10.1136/bmj.h6326},
	doi = {10.1136/bmj.h6326},
	language = {en},
	urldate = {2015-12-16},
	journal = {BMJ},
	author = {Chapman, Cath and Slade, Tim},
	month = dec,
	year = {2015},
	pages = {h6326}
}

@article{eldan-power-2015,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.03965},
	abstract = {We show that there are simple functions expressible by small 3-layer feedforward neural networks, which cannot be approximated by a 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for most continuous activation functions, such as rectified linear units and sigmoids, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks.},
	urldate = {2015-12-15},
	journal = {arXiv:1512.03965 [cs, stat]},
	author = {Eldan, Ronen and Shamir, Ohad},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03965},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QS3D46ZH/1512.html:text/html;Eldan_Shamir_2015_The Power of Depth for Feedforward Neural Networks.pdf:/home/user/Zotero/storage/NJU7M9E8/Eldan_Shamir_2015_The Power of Depth for Feedforward Neural Networks.pdf:application/pdf}
}

@article{ranzato-sequence-2015-2,
	title = {Sequence {Level} {Training} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06732},
	abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
	urldate = {2015-12-15},
	journal = {arXiv:1511.06732 [cs]},
	author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06732},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3G6I33CA/1511.html:text/html;Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:/home/user/Zotero/storage/FB45DKPK/Ranzato et al_2015_Sequence Level Training with Recurrent Neural Networks.pdf:application/pdf}
}

@article{aaronson-sculpting-2015,
	title = {Sculpting {Quantum} {Speedups}},
	url = {http://arxiv.org/abs/1512.04016},
	abstract = {Given a problem which is intractable for both quantum and classical algorithms, can we find a sub-problem for which quantum algorithms provide an exponential advantage? We refer to this problem as the "sculpting problem." In this work, we give a full characterization of sculptable functions in the query complexity setting. We show that a total function f can be restricted to a promise P such that Q(f{\textbar}\_P)=O(polylog(N)) and R(f{\textbar}\_P)=N{\textasciicircum}{Omega(1)}, if and only if f has a large number of inputs with large certificate complexity. The proof uses some interesting techniques: for one direction, we introduce new relationships between randomized and quantum query complexity in various settings, and for the other direction, we use a recent result from communication complexity due to Klartag and Regev. We also characterize sculpting for other query complexity measures, such as R(f) vs. R\_0(f) and R\_0(f) vs. D(f). Along the way, we prove some new relationships for quantum query complexity: for example, a nearly quadratic relationship between Q(f) and D(f) whenever the promise of f is small. This contrasts with the recent super-quadratic query complexity separations, showing that the maximum gap between classical and quantum query complexities is indeed quadratic in various settings - just not for total functions! Lastly, we investigate sculpting in the Turing machine model. We show that if there is any BPP-bi-immune language in BQP, then every language outside BPP can be restricted to a promise which places it in PromiseBQP but not in PromiseBPP. Under a weaker assumption, that some problem in BQP is hard on average for P/poly, we show that every paddable language outside BPP is sculptable in this way.},
	urldate = {2015-12-15},
	journal = {arXiv:1512.04016 [quant-ph]},
	author = {Aaronson, Scott and Ben-David, Shalev},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.04016},
	keywords = {Computer Science - Computational Complexity, Quantum Physics},
	file = {Aaronson_Ben-David_2015_Sculpting Quantum Speedups.pdf:/home/user/Zotero/storage/JZ2DQBKJ/Aaronson_Ben-David_2015_Sculpting Quantum Speedups.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GZB3MPXC/1512.html:text/html}
}

@article{balkir-sentence-2015,
	title = {Sentence {Entailment} in {Compositional} {Distributional} {Semantics}},
	url = {http://arxiv.org/abs/1512.04419},
	abstract = {Distributional semantic models provide vector representations for words by gathering co-occurrence frequencies from corpora of text. Compositional distributional models extend these representations from words to phrases and sentences. In categorical compositional distributional semantics these representations are built in such a manner that meanings of phrases and sentences are functions of their grammatical structure and the meanings of the words therein. These models have been applied to reasoning about phrase and sentence level similarity. In this paper, we argue for and prove that these models can also be used to reason about phrase and sentence level entailment. We provide preliminary experimental results on a toy entailment dataset.},
	urldate = {2015-12-15},
	journal = {arXiv:1512.04419 [cs, math]},
	author = {Balkir, Esma and Kartsaklis, Dimitri and Sadrzadeh, Mehrnoosh},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.04419},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.7, Mathematics - Category Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/F8ZNQPNR/1512.html:text/html;Balkir et al_2015_Sentence Entailment in Compositional Distributional Semantics.pdf:/home/user/Zotero/storage/WMVWAVI8/Balkir et al_2015_Sentence Entailment in Compositional Distributional Semantics.pdf:application/pdf}
}

@article{pham-dropout-2013,
	title = {Dropout improves {Recurrent} {Neural} {Networks} for {Handwriting} {Recognition}},
	url = {http://arxiv.org/abs/1312.4569},
	abstract = {Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.},
	urldate = {2015-12-15},
	journal = {arXiv:1312.4569 [cs]},
	author = {Pham, Vu and Bluche, Théodore and Kermorvant, Christopher and Louradour, Jérôme},
	month = nov,
	year = {2013},
	note = {arXiv: 1312.4569},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ERBHAK6G/1312.html:text/html;Pham et al_2013_Dropout improves Recurrent Neural Networks for Handwriting Recognition.pdf:/home/user/Zotero/storage/B897DRN3/Pham et al_2013_Dropout improves Recurrent Neural Networks for Handwriting Recognition.pdf:application/pdf}
}

@article{zaremba-recurrent-2014,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	urldate = {2015-12-15},
	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.2329},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VPCKVP2A/1409.html:text/html;Zaremba et al_2014_Recurrent Neural Network Regularization.pdf:/home/user/Zotero/storage/WPBKRFDV/Zaremba et al_2014_Recurrent Neural Network Regularization.pdf:application/pdf}
}

@article{gong-modeling-2015,
	title = {Modeling {Coevolution} between {Language} and {Memory} {Capacity} during {Language} {Origin}},
	volume = {10},
	url = {http://dx.doi.org/10.1371/journal.pone.0142281},
	doi = {10.1371/journal.pone.0142281},
	abstract = {Memory is essential to many cognitive tasks including language. Apart from empirical studies of memory effects on language acquisition and use, there lack sufficient evolutionary explorations on whether a high level of memory capacity is prerequisite for language and whether language origin could influence memory capacity. In line with evolutionary theories that natural selection refined language-related cognitive abilities, we advocated a coevolution scenario between language and memory capacity, which incorporated the genetic transmission of individual memory capacity, cultural transmission of idiolects, and natural and cultural selections on individual reproduction and language teaching. To illustrate the coevolution dynamics, we adopted a multi-agent computational model simulating the emergence of lexical items and simple syntax through iterated communications. Simulations showed that: along with the origin of a communal language, an initially-low memory capacity for acquired linguistic knowledge was boosted; and such coherent increase in linguistic understandability and memory capacities reflected a language-memory coevolution; and such coevolution stopped till memory capacities became sufficient for language communications. Statistical analyses revealed that the coevolution was realized mainly by natural selection based on individual communicative success in cultural transmissions. This work elaborated the biology-culture parallelism of language evolution, demonstrated the driving force of culturally-constituted factors for natural selection of individual cognitive abilities, and suggested that the degree difference in language-related cognitive abilities between humans and nonhuman animals could result from a coevolution with language.},
	number = {11},
	urldate = {2015-12-14},
	journal = {PLoS ONE},
	author = {Gong, Tao and Shuai, Lan},
	month = nov,
	year = {2015},
	pages = {e0142281},
	file = {Gong_Shuai_2015_Modeling Coevolution between Language and Memory Capacity during Language Origin.pdf:/home/user/Zotero/storage/QW8TH55S/Gong_Shuai_2015_Modeling Coevolution between Language and Memory Capacity during Language Origin.pdf:application/pdf}
}

@article{czerwinski-characterization-2014,
	title = {A {Characterization} for {Decidable} {Separability} by {Piecewise} {Testable} {Languages}},
	url = {http://arxiv.org/abs/1410.1042},
	abstract = {The separability problem for word languages of a class \$\mathcal{C}\$ by languages of a class \$\mathcal{S}\$ asks, for two given languages \$I\$ and \$E\$ from \$\mathcal{C}\$, whether there exists a language \$S\$ from \$\mathcal{S}\$ that includes \$I\$ and excludes \$E\$, that is, \$I \subseteq S\$ and \$S\cap E = \emptyset\$. In this work, we assume some mild closure properties for \$\mathcal{C}\$ and study for which such classes \$\mathcal{C}\$ separability by piecewise testable languages (PTL) is decidable. We characterize these classes in terms of decidability of (two variants of) an unboundedness problem. From this we deduce that separability by PTL is decidable for a number of language classes, such as the context-free languages and languages of labeled vector addition systems. Furthermore, it follows that separability by PTL is decidable if and only if one can compute for any language of the class its downward closure with respect to the scattered substring ordering (i.e., if the set of scattered substrings of any language of the class is effectively regular). The obtained decidability results contrast known undecidability results. In fact, for all the (non-regular) language classes we present as examples with decidable separability, it is undecidable whether a given language is a PTL itself. Our characterization involves a result of independent interest, which states that for any kind of languages \$I\$ and \$E\$, non-separability is equivalent to the existence of common patterns in \$I\$ and \$E\$.},
	urldate = {2015-12-14},
	journal = {arXiv:1410.1042 [cs]},
	author = {Czerwiński, Wojciech and Martens, Wim and van Rooijen, Lorijn and Zeitoun, Marc and Zetzsche, Georg},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.1042},
	keywords = {Computer Science - Formal Languages and Automata Theory, F.4.3, 68Q45},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VUAMME8I/1410.html:text/html;Czerwiński et al_2014_A Characterization for Decidable Separability by Piecewise Testable Languages.pdf:/home/user/Zotero/storage/4CN62F7V/Czerwiński et al_2014_A Characterization for Decidable Separability by Piecewise Testable Languages.pdf:application/pdf}
}

@article{sennrich-improving-2015,
	title = {Improving {Neural} {Machine} {Translation} {Models} with {Monolingual} {Data}},
	url = {http://arxiv.org/abs/1511.06709},
	abstract = {Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for neural machine translation (NMT). In contrast to previous work, which integrates a separately trained RNN language model into an NMT architecture, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to include monolingual training data in the training process. Through our use of monolingual data, we obtain substantial improvements on the WMT 15 (+2.8--3.4 BLEU) task for English-{\textgreater}German, and for the low-resourced IWSLT 14 task Turkish-{\textgreater}English (+2.1--3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task for English-{\textgreater}German.},
	urldate = {2015-12-14},
	journal = {arXiv:1511.06709 [cs]},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06709},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RC5H6JRU/1511.html:text/html;Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:/home/user/Zotero/storage/XMC38MBH/Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:application/pdf}
}

@article{lee-reasoning-2015-1,
	title = {Reasoning in {Vector} {Space}: {An} {Exploratory} {Study} of {Question} {Answering}},
	shorttitle = {Reasoning in {Vector} {Space}},
	url = {http://arxiv.org/abs/1511.06426},
	abstract = {Question answering tasks have shown remarkable progress with distributed vector representations. In this paper, we look into the recently proposed Facebook 20 tasks (FB20). Finding the answers for questions in FB20 requires complex reasoning. Because the previous work on FB20 consists of end-to-end models, it is unclear whether errors come from imperfect understanding of semantics or in certain steps of the reasoning. To address this issue, we propose two vector space models inspired by tensor product representation (TPR) to perform analysis, knowledge representation, and reasoning based on common-sense inference. We achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches due to the special two-dimensional relationships identified from this study. The exploration reported in this paper and our subsequent work on generalizing the current model to the TPR formalism suggest the feasibility of developing further reasoning models in tensor space with learning capabilities.},
	urldate = {2015-12-14},
	journal = {arXiv:1511.06426 [cs]},
	author = {Lee, Moontae and He, Xiaodong and Yih, Wen-tau and Gao, Jianfeng and Deng, Li and Smolensky, Paul},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06426},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9BIRVPFM/1511.html:text/html;Lee et al_2015_Reasoning in Vector Space.pdf:/home/user/Zotero/storage/55KKSIHD/Lee et al_2015_Reasoning in Vector Space.pdf:application/pdf}
}

@article{bohannon-how-2015,
	title = {How to hijack a journal},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/350/6263/903},
	doi = {10.1126/science.350.6263.903},
	language = {en},
	number = {6263},
	urldate = {2015-12-14},
	journal = {Science},
	author = {Bohannon, John},
	month = nov,
	year = {2015},
	pmid = {26586744},
	pages = {903--905},
	file = {Bohannon_2015_How to hijack a journal.pdf:/home/user/Zotero/storage/93DQPITF/Bohannon_2015_How to hijack a journal.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/4KRGZU7J/903.html:text/html}
}

@article{yang-how-2015,
	title = {How {Distance} {Affects} {Semantic} {Integration} in {Discourse}: {Evidence} from {Event}-{Related} {Potentials}},
	volume = {10},
	shorttitle = {How {Distance} {Affects} {Semantic} {Integration} in {Discourse}},
	url = {http://dx.doi.org/10.1371/journal.pone.0142967},
	doi = {10.1371/journal.pone.0142967},
	abstract = {Event-related potentials were used to investigate whether semantic integration in discourse is influenced by the number of intervening sentences between the endpoints of integration. Readers read discourses in which the last sentence contained a critical word that was either congruent or incongruent with the information introduced in the first sentence. Furthermore, for the short discourses, the first and last sentence were intervened by only one sentence while for the long discourses, they were intervened by three sentences. We found that the incongruent words elicited an N400 effect for both the short and long discourses. However, a P600 effect was only observed for the long discourses, but not for the short ones. These results suggest that although readers can successfully integrate upcoming words into the existing discourse representation, the effort required for this integration process is modulated by the number of intervening sentences. Thus, discourse distance as measured by the number of intervening sentences should be taken as an important factor for semantic integration in discourse.},
	number = {11},
	urldate = {2015-12-14},
	journal = {PLoS ONE},
	author = {Yang, Xiaohong and Chen, Shuang and Chen, Xuhai and Yang, Yufang},
	month = nov,
	year = {2015},
	pages = {e0142967},
	file = {Yang et al_2015_How Distance Affects Semantic Integration in Discourse.pdf:/home/user/Zotero/storage/6PN5KEU8/Yang et al_2015_How Distance Affects Semantic Integration in Discourse.pdf:application/pdf}
}

@article{yang-neural-2015-1,
	title = {Neural {Self} {Talk}: {Image} {Understanding} via {Continuous} {Questioning} and {Answering}},
	shorttitle = {Neural {Self} {Talk}},
	url = {http://arxiv.org/abs/1512.03460},
	abstract = {In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.},
	urldate = {2015-12-14},
	journal = {arXiv:1512.03460 [cs]},
	author = {Yang, Yezhou and Li, Yi and Fermuller, Cornelia and Aloimonos, Yiannis},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03460},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, I.2.10, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3CS6M2HK/1512.html:text/html;Yang et al_2015_Neural Self Talk.pdf:/home/user/Zotero/storage/7DUVTBKD/Yang et al_2015_Neural Self Talk.pdf:application/pdf}
}

@article{park-bayesian-2014,
	title = {Bayesian {Manifold} {Learning}: {The} {Locally} {Linear} {Latent} {Variable} {Model} ({LL}-{LVM})},
	shorttitle = {Bayesian {Manifold} {Learning}},
	url = {http://arxiv.org/abs/1410.6791},
	abstract = {We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.},
	urldate = {2015-12-14},
	journal = {arXiv:1410.6791 [stat]},
	author = {Park, Mijung and Jitkrittum, Wittawat and Qamar, Ahmad and Szabo, Zoltan and Buesing, Lars and Sahani, Maneesh},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.6791},
	keywords = {G.3, Statistics - Machine Learning, I.2.6, 62F15},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XBXIF92S/1410.html:text/html;Park et al_2014_Bayesian Manifold Learning.pdf:/home/user/Zotero/storage/DCUHI48W/Park et al_2014_Bayesian Manifold Learning.pdf:application/pdf}
}

@article{karimi-classical-2015,
	title = {Classical entanglement?},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/350/6265/1172},
	doi = {10.1126/science.aad7174},
	language = {en},
	number = {6265},
	urldate = {2015-12-14},
	journal = {Science},
	author = {Karimi, Ebrahim and Boyd, Robert W.},
	month = dec,
	year = {2015},
	pages = {1172--1173},
	file = {Karimi_Boyd_2015_Classical entanglement.pdf:/home/user/Zotero/storage/JFF7KDSK/Karimi_Boyd_2015_Classical entanglement.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7VMPATWM/1172.html:text/html}
}

@article{fawcett-can-2015,
	title = {Can {Paris} pledges avert severe climate change?},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/350/6265/1168},
	doi = {10.1126/science.aad5761},
	language = {en},
	number = {6265},
	urldate = {2015-12-14},
	journal = {Science},
	author = {Fawcett, Allen A. and Iyer, Gokul C. and Clarke, Leon E. and Edmonds, James A. and Hultman, Nathan E. and McJeon, Haewon C. and Rogelj, Joeri and Schuler, Reed and Alsalam, Jameel and Asrar, Ghassem R. and Creason, Jared and Jeong, Minji and McFarland, James and Mundra, Anupriya and Shi, Wenjing},
	month = dec,
	year = {2015},
	pmid = {26612835},
	pages = {1168--1169},
	file = {Fawcett et al_2015_Can Paris pledges avert severe climate change.pdf:/home/user/Zotero/storage/XWQFNK6V/Fawcett et al_2015_Can Paris pledges avert severe climate change.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/M7R67T59/1168.html:text/html}
}

@article{kim-neural-2015,
	title = {Neural {Substrates} {Related} to {Motor} {Memory} with {Multiple} {Timescales} in {Sensorimotor} {Adaptation}},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pbio.1002312},
	doi = {10.1371/journal.pbio.1002312},
	abstract = {A model-based functional MRI study reveals the existence of brain regions associated with four distinct timescales in motor adaptation.},
	number = {12},
	urldate = {2015-12-14},
	journal = {PLoS Biol},
	author = {Kim, Sungshin and Ogawa, Kenji and Lv, Jinchi and Schweighofer, Nicolas and Imamizu, Hiroshi},
	month = dec,
	year = {2015},
	pages = {e1002312},
	file = {Kim et al_2015_Neural Substrates Related to Motor Memory with Multiple Timescales in.pdf:/home/user/Zotero/storage/2T5ZSTZE/Kim et al_2015_Neural Substrates Related to Motor Memory with Multiple Timescales in.pdf:application/pdf}
}

@article{weaver-motor-2015,
	title = {Motor {Learning} {Unfolds} over {Different} {Timescales} in {Distinct} {Neural} {Systems}},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pbio.1002313},
	doi = {10.1371/journal.pbio.1002313},
	abstract = {A new study reveals the time-resolved brain map of activity involved in the formation of motor memories, from seconds to hours and from the frontal and parietal lobes to the cerebellum. Read the Research Article.},
	number = {12},
	urldate = {2015-12-14},
	journal = {PLoS Biol},
	author = {Weaver, Janelle},
	month = dec,
	year = {2015},
	pages = {e1002313},
	file = {Weaver_2015_Motor Learning Unfolds over Different Timescales in Distinct Neural Systems.pdf:/home/user/Zotero/storage/2GG24VGE/Weaver_2015_Motor Learning Unfolds over Different Timescales in Distinct Neural Systems.pdf:application/pdf}
}

@article{gal-bayesian-2015-1,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Bernoulli} {Approximate} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1506.02158},
	abstract = {Convolutional neural networks (convnets) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use convnets with small data -- as convnets overfit quickly. We present an efficient Bayesian convnet, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the convnet's kernels. To make this possible we present new theoretical results casting dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in the field with no increase in time complexity. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.},
	urldate = {2015-12-14},
	journal = {arXiv:1506.02158 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02158},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QV6BBXCQ/1506.html:text/html;Gal_Ghahramani_2015_Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational.pdf:/home/user/Zotero/storage/HDMBH8WB/Gal_Ghahramani_2015_Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational.pdf:application/pdf}
}

@article{perez-evaluation-2015,
	title = {Evaluation and revision of inferential comprehension in narrative texts: an eye movement study},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Evaluation and revision of inferential comprehension in narrative texts},
	url = {http://dx.doi.org/10.1080/23273798.2015.1115883},
	doi = {10.1080/23273798.2015.1115883},
	abstract = {We investigated how adult readers evaluate and revise their situation model, by monitoring their eye movements as they read narrative texts and critical sentences. In each text, a short introduction primed an inference, followed by a concept that was either expected (e.g. “oven”) or unexpected (e.g. “grill”). Eye movements showed that readers detected a mismatch between the unexpected information and their prior interpretation, confirming their ability to evaluate inferential information. Subsequently, a critical sentence included a word that was either congruent (e.g. “roasted”) or incongruent (e.g. “barbecued”) with the expected but not the unexpected concept. Readers spent less time reading the congruent than the incongruent word, reflecting the facilitation of prior information. In addition, when the unexpected concept had been presented, participants with lower verbal (but not visuospatial) working memory span exhibited longer reading times and made more regressions on encountering congruent information, indicating difficulty in revising their situation model.},
	number = {0},
	urldate = {2015-12-14},
	journal = {Language, Cognition and Neuroscience},
	author = {Pérez, Ana and Joseph, Holly S. S. L. and Bajo, Teresa and Nation, Kate},
	month = nov,
	year = {2015},
	pages = {1--18},
	file = {Pérez et al_2015_Evaluation and revision of inferential comprehension in narrative texts.pdf:/home/user/Zotero/storage/7STP88AQ/Pérez et al_2015_Evaluation and revision of inferential comprehension in narrative texts.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GAD3KP7K/23273798.2015.html:text/html}
}

@article{tollefson-is-2015,
	title = {Is the 2 °{C} world a fantasy?},
	volume = {527},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/527436a},
	doi = {10.1038/527436a},
	number = {7579},
	urldate = {2015-12-14},
	journal = {Nature},
	author = {Tollefson, Jeff},
	month = nov,
	year = {2015},
	pages = {436--438}
}

@article{hansen-quick-2015,
	title = {A quick look at how photoelectrodes work},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/350/6264/1030},
	doi = {10.1126/science.aad6060},
	language = {en},
	number = {6264},
	urldate = {2015-12-14},
	journal = {Science},
	author = {Hansen, Ole and Seger, Brian and Vesborg, Peter C. K. and Chorkendorff, Ib},
	month = nov,
	year = {2015},
	pmid = {26612935},
	pages = {1030--1031},
	file = {Hansen et al_2015_A quick look at how photoelectrodes work.pdf:/home/user/Zotero/storage/5B93Z9SU/Hansen et al_2015_A quick look at how photoelectrodes work.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HN5XP7TT/1030.html:text/html}
}

@article{lewis-understanding-2015,
	title = {Understanding {China}'s non–fossil energy targets},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/350/6264/1034},
	doi = {10.1126/science.aad1084},
	language = {en},
	number = {6264},
	urldate = {2015-12-14},
	journal = {Science},
	author = {Lewis, Joanna I. and Fridley, David G. and Price, Lynn K. and Lu, Hongyou and Romankiewicz, John P.},
	month = nov,
	year = {2015},
	pmid = {26612938},
	pages = {1034--1036},
	file = {Lewis et al_2015_Understanding China's non–fossil energy targets.pdf:/home/user/Zotero/storage/9QPUMZAQ/Lewis et al_2015_Understanding China's non–fossil energy targets.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9A76M6EI/1034.html:text/html}
}

@article{maesani-fluctuation-driven-2015,
	title = {Fluctuation-{Driven} {Neural} {Dynamics} {Reproduce} {Drosophila} {Locomotor} {Patterns}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004577},
	doi = {10.1371/journal.pcbi.1004577},
	abstract = {Author Summary The brain is never quiet. Even in the absence of environmental cues, neurons receive and produce an ongoing barrage of fluctuating signals. These fluctuations are well studied in the sensory periphery but their potential influence on central circuits and behavior are unknown. In particular, activity fluctuations in action selection circuits—neural populations that drive an animal’s actions from moment to moment—may strongly influence behavior. To shed light on the influence of activity fluctuations on action timing, we developed a computational approach for automatically generating neural network models that reproduce large-scale, high-resolution behavioral measurements of freely walking Drosophila melanogaster. We found that models require stochastic activity fluctuations to reproduce complex Drosophila locomotor patterns. Specific fluctuation-driven dynamics allow these models to produce short and long bouts of locomotion in the absence of sensory cues and to reduce locomotor activity after sensory stimulation. These results support a role for ongoing activity fluctuations in the timing of animal behavior and reveal how behavioral shifts can be brought about through changes in the dynamics of neural circuits. Thus, simple dynamical mechanisms may underlie complex patterns of animal behavior.},
	number = {11},
	urldate = {2015-12-14},
	journal = {PLoS Comput Biol},
	author = {Maesani, Andrea and Ramdya, Pavan and Cruchet, Steeve and Gustafson, Kyle and Benton, Richard and Floreano, Dario},
	month = nov,
	year = {2015},
	pages = {e1004577},
	file = {Maesani et al_2015_Fluctuation-Driven Neural Dynamics Reproduce Drosophila Locomotor Patterns.pdf:/home/user/Zotero/storage/SUXJC74E/Maesani et al_2015_Fluctuation-Driven Neural Dynamics Reproduce Drosophila Locomotor Patterns.pdf:application/pdf}
}

@article{lau-systematic-2015,
	title = {A {Systematic} {Bayesian} {Integration} of {Epidemiological} and {Genetic} {Data}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004633},
	doi = {10.1371/journal.pcbi.1004633},
	abstract = {Author Summary In the midst of increasingly available sequence data of pathogens, a key challenge is to better integrate these data with traditional epidemiological data, with the proximate goal of reliable prediction and the ultimate aim of effective management of disease outbreaks. Although substantial advances have been made for such an integration, and they have improved our understandings of many disease dynamics which are not available otherwise, current methods have relied on fast algorithms, rather than achieving a systematic integration and accurate inference of the joint epidemiological-evolutionary process. Building on methods in current literature, this paper describes a novel Bayesian approach for systematically integrating these two streams of data. We propose a computationally tractable Bayesian inferential algorithm which takes the full joint epidemiological-evolutionary process into account. Using this algorithm, we study systematically the value of genetic data, providing valuable insights into future sampling designs. The algorithm is subsequently applied to real-world dataset describing the spread of animal foot-and-mouth disease in the UK, demonstrating the importance of such a systematic integration achieved with our methodology.},
	number = {11},
	urldate = {2015-12-14},
	journal = {PLoS Comput Biol},
	author = {Lau, Max S. Y. and Marion, Glenn and Streftaris, George and Gibson, Gavin},
	month = nov,
	year = {2015},
	pages = {e1004633},
	file = {Lau et al_2015_A Systematic Bayesian Integration of Epidemiological and Genetic Data.pdf:/home/user/Zotero/storage/E7HNCU29/Lau et al_2015_A Systematic Bayesian Integration of Epidemiological and Genetic Data.pdf:application/pdf}
}

@article{zhou-c-lstm-2015,
	title = {A {C}-{LSTM} {Neural} {Network} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1511.08630},
	abstract = {Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.},
	urldate = {2015-12-14},
	journal = {arXiv:1511.08630 [cs]},
	author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C. M.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08630},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UPD3EP9U/1511.html:text/html;Zhou et al_2015_A C-LSTM Neural Network for Text Classification.pdf:/home/user/Zotero/storage/95723F5B/Zhou et al_2015_A C-LSTM Neural Network for Text Classification.pdf:application/pdf}
}

@article{seymour-multimodal-2015,
	title = {Multimodal {Skip}-gram {Using} {Convolutional} {Pseudowords}},
	url = {http://arxiv.org/abs/1511.04024},
	abstract = {This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional "pseudowords:" embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.},
	urldate = {2015-12-14},
	journal = {arXiv:1511.04024 [cs]},
	author = {Seymour, Zachary and Li, Yingming and Zhang, Zhongfei},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04024},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C9JKQVWW/1511.html:text/html;Seymour et al_2015_Multimodal Skip-gram Using Convolutional Pseudowords.pdf:/home/user/Zotero/storage/XTZX2582/Seymour et al_2015_Multimodal Skip-gram Using Convolutional Pseudowords.pdf:application/pdf}
}

@article{sturm-separation-2015,
	title = {Separation of {Quantified} {First}-{Order} {Variables} entails {Decidability}},
	url = {http://arxiv.org/abs/1511.08999},
	abstract = {We introduce a fragment of first-order logic with equality which strictly generalizes two already well-known ones -- the Bernays-Sch\"onfinkel-Ramsey fragment (BSR) and the Monadic fragment. Satisfiability remains decidable in the new fragment. The defining principle is the separation of universally quantified variables from existentially quantified ones on the level of atoms. Thus, our classification neither rests on restrictions of quantifier prefixes (as in the BSR case) nor on restrictions with respect to the arity of the occurring predicate symbols (as in the monadic case).},
	urldate = {2015-12-14},
	journal = {arXiv:1511.08999 [cs]},
	author = {Sturm, Thomas and Voigt, Marco and Weidenbach, Christoph},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08999},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ETKNH97Q/1511.html:text/html;Sturm et al_2015_Separation of Quantified First-Order Variables entails Decidability.pdf:/home/user/Zotero/storage/5R3ZJHTP/Sturm et al_2015_Separation of Quantified First-Order Variables entails Decidability.pdf:application/pdf}
}

@article{pierron-quantifier-2015,
	title = {Quantifier {Alternation} for {Infinite} {Words}},
	url = {http://arxiv.org/abs/1511.09011},
	abstract = {We investigate the expressive power of quantifier alternation hierarchy of first-order logic over words. This hierarchy includes the classes \${\Sigma}\_i\$ (sentences having at most \$i\$ blocks of quantifiers starting with an \$\exists\$) and \$\mathcal{B}{\Sigma}\_i\$ (Boolean combinations of \${\Sigma}\_i\$ sentences). So far, this expressive power has been effectively characterized for the lower levels only. Recently, a breakthrough was made over finite words, and decidable characterizations were obtained for \$\mathcal{B}{\Sigma}\_2\$ and \${\Sigma}\_3\$, by relying on a decision problem called separation, and solving it for \${\Sigma}\_2\$. The contribution of this paper is a generalization of these results to the setting of infinite words: we solve separation for \${\Sigma}\_2\$ and \${\Sigma}\_3\$, and obtain decidable characterizations of \$\mathcal{B}{\Sigma}\_2\$ and \${\Sigma}\_3\$ as consequences.},
	urldate = {2015-12-14},
	journal = {arXiv:1511.09011 [cs]},
	author = {Pierron, Théo and Place, Thomas and Zeitoun, Marc},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.09011},
	keywords = {Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/SDC655PX/1511.html:text/html;Pierron et al_2015_Quantifier Alternation for Infinite Words.pdf:/home/user/Zotero/storage/PNISSHE2/Pierron et al_2015_Quantifier Alternation for Infinite Words.pdf:application/pdf}
}

@article{hoi-probing-2015,
	title = {Probing the quantum vacuum with an artificial atom in front of a mirror},
	volume = {11},
	copyright = {© 2015 Nature Publishing Group},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v11/n12/full/nphys3484.html?WT.ec\_id=NPHYS-201512&spMailingID=50147943&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=820146428&spReportId=ODIwMTQ2NDI4S0},
	doi = {10.1038/nphys3484},
	abstract = {Quantum fluctuations of the vacuum are both a surprising and fundamental phenomenon of nature. Understood as virtual photons, they still have a very real impact, for instance, in the Casimir effects and the lifetimes of atoms. Engineering vacuum fluctuations is therefore becoming increasingly important to emerging technologies. Here, we shape vacuum fluctuations using a superconducting circuit analogue of a mirror, creating regions in space where they are suppressed. Moving an artificial atom through these regions and measuring the spontaneous emission lifetime of the atom provides us with the spectral density of the vacuum fluctuations. Using the paradigm of waveguide quantum electrodynamics, we significantly improve over previous studies of the interaction of an atom with its mirror image, observing a spectral density as low as 0.02 quanta, a factor of 50 below the mirrorless result. This demonstrates that we can hide the atom from the vacuum, even though it is exposed in free space.},
	language = {en},
	number = {12},
	urldate = {2015-12-14},
	journal = {Nature Physics},
	author = {Hoi, I.-C. and Kockum, A. F. and Tornberg, L. and Pourkabirian, A. and Johansson, G. and Delsing, P. and Wilson, C. M.},
	month = dec,
	year = {2015},
	keywords = {Quantum mechanics, Superconducting properties and materials},
	pages = {1045--1049},
	file = {Hoi et al_2015_Probing the quantum vacuum with an artificial atom in front of a mirror.pdf:/home/user/Zotero/storage/B8UZPK2V/Hoi et al_2015_Probing the quantum vacuum with an artificial atom in front of a mirror.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/A954XVMN/nphys3484.html:text/html}
}

@article{buchanan-non-optimal-2015,
	title = {Non-optimal optimization},
	volume = {11},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v11/n12/full/nphys3586.html?WT.ec\_id=NPHYS-201512&spMailingID=50147943&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=820146428&spReportId=ODIwMTQ2NDI4S0},
	doi = {10.1038/nphys3586},
	language = {en},
	number = {12},
	urldate = {2015-12-14},
	journal = {Nature Physics},
	author = {Buchanan, Mark},
	month = dec,
	year = {2015},
	keywords = {Physics, Economics},
	pages = {984--984},
	file = {Buchanan_2015_Non-optimal optimization.pdf:/home/user/Zotero/storage/TIWH87S8/Buchanan_2015_Non-optimal optimization.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HJJZMJX2/nphys3586.html:text/html}
}

@article{bach-anxiety-like-2015,
	title = {Anxiety-{Like} {Behavioural} {Inhibition} {Is} {Normative} under {Environmental} {Threat}-{Reward} {Correlations}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004646},
	doi = {10.1371/journal.pcbi.1004646},
	abstract = {Author Summary Behavioural inhibition is observed in situations of anxiety, both in animals and humans. In some situations, it is not clear how it minimises harm or maximises benefit for the agent, and can even appear counterproductive. This prevents an understanding of the underlying neural computations. Here, we furnish the first formal assessment of its adaptive value in a controlled anxiety model, and confirm predictions in four experiments with humans. Results may suggest a neural implementation that relies on online cost minimisation. This finding could afford a better understanding of human anxiety disorder and the underlying neural computations.},
	number = {12},
	urldate = {2015-12-13},
	journal = {PLoS Comput Biol},
	author = {Bach, Dominik R.},
	month = dec,
	year = {2015},
	pages = {e1004646},
	file = {Bach_2015_Anxiety-Like Behavioural Inhibition Is Normative under Environmental.pdf:/home/user/Zotero/storage/5GW8NQJV/Bach_2015_Anxiety-Like Behavioural Inhibition Is Normative under Environmental.pdf:application/pdf}
}

@article{ariel-locust-2015,
	title = {Locust {Collective} {Motion} and {Its} {Modeling}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004522},
	doi = {10.1371/journal.pcbi.1004522},
	abstract = {Over the past decade, technological advances in experimental and animal tracking techniques have motivated a renewed theoretical interest in animal collective motion and, in particular, locust swarming. This review offers a comprehensive biological background followed by comparative analysis of recent models of locust collective motion, in particular locust marching, their settings, and underlying assumptions. We describe a wide range of recent modeling and simulation approaches, from discrete agent-based models of self-propelled particles to continuous models of integro-differential equations, aimed at describing and analyzing the fascinating phenomenon of locust collective motion. These modeling efforts have a dual role: The first views locusts as a quintessential example of animal collective motion. As such, they aim at abstraction and coarse-graining, often utilizing the tools of statistical physics. The second, which originates from a more biological perspective, views locust swarming as a scientific problem of its own exceptional merit. The main goal should, thus, be the analysis and prediction of natural swarm dynamics. We discuss the properties of swarm dynamics using the tools of statistical physics, as well as the implications for laboratory experiments and natural swarms. Finally, we stress the importance of a combined-interdisciplinary, biological-theoretical effort in successfully confronting the challenges that locusts pose at both the theoretical and practical levels.},
	number = {12},
	urldate = {2015-12-13},
	journal = {PLoS Comput Biol},
	author = {Ariel, Gil and Ayali, Amir},
	month = dec,
	year = {2015},
	pages = {e1004522},
	file = {Ariel_Ayali_2015_Locust Collective Motion and Its Modeling.pdf:/home/user/Zotero/storage/FZTRIWA8/Ariel_Ayali_2015_Locust Collective Motion and Its Modeling.pdf:application/pdf}
}

@article{ariel-locust-2015-1,
	title = {Locust {Collective} {Motion} and {Its} {Modeling}},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004522},
	doi = {10.1371/journal.pcbi.1004522},
	abstract = {Over the past decade, technological advances in experimental and animal tracking techniques have motivated a renewed theoretical interest in animal collective motion and, in particular, locust swarming. This review offers a comprehensive biological background followed by comparative analysis of recent models of locust collective motion, in particular locust marching, their settings, and underlying assumptions. We describe a wide range of recent modeling and simulation approaches, from discrete agent-based models of self-propelled particles to continuous models of integro-differential equations, aimed at describing and analyzing the fascinating phenomenon of locust collective motion. These modeling efforts have a dual role: The first views locusts as a quintessential example of animal collective motion. As such, they aim at abstraction and coarse-graining, often utilizing the tools of statistical physics. The second, which originates from a more biological perspective, views locust swarming as a scientific problem of its own exceptional merit. The main goal should, thus, be the analysis and prediction of natural swarm dynamics. We discuss the properties of swarm dynamics using the tools of statistical physics, as well as the implications for laboratory experiments and natural swarms. Finally, we stress the importance of a combined-interdisciplinary, biological-theoretical effort in successfully confronting the challenges that locusts pose at both the theoretical and practical levels.},
	number = {12},
	urldate = {2015-12-13},
	journal = {PLoS Comput Biol},
	author = {Ariel, Gil and Ayali, Amir},
	month = dec,
	year = {2015},
	pages = {e1004522},
	file = {Ariel_Ayali_2015_Locust Collective Motion and Its Modeling.pdf:/home/user/Zotero/storage/8KT2CEG7/Ariel_Ayali_2015_Locust Collective Motion and Its Modeling.pdf:application/pdf}
}

@article{friston-generalised-2010,
	title = {Generalised {Filtering}},
	volume = {2010},
	issn = {1024-123X, 1563-5147},
	url = {http://www.hindawi.com/journals/mpe/2010/621670/},
	doi = {10.1155/2010/621670},
	language = {en},
	urldate = {2016-01-25},
	journal = {Mathematical Problems in Engineering},
	author = {Friston, Karl and Stephan, Klaas and Li, Baojuan and Daunizeau, Jean},
	year = {2010},
	pages = {1--34},
	file = {Generalised Filtering - Generalised Filtering.pdf:/home/user/Zotero/storage/AZ5T2GRJ/Generalised Filtering.pdf:application/pdf}
}

@article{ginzburg-theory-2007,
	title = {On the theory of superconductivity},
	volume = {2},
	issn = {1827-6121},
	url = {http://link.springer.com/article/10.1007/BF02731579},
	doi = {10.1007/BF02731579},
	abstract = {Summary Macroscopic theory of superconductivity valid for magnetic fields of arbitrary magnitude and the behaviour of superconductors in weak high frequency fields are discussed. The problem of formulating a microscopic theory of superconductivity is also considered.},
	language = {en},
	number = {6},
	urldate = {2016-01-25},
	journal = {Il Nuovo Cimento (1955-1965)},
	author = {Ginzburg, V. L.},
	month = oct,
	year = {2007},
	keywords = {Physics, general},
	pages = {1234--1250},
	file = {Full Text PDF:/home/user/Zotero/storage/NVZND9NP/Ginzburg - 2007 - On the theory of superconductivity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/NIKT8UPE/10.html:text/html}
}

@misc{noauthor-variational-2016,
	title = {Variational {Bayesian} methods},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Variational\_Bayesian\_methods&oldid=700819699},
	abstract = {Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning. They are typically used in complex statistical models consisting of observed variables (usually termed "data") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model. As is typical in Bayesian inference, the parameters and latent variables are grouped together as "unobserved variables". Variational Bayesian methods are primarily used for two purposes:
To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables.
To derive a lower bound for the marginal likelihood (sometimes called the "evidence") of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables). This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.)
In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods — particularly, Markov chain Monte Carlo methods such as Gibbs sampling — for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to directly evaluate or sample from. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, Variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior.
Variational Bayes can be seen as an extension of the EM (expectation-maximization) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables. As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically.
For many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed. However, deriving the set of equations used to iteratively update the parameters often requires a large amount of work compared with deriving the comparable Gibbs sampling equations. This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables.},
	language = {en},
	urldate = {2016-01-22},
	journal = {Wikipedia, the free encyclopedia},
	month = jan,
	year = {2016},
	note = {Page Version ID: 700819699},
	file = {Snapshot:/home/user/Zotero/storage/CC5FNBHT/index.html:text/html}
}

@article{mayo-defense-1981,
	title = {In defense of the {Neyman}-{Pearson} theory of confidence intervals},
	url = {http://www.jstor.org/stable/187185},
	urldate = {2016-01-22},
	journal = {Philosophy of Science},
	author = {Mayo, Deborah G.},
	year = {1981},
	pages = {269--280},
	file = {In Defense of the Neyman-Pearson Theory of Confidence Intervals.pdf:/home/user/Zotero/storage/FUMTTHRM/In Defense of the Neyman-Pearson Theory of Confidence Intervals.pdf:application/pdf}
}

@article{friston-free-2006,
	title = {A free energy principle for the brain},
	volume = {100},
	issn = {09284257},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S092842570600060X},
	doi = {10.1016/j.jphysparis.2006.10.001},
	language = {en},
	number = {1-3},
	urldate = {2016-01-22},
	journal = {Journal of Physiology-Paris},
	author = {Friston, Karl and Kilner, James and Harrison, Lee},
	month = jul,
	year = {2006},
	pages = {70--87},
	file = {doi\:10.1016/j.jphysparis.2006.10.001 - A free energy principle for the brain.pdf:/home/user/Zotero/storage/TZENME4Q/A free energy principle for the brain.pdf:application/pdf}
}

@article{friston-knowing-2015,
	title = {Knowing one's place: a free-energy approach to pattern regulation},
	volume = {12},
	issn = {1742-5689, 1742-5662},
	shorttitle = {Knowing one's place},
	url = {http://rsif.royalsocietypublishing.org/cgi/doi/10.1098/rsif.2014.1383},
	doi = {10.1098/rsif.2014.1383},
	language = {en},
	number = {105},
	urldate = {2016-01-22},
	journal = {Journal of The Royal Society Interface},
	author = {Friston, K. and Levin, M. and Sengupta, B. and Pezzulo, G.},
	month = mar,
	year = {2015},
	pages = {20141383--20141383},
	file = {untitled - Knowing ones place.pdf:/home/user/Zotero/storage/9BSPXIU9/Knowing ones place.pdf:application/pdf}
}

@article{friston-active-2015,
	title = {Active inference and epistemic value},
	volume = {6},
	issn = {1758-8928, 1758-8936},
	url = {http://www.tandfonline.com/doi/full/10.1080/17588928.2015.1020053},
	doi = {10.1080/17588928.2015.1020053},
	language = {en},
	number = {4},
	urldate = {2016-01-22},
	journal = {Cognitive Neuroscience},
	author = {Friston, Karl and Rigoli, Francesco and Ognibene, Dimitri and Mathys, Christoph and Fitzgerald, Thomas and Pezzulo, Giovanni},
	month = oct,
	year = {2015},
	pages = {187--214},
	file = {Active inference and epistemic value - Active inference and epistemic value.pdf:/home/user/Zotero/storage/CKU5EHHR/Active inference and epistemic value.pdf:application/pdf}
}

@article{friston-duet-2015,
	title = {A {Duet} for one},
	volume = {36},
	issn = {10538100},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S105381001400230X},
	doi = {10.1016/j.concog.2014.12.003},
	language = {en},
	urldate = {2016-01-22},
	journal = {Consciousness and Cognition},
	author = {Friston, Karl and Frith, Christopher},
	month = nov,
	year = {2015},
	pages = {390--405},
	file = {A Duet for one - A Duet for one.pdf:/home/user/Zotero/storage/FA5BD7HK/A Duet for one.pdf:application/pdf}
}

@article{friston-anatomy-2014,
	title = {The anatomy of choice: dopamine and decision-making},
	volume = {369},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The anatomy of choice},
	url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0481},
	doi = {10.1098/rstb.2013.0481},
	language = {en},
	number = {1655},
	urldate = {2016-01-22},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Friston, K. and Schwartenbeck, P. and FitzGerald, T. and Moutoussis, M. and Behrens, T. and Dolan, R. J.},
	month = sep,
	year = {2014},
	pages = {20130481--20130481},
	file = {untitled - The anatomy of choice dopamine and decision-making.pdf:/home/user/Zotero/storage/7DKNKJG8/The anatomy of choice dopamine and decision-making.pdf:application/pdf}
}

@article{friston-anatomy-2013,
	title = {The anatomy of choice: active inference and agency},
	volume = {7},
	issn = {1662-5161},
	shorttitle = {The anatomy of choice},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2013.00598/abstract},
	doi = {10.3389/fnhum.2013.00598},
	urldate = {2016-01-22},
	journal = {Frontiers in Human Neuroscience},
	author = {Friston, Karl and Schwartenbeck, Philipp and FitzGerald, Thomas and Moutoussis, Michael and Behrens, Timothy and Dolan, Raymond J.},
	year = {2013},
	file = {The anatomy of choice\: active inference and agency - The anatomy of choice active inference and agency.pdf:/home/user/Zotero/storage/VXM69UC8/The anatomy of choice active inference and agency.pdf:application/pdf}
}

@article{friston-life-2013,
	title = {Life as we know it},
	volume = {10},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/cgi/doi/10.1098/rsif.2013.0475},
	doi = {10.1098/rsif.2013.0475},
	language = {en},
	number = {86},
	urldate = {2016-01-22},
	journal = {Journal of The Royal Society Interface},
	author = {Friston, K.},
	month = jul,
	year = {2013},
	pages = {20130475--20130475},
	file = {untitled - Life as we know it.pdf:/home/user/Zotero/storage/EBACR9EP/Life as we know it.pdf:application/pdf}
}

@article{karl-free-2012,
	title = {A {Free} {Energy} {Principle} for {Biological} {Systems}},
	volume = {14},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/14/11/2100/},
	doi = {10.3390/e14112100},
	language = {en},
	number = {12},
	urldate = {2016-01-22},
	journal = {Entropy},
	author = {Karl, Friston},
	month = oct,
	year = {2012},
	pages = {2100--2121},
	file = {A Free Energy Principle for Biological Systems - A Free Energy Principle for Biological Systems.pdf:/home/user/Zotero/storage/JFVHX4C8/A Free Energy Principle for Biological Systems.pdf:application/pdf}
}

@article{bastos-canonical-2012,
	title = {Canonical {Microcircuits} for {Predictive} {Coding}},
	volume = {76},
	issn = {08966273},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627312009592},
	doi = {10.1016/j.neuron.2012.10.038},
	language = {en},
	number = {4},
	urldate = {2016-01-22},
	journal = {Neuron},
	author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
	month = nov,
	year = {2012},
	pages = {695--711},
	file = {Canonical Microcircuits for Predictive Coding - Canonical Microcircuits for Predictive Coding.pdf:/home/user/Zotero/storage/FZRZ23NE/Canonical Microcircuits for Predictive Coding.pdf:application/pdf}
}

@article{brown-active-2013,
	title = {Active inference, sensory attenuation and illusions},
	volume = {14},
	issn = {1612-4782, 1612-4790},
	url = {http://link.springer.com/10.1007/s10339-013-0571-3},
	doi = {10.1007/s10339-013-0571-3},
	language = {en},
	number = {4},
	urldate = {2016-01-22},
	journal = {Cognitive Processing},
	author = {Brown, Harriet and Adams, Rick A. and Parees, Isabel and Edwards, Mark and Friston, Karl},
	month = nov,
	year = {2013},
	pages = {411--427},
	file = {Active inference sensory attenuation and illusions.pdf:/home/user/Zotero/storage/GM4B53RN/Active inference sensory attenuation and illusions.pdf:application/pdf}
}

@article{adams-computational-2013,
	title = {The {Computational} {Anatomy} of {Psychosis}},
	volume = {4},
	issn = {1664-0640},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyt.2013.00047/abstract},
	doi = {10.3389/fpsyt.2013.00047},
	urldate = {2016-01-22},
	journal = {Frontiers in Psychiatry},
	author = {Adams, Rick A. and Stephan, Klaas Enno and Brown, Harriet R. and Frith, Christopher D. and Friston, Karl J.},
	year = {2013},
	file = {The computational anatomy of psychosis - The computational anatomy of psychosis.pdf:/home/user/Zotero/storage/22HH2KBP/The computational anatomy of psychosis.pdf:application/pdf}
}

@article{adams-predictions-2013,
	title = {Predictions not commands: active inference in the motor system},
	volume = {218},
	issn = {1863-2653, 1863-2661},
	shorttitle = {Predictions not commands},
	url = {http://link.springer.com/10.1007/s00429-012-0475-5},
	doi = {10.1007/s00429-012-0475-5},
	language = {en},
	number = {3},
	urldate = {2016-01-22},
	journal = {Brain Structure and Function},
	author = {Adams, Rick A. and Shipp, Stewart and Friston, Karl J.},
	month = may,
	year = {2013},
	pages = {611--643},
	file = {Predictions not commands - active inference in the motor system.pdf:/home/user/Zotero/storage/DVTRMB56/Predictions not commands - active inference in the motor system.pdf:application/pdf}
}

@article{friston-perceptions-2012,
	title = {Perceptions as {Hypotheses}: {Saccades} as {Experiments}},
	volume = {3},
	issn = {1664-1078},
	shorttitle = {Perceptions as {Hypotheses}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00151/abstract},
	doi = {10.3389/fpsyg.2012.00151},
	urldate = {2016-01-22},
	journal = {Frontiers in Psychology},
	author = {Friston, Karl and Adams, Rick A. and Perrinet, Laurent and Breakspear, Michael},
	year = {2012},
	file = {Perceptions as hypotheses\: saccades as experiments - Perceptions as hypotheses saccades as experiments.pdf:/home/user/Zotero/storage/73EMT7GJ/Perceptions as hypotheses saccades as experiments.pdf:application/pdf}
}

@article{hobson-waking-2012,
	title = {Waking and dreaming consciousness: {Neurobiological} and functional considerations},
	volume = {98},
	issn = {03010082},
	shorttitle = {Waking and dreaming consciousness},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0301008212000706},
	doi = {10.1016/j.pneurobio.2012.05.003},
	language = {en},
	number = {1},
	urldate = {2016-01-22},
	journal = {Progress in Neurobiology},
	author = {Hobson, J.A. and Friston, K.J.},
	month = jul,
	year = {2012},
	pages = {82--98},
	file = {0c4c943f23689274da2b74e8d4ffeb70 - Waking and Dreaming Consciousness Neurobiological and Functional Considerations.pdf:/home/user/Zotero/storage/7IZQ2BW9/Waking and Dreaming Consciousness Neurobiological and Functional Considerations.pdf:application/pdf}
}

@article{brown-free-energy-2012,
	title = {Free-{Energy} and {Illusions}: {The} {Cornsweet} {Effect}},
	volume = {3},
	issn = {1664-1078},
	shorttitle = {Free-{Energy} and {Illusions}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00043/abstract},
	doi = {10.3389/fpsyg.2012.00043},
	urldate = {2016-01-22},
	journal = {Frontiers in Psychology},
	author = {Brown, Harriet and Friston, Karl J.},
	year = {2012},
	file = {Free-energy and illusions\: the Cornsweet effect - Free-energy and illusions the Cornsweet effect.pdf:/home/user/Zotero/storage/BA8E5X9T/Free-energy and illusions the Cornsweet effect.pdf:application/pdf}
}

@article{friston-dopamine-2012,
	title = {Dopamine, {Affordance} and {Active} {Inference}},
	volume = {8},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1002327},
	doi = {10.1371/journal.pcbi.1002327},
	language = {en},
	number = {1},
	urldate = {2016-01-22},
	journal = {PLoS Computational Biology},
	author = {Friston, Karl J. and Shiner, Tamara and FitzGerald, Thomas and Galea, Joseph M. and Adams, Rick and Brown, Harriet and Dolan, Raymond J. and Moran, Rosalyn and Stephan, Klaas Enno and Bestmann, Sven},
	editor = {Sporns, Olaf},
	month = jan,
	year = {2012},
	pages = {e1002327},
	file = {pcbi.1002327 1..20 - Dopamine Affordance and Active Inference.pdf:/home/user/Zotero/storage/DHFA5SEH/Dopamine Affordance and Active Inference.pdf:application/pdf}
}

@article{friston-reinforcement-2009,
	title = {Reinforcement {Learning} or {Active} {Inference}?},
	volume = {4},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0006421},
	doi = {10.1371/journal.pone.0006421},
	language = {en},
	number = {7},
	urldate = {2016-01-22},
	journal = {PLoS ONE},
	author = {Friston, Karl J. and Daunizeau, Jean and Kiebel, Stefan J.},
	editor = {Sporns, Olaf},
	month = jul,
	year = {2009},
	pages = {e6421},
	file = {pone.0006421 1..13 - Reinforcement Learning or Active Inference.pdf:/home/user/Zotero/storage/GTNGS8M8/Reinforcement Learning or Active Inference.pdf:application/pdf}
}

@article{friston-action-2010,
	title = {Action and behavior: a free-energy formulation},
	volume = {102},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Action and behavior},
	url = {http://link.springer.com/10.1007/s00422-010-0364-z},
	doi = {10.1007/s00422-010-0364-z},
	language = {en},
	number = {3},
	urldate = {2016-01-22},
	journal = {Biological Cybernetics},
	author = {Friston, Karl J. and Daunizeau, Jean and Kilner, James and Kiebel, Stefan J.},
	month = mar,
	year = {2010},
	pages = {227--260},
	file = {Action and behavior A free-energy formulation.pdf:/home/user/Zotero/storage/3IJICHQS/Action and behavior A free-energy formulation.pdf:application/pdf}
}

@book{paul-principles-1891,
	title = {Principles of the history of language},
	url = {http://archive.org/details/cu31924026442586},
	abstract = {The metadata below describe the original scanning. Follow the "All Files: HTTP" link in the "View the book" box to the left to find XML files that contain more metadata about the original images and the derived formats (OCR results, PDF etc.). See also the What is the directory structure for the texts? FAQ for information about file content and naming conventions.; Translation of Principien der Sprachgeschichte; Includes index; Spine title: Paul's principles of language},
	language = {eng},
	urldate = {2016-01-22},
	publisher = {London ; New York : Longmans, Green},
	author = {Paul, Hermann and Strong, Herbert Augustus},
	collaborator = {{Cornell University Library}},
	year = {1891},
	keywords = {historical linguistics},
	file = {Principles of the history of language - cu31924026442586.pdf:/home/user/Zotero/storage/86M7QKUQ/cu31924026442586.pdf:application/pdf}
}

@inproceedings{arora-provable-2014,
	title = {Provable {Bounds} for {Learning} {Some} {Deep} {Representations} - arora14.pdf},
	url = {http://jmlr.org/proceedings/papers/v32/arora14.pdf},
	urldate = {2016-01-20},
	author = {Arora, Sanjeev},
	year = {2014},
	file = {Provable Bounds for Learning Some Deep Representations - arora14.pdf:/home/user/Zotero/storage/AZQ4AJ3B/arora14.pdf:application/pdf}
}

@article{robinson-counterexamples-1975,
	title = {Some {Counterexamples} to the {Theory} of {Confidence} {Intervals}},
	volume = {62},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2334498},
	doi = {10.2307/2334498},
	abstract = {Some families of distributions are presented for which certain Neyman confidence intervals have very poor conditional properties. In each case there is a 50\% Neyman confidence interval I(x) for a parameter θ and a subset A of the sample space such that the conditional probability that I(X) covers θ given that X belongs to A is less than 0.2 for all θ and the conditional probability that I(X) covers θ given that X is not in A is at least 0.8 for all θ. The families of distributions are somewhat alike. More than one example is presented in order to show that the theory of confidence intervals cannot easily step around the difficulties presented to it.},
	number = {1},
	urldate = {2016-01-22},
	journal = {Biometrika},
	author = {Robinson, G. K.},
	year = {1975},
	pages = {155--161},
	file = {JSTOR Full Text PDF:/home/user/Zotero/storage/GC2VT595/Robinson - 1975 - Some Counterexamples to the Theory of Confidence I.pdf:application/pdf}
}

@article{freedman-asymptotic-1963,
	title = {On the asymptotic behavior of {Bayes}' estimates in the discrete case},
	url = {http://www.jstor.org/stable/2238346},
	urldate = {2016-01-22},
	journal = {The Annals of Mathematical Statistics},
	author = {Freedman, David A.},
	year = {1963},
	pages = {1386--1403},
	file = {2238346.pdf:/home/user/Zotero/storage/727NWMMD/2238346.pdf:application/pdf}
}

@book{galecki-linear-2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Linear {Mixed}-{Effects} {Models} {Using} {R}},
	isbn = {978-1-4614-3899-1 978-1-4614-3900-4},
	url = {http://link.springer.com/10.1007/978-1-4614-3900-4},
	urldate = {2016-01-20},
	publisher = {Springer New York},
	author = {Gałecki, Andrzej and Burzykowski, Tomasz},
	year = {2013},
	file = {bok%3A978-1-4614-3900-4.pdf:/home/user/Zotero/storage/KK74AAJ6/bok%3A978-1-4614-3900-4.pdf:application/pdf}
}

@article{chung-gated-2015,
	title = {Gated {Feedback} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.02367},
	abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
	urldate = {2016-01-20},
	journal = {arXiv:1502.02367 [cs, stat]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.02367},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1502.02367 PDF:/home/user/Zotero/storage/QPARTUCV/Chung et al. - 2015 - Gated Feedback Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/BJWZF9UC/1502.html:text/html}
}

@article{koutnik-clockwork-2014,
	title = {A {Clockwork} {RNN}},
	url = {http://arxiv.org/abs/1402.3511},
	abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
	urldate = {2016-01-20},
	journal = {arXiv:1402.3511 [cs]},
	author = {Koutník, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Jürgen},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.3511},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1402.3511 PDF:/home/user/Zotero/storage/VEDPSVQJ/Koutník et al. - 2014 - A Clockwork RNN.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/EGBBRKKB/1402.html:text/html}
}

@incollection{bott-verbs-2014,
	title = {From verbs to discourse: {A} novel account of implicit causality},
	shorttitle = {From verbs to discourse},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-05675-3\_9},
	urldate = {2016-01-20},
	booktitle = {Psycholinguistic approaches to meaning and understanding across languages},
	publisher = {Springer},
	author = {Bott, Oliver and Solstad, Torgrim},
	year = {2014},
	pages = {213--251},
	file = {index.php:/home/user/Zotero/storage/5VBZDTNJ/index.pdf:application/pdf}
}

@incollection{bott-cross-linguistic-2014,
	title = {Cross-linguistic variation in the processing of aspect},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-05675-3\_4},
	urldate = {2016-01-20},
	booktitle = {Psycholinguistic approaches to meaning and understanding across languages},
	publisher = {Springer},
	author = {Bott, Oliver and Hamm, Fritz},
	year = {2014},
	pages = {83--109},
	file = {aspect_bott_hamm_final_with_corrections.dvi - index.php:/home/user/Zotero/storage/5JBEDC79/index.pdf:application/pdf}
}

@article{poschmann-relative-2015,
	title = {Relative clause extraposition and prosody in {German}},
	url = {http://link.springer.com/article/10.1007/s11049-015-9314-8},
	urldate = {2016-01-20},
	journal = {Natural Language \& Linguistic Theory},
	author = {Poschmann, Claudia and Wagner, Michael},
	year = {2015},
	pages = {1--46},
	file = {poschmannwagner2015.pdf:/home/user/Zotero/storage/QKFNTAFF/poschmannwagner2015.pdf:application/pdf}
}

@article{hamlaoui-acoustic-2015,
	title = {Acoustic correlates of focus marking in {Polish}},
	url = {http://www.zas.gwz-berlin.de/fileadmin/mitarbeiter/hamlaoui/PolishFocus\_ICPHS.pdf},
	urldate = {2016-01-20},
	journal = {proceedings of the 18th ICPhS},
	author = {Hamlaoui, Fatima and Zygis, Marzena and Engelmann, Jonas and Wagner, Michael},
	year = {2015},
	file = {Hamlaoui_etal_2015_polish.pdf:/home/user/Zotero/storage/B2KZQ7NP/Hamlaoui_etal_2015_polish.pdf:application/pdf}
}

@article{tanner-production-nodate,
	title = {{PRODUCTION} {PLANNING} {AND} {CORONAL} {STOP} {DELETION} {IN} {SPONTANEOUS} {SPEECH}},
	url = {http://www.researchgate.net/profile/Morgan\_Sonderegger/publication/275963130\_Production\_planning\_and\_coronal\_stop\_deletion\_in\_spontaneous\_speech/links/554bce050cf29752ee7ebb2f.pdf},
	urldate = {2016-01-20},
	author = {Tanner, James and Sonderegger, Morgan and Wagner, Michael},
	file = {tannerEtAl2015Icphs.pdf:/home/user/Zotero/storage/6MAZ98JB/tannerEtAl2015Icphs.pdf:application/pdf}
}

@article{wagner-accessibility-2015,
	title = {Accessibility is no alternative to alternatives},
	volume = {30},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2014.959532},
	number = {1-2},
	urldate = {2016-01-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Wagner, Michael and Klassen, Jeffrey},
	year = {2015},
	pages = {212--233},
	file = {wagnerklassen14alternatives.pdf:/home/user/Zotero/storage/W6MTUU5N/wagnerklassen14alternatives.pdf:application/pdf}
}

@article{mcclay-accented-nodate,
	title = {Accented {Pronouns} and {Contrast}},
	url = {http://semanticsarchive.net/Archive/2I2ODdhM/mcclaywagner2014cls.pdf},
	urldate = {2016-01-20},
	author = {McClay, Elise and Wagner, Michael},
	file = {mcclaywagner2014cls.pdf:/home/user/Zotero/storage/9RNGJNE6/mcclaywagner2014cls.pdf:application/pdf}
}

@article{wagner-incomplete-nodate,
	title = {Incomplete {Answers} and the {Rise}-{Fall}-{Rise} {Contour}},
	url = {http://www.illc.uva.nl/semdial/dialdam/papers/WagnerEtal\_dialdam.pdf},
	urldate = {2016-01-20},
	author = {Wagner, Michael and McClay, Elise and Mak, Lauren},
	file = {WagnerEtal_dialdam.pdf:/home/user/Zotero/storage/SU4CX37R/WagnerEtal_dialdam.pdf:application/pdf}
}

@article{alain-what-2012,
	title = {What {Regularized} {Auto}-{Encoders} {Learn} from the {Data} {Generating} {Distribution}},
	url = {http://arxiv.org/abs/1211.4246},
	abstract = {What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
	urldate = {2016-01-20},
	journal = {arXiv:1211.4246 [cs, stat]},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.4246},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1211.4246 PDF:/home/user/Zotero/storage/4KFM5UFW/Alain and Bengio - 2012 - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/XA3AAJ5I/1211.html:text/html}
}

@article{choromanska-loss-2014,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	url = {http://arxiv.org/abs/1412.0233},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	urldate = {2016-01-20},
	journal = {arXiv:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
	month = nov,
	year = {2014},
	note = {arXiv: 1412.0233},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1412.0233 PDF:/home/user/Zotero/storage/V5XM3448/Choromanska et al. - 2014 - The Loss Surfaces of Multilayer Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GZAVH6N9/1412.html:text/html}
}

@article{szegedy-intriguing-2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2016-01-20},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KSVEEA8S/1312.html:text/html;Szegedy et al_2013_Intriguing properties of neural networks.pdf:/home/user/Zotero/storage/5VI3SPVM/Szegedy et al_2013_Intriguing properties of neural networks.pdf:application/pdf}
}

@article{melnattur-learning-2015-1,
	title = {Learning and {Memory}: {Do} {Bees} {Dream}?},
	volume = {25},
	issn = {0960-9822},
	shorttitle = {Learning and {Memory}},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982215010805},
	doi = {10.1016/j.cub.2015.09.001},
	abstract = {Summary
In mammals, evidence for memory reactivation during sleep highlighted the important role that sleep plays in memory consolidation. A new study reports that memory reactivation is evolutionarily conserved and can also be found in the honeybee.},
	number = {21},
	urldate = {2016-01-20},
	journal = {Current Biology},
	author = {Melnattur, Krishna and Dissel, Stephane and Shaw, Paul J.},
	month = nov,
	year = {2015},
	pages = {R1040--R1041},
	file = {Melnattur et al_2015_Learning and Memory.pdf:/home/user/Zotero/storage/SGC55B78/Melnattur et al_2015_Learning and Memory.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/WI2RJGNF/S0960982215010805.html:text/html}
}

@article{bentz-languages-2013,
	title = {Languages with more second language learners tend to lose nominal case},
	volume = {3},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-13030105},
	number = {1},
	urldate = {2016-01-20},
	journal = {Language Dynamics and Change},
	author = {Bentz, Christian and Winter, Bodo},
	year = {2013},
	pages = {1--27},
	file = {bentz_winter_L2_case.pdf:/home/user/Zotero/storage/3596PBXN/bentz_winter_L2_case.pdf:application/pdf}
}

@article{magri-how-2015,
	title = {How to keep the {HG} weights non-negative: the truncated {Perceptron} reweighing rule},
	volume = {3},
	shorttitle = {How to keep the {HG} weights non-negative},
	url = {http://jlm.ipipan.waw.pl/ojs/index.php/JLM/article/view/115},
	number = {2},
	urldate = {2016-01-15},
	journal = {Journal of Language Modelling},
	author = {Magri, Giorgio},
	year = {2015},
	pages = {345--375},
	file = {108.pdf:/home/user/Zotero/storage/BXJ7UUSF/108.pdf:application/pdf}
}

@article{dalrymple-economy-2015,
	title = {Economy of expression as a principle of syntax},
	url = {http://ora.ox.ac.uk/objects/uuid:cad317f8-5067-492c-be18-9f8e5a05c67e},
	urldate = {2016-01-15},
	journal = {Journal of Language Modelling},
	author = {Dalrymple, M. and Kaplan, R. M. and King, T. H.},
	year = {2015},
	file = {110.pdf:/home/user/Zotero/storage/U3KT3MQX/110.pdf:application/pdf}
}

@article{hammarstrom-quantifying-2014,
	title = {Quantifying {Geographical} {Determinants} of {Large}-{Scale} {Distributions} of {Linguistic} {Features}},
	volume = {4},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00401002},
	doi = {10.1163/22105832-00401002},
	abstract = {In the recent past the work on large-scale linguistic distributions across the globe has intensified considerably. Work on macro-areal relationships in Africa (Güldemann, 2010) suggests that the shape of convergence areas may be determined by climatic factors and geophysical features such as mountains, water bodies, coastlines, etc. Worldwide data is now available for geophysical features as well as linguistic features, including numeral systems and basic constituent order. We explore the possibility that the shape of areal aggregations of individual features in these two linguistic domains correlates with Köppen-Geiger climate zones. Furthermore, we test the hypothesis that the shape of such areal feature aggregations is determined by the contour of adjacent geophysical features like mountain ranges or coastlines. In these first basic tests, we do not find clear evidence that either Köppen-Geiger climate zones or the contours of geophysical features are major predictors for the linguistic data at hand.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Hammarström, Harald and Güldemann, Tom},
	month = jan,
	year = {2014},
	keywords = {word order, geographical influence on language, linguistic areas, numeral systems},
	pages = {87--115},
	file = {Hammarström_Güldemann_2014_Quantifying Geographical Determinants of Large-Scale Distributions of.pdf:/home/user/Zotero/storage/BIEAEPPM/Hammarström_Güldemann_2014_Quantifying Geographical Determinants of Large-Scale Distributions of.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/H86P7957/22105832-00401002.html:text/html}
}

@article{michael-exploring-2014,
	title = {Exploring {Phonological} {Areality} in the {Circum}-{Andean} {Region} {Using} a {Naive} {Bayes} {Classifier}},
	volume = {4},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00401004},
	doi = {10.1163/22105832-00401004},
	abstract = {This paper describes the Core and Periphery technique: a quantitative method for exploring areality that uses a naive Bayes classifier, a statistical tool for inferring class membership based on training sets assembled from members of the classes in question. The Core and Periphery technique is applied to the exploration of phonological areality in the Andes and surrounding lowland regions, based on the South American Phonological Inventory Database (SAPhon 1.1.3; Michael et al., 2013). Evidence is found for a phonological area centering on the Andean highlands, and extending to parts of the northern and central Andean foothills regions, the Chaco, and Patagonia. Evidence is also found for Southern and North-Central phonological sub-areas within this larger phonological area.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Michael, Lev and Chang, Will and Stark, Tammy},
	month = jan,
	year = {2014},
	keywords = {Andean languages, linguistic areality, naive Bayes classifier, South American languages},
	pages = {27--86},
	file = {Michael et al_2014_Exploring Phonological Areality in the Circum-Andean Region Using a Naive Bayes.pdf:/home/user/Zotero/storage/CNI3ZWPI/Michael et al_2014_Exploring Phonological Areality in the Circum-Andean Region Using a Naive Bayes.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/SZF69AXZ/22105832-00401004.html:text/html}
}

@article{chang-relaxed-2014,
	title = {A {Relaxed} {Admixture} {Model} of {Language} {Contact}},
	volume = {4},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00401005},
	doi = {10.1163/22105832-00401005},
	abstract = {Under conditions of language contact, a language may gain features from its neighbors that it is unlikely to have gained endogenously. We describe a method for evaluating pairs of languages for potential contact by comparing a null hypothesis, in which a target language obtained all its features by inheritance, with an alternative hypothesis in which the target language obtained its features via inheritance and via contact with a proposed donor language. Under the alternative hypothesis, the donor may influence the target to gain features, but not to lose features. When applied to a database of phonological characters in South American languages, this method proves useful for detecting the effects of relatively mild and recent contact, and for highlighting several potential linguistic areas in South America.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Chang, Will and Michael, Lev},
	month = jan,
	year = {2014},
	keywords = {linguistic areality, language contact, phonological inventory, probabilistic generative model, South America, Upper Xingú},
	pages = {1--26},
	file = {Chang_Michael_2014_A Relaxed Admixture Model of Language Contact.pdf:/home/user/Zotero/storage/IZ8GMRE3/Chang_Michael_2014_A Relaxed Admixture Model of Language Contact.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/WQBN2TRJ/22105832-00401005.html:text/html}
}

@article{ross-affixes-2014,
	title = {Affixes in {Language} {Change}},
	volume = {4},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00402002},
	doi = {10.1163/22105832-00402002},
	number = {2},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Ross, Malcolm},
	month = jan,
	year = {2014},
	pages = {271--284},
	file = {Ross_2014_Affixes in Language Change.pdf:/home/user/Zotero/storage/INU3ARQG/Ross_2014_Affixes in Language Change.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/JXIFVQ5J/22105832-00402002.html:text/html}
}

@article{list-using-2014,
	title = {Using {Phylogenetic} {Networks} to {Model} {Chinese} {Dialect} {History}},
	volume = {4},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00402008},
	doi = {10.1163/22105832-00402008},
	abstract = {The idea that language history is best visualized by a branching tree has been controversially discussed in the linguistic world and many alternative theories have been proposed. The reluctance of many scholars to accept the tree as the natural metaphor for language history was due to conflicting signals in linguistic data: many resemblances would simply not point to a unique tree. Despite these observations, the majority of automatic approaches applied to language data has been based on the tree model, while network approaches have rarely been applied. Due to the specific sociolinguistic situation in China, where very divergent varieties have been developing under the roof of a common culture and writing system, the history of the Chinese dialects is complex and intertwined. They are therefore a good test case for methods which no longer take the family tree as their primary model. Here we use a network approach to study the lexical history of 40 Chinese dialects. In contrast to previous approaches, our method is character-based and captures both vertical and horizontal aspects of language history. According to our results, the majority of characters in our data (about 54\%) cannot be readily explained with the help of a given tree model. The borrowing events inferred by our method do not only reflect general uncertainties of Chinese dialect classification, they also reveal the strong influence of the standard language on Chinese dialect history.},
	number = {2},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {List, Johann-Mattis and Shijulal, Nelson-Sathi and Martin, William and Geisler, Hans},
	month = jan,
	year = {2014},
	keywords = {Chinese languages, Chinese linguistics, lexical borrowing, phylogenetic networks, tree model},
	pages = {222--252},
	file = {List et al_2014_Using Phylogenetic Networks to Model Chinese Dialect History.pdf:/home/user/Zotero/storage/M9ENG6MD/List et al_2014_Using Phylogenetic Networks to Model Chinese Dialect History.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/F38HFCC8/22105832-00402008.html:text/html}
}

@article{alday-be-2015,
	title = {Be {Careful} {When} {Assuming} the {Obvious}},
	volume = {5},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00501008},
	doi = {10.1163/22105832-00501008},
	abstract = {Ferrer-i-Cancho (this volume) presents a mathematical model of both the synchronic and diachronic nature of word order based on the assumption that memory costs are a never decreasing function of distance and a few very general linguistic assumptions. However, even these minimal and seemingly obvious assumptions are not as safe as they appear in light of recent typological and psycholinguistic evidence. The interaction of word order and memory has further depths to be explored.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Alday, Phillip M.},
	month = jan,
	year = {2015},
	keywords = {word order, head placement, language dynamics, memory and language},
	pages = {138--146},
	file = {Alday_2015_Be Careful When Assuming the Obvious.pdf:/home/user/Zotero/storage/ARVWTKXI/Alday_2015_Be Careful When Assuming the Obvious.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/T2HV6VC9/22105832-00501008.html:text/html}
}

@article{ferrer-i-cancho-placement-2015,
	title = {The {Placement} of the {Head} that {Minimizes} {Online} {Memory}},
	volume = {5},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00501007},
	doi = {10.1163/22105832-00501007},
	abstract = {It is well known that the length of a syntactic dependency determines its online memory cost. Thus, the problem of the placement of a head and its dependents (complements or modifiers) that minimizes online memory is equivalent to the problem of the minimum linear arrangement of a star tree. However, how that length is translated into cognitive cost is not known. This study shows that the online memory cost is minimized when the head is placed at the center, regardless of the function that transforms length into cost, provided only that this function is strictly monotonically increasing. Online memory defines a quasi-convex adaptive landscape with a single central minimum if the number of elements is odd and two central minima if that number is even. We discuss various aspects of the dynamics of word order of subject (S), verb (V) and object (O) from a complex systems perspective and suggest that word orders tend to evolve by swapping adjacent constituents from an initial or early SOV configuration that is attracted towards a central word order by online memory minimization. We also suggest that the stability of SVO is due to at least two factors, the quasi-convex shape of the adaptive landscape in the online memory dimension and online memory adaptations that avoid regression to SOV. Although OVS is also optimal for placing the verb at the center, its low frequency is explained by its long distance to the seminal SOV in the permutation space.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Ferrer-i-Cancho, Ramon},
	month = jan,
	year = {2015},
	keywords = {word order, Language evolution, head placement, language dynamics, adaptive landscape, neutrality},
	pages = {114--137},
	file = {Ferrer-i-Cancho_2015_The Placement of the Head that Minimizes Online Memory.pdf:/home/user/Zotero/storage/9JAK495H/Ferrer-i-Cancho_2015_The Placement of the Head that Minimizes Online Memory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IEWRXXQ3/22105832-00501007.html:text/html}
}

@article{jansson-modeling-2015,
	title = {Modeling the {Evolution} of {Creoles}},
	volume = {5},
	issn = {2210-5832},
	url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22105832-00501005},
	doi = {10.1163/22105832-00501005},
	abstract = {Various theories have been proposed regarding the origin of creole languages. Describing a process where only the end result is documented involves several methodological difficulties. In this paper we try to address some of the issues by using a novel mathematical model together with detailed empirical data on the origin and structure of Mauritian Creole. Our main focus is on whether Mauritian Creole may have originated only from a mutual desire to communicate, without a target language or prestige bias. Our conclusions are affirmative. With a confirmation bias towards learning from successful communication, the model predicts Mauritian Creole better than any of the input languages, including the lexifier French, thus providing a compelling and specific hypothetical model of how creoles emerge. The results also show that it may be possible for a creole to develop quickly after first contact, and that it was created mostly from material found in the input languages, but without inheriting their morphology.},
	number = {1},
	urldate = {2016-01-14},
	journal = {Language Dynamics and Change},
	author = {Jansson, Fredrik and Parkvall, Mikael and Strimling, Pontus},
	month = jan,
	year = {2015},
	keywords = {mathematical modeling, cultural evolution, Creoles, pidgins, simulation},
	pages = {1--51},
	file = {Jansson et al_2015_Modeling the Evolution of Creoles.pdf:/home/user/Zotero/storage/4JZVVX2W/Jansson et al_2015_Modeling the Evolution of Creoles.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EGN9SUAU/22105832-00501005.html:text/html}
}

@article{schumacher-backward--2015,
	title = {Backward- and {Forward}-{Looking} {Potential} of {Anaphors}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01746/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w3-2016},
	doi = {10.3389/fpsyg.2015.01746},
	abstract = {Personal pronouns and demonstratives contribute differently to the encoding of information in the mental model and they serve distinct backward- and forward-looking functions. While (unstressed) personal pronouns are the default means to indicate coreference with the most prominent discourse entity (backward-looking function) and typically mark the maintenance of the current topic, demonstratives are used to refer to a less prominent entity and serve the additional forward-looking function of signaling a possible topic shift. In Experiment 1, we present an ERP study that examines the time course of processing personal and d-pronouns in German (er vs. der) and assesses the impact of two prominence features of the antecedent, thematic role and sentential position, as well as neurophysiological correlates of backward- and forward-looking functions of referential expressions. We tested the comprehension of personal and d-pronouns following context sentences containing two potential antecedents. In addition to the factor pronoun type (er vs. der), we varied the verb type (active accusative verbs vs. dative experiencer verbs) and the thematic role order (canonical vs. non-canonical) in the context sentences to vary the antecedent's prominence. Time-locked to pronoun-onset, the ERPs revealed a general biphasic N400-Late Positivity for d-pronouns over personal pronouns with further subtle interactions of the prominence-lending cues in the early time window. The findings indicate that the calculation of the referential candidates' prominence (backward-looking function) is guided by thematic role and positional information. Thematic role information, in combination with initial position, thus represents a central predictor during referential processing. Coreference with a less prominent entity (assumed for d-pronouns) results in processing costs (N400). The additional topic shift signaled by d-pronouns (forward-looking function) results in attentional reorienting (Late Positivity). This is further supported by Experiment 2, a story continuation study, which showed that personal pronouns trigger topic maintenance, while d-pronouns yield topic shifts.},
	urldate = {2016-01-14},
	journal = {Language Sciences},
	author = {Schumacher, Petra B. and Backhaus, Jana and Dangl, Manuel},
	year = {2015},
	keywords = {pronoun resolution, N400, agentivity, ERP, Late Positivity, position, prominence, topic shift},
	pages = {1746},
	file = {Schumacher et al_2015_Backward- and Forward-Looking Potential of Anaphors.pdf:/home/user/Zotero/storage/XQAGEB3H/Schumacher et al_2015_Backward- and Forward-Looking Potential of Anaphors.pdf:application/pdf}
}

@article{zaccarella-merge-2015,
	title = {Merge in the {Human} {Brain}: {A} {Sub}-{Region} {Based} {Functional} {Investigation} in the {Left} {Pars} {Opercularis}},
	shorttitle = {Merge in the {Human} {Brain}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01818/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w3-2016},
	doi = {10.3389/fpsyg.2015.01818},
	abstract = {Language is thought to represent one of the most complex cognitive functions in humans. Here we break down complexity of language to its most basic syntactic computation which hierarchically binds single words together to form larger phrases and sentences. So far, the neural implementation of this basic operation has only been inferred indirectly from studies investigating more complex linguistic phenomena. In the present sub-region based functional magnetic resonance imaging (fMRI) study we directly assessed the neuroanatomical nature of this process. Our results showed that syntactic phrases—compared to word-list sequences—corresponded to increased neural activity in the ventral-anterior portion of the left pars opercularis [Brodmann Area (BA) 44], whereas the adjacently located deep frontal operculum/anterior insula (FOP/aINS), a phylogenetically older and less specialized region, was found to be equally active for both conditions. Crucially, the functional activity of syntactic binding was confined to one out of five clusters proposed by a recent fine-grained sub-anatomical parcellation for BA 44, with consistency across individuals. Neuroanatomically, the present results call for a redefinition of BA 44 as a region with internal functional specializations. Neurocomputationally, they support the idea of invariance within BA 44 in the location of activation across participants for basic syntactic building processing.},
	urldate = {2016-01-14},
	journal = {Language Sciences},
	author = {Zaccarella, Emiliano and Friederici, Angela D.},
	year = {2015},
	keywords = {Syntax, fMRI, clusters, merge, pars opercularis},
	pages = {1818},
	file = {Zaccarella_Friederici_2015_Merge in the Human Brain.pdf:/home/user/Zotero/storage/A7GA57WV/Zaccarella_Friederici_2015_Merge in the Human Brain.pdf:application/pdf}
}

@article{jung-rhythmic-2015,
	title = {Rhythmic {Effects} of {Syntax} {Processing} in {Music} and {Language}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01762/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w3-2016},
	doi = {10.3389/fpsyg.2015.01762},
	abstract = {Music and language are human cognitive and neural functions that share many structural similarities. Past theories posit a sharing of neural resources between syntax processing in music and language (Patel, 2003), and a dynamic attention network that governs general temporal processing (Large and Jones, 1999). Both make predictions about music and language processing over time. Experiment 1 of this study investigates the relationship between rhythmic expectancy and musical and linguistic syntax in a reading time paradigm. Stimuli (adapted from Slevc et al., 2009) were sentences broken down into segments; each sentence segment was paired with a musical chord and presented at a fixed inter-onset interval. Linguistic syntax violations appeared in a garden-path design. During the critical region of the garden-path sentence, i.e., the particular segment in which the syntactic unexpectedness was processed, expectancy violations for language, music, and rhythm were each independently manipulated: musical expectation was manipulated by presenting out-of-key chords and rhythmic expectancy was manipulated by perturbing the fixed inter-onset interval such that the sentence segments and musical chords appeared either early or late. Reading times were recorded for each sentence segment and compared for linguistic, musical, and rhythmic expectancy. Results showed main effects of rhythmic expectancy and linguistic syntax expectancy on reading time. There was also an effect of rhythm on the interaction between musical and linguistic syntax: effects of violations in musical and linguistic syntax showed significant interaction only during rhythmically expected trials. To test the effects of our experimental design on rhythmic and linguistic expectancies, independently of musical syntax, Experiment 2 used the same experimental paradigm, but the musical factor was eliminated—linguistic stimuli were simply presented silently, and rhythmic expectancy was manipulated at the critical region. Experiment 2 replicated effects of rhythm and language, without an interaction. Together, results suggest that the interaction of music and language syntax processing depends on rhythmic expectancy, and support a merging of theories of music and language syntax processing with dynamic models of attentional entrainment.},
	urldate = {2016-01-14},
	journal = {Auditory Cognitive Neuroscience},
	author = {Jung, Harim and Sontag, Samuel and Park, YeBin S. and Loui, Psyche},
	year = {2015},
	keywords = {language, Syntax, expectancy, harmony, music, rhythm},
	pages = {1762},
	file = {Jung et al_2015_Rhythmic Effects of Syntax Processing in Music and Language.pdf:/home/user/Zotero/storage/25XQFKB5/Jung et al_2015_Rhythmic Effects of Syntax Processing in Music and Language.pdf:application/pdf}
}

@article{feng-implicit-2016,
	title = {Implicit {Distortion} and {Fertility} {Models} for {Attention}-based {Encoder}-{Decoder} {NMT} {Model}},
	url = {http://arxiv.org/abs/1601.03317},
	abstract = {Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder frame- work. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly dam- aged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoder- decoder and compare them with other models on machine translation. Our pro- posed method achieved an improvement of 2 BLEU points over the original attention- based encoder-decoder.},
	urldate = {2016-01-14},
	journal = {arXiv:1601.03317 [cs]},
	author = {Feng, Shi and Liu, Shujie and Li, Mu and Zhou, Ming},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.03317},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CFZ8MZUC/1601.html:text/html;Feng et al_2016_Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder.pdf:/home/user/Zotero/storage/I43S33FR/Feng et al_2016_Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder.pdf:application/pdf}
}

@article{gomez-rodriguez-scarcity-2016,
	title = {The scarcity of crossing dependencies: a direct outcome of a specific constraint?},
	shorttitle = {The scarcity of crossing dependencies},
	url = {http://arxiv.org/abs/1601.03210},
	abstract = {Crossing syntactic dependencies have been observed to be infrequent in natural language, to the point that some syntactic theories and formalisms disregard them entirely. This leads to the question of whether the scarcity of crossings in languages arises from an independent and specific constraint on crossings. We provide statistical evidence suggesting that this is not the case, as the proportion of dependency crossings in a wide range of natural language treebanks can be accurately estimated by a simple predictor based on the local probability that two dependencies cross given their lengths. The relative error of this predictor never exceeds 5\% on average, whereas a baseline predictor assuming a random ordering of the words of a sentence incurs a relative error that is at least 6 times greater. Our results suggest that the low frequency of crossings in natural languages is neither originated by hidden knowledge of language nor by the undesirability of crossings per se, but as a mere side effect of the principle of dependency length minimization.},
	urldate = {2016-01-14},
	journal = {arXiv:1601.03210 [physics]},
	author = {Gómez-Rodríguez, Carlos and Ferrer-i-Cancho, Ramon},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.03210},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UNJCHPRN/1601.html:text/html;Gómez-Rodríguez_Ferrer-i-Cancho_2016_The scarcity of crossing dependencies.pdf:/home/user/Zotero/storage/B5WBFXZ6/Gómez-Rodríguez_Ferrer-i-Cancho_2016_The scarcity of crossing dependencies.pdf:application/pdf}
}

@article{sedghi-provable-2014,
	title = {Provable {Tensor} {Methods} for {Learning} {Mixtures} of {Generalized} {Linear} {Models}},
	url = {http://arxiv.org/abs/1412.3046},
	abstract = {We consider the problem of learning mixtures of generalized linear models (GLM) which arise in classification and regression problems. Typical learning approaches such as expectation maximization (EM) or variational Bayes can get stuck in spurious local optima. In contrast, we present a tensor decomposition method which is guaranteed to correctly recover the parameters. The key insight is to employ certain feature transformations of the input, which depend on the input generative model. Specifically, we employ score function tensors of the input and compute their cross-correlation with the response variable. We establish that the decomposition of this tensor consistently recovers the parameters, under mild non-degeneracy conditions. We demonstrate that the computational and sample complexity of our method is a low order polynomial of the input and the latent dimensions.},
	urldate = {2016-01-14},
	journal = {arXiv:1412.3046 [cs, stat]},
	author = {Sedghi, Hanie and Janzamin, Majid and Anandkumar, Anima},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3046},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/EZUHI9NQ/1412.html:text/html;Sedghi et al_2014_Provable Tensor Methods for Learning Mixtures of Generalized Linear Models.pdf:/home/user/Zotero/storage/ZQR3P8PV/Sedghi et al_2014_Provable Tensor Methods for Learning Mixtures of Generalized Linear Models.pdf:application/pdf}
}

@article{peever-neuroscience:-2016,
	title = {Neuroscience: {A} {Distributed} {Neural} {Network} {Controls} {REM} {Sleep}},
	volume = {26},
	issn = {0960-9822},
	shorttitle = {Neuroscience},
	url = {http://www.cell.com/article/S096098221501372X/abstract},
	doi = {10.1016/j.cub.2015.11.011},
	abstract = {How does the brain control dreams? New science shows that a small node of cells in the medulla — the most primitive part of the brain — may function to control REM sleep, the brain state that underlies dreaming.},
	language = {English},
	number = {1},
	urldate = {2016-01-14},
	journal = {Current Biology},
	author = {Peever, John and Fuller, Patrick M.},
	month = nov,
	year = {2016},
	pages = {R34--R35},
	file = {Peever_Fuller_2016_Neuroscience.pdf:/home/user/Zotero/storage/H3CPTHND/Peever_Fuller_2016_Neuroscience.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HQHZ88SS/S0960-9822(15)01372-X.html:text/html}
}

@article{gardner-deep-2015,
	title = {Deep {Manifold} {Traversal}: {Changing} {Labels} with {Convolutional} {Features}},
	shorttitle = {Deep {Manifold} {Traversal}},
	url = {http://arxiv.org/abs/1511.06421},
	abstract = {Machine learning is increasingly used in high impact applications such as prediction of hospital re-admission, cancer screening or bio-medical research applications. As predictions become increasingly accurate, practitioners may be interested in identifying actionable changes to inputs in order to alter their class membership. For example, a doctor might want to know what changes to a patient's status would predict him/her to not be re-admitted to the hospital soon. Szegedy et al. (2013b) demonstrated that identifying such changes can be very hard in image classification tasks. In fact, tiny, imperceptible changes can result in completely different predictions without any change to the true class label of the input. In this paper we ask the question if we can make small but meaningful changes in order to truly alter the class membership of images from a source class to a target class. To this end we propose deep manifold traversal, a method that learns the manifold of natural images and provides an effective mechanism to move images from one area (dominated by the source class) to another (dominated by the target class).The resulting algorithm is surprisingly effective and versatile. It allows unrestricted movements along the image manifold and only requires few images from source and target to identify meaningful changes. We demonstrate that the exact same procedure can be used to change an individual's appearance of age, facial expressions or even recolor black and white images.},
	urldate = {2016-01-14},
	journal = {arXiv:1511.06421 [cs, stat]},
	author = {Gardner, Jacob R. and Kusner, Matt J. and Li, Yixuan and Upchurch, Paul and Weinberger, Kilian Q. and Hopcroft, John E.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06421},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TD3KK2DJ/1511.html:text/html;Gardner et al_2015_Deep Manifold Traversal.pdf:/home/user/Zotero/storage/UEV3SDMV/Gardner et al_2015_Deep Manifold Traversal.pdf:application/pdf}
}

@article{janzamin-beating-2015,
	title = {Beating the {Perils} of {Non}-{Convexity}: {Guaranteed} {Training} of {Neural} {Networks} using {Tensor} {Methods}},
	shorttitle = {Beating the {Perils} of {Non}-{Convexity}},
	url = {http://arxiv.org/abs/1506.08473},
	abstract = {Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for learnability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we propose a computationally efficient method with guaranteed risk bounds for training neural networks with one hidden layer.},
	urldate = {2016-01-14},
	journal = {arXiv:1506.08473 [cs, stat]},
	author = {Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08473},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/DBE7JD7Q/1506.html:text/html;Janzamin et al_2015_Beating the Perils of Non-Convexity.pdf:/home/user/Zotero/storage/XPWZ96DS/Janzamin et al_2015_Beating the Perils of Non-Convexity.pdf:application/pdf}
}

@article{giryes-deep-2015,
	title = {Deep {Neural} {Networks} with {Random} {Gaussian} {Weights}: {A} {Universal} {Classification} {Strategy}?},
	shorttitle = {Deep {Neural} {Networks} with {Random} {Gaussian} {Weights}},
	url = {http://arxiv.org/abs/1504.08291},
	abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
	urldate = {2016-01-14},
	journal = {arXiv:1504.08291 [cs, stat]},
	author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.08291},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, I.5.1, 62M45},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/G5AMEEUE/1504.html:text/html;Giryes et al_2015_Deep Neural Networks with Random Gaussian Weights.pdf:/home/user/Zotero/storage/IEZIZ583/Giryes et al_2015_Deep Neural Networks with Random Gaussian Weights.pdf:application/pdf}
}

@article{hosseini-asl-deep-2015,
	title = {Deep {Learning} of {Part}-based {Representation} of {Data} {Using} {Sparse} {Autoencoders} with {Nonnegativity} {Constraints}},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1601.02733},
	doi = {10.1109/TNNLS.2015.2479223},
	abstract = {We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (NCAE), that learns features which show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text dataset. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and Nonnegative Matrix Factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network.},
	urldate = {2016-01-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Hosseini-Asl, Ehsan and Zurada, Jacek M. and Nasraoui, Olfa},
	year = {2015},
	note = {arXiv: 1601.02733},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	pages = {1--13},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/I75JB4MI/1601.html:text/html;Hosseini-Asl et al_2015_Deep Learning of Part-based Representation of Data Using Sparse Autoencoders.pdf:/home/user/Zotero/storage/ES4NV6CH/Hosseini-Asl et al_2015_Deep Learning of Part-based Representation of Data Using Sparse Autoencoders.pdf:application/pdf}
}

@article{cheng-syllable-2015,
	title = {Syllable {Structure} {Universals} and {Native} {Language} {Interference} in {Second} {Language} {Perception} and {Production}: {Positional} {Asymmetry} and {Perceptual} {Links} to {Accentedness}},
	shorttitle = {Syllable {Structure} {Universals} and {Native} {Language} {Interference} in {Second} {Language} {Perception} and {Production}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01801/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w3-2016},
	doi = {10.3389/fpsyg.2015.01801},
	abstract = {The present study investigated how syllable structure differences between the first Language (L1) and the second language (L2) affect L2 consonant perception and production at syllable-initial and syllable-final positions. The participants were Mandarin-speaking college students who studied English as a second language. Monosyllabic English words were used in the perception test. Production was recorded from each Chinese subject and rated for accentedness by two native speakers of English. Consistent with previous studies, significant positional asymmetry effects were found across speech sound categories in terms of voicing, place of articulation, and manner of articulation. Furthermore, significant correlations between perception and accentedness ratings were found at the syllable onset position but not for the coda. Many exceptions were also found, which could not be solely accounted for by differences in L1–L2 syllabic structures. The results show a strong effect of language experience at the syllable level, which joins force with acoustic, phonetic, and phonemic properties of individual consonants in influencing positional asymmetry in both domains of L2 segmental perception and production. The complexities and exceptions call for further systematic studies on the interactions between syllable structure universals and native language interference with refined theoretical models to specify the links between perception and production in second language acquisition.},
	urldate = {2016-01-14},
	journal = {Language Sciences},
	author = {Cheng, Bing and Zhang, Yang},
	year = {2015},
	keywords = {speech perception, speech production, accentedness, allophonic variations, native language neural commitment, phonetic learning, syllable structure},
	pages = {1801},
	file = {Cheng_Zhang_2015_Syllable Structure Universals and Native Language Interference in Second.pdf:/home/user/Zotero/storage/HGRFCGCQ/Cheng_Zhang_2015_Syllable Structure Universals and Native Language Interference in Second.pdf:application/pdf}
}

@article{ioffe-batch-2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2016-01-14},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/63KSP4S5/1502.html:text/html;Ioffe_Szegedy_2015_Batch Normalization.pdf:/home/user/Zotero/storage/WWHTR9EE/Ioffe_Szegedy_2015_Batch Normalization.pdf:application/pdf}
}

@article{graves-generating-2013-1,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2016-01-13},
	journal = {arXiv:1308.0850 [cs]},
	author = {Graves, Alex},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.0850},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/62NG2KFQ/1308.html:text/html;Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:/home/user/Zotero/storage/5IZ5JCVH/Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf:application/pdf}
}

@inproceedings{sutskever-sequence-2014-1,
	title = {Sequence to sequence learning with neural networks},
	url = {http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces},
	urldate = {2016-01-13},
	booktitle = {Advances in neural information processing systems},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc VV},
	year = {2014},
	pages = {3104--3112},
	file = {Sequence to Sequence Learning with Neural Networks - 5346-sequence-to-sequence-learning-with-neural-networks.pdf:/home/user/Zotero/storage/ZH8PSNTD/5346-sequence-to-sequence-learning-with-neural-networks.pdf:application/pdf}
}

@article{ioffe-batch-2015-1,
	title = {Batch normalization: {Accelerating} deep network training by reducing internal covariate shift},
	shorttitle = {Batch normalization},
	url = {http://arxiv.org/abs/1502.03167},
	urldate = {2016-01-13},
	journal = {arXiv preprint arXiv:1502.03167},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	file = {() - 1502.03167v3.pdf:/home/user/Zotero/storage/SW2IAJ9Z/1502.03167v3.pdf:application/pdf}
}

@article{luong-multi-task-2015-1,
	title = {Multi-task {Sequence} to {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.06114},
	abstract = {Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three settings to multi-task sequence to sequence learning: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve translation quality by up to 1.5 BLEU points. Additionaly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the context of multi-task sequence to sequence learning.},
	urldate = {2016-01-13},
	journal = {arXiv:1511.06114 [cs, stat]},
	author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06114},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3FQJ4UZC/1511.html:text/html;Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:/home/user/Zotero/storage/AXSPZKCV/Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:application/pdf}
}

@article{tang-recurrent-2015,
	title = {Recurrent {Neural} {Network} {Training} with {Dark} {Knowledge} {Transfer}},
	url = {http://arxiv.org/abs/1505.04630},
	abstract = {Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.},
	urldate = {2016-01-13},
	journal = {arXiv:1505.04630 [cs, stat]},
	author = {Tang, Zhiyuan and Wang, Dong and Zhang, Zhiyong},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04630},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VKVV3M5H/1505.html:text/html;Tang et al_2015_Recurrent Neural Network Training with Dark Knowledge Transfer.pdf:/home/user/Zotero/storage/6GFAJIHA/Tang et al_2015_Recurrent Neural Network Training with Dark Knowledge Transfer.pdf:application/pdf}
}

@article{wieting-towards-2015-1,
	title = {Towards {Universal} {Paraphrastic} {Sentence} {Embeddings}},
	url = {http://arxiv.org/abs/1511.08198},
	abstract = {In this paper, we show how to create paraphrastic sentence embeddings using the Paraphrase Database (Ganitkevitch et al., 2013), an extensive semantic resource with millions of phrase pairs. We consider several compositional architectures and evaluate them on 24 textual similarity datasets encompassing domains such as news, tweets, web forums, news headlines, machine translation output, glosses, and image and video captions. We present the interesting result that simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks and that these simpler architectures allow us to learn models with superior generalization. Our models are efficient, very easy to use, and competitive with task-tuned systems. We make them available to the research community with the hope that they can serve as the new baseline for further work on universal paraphrastic sentence embeddings.},
	urldate = {2016-01-13},
	journal = {arXiv:1511.08198 [cs]},
	author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08198},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/7ZDDDVC2/1511.html:text/html;Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:/home/user/Zotero/storage/BJV4N27K/Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:application/pdf}
}

@article{michaeli-nonparametric-2015,
	title = {Nonparametric {Canonical} {Correlation} {Analysis}},
	url = {http://arxiv.org/abs/1511.04839},
	abstract = {Canonical correlation analysis (CCA) is a fundamental technique in multi-view data analysis and representation learning. Several nonlinear extensions of the classical linear CCA method have been proposed, including kernel and deep neural network methods. These approaches restrict attention to certain families of nonlinear projections, which the user must specify (by choosing a kernel or a neural network architecture), and are computationally demanding. Interestingly, the theory of nonlinear CCA without any functional restrictions, has been studied in the population setting by Lancaster already in the 50's. However, these results, have not inspired practical algorithms. In this paper, we revisit Lancaster's theory, and use it to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the most correlated nonlinear projections of two random vectors can be expressed in terms of the singular value decomposition of a certain operator associated with their joint density. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without having to compute the inverse of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. PLCCA turns out to have a similar form to the classical linear CCA, but with a nonparametric regression term replacing the linear regression in CCA. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and achieve better performance than kernel CCA and comparable performance to deep CCA.},
	urldate = {2016-01-12},
	journal = {arXiv:1511.04839 [cs, stat]},
	author = {Michaeli, Tomer and Wang, Weiran and Livescu, Karen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04839},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/73CTU45N/1511.html:text/html;Michaeli et al_2015_Nonparametric Canonical Correlation Analysis.pdf:/home/user/Zotero/storage/JJDAUUXP/Michaeli et al_2015_Nonparametric Canonical Correlation Analysis.pdf:application/pdf}
}

@article{burda-importance-2015,
	title = {Importance {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
	urldate = {2016-01-12},
	journal = {arXiv:1509.00519 [cs, stat]},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.00519},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XNKKWFSU/1509.html:text/html;Burda et al_2015_Importance Weighted Autoencoders.pdf:/home/user/Zotero/storage/84N88R66/Burda et al_2015_Importance Weighted Autoencoders.pdf:application/pdf}
}

@article{wang-bayesian-2013,
	title = {Bayesian {Optimization} in a {Billion} {Dimensions} via {Random} {Embeddings}},
	url = {http://arxiv.org/abs/1301.1942},
	abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
	urldate = {2016-01-12},
	journal = {arXiv:1301.1942 [cs, stat]},
	author = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and de Freitas, Nando},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.1942},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CA67PGAQ/1301.html:text/html;Wang et al_2013_Bayesian Optimization in a Billion Dimensions via Random Embeddings.pdf:/home/user/Zotero/storage/WN2FHNA9/Wang et al_2013_Bayesian Optimization in a Billion Dimensions via Random Embeddings.pdf:application/pdf}
}

@article{almeida-reduction-2015,
	title = {Reduction of {Nondeterministic} {Tree} {Automata}},
	url = {http://arxiv.org/abs/1512.08823},
	abstract = {We present an efficient algorithm to reduce the size of nondeterministic tree automata, while retaining their language. It is based on new transition pruning techniques, and quotienting of the state space w.r.t. suitable equivalences. It uses criteria based on combinations of downward and upward simulation preorder on trees, and the more general downward and upward language inclusions. Since tree-language inclusion is EXPTIME-complete, we describe methods to compute good approximations in polynomial time. We implemented our algorithm as a module of the well-known libvata tree automata library, and tested its performance on a given collection of tree automata from various applications of libvata in regular model checking and shape analysis, as well as on various classes of randomly generated tree automata. Our algorithm yields substantially smaller and sparser automata than all previously known reduction techniques, and it is still fast enough to handle large instances.},
	urldate = {2016-01-12},
	journal = {arXiv:1512.08823 [cs]},
	author = {Almeida, Ricardo and Holík, Lukáš and Mayr, Richard},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.08823},
	keywords = {Computer Science - Formal Languages and Automata Theory, F.1.1, 68Q45, D.2.4},
	file = {Almeida et al_2015_Reduction of Nondeterministic Tree Automata.pdf:/home/user/Zotero/storage/4R8M8ADQ/Almeida et al_2015_Reduction of Nondeterministic Tree Automata.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/DWNSNPNS/1512.html:text/html}
}

@article{brough-automaton-2016,
	title = {Automaton semigroups: new construction results and examples of non-automaton semigroups},
	shorttitle = {Automaton semigroups},
	url = {http://arxiv.org/abs/1601.01168},
	abstract = {This paper studies the class of automaton semigroups from two perspectives: closure under constructions, and examples of semigroups that are not automaton semigroups. We prove that (semigroup) free products of finite semigroups always arise as automaton semigroups, and that the class of automaton monoids is closed under forming wreath products with finite monoids. We also consider closure under certain kinds of Rees matrix constructions, strong semilattices, and small extensions. Finally, we prove that no subsemigroup of \$(\mathbb{N}, +)\$ arises as an automaton semigroup. (Previously, \$(\mathbb{N},+)\$ itself was the unique example of a finitely generated residually finite semigroup that was known not to arise as an automaton semigroup.)},
	urldate = {2016-01-12},
	journal = {arXiv:1601.01168 [cs, math]},
	author = {Brough, Tara and Cain, Alan J.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01168},
	keywords = {Computer Science - Formal Languages and Automata Theory, 20M35, 68Q45, Mathematics - Group Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GFWFSU7G/1601.html:text/html;Brough_Cain_2016_Automaton semigroups.pdf:/home/user/Zotero/storage/KCGJH2WX/Brough_Cain_2016_Automaton semigroups.pdf:application/pdf}
}

@article{dong-language-2016,
	title = {Language to {Logical} {Form} with {Neural} {Attention}},
	url = {http://arxiv.org/abs/1601.01280},
	abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper, we present a general method based on an attention-enhanced sequence-to-sequence model. We encode input sentences into vector representations using recurrent neural networks, and generate their logical forms by conditioning the output on the encoding vectors. The model is trained in an end-to-end fashion to maximize the likelihood of target logical forms given the natural language inputs. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
	urldate = {2016-01-12},
	journal = {arXiv:1601.01280 [cs]},
	author = {Dong, Li and Lapata, Mirella},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01280},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/83XJ7ACC/1601.html:text/html;Dong_Lapata_2016_Language to Logical Form with Neural Attention.pdf:/home/user/Zotero/storage/FESQQ7I4/Dong_Lapata_2016_Language to Logical Form with Neural Attention.pdf:application/pdf}
}

@article{cohn-incorporating-2016,
	title = {Incorporating {Structural} {Alignment} {Biases} into an {Attentional} {Neural} {Translation} {Model}},
	url = {http://arxiv.org/abs/1601.01085},
	abstract = {Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.},
	urldate = {2016-01-12},
	journal = {arXiv:1601.01085 [cs]},
	author = {Cohn, Trevor and Hoang, Cong Duy Vu and Vymolova, Ekaterina and Yao, Kaisheng and Dyer, Chris and Haffari, Gholamreza},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01085},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UHA3EUI2/1601.html:text/html;Cohn et al_2016_Incorporating Structural Alignment Biases into an Attentional Neural.pdf:/home/user/Zotero/storage/PAJ83UHW/Cohn et al_2016_Incorporating Structural Alignment Biases into an Attentional Neural.pdf:application/pdf}
}

@article{firat-multi-way-2016-1,
	title = {Multi-{Way}, {Multilingual} {Neural} {Machine} {Translation} with a {Shared} {Attention} {Mechanism}},
	url = {http://arxiv.org/abs/1601.01073},
	abstract = {We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.},
	urldate = {2016-01-12},
	journal = {arXiv:1601.01073 [cs, stat]},
	author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01073},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TT7R82HG/1601.html:text/html;Firat et al_2016_Multi-Way, Multilingual Neural Machine Translation with a Shared Attention.pdf:/home/user/Zotero/storage/HEFWMAPC/Firat et al_2016_Multi-Way, Multilingual Neural Machine Translation with a Shared Attention.pdf:application/pdf}
}

@article{wagner-givenness-2012,
	title = {A givenness illusion},
	volume = {27},
	issn = {0169-0965},
	url = {http://dx.doi.org/10.1080/01690965.2011.607713},
	doi = {10.1080/01690965.2011.607713},
	abstract = {Constituents that encode information that is salient in the discourse or “given” are often prosodically reduced and remain unaccented. What is given and new is usually defined at the level of meaning: given expressions are those that refer to salient referents or predicates that have been made salient by the previous discourse. This paper presents evidence from two production studies that sometimes, a constituent that semantically should be contrastive, and hence accentable, is treated prosodically as if it was given, and placing an accent on it is consistently avoided—an illusory case of givenness. This effect can be explained by assuming that givenness is not only evaluated in terms of semantic content, but also at the phonological level. Prosodically marking a semantic contrast requires the presence of a phonological contrast. This effect thus provides evidence that the notion of “antecedent” relevant for prosodic givenness-marking needs to include reference to linguistic form, and not just to referential meaning.},
	number = {10},
	urldate = {2016-01-12},
	journal = {Language and Cognitive Processes},
	author = {Wagner, Michael},
	month = dec,
	year = {2012},
	pages = {1433--1458},
	file = {Snapshot:/home/user/Zotero/storage/FRW4JPNK/01690965.2011.html:text/html;Wagner_2012_A givenness illusion.pdf:/home/user/Zotero/storage/RHKRHEI8/Wagner_2012_A givenness illusion.pdf:application/pdf}
}

@article{wagner-contrastive-2012,
	title = {Contrastive topics decomposed},
	volume = {5},
	issn = {1937-8912},
	url = {http://semprag.org/article/view/1231},
	doi = {10.3765/sp.5.8},
	number = {0},
	journal = {Semantics and Pragmatics},
	author = {Wagner, Michael},
	month = dec,
	year = {2012},
	pages = {8:1--54},
	file = {Contrastive topics decomposed | Wagner | Semantics and Pragmatics:/home/user/Zotero/storage/6TQZQ52I/sp.5.html:text/html}
}

@misc{noauthor-contrastive-nodate,
	title = {Contrastive topics decomposed {\textbar} {Wagner} {\textbar} {Semantics} and {Pragmatics}},
	url = {http://semprag.org/article/view/sp.5.8},
	urldate = {2016-01-12},
	file = {Contrastive topics decomposed | Wagner | Semantics and Pragmatics:/home/user/Zotero/storage/JTP7PURT/sp.5.html:text/html}
}

@book{dancygier-viewpoint-2012,
	title = {Viewpoint in {Language}},
	abstract = {What makes us talk about viewpoint and perspective in linguistic analyses and in literary texts, as well as in landscape art? Is this shared vocabulary marking real connections between the disparate phenomena? This volume argues that human cognition is not only rooted in the human body, but also inherently 'viewpointed' as a result; consequently, so are language and communication. Dancygier and Sweetser bring together researchers who do not typically meet on common ground: analysts of narrative and literary style, linguists examining the uses of grammatical forms in signed and spoken languages, and analysts of gesture accompanying speech. Using models developed within cognitive linguistics, the book uncovers surprising functional similarities across various communicative forms, arguing for specific cognitive underpinnings of such correlations. What emerges is a new understanding of the role and structure of viewpoint and a groundbreaking methodology for investigating communicative choices across various modalities and discourse contexts.},
	language = {English},
	publisher = {Cambridge University Press},
	editor = {Dancygier, Barbara and Sweetser, Eve},
	month = mar,
	year = {2012}
}

@article{jocham-reward-guided-2016,
	title = {Reward-{Guided} {Learning} with and without {Causal} {Attribution}},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001112/abstract},
	doi = {10.1016/j.neuron.2016.02.018},
	language = {English},
	number = {0},
	urldate = {2016-03-18},
	journal = {Neuron},
	author = {Jocham, Gerhard and Brodersen, Kay H. and Constantinescu, Alexandra O. and Kahn, Martin C. and Ianni, Angela M. and Walton, Mark E. and Rushworth, Matthew F. S. and Behrens, Timothy E. J.},
	month = mar,
	year = {2016},
	file = {Jocham et al_2016_Reward-Guided Learning with and without Causal Attribution.pdf:/home/user/Zotero/storage/RRJ649E4/Jocham et al_2016_Reward-Guided Learning with and without Causal Attribution.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/S4499AB5/S0896-6273(16)00111-2.html:text/html}
}

@article{bi-object-2016,
	title = {Object {Domain} and {Modality} in the {Ventral} {Visual} {Pathway}},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {http://www.cell.com/article/S1364661316000437/abstract},
	doi = {10.1016/j.tics.2016.02.002},
	language = {English},
	number = {0},
	urldate = {2016-03-18},
	journal = {Trends in Cognitive Sciences},
	author = {Bi, Yanchao and Wang, Xiaoying and Caramazza, Alfonso},
	month = mar,
	year = {2016},
	file = {Bi et al_2016_Object Domain and Modality in the Ventral Visual Pathway.pdf:/home/user/Zotero/storage/M8QKKUSD/Bi et al_2016_Object Domain and Modality in the Ventral Visual Pathway.pdf:application/pdf}
}

@article{noauthor-africas-2016,
	title = {Africa’s elite},
	volume = {531},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/531275a},
	doi = {10.1038/531275a},
	number = {7594},
	urldate = {2016-03-18},
	journal = {Nature},
	month = mar,
	year = {2016},
	pages = {275--275}
}

@article{semeniuta-recurrent-2016,
	title = {Recurrent {Dropout} without {Memory} {Loss}},
	url = {http://arxiv.org/abs/1603.05118},
	abstract = {This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05118 [cs]},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05118},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/H6THCR2C/1603.html:text/html;Semeniuta et al_2016_Recurrent Dropout without Memory Loss.pdf:/home/user/Zotero/storage/WCG7BP5P/Semeniuta et al_2016_Recurrent Dropout without Memory Loss.pdf:application/pdf}
}

@article{karpova-editorial-nodate,
	title = {Editorial overview: {Neurobiology} of cognitive behavior: {Complexity} of neural computation and cognition},
	issn = {0959-4388},
	shorttitle = {Editorial overview},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438816300125},
	doi = {10.1016/j.conb.2016.03.003},
	urldate = {2016-03-18},
	journal = {Current Opinion in Neurobiology},
	author = {Karpova, Alla and Kiani, Roozbeh},
	file = {Karpova_Kiani_Editorial overview.pdf:/home/user/Zotero/storage/CMSD55EN/Karpova_Kiani_Editorial overview.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/P69NH9PF/S0959438816300125.html:text/html}
}

@article{sanchez-information-2016,
	title = {Information {Thermodynamics} of {Cytosine} {DNA} {Methylation}},
	volume = {11},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0150427},
	doi = {10.1371/journal.pone.0150427},
	abstract = {Cytosine DNA methylation (CDM) is a stable epigenetic modification to the genome and a widespread regulatory process in living organisms that involves multicomponent molecular machines. Genome-wide cytosine methylation patterning participates in the epigenetic reprogramming of a cell, suggesting that the biological information contained within methylation positions may be amenable to decoding. Adaptation to a new cellular or organismal environment also implies the potential for genome-wide redistribution of CDM changes that will ensure the stability of DNA molecules. This raises the question of whether or not we would be able to sort out the regulatory methylation signals from the CDM background (“noise”) induced by thermal fluctuations. Here, we propose a novel statistical and information thermodynamic description of the CDM changes to address the last question. The physical basis of our statistical mechanical model was evaluated in two respects: 1) the adherence to Landauer’s principle, according to which molecular machines must dissipate a minimum energy  ε  =  k   B   T ln 2 at each logic operation, where  k   B   is the Boltzmann constant, and  T  is the absolute temperature and 2) whether or not the binary stretch of methylation marks on the DNA molecule comprise a language of sorts, properly constrained by thermodynamic principles. The study was performed for genome-wide methylation data from 152 ecotypes and 40 trans-generational variations of  Arabidopsis thaliana  and 93 human tissues. The DNA persistence length, a basic mechanical property altered by CDM, was estimated with values from 39 to 66.9 nm. Classical methylome analysis can be retrieved by applying information thermodynamic modelling, which is able to discriminate signal from noise. Our finding suggests that the CDM signal comprises a language scheme properly constrained by molecular thermodynamic principles, which is part of an epigenomic communication system that obeys the same thermodynamic rules as do current human communication systems.},
	number = {3},
	urldate = {2016-03-18},
	journal = {PLOS ONE},
	author = {Sanchez, Robersy and Mackenzie, Sally A.},
	month = mar,
	year = {2016},
	keywords = {language, Arabidopsis thaliana, Background noise (acoustics), Cytosine, DNA methylation, Methylation, Probability distribution, Thermodynamics},
	pages = {e0150427},
	file = {Sanchez_Mackenzie_2016_Information Thermodynamics of Cytosine DNA Methylation.pdf:/home/user/Zotero/storage/IG9ICFV2/Sanchez_Mackenzie_2016_Information Thermodynamics of Cytosine DNA Methylation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3BUG2UKJ/article.html:text/html}
}

@article{weisberg-gender-2011,
	title = {Gender {Differences} in {Personality} across the {Ten} {Aspects} of the {Big} {Five}},
	volume = {2},
	issn = {1664-1078},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3149680/},
	doi = {10.3389/fpsyg.2011.00178},
	abstract = {This paper investigates gender differences in personality traits, both at the level of the Big Five and at the sublevel of two aspects within each Big Five domain. Replicating previous findings, women reported higher Big Five Extraversion, Agreeableness, and Neuroticism scores than men. However, more extensive gender differences were found at the level of the aspects, with significant gender differences appearing in both aspects of every Big Five trait. For Extraversion, Openness, and Conscientiousness, the gender differences were found to diverge at the aspect level, rendering them either small or undetectable at the Big Five level. These findings clarify the nature of gender differences in personality and highlight the utility of measuring personality at the aspect level.},
	urldate = {2016-03-18},
	journal = {Frontiers in Psychology},
	author = {Weisberg, Yanna J. and DeYoung, Colin G. and Hirsh, Jacob B.},
	month = aug,
	year = {2011},
	pmid = {21866227},
	pmcid = {PMC3149680},
	file = {Weisberg et al_2011_Gender Differences in Personality across the Ten Aspects of the Big Five.pdf:/home/user/Zotero/storage/X2PUZQI7/Weisberg et al_2011_Gender Differences in Personality across the Ten Aspects of the Big Five.pdf:application/pdf}
}

@article{sennrich-improving-2015-1,
	title = {Improving {Neural} {Machine} {Translation} {Models} with {Monolingual} {Data}},
	url = {http://arxiv.org/abs/1511.06709},
	abstract = {Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English{\textless}-{\textgreater}German (+2.8-3.4 BLEU), and for the low-resourced IWSLT 14 task Turkish-{\textgreater}English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English-{\textgreater}German.},
	urldate = {2016-03-18},
	journal = {arXiv:1511.06709 [cs]},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06709},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NHCDQ3BH/1511.html:text/html;Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:/home/user/Zotero/storage/ZE5XI6IS/Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:application/pdf}
}

@article{krebs-two-variable-2016,
	title = {Two-variable {Logic} with a {Between} {Predicate}},
	url = {http://arxiv.org/abs/1603.05625},
	abstract = {We study an extension of FO{\textasciicircum}2[{\textless}], first-order logic interpreted in finite words, in which formulas are restricted to use only two variables. We adjoin to this language two-variable atomic formulas that say, `the letter a appears between positions x and y'. This is, in a sense, the simplest property that is not expressible using only two variables. We present several logics, both first-order and temporal, that have the same expressive power, and find matching lower and upper bounds for the complexity of satisfiability for each of these formulations. We also give an effective necessary condition, in terms of the syntactic monoid of a regular language, for a property to be expressible in this logic. We show that this condition is also sufficient for words over a two-letter alphabet. This algebraic analysis allows us us to prove, among other things, that our new logic has strictly less expressive power than full first-order logic FO[{\textless}].},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05625 [cs]},
	author = {Krebs, Andreas and Lodaya, Kamal and Pandya, Paritosh and Straubing, Howard},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05625},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/P9X9R92J/1603.html:text/html;Krebs et al_2016_Two-variable Logic with a Between Predicate.pdf:/home/user/Zotero/storage/RINW863F/Krebs et al_2016_Two-variable Logic with a Between Predicate.pdf:application/pdf}
}

@article{liu-short-2016,
	title = {A {Short} {Note} on {Infinite} {Union}/{Intersection} of {Omega} {Regular} {Languages}},
	url = {http://arxiv.org/abs/1603.05426},
	abstract = {We in this paper show that omega regular languages are not closed under infinite union and intersection. As an attempt, we propose to add step variables and quantifiers to temporal logics to enhance the expressiveness of the underlying logic. We also show that doing this would cause undecidability in satisfiability, even if for a rather limited fragment of temporal logic.},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05426 [cs]},
	author = {Liu, Wanwei},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05426},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UJ7Z8KN9/1603.html:text/html;Liu_2016_A Short Note on Infinite Union-Intersection of Omega Regular Languages.pdf:/home/user/Zotero/storage/RP5PQRER/Liu_2016_A Short Note on Infinite Union-Intersection of Omega Regular Languages.pdf:application/pdf}
}

@article{fleischer-operations-2016,
	title = {Operations on {Weakly} {Recognizing} {Morphisms}},
	url = {http://arxiv.org/abs/1603.05376},
	abstract = {Weakly recognizing morphisms from free semigroups onto finite semigroups are a classical way for defining the class of omega-regular languages, i.e., a set of infinite words is weakly recognizable by such a morphism if and only if it is accepted by some B\"uchi automaton. We consider the descriptional complexity of various constructions for weakly recognizing morphisms. This includes the conversion from and to B\"uchi automata, the conversion into strongly recognizing morphisms, and complementation. For some problems, we are able to give more precise bounds in the case of binary alphabets or simple semigroups.},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05376 [cs]},
	author = {Fleischer, Lukas and Kufleitner, Manfred},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05376},
	keywords = {Computer Science - Formal Languages and Automata Theory, F.4.3, F.2.2},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CX3Z487H/1603.html:text/html;Fleischer_Kufleitner_2016_Operations on Weakly Recognizing Morphisms.pdf:/home/user/Zotero/storage/U34ZPUZD/Fleischer_Kufleitner_2016_Operations on Weakly Recognizing Morphisms.pdf:application/pdf}
}

@article{vera-modeling-2016,
	title = {Modeling the self-organization of vocabularies under phonological similarity effects},
	url = {http://arxiv.org/abs/1603.05354},
	abstract = {This work develops a computational model (by Automata Networks) of short-term memory constraints involved in the formation of linguistic conventions on artificial populations of speakers. The individuals confound phonologically similar words according to a predefined parameter. The main hypothesis of this paper is that there is a critical range of working memory capacities, in particular, a critical phonological degree of confusion, which implies drastic changes in the final consensus of the entire population. A theoretical result proves the convergence of a particular case of the model. Computer simulations describe the evolution of an energy function that measures the amount of local agreement between individuals. The main finding is the appearance of sudden changes in the energy function at critical parameters. Finally, the results are related to previous work on the absence of stages in the formation of languages.},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05354 [physics]},
	author = {Vera, Javier},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05354},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/EVE87N8F/1603.html:text/html;Vera_2016_Modeling the self-organization of vocabularies under phonological similarity.pdf:/home/user/Zotero/storage/ETMBGQBB/Vera_2016_Modeling the self-organization of vocabularies under phonological similarity.pdf:application/pdf}
}

@article{vera-self-organization-2016,
	title = {Self-organization of vocabularies under different interaction orders},
	url = {http://arxiv.org/abs/1603.05350},
	abstract = {Traditionally, the formation of linguistic conventions has been studied by agent-based models (specially, the Naming Game) in which random pairs of agents negotiate word-meaning associations at each discrete time step. This paper proposes a first approximation to a novel question: To what extent the negotiation of word-meaning associations is influenced by the order in which the individuals interact? Automata Networks provide the adequate mathematical framework to explore this question. A theoretical result shows that if all individuals are updated at the same time a periodic behavior may appear. Computer simulations suggest that on two-dimensional lattices the typical features of the formation of linguistic conventions are recovered under random schemes that update small fractions of the population at the same time.},
	urldate = {2016-03-18},
	journal = {arXiv:1603.05350 [physics]},
	author = {Vera, Javier},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05350},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/N5HS6Q3T/1603.html:text/html;Vera_2016_Self-organization of vocabularies under different interaction orders.pdf:/home/user/Zotero/storage/R23RXQ2M/Vera_2016_Self-organization of vocabularies under different interaction orders.pdf:application/pdf}
}

@article{han-variational-2015,
	title = {Variational {Gaussian} {Copula} {Inference}},
	url = {http://arxiv.org/abs/1506.05860},
	abstract = {We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a {semiparametric} and {automated} variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors.},
	urldate = {2016-03-18},
	journal = {arXiv:1506.05860 [cs, stat]},
	author = {Han, Shaobo and Liao, Xuejun and Dunson, David B. and Carin, Lawrence},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.05860},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UQWR65GN/1506.html:text/html;Han et al_2015_Variational Gaussian Copula Inference.pdf:/home/user/Zotero/storage/PZTZCMZH/Han et al_2015_Variational Gaussian Copula Inference.pdf:application/pdf}
}

@article{waqas-how-2016,
	title = {How {I} made my own opportunities},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6279/1358},
	doi = {10.1126/science.351.6279.1358},
	language = {en},
	number = {6279},
	urldate = {2016-03-18},
	journal = {Science},
	author = {Waqas, Ahmed},
	month = mar,
	year = {2016},
	pages = {1358--1358},
	file = {Snapshot:/home/user/Zotero/storage/6AVWWPB7/1358.html:text/html;Waqas_2016_How I made my own opportunities.pdf:/home/user/Zotero/storage/JPUH54WH/Waqas_2016_How I made my own opportunities.pdf:application/pdf}
}

@article{cornwall-efforts-2016,
	title = {Efforts to link climate change to severe weather gain ground},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6279/1249},
	doi = {10.1126/science.351.6279.1249},
	abstract = {Scientists are tying climate change to individual cases of extreme weather with increasing confidence and speed. Although people have long said it's impossible to blame climate change for any single weather event, that's no longer the case, according to a report issued 11 March by a panel of scientists for the National Academies of Sciences, Engineering, and Medicine. Computer models of the climate, paired with historic weather records, are now being used to estimate whether the odds of a particular event—such as a heat wave—are higher in a world with current greenhouse gas levels. The panel said heat waves and cold snaps are producing the most reliable studies, whereas droughts and severe rainstorms can be examined with some confidence. Hurricanes and tornadoes, however, continue to elude such analyses. This emerging science of event attribution could have legal and diplomatic implications, as nations and people harmed by such episodes consider seeking compensation for damage caused by extreme weather from greenhouse gas polluters.
U.S. academies find researchers getting better at attributing heat waves, cold snaps, and other events to climate trends.
U.S. academies find researchers getting better at attributing heat waves, cold snaps, and other events to climate trends.},
	language = {en},
	number = {6279},
	urldate = {2016-03-18},
	journal = {Science},
	author = {Cornwall, Warren},
	month = mar,
	year = {2016},
	pages = {1249--1250},
	file = {Cornwall_2016_Efforts to link climate change to severe weather gain ground.pdf:/home/user/Zotero/storage/BVXWRIXW/Cornwall_2016_Efforts to link climate change to severe weather gain ground.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/6HN8ZM2A/1249.html:text/html}
}

@article{lindskog-no-2016,
	title = {No evidence of learning in non-symbolic numerical tasks – {A} comment on {Park} and {Brannon} (2014)},
	volume = {150},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716300051},
	doi = {10.1016/j.cognition.2016.01.005},
	abstract = {Two recent studies – one of which was published in this journal – claimed to have found that learning on a non-symbolic arithmetic task improved performance on a symbolic arithmetic task (Park \&amp; Brannon, 2013, 2014). This finding has potentially far-reaching implications, because it would constitute evidence for a causal link between the Approximate Number System (ANS) and symbolic-math ability. Here, we argue that, due to the methodology used in both studies, the interpretation of data in terms of an improvement in ANS performance is problematic. We provide arguments and simulations showing that the trends in the data are similar to what one would expect for a non-learning observer. We discuss the implications for the original interpretation in terms of causality between non-symbolic and symbolic arithmetic performance.},
	urldate = {2016-03-16},
	journal = {Cognition},
	author = {Lindskog, Marcus and Winman, Anders},
	month = may,
	year = {2016},
	keywords = {Approximate Number System, Cognitive training, Numerical cognition},
	pages = {243--247},
	file = {Lindskog_Winman_2016_No evidence of learning in non-symbolic numerical tasks – A comment on Park and.pdf:/home/user/Zotero/storage/BJ9XB8TZ/Lindskog_Winman_2016_No evidence of learning in non-symbolic numerical tasks – A comment on Park and.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/3DG52WUN/S0010027716300051.html:text/html}
}

@article{ayars-can-2016,
	title = {Can model-free reinforcement learning explain deontological moral judgments?},
	volume = {150},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716300300},
	doi = {10.1016/j.cognition.2016.02.002},
	abstract = {Dual-systems frameworks propose that moral judgments are derived from both an immediate emotional response, and controlled/rational cognition. Recently Cushman (2013) proposed a new dual-system theory based on model-free and model-based reinforcement learning. Model-free learning attaches values to actions based on their history of reward and punishment, and explains some deontological, non-utilitarian judgments. Model-based learning involves the construction of a causal model of the world and allows for far-sighted planning; this form of learning fits well with utilitarian considerations that seek to maximize certain kinds of outcomes. I present three concerns regarding the use of model-free reinforcement learning to explain deontological moral judgment. First, many actions that humans find aversive from model-free learning are not judged to be morally wrong. Moral judgment must require something in addition to model-free learning. Second, there is a dearth of evidence for central predictions of the reinforcement account—e.g., that people with different reinforcement histories will, all else equal, make different moral judgments. Finally, to account for the effect of intention within the framework requires certain assumptions which lack support. These challenges are reasonable foci for future empirical/theoretical work on the model-free/model-based framework.},
	urldate = {2016-03-16},
	journal = {Cognition},
	author = {Ayars, Alisabeth},
	month = may,
	year = {2016},
	keywords = {reinforcement learning, Dual-system, Model-based, Model-free, Moral judgment},
	pages = {232--242},
	file = {Ayars_2016_Can model-free reinforcement learning explain deontological moral judgments.pdf:/home/user/Zotero/storage/X5T6WF3X/Ayars_2016_Can model-free reinforcement learning explain deontological moral judgments.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DESTDEXD/S0010027716300300.html:text/html}
}

@article{barcelo-order-invariant-2016,
	title = {Order-{Invariant} {Types} and {Their} {Applications}},
	url = {http://arxiv.org/abs/1603.04309},
	abstract = {Our goal is to show that the standard model-theoretic concept of types can be applied in the study of order-invariant properties, i.e., properties definable in a logic in the presence of an auxiliary order relation, but not actually dependent on that order relation. This is somewhat surprising since order-invariant properties are more of a combinatorial rather than a logical object. We provide two applications of this notion. One is a proof, from the basic principles, of a theorem by Courcelle stating that over trees, order-invariant MSO properties are expressible in MSO with counting quantifiers. The other is an analog of the Feferman-Vaught theorem for order-invariant properties.},
	urldate = {2016-03-15},
	journal = {arXiv:1603.04309 [cs]},
	author = {Barcelo, Pablo and Libkin, Leonid},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04309},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/STRU6X3T/1603.html:text/html;Barcelo_Libkin_2016_Order-Invariant Types and Their Applications.pdf:/home/user/Zotero/storage/NU5PAKNG/Barcelo_Libkin_2016_Order-Invariant Types and Their Applications.pdf:application/pdf}
}

@article{kiperwasser-simple-2016,
	title = {Simple and {Accurate} {Dependency} {Parsing} {Using} {Bidirectional} {LSTM} {Feature} {Representations}},
	url = {http://arxiv.org/abs/1603.04351},
	abstract = {We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.},
	urldate = {2016-03-15},
	journal = {arXiv:1603.04351 [cs]},
	author = {Kiperwasser, Eliyahu and Goldberg, Yoav},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04351},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZP4GA68B/1603.html:text/html;Kiperwasser_Goldberg_2016_Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature.pdf:/home/user/Zotero/storage/KBKWFPXX/Kiperwasser_Goldberg_2016_Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature.pdf:application/pdf}
}

@article{sauppe-verbal-2016,
	title = {Verbal {Semantics} {Drives} {Early} {Anticipatory} {Eye} {Movements} during the {Comprehension} of {Verb}-{Initial} {Sentences}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00095/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00095},
	abstract = {Studies on anticipatory processes during sentence comprehension often focus on the prediction of postverbal direct objects. In subject-initial languages (the target of most studies so far), however, the position in the sentence, the syntactic function, and the semantic role of arguments are often conflated. For example, in the sentence “The frog will eat the fly” the syntactic object (“fly”) is at the same time also the last word and the patient argument of the verb. It is therefore not apparent which kind of information listeners orient to for predictive processing during sentence comprehension. A visual world eye tracking study on the verb-initial language Tagalog (Austronesian) tested what kind of information listeners use to anticipate upcoming postverbal linguistic input. The grammatical structure of Tagalog allows to test whether listeners' anticipatory gaze behavior is guided by predictions of the linear order of words, by syntactic functions (e.g., subject/object), or by semantic roles (agent/patient). Participants heard sentences of the type “Eat frog fly” or “Eat fly frog” (both meaning “The frog will eat the fly”) while looking at displays containing an agent referent (“frog”), a patient referent (“fly”) and a distractor. The verb carried morphological marking that allowed the order and syntactic function of agent and patient to be inferred. After having heard the verb, listeners fixated on the agent irrespective of its syntactic function or position in the sentence. While hearing the first-mentioned argument, listeners fixated on the corresponding referent in the display accordingly and then initiated saccades to the last-mentioned referent before it was encountered. The results indicate that listeners used verbal semantics to identify referents and their semantic roles early; information about word order or syntactic functions did not influence anticipatory gaze behavior directly after the verb was heard. In this verb-initial language, event semantics takes early precedence during the comprehension of sentences, while arguments are anticipated temporally more local to when they are encountered. The current experiment thus helps to better understand anticipation during language processing by employing linguistic structures not available in previously studied subject-initial languages.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Sauppe, Sebastian},
	year = {2016},
	keywords = {Sentence comprehension, prediction, anticipation, Tagalog, verb-initial word order, visual world eye tracking},
	pages = {95}
}

@article{harris-lexical-2016,
	title = {Lexical {Stress} and {Linguistic} {Predictability} {Influence} {Proofreading} {Behavior}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00096/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00096},
	abstract = {There is extensive evidence that the segmental (i.e., phonemic) layer of phonology is routinely activated during reading, but little is known about whether phonological activation extends beyond phonemes to subsegmental layers (which include articulatory information, such as voicing) and suprasegmental layers (which include prosodic information, such as lexical stress). In three proofreading experiments, we show that spelling errors are detected more reliably in syllables that are stressed than in syllables that are unstressed if comprehension is a goal of the reader, indicating that suprasegmental phonology is both active during silent reading and can influence orthographic processes. In Experiment 1, participants received instructions to read for both errors and comprehension, and we found that the effect of lexical stress interacted with linguistic predictability, such that detection of errors in more predictable words was aided by stress but detection of errors in less predictable words was not. This finding suggests that lexical stress patterns can be accessed prelexically if an upcoming word is sufficiently predictable from context. Participants with stronger vocabularies showed decreased effects of stress on task performance, which is consistent with previous findings that more skilled readers are less swayed by phonological information in decisions about orthographic form. In two subsequent experiments, participants were instructed to read only for errors (Experiment 2) or only for comprehension (Experiment 3); the effect of stress disappeared when participants read for errors and reappeared when participants read for comprehension, reconfirming our hypothesis that predictability is a driver of lexical stress effects. In all experiments, errors were detected more reliably in words that were difficult to predict from context than in words that were highly predictable. Taken together, this series of experiments contributes two important findings to the field of reading and cognition: (1) The prosodic property of lexical stress can influence orthographic processing, and (2) Predictability inhibits the detection of errors in written language processing.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Harris, Lindsay N. and Perfetti, Charles A.},
	year = {2016},
	keywords = {error detection, lexical stress, orthographic processing, proofreading, spelling},
	pages = {96}
}

@article{everett-evaluation-2016,
	title = {An {Evaluation} of {Universal} {Grammar} and the {Phonological} {Mind}1},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00015/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00015},
	abstract = {This paper argues against the hypothesis of a “phonological mind” advanced by Berent. It establishes that there is no evidence that phonology is innate and that, in fact, the simplest hypothesis seems to be that phonology is learned like other human abilities. Moreover, the paper fleshes out the original claim of Philip Lieberman that Universal Grammar predicts that not everyone should be able to learn every language, i.e., the opposite of what UG is normally thought to predict. The paper also underscores the problem that the absence of recursion in Pirahã represents for Universal Grammar proposals.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Everett, Daniel L.},
	year = {2016},
	keywords = {Syntax, recursion, Phonology, linguistic universals, universal grammar},
	pages = {15}
}

@article{roeper-multiple-2016,
	title = {Multiple {Grammars} and the {Logic} of {Learnability} in {Second} {Language} {Acquisition}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00014/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00014},
	abstract = {The core notion of modern Universal Grammar is that language ability requires abstract representation in terms of hierarchy, movement operations, abstract features on words, and fixed mapping to meaning. These mental structures are a step toward integrating representational knowledge of all kinds into a larger model of cognitive psychology. Examining first and second language at once provides clues as to how abstractly we should represent this knowledge. The abstract nature of grammar allows both the formulation of many grammars and the possibility that a rule of one grammar could apply to another grammar. We argue that every language contains Multiple Grammars which may reflect different language families. We develop numerous examples of how the same abstract rules can apply in various languages and develop a theory of how language modules (case-marking, topicalization, and quantification) interact to predict L2 acquisition paths. In particular we show in depth how Germanic Verb-second operations, based on Verb-final structure, can apply in English. The argument is built around how and where V2 from German can apply in English, seeking to explain the crucial contrast: “nothing” yelled out Bill/*“nothing” yelled Bill out in terms of the necessary abstractness of the V2 rule.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Roeper, Tom W.},
	year = {2016},
	keywords = {minimalism, Transfer, acceptability/grammaticality judgments, interfaces, learnability, multiple grammars, verb-second},
	pages = {14}
}

@article{de-diego-balaguer-temporal-2016,
	title = {Temporal {Attention} as a {Scaffold} for {Language} {Development}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00044/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00044},
	abstract = {Language is one of the most fascinating abilities that humans possess. Infants demonstrate an amazing repertoire of linguistic abilities from very early on and reach an adult-like form incredibly fast. However, language is not acquired all at once but in an incremental fashion. In this article we propose that the attentional system may be one of the sources for this developmental trajectory in language acquisition. At birth, infants are endowed with an attentional system fully driven by salient stimuli in their environment, such as prosodic information (e.g., rhythm or pitch). Early stages of language acquisition could benefit from this readily available, stimulus-driven attention to simplify the complex speech input and allow word segmentation. At later stages of development, infants are progressively able to selectively attend to specific elements while disregarding others. This attentional ability could allow them to learn distant non-adjacent rules needed for morphosyntactic acquisition. Because non-adjacent dependencies occur at distant moments in time, learning these dependencies may require correctly orienting attention in the temporal domain. Here, we gather evidence uncovering the intimate relationship between the development of attention and language. We aim to provide a novel approach to human development, bridging together temporal attention and language acquisition.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {de Diego-Balaguer, Ruth and Martinez-Alvarez, Anna and Pons, Ferran},
	year = {2016},
	keywords = {Language Development, statistical learning, rule learning, Attention, word segmentation, infancy, morphosyntactic development, temporal orienting},
	pages = {44}
}

@article{wei-structural-2016,
	title = {Structural {Priming} and {Frequency} {Effects} {Interact} in {Chinese} {Sentence} {Comprehension}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00045/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00045},
	abstract = {Previous research in several European languages has shown that the language processing system is sensitive to both structural frequency and structural priming effects. However, it is currently not clear whether these two types of effects interact during online sentence comprehension, especially for languages that do not have morphological markings. To explore this issue, the present study investigated the possible interplay between structural priming and frequency effects for sentences containing the Chinese ambiguous construction V NP1 de NP2 in a self-paced reading experiment. The sentences were disambiguated to either the more frequent/preferred NP structure or the less frequent VP structure. Each target sentence was preceded by a prime sentence of three possible types: NP primes, VP primes, and neutral primes. When the ambiguous construction V NP1 de NP2 was disambiguated to the dispreferred VP structure, participants experienced more processing difficulty following an NP prime relative to following a VP prime or a neutral baseline. When the ambiguity was resolved to the preferred NP structure, prime type had no effect. These results suggest that structural priming in comprehension is modulated by the baseline frequency of alternative structures, with the less frequent structure being more subject to structural priming effects. These results are discussed in the context of the error-based, implicit learning account of structural priming.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Wei, Hang and Dong, Yanping and Boland, Julie E. and Yuan, Fang},
	year = {2016},
	keywords = {Sentence comprehension, baseline frequency, inverse preference effects, Mandarin Chinese, structural priming},
	pages = {45}
}

@article{sczesny-can-2016,
	title = {Can {Gender}-{Fair} {Language} {Reduce} {Gender} {Stereotyping} and {Discrimination}?},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00025/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w12-2016},
	doi = {10.3389/fpsyg.2016.00025},
	abstract = {Gender-fair language (GFL) aims at reducing gender stereotyping and discrimination. Two principle strategies have been employed to make languages gender-fair and to treat women and men symmetrically: neutralization and feminization. Neutralization is achieved, for example, by replacing male-masculine forms (policeman) with gender-unmarked forms (police officer), whereas feminization relies on the use of feminine forms to make female referents visible (i.e., the applicant… he or she instead of the applicant… he). By integrating research on (1) language structures, (2) language policies, and (3) individual language behavior, we provide a critical review of how GFL contributes to the reduction of gender stereotyping and discrimination. Our review provides a basis for future research and for scientifically based policy-making.},
	urldate = {2016-03-15},
	journal = {Language Sciences},
	author = {Sczesny, Sabine and Formanowicz, Magda and Moser, Franziska},
	year = {2016},
	keywords = {gender equality, gender stereotypes, gender-fair language, social change, social discrimination},
	pages = {25}
}

@article{zaremba-reinforcement-2015,
	title = {Reinforcement {Learning} {Neural} {Turing} {Machines} - {Revised}},
	url = {http://arxiv.org/abs/1505.00521},
	abstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.},
	urldate = {2016-03-14},
	journal = {arXiv:1505.00521 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	month = may,
	year = {2015},
	note = {arXiv: 1505.00521},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/7JPFZMID/1505.html:text/html;Zaremba_Sutskever_2015_Reinforcement Learning Neural Turing Machines - Revised.pdf:/home/user/Zotero/storage/FGZTISGD/Zaremba_Sutskever_2015_Reinforcement Learning Neural Turing Machines - Revised.pdf:application/pdf}
}

@incollection{edelman-random-2013,
	title = {Random matrix theory and its innovative applications},
	url = {http://link.springer.com/10.1007/978-1-4614-5389-5\_5},
	urldate = {2016-03-14},
	booktitle = {Advances in {Applied} {Mathematics}, {Modeling}, and {Computational} {Science}},
	publisher = {Springer},
	author = {Edelman, Alan and Wang, Yuyang},
	year = {2013},
	pages = {91--116},
	file = {random_matrix_theory_innovative.pdf:/home/user/Zotero/storage/ZSXBTRA3/random_matrix_theory_innovative.pdf:application/pdf}
}

@book{tao-topics-2012,
	title = {Topics in random matrix theory},
	volume = {132},
	url = {https://books.google.com/books?hl=en&lr=&id=L51VAwAAQBAJ&oi=fnd&pg=PR9&dq=%22laws+for+eigenvalue+spacing+distributions+of+Wigner%22+%22of+eigenvalues+of+a+Wigner+random+matrix,+or+the%22+%22instance+in+my+books+%5BTa2011,+Ta2010%5D).+If+this+text+is%22+&ots=7VjlKhPl-M&sig=0ONRleSSsfPLsEfQlu-1gSJUVUo},
	urldate = {2016-03-14},
	publisher = {American Mathematical Soc.},
	author = {Tao, Terence},
	year = {2012},
	file = {matrix-book.pdf:/home/user/Zotero/storage/KZSM9PMI/matrix-book.pdf:application/pdf}
}

@article{fulton-introduction-nodate,
	title = {An {Introduction} to {Random} {Matrices}},
	url = {http://www.langtoninfo.com/web\_content/9780521194525\_frontmatter.pdf},
	urldate = {2016-03-14},
	author = {FULTON, W. and KATOK, A. and KIRWAN, F. and SARNAK, P. and SIMON, B. and TOTARO, B.},
	file = {cupbooktemp0809.dvi - cupbook.pdf:/home/user/Zotero/storage/E7CUNNAT/cupbook.pdf:application/pdf}
}

@article{paperno-when-2016,
	title = {When the {Whole} is {Less} than the {Sum} of its {Parts}: {How} {Composition} {Affects} {PMI} {Values} in {Distributional} {Semantic} {Vectors}},
	issn = {0891-2017},
	shorttitle = {When the {Whole} is {Less} than the {Sum} of its {Parts}},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00250},
	doi = {10.1162/COLI\_a\_00250},
	abstract = {Distributional semantic models, deriving vector-based word representations from patterns of word usage in corpora, have many useful applications (Turney and Pantel 2010). Recently, there has been interest in compositional distributional models, which derive vectors for phrases from representations of their constituent words (Mitchell and Lapata 2010). Often, the values of distributional vectors are Pointwise Mutual Information (PMI) scores obtained from raw co-occurrence counts. In this paper we study the relation between the PMI dimensions of a phrase vector and its components in order to gain insights into which operations an adequate composition model should perform. We show mathematically that the difference between the PMI dimension of a phrase vector and the sum of PMIs in the corresponding dimensions of the phrases' parts is an independently interpretable value, namely a quantification of the impact of the context associated to the relevant dimension on the phrase's internal cohesion, as also measured by PMI. We then explore this quantity empirically, through an analysis of adjective-noun composition.},
	urldate = {2016-03-14},
	journal = {Computational Linguistics},
	author = {Paperno, Denis and Baroni, Marco},
	month = mar,
	year = {2016},
	pages = {1--10},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/T5FSBK78/COLI_a_00250.html:text/html;Paperno_Baroni_2016_When the Whole is Less than the Sum of its Parts.pdf:/home/user/Zotero/storage/CE2PPHMC/Paperno_Baroni_2016_When the Whole is Less than the Sum of its Parts.pdf:application/pdf}
}

@article{ling-mining-2016,
	title = {Mining {Parallel} {Corpora} {From} {Sina} {Weibo} and {Twitter}},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00249},
	doi = {10.1162/COLI\_a\_00249},
	abstract = {Microblogs such as Twitter, Facebook, and Sina Weibo (China's equivalent of Twitter), are a remarkable linguistic resource. In contrast to content from edited genres such as newswire, microblogs contain discussions of virtually every topic by numerous individuals in different languages and dialects and in different styles. In this work, we show that some microblog users post “self-translated” messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for finding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we give an optimally efficient dynamic programming algorithm for this. Using our method, we extract nearly 3M Chinese–English parallel segments from Sina Weibo using a targeted crawl of Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we are obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as in terms of utility as training data for a Chinese–English machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content.},
	urldate = {2016-03-14},
	journal = {Computational Linguistics},
	author = {Ling, Wang and Marujo, Luís and Dyer, Chris and Black, Alan W and Trancoso, Isabel},
	month = mar,
	year = {2016},
	pages = {1--55},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/IUPFEJBD/COLI_a_00249.html:text/html;Ling et al_2016_Mining Parallel Corpora From Sina Weibo and Twitter.pdf:/home/user/Zotero/storage/T3V75EHX/Ling et al_2016_Mining Parallel Corpora From Sina Weibo and Twitter.pdf:application/pdf}
}

@article{gildea-synchronous-2016,
	title = {Synchronous {Context}-{Free} {Grammars} and {Optimal} {Parsing} {Strategies}},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00246},
	doi = {10.1162/COLI\_a\_00246},
	abstract = {The complexity of parsing with Synchronous Context-Free Grammars is polynomial in the sentence length for a fixed grammar, but the degree of the polynomial depends on the grammar. Specifically, the degree depends on the length of rules, the permutations represented by the rules, and the parsing strategy adopted to decompose the recognition of a rule into smaller steps. We address the problem of finding the best parsing strategy for a rule, in terms of space and time complexity. We show that it is NP-hard to find the binary strategy with the lowest space complexity. We also show that any algorithm for finding the strategy with the lowest time complexity would imply improved approximation algorithms for finding the treewidth of general graphs.},
	urldate = {2016-03-14},
	journal = {Computational Linguistics},
	author = {Gildea, Daniel and Satta, Giorgio},
	month = mar,
	year = {2016},
	pages = {1--64},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/9NSKFB2T/COLI_a_00246.html:text/html;Gildea_Satta_2016_Synchronous Context-Free Grammars and Optimal Parsing Strategies.pdf:/home/user/Zotero/storage/VR49EVJU/Gildea_Satta_2016_Synchronous Context-Free Grammars and Optimal Parsing Strategies.pdf:application/pdf}
}

@article{mccarthy-word-2016,
	title = {Word {Sense} {Clustering} and {Clusterability}},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00247},
	doi = {10.1162/COLI\_a\_00247},
	abstract = {Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.},
	urldate = {2016-03-14},
	journal = {Computational Linguistics},
	author = {McCarthy, Diana and Apidianaki, Marianna and Erk, Katrin},
	month = mar,
	year = {2016},
	pages = {1--43},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/QEMGKK6M/COLI_a_00247.html:text/html;McCarthy et al_2016_Word Sense Clustering and Clusterability.pdf:/home/user/Zotero/storage/V4N53FRH/McCarthy et al_2016_Word Sense Clustering and Clusterability.pdf:application/pdf}
}

@article{bisazza-survey-2016,
	title = {A {Survey} of {Word} {Reordering} in {Statistical} {Machine} {Translation}: {Computational} {Models} and {Language} {Phenomena}},
	issn = {0891-2017},
	shorttitle = {A {Survey} of {Word} {Reordering} in {Statistical} {Machine} {Translation}},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00245},
	doi = {10.1162/COLI\_a\_00245},
	abstract = {Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials.},
	urldate = {2016-03-14},
	journal = {Computational Linguistics},
	author = {Bisazza, Arianna and Federico, Marcello},
	month = mar,
	year = {2016},
	pages = {1--67},
	file = {Bisazza_Federico_2016_A Survey of Word Reordering in Statistical Machine Translation.pdf:/home/user/Zotero/storage/C77BCZV8/Bisazza_Federico_2016_A Survey of Word Reordering in Statistical Machine Translation.pdf:application/pdf;Computational Linguistics Snapshot:/home/user/Zotero/storage/T99JM3KJ/COLI_a_00245.html:text/html}
}

@article{bonyadi-particle-2016,
	title = {Particle swarm optimization for single objective continuous space problems: a review},
	issn = {1063-6560},
	shorttitle = {Particle swarm optimization for single objective continuous space problems},
	url = {http://dx.doi.org/10.1162/EVCO\_r\_00180},
	doi = {10.1162/EVCO\_r\_00180},
	abstract = {This paper reviews recent studies on Particle Swarm Optimization (PSO) algorithm. The review has been focused on high impact recent articles that have analyzed and/or modified PSO algorithms. This paper also presents some potential areas for future study.},
	urldate = {2016-03-14},
	journal = {Evolutionary Computation},
	author = {Bonyadi, Mohammad Reza and Michalewicz, Zbigniew},
	month = mar,
	year = {2016},
	file = {Evolutionary Computation Snapshot:/home/user/Zotero/storage/8ZIP762S/EVCO_r_00180.html:text/html}
}

@article{rajan-recurrent-2016,
	title = {Recurrent {Network} {Models} of {Sequence} {Generation} and {Memory}},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001021/abstract},
	doi = {10.1016/j.neuron.2016.02.009},
	language = {English},
	number = {0},
	urldate = {2016-03-14},
	journal = {Neuron},
	author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
	month = mar,
	year = {2016},
	file = {Rajan et al_2016_Recurrent Network Models of Sequence Generation and Memory.pdf:/home/user/Zotero/storage/GSHGF2JC/Rajan et al_2016_Recurrent Network Models of Sequence Generation and Memory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HRSHB6PS/S0896-6273(16)00102-1.html:text/html}
}

@article{i-cancho-least-2003,
	title = {Least effort and the origins of scaling in human language},
	volume = {100},
	url = {http://www.pnas.org/content/100/3/788.short},
	number = {3},
	urldate = {2016-03-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {i Cancho, Ramon Ferrer and Solé, Ricard V.},
	year = {2003},
	pages = {788--791},
	file = {788.full.pdf:/home/user/Zotero/storage/TZBCK35H/788.full.pdf:application/pdf}
}

@article{dawid-game-2004,
	title = {Game theory, maximum entropy, minimum discrepancy and robust {Bayesian} decision theory},
	volume = {32},
	issn = {0090-5364},
	url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1091626173/},
	doi = {10.1214/009053604000000553},
	language = {en},
	number = {4},
	urldate = {2016-03-13},
	journal = {The Annals of Statistics},
	author = {Dawid, A. Philip and Gr�nwald, Peter D.},
	month = aug,
	year = {2004},
	pages = {1367--1433},
	file = {1091626173:/home/user/Zotero/storage/78WRG4FP/1091626173.pdf:application/pdf}
}

@article{pierron-quantifier-2015-1,
	title = {Quantifier {Alternation} for {Infinite} {Words}},
	url = {http://arxiv.org/abs/1511.09011},
	urldate = {2016-03-13},
	journal = {arXiv preprint arXiv:1511.09011},
	author = {Pierron, Théo and Place, Thomas and Zeitoun, Marc},
	year = {2015},
	file = {PPZ-FOSSACS16.pdf:/home/user/Zotero/storage/MVA59TGI/PPZ-FOSSACS16.pdf:application/pdf}
}

@misc{noauthor-zipfs-nodate,
	title = {Zipf's law from a communicative phase transition - {Springer}},
	url = {http://link.springer.com/article/10.1140%2Fepjb%2Fe2005-00340-y#/page-1},
	urldate = {2016-03-13},
	file = {Zipf's law from a communicative phase transition - Springer:/home/user/Zotero/storage/U26JECZT/10.html:text/html}
}

@article{turner-why-2016,
	title = {Why more is better: {Simultaneous} modeling of {EEG}, {fMRI}, and behavioral data},
	volume = {128},
	issn = {1053-8119},
	shorttitle = {Why more is better},
	url = {http://www.sciencedirect.com/science/article/pii/S105381191501143X},
	doi = {10.1016/j.neuroimage.2015.12.030},
	abstract = {The need to test a growing number of theories in cognitive science has led to increased interest in inferential methods that integrate multiple data modalities. In this manuscript, we show how a method for integrating three data modalities within a single framework provides (1) more detailed descriptions of cognitive processes and (2) more accurate predictions of unobserved data than less integrative methods. Specifically, we show how combining either EEG and fMRI with a behavioral model can perform substantially better than a behavioral-data-only model in both generative and predictive modeling analyses. We then show how a trivariate model – a model including EEG, fMRI, and behavioral data – outperforms bivariate models in both generative and predictive modeling analyses. Together, these results suggest that within an appropriate modeling framework, more data can be used to better constrain cognitive theory, and to generate more accurate predictions for behavioral and neural data.},
	urldate = {2016-03-13},
	journal = {NeuroImage},
	author = {Turner, Brandon M. and Rodriguez, Christian A. and Norcia, Tony M. and McClure, Samuel M. and Steyvers, Mark},
	month = mar,
	year = {2016},
	keywords = {Bayesian modeling, fMRI, EEG, Joint modeling framework, Linear Ballistic Accumulator model},
	pages = {96--115},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/MXRM9EAW/S105381191501143X.html:text/html;Turner et al_2016_Why more is better.pdf:/home/user/Zotero/storage/BZN34JRU/Turner et al_2016_Why more is better.pdf:application/pdf}
}

@article{jarvis-avian-2005,
	title = {Avian brains and a new understanding of vertebrate brain evolution},
	volume = {6},
	copyright = {© 2005 Nature Publishing Group},
	issn = {1471-003X},
	url = {http://www.nature.com/nrn/journal/v6/n2/full/nrn1606.html},
	doi = {10.1038/nrn1606},
	abstract = {We believe that names have a powerful influence on the experiments we do and the way in which we think. For this reason, and in the light of new evidence about the function and evolution of the vertebrate brain, an international consortium of neuroscientists has reconsidered the traditional, 100-year-old terminology that is used to describe the avian cerebrum. Our current understanding of the avian brain — in particular the neocortex-like cognitive functions of the avian pallium — requires a new terminology that better reflects these functions and the homologies between avian and mammalian brains.},
	language = {en},
	number = {2},
	urldate = {2016-03-13},
	journal = {Nature Reviews Neuroscience},
	author = {Jarvis, Erich D. and Güntürkün, Onur and Bruce, Laura and Csillag, András and Karten, Harvey and Kuenzel, Wayne and Medina, Loreta and Paxinos, George and Perkel, David J. and Shimizu, Toru and Striedter, Georg and Wild, J. Martin and Ball, Gregory F. and Dugas-Ford, Jennifer and Durand, Sarah E. and Hough, Gerald E. and Husband, Scott and Kubikova, Lubica and Lee, Diane W. and Mello, Claudio V. and Powers, Alice and Siang, Connie and Smulders, Tom V. and Wada, Kazuhiro and White, Stephanie A. and Yamamoto, Keiko and Yu, Jing and Reiner, Anton and Butler, Ann B.},
	month = feb,
	year = {2005},
	pages = {151--159},
	file = {Jarvis et al_2005_Avian brains and a new understanding of vertebrate brain evolution.pdf:/home/user/Zotero/storage/GUB5984R/Jarvis et al_2005_Avian brains and a new understanding of vertebrate brain evolution.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VGHFRT5R/nrn1606.html:text/html}
}

@article{clayton-avian-2015,
	title = {Avian {Models} for {Human} {Cognitive} {Neuroscience}: {A} {Proposal}},
	volume = {86},
	issn = {0896-6273},
	shorttitle = {Avian {Models} for {Human} {Cognitive} {Neuroscience}},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627315003748},
	doi = {10.1016/j.neuron.2015.04.024},
	abstract = {Research on avian cognitive neuroscience over the past two decades has revealed the avian brain to be a better model for understanding human cognition than previously thought, despite differences in the neuroarchitecture of avian and mammalian brains. The brain, behavior, and cognition of songbirds have provided an excellent model of human cognition in one domain, namely learning human language and the production of speech. There are other important behavioral candidates of avian cognition, however, notably the capacity of corvids to remember the past and plan for the future, as well as their ability to think about another’s perspective, and physical reasoning. We review this work and assess the evidence that the corvid brain can support such a cognitive architecture. We propose potential applications of these behavioral paradigms for cognitive neuroscience, including recent work on single-cell recordings and neuroimaging in corvids. Finally, we discuss their impact on understanding human developmental cognition.},
	number = {6},
	urldate = {2016-03-13},
	journal = {Neuron},
	author = {Clayton, Nicola S. and Emery, Nathan J.},
	month = jun,
	year = {2015},
	pages = {1330--1342},
	file = {Clayton_Emery_2015_Avian Models for Human Cognitive Neuroscience.pdf:/home/user/Zotero/storage/ARP5ZPBJ/Clayton_Emery_2015_Avian Models for Human Cognitive Neuroscience.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/US9N672Z/S0896627315003748.html:text/html}
}

@article{ji-latent-2016,
	title = {A {Latent} {Variable} {Recurrent} {Neural} {Network} for {Discourse} {Relation} {Language} {Models}},
	url = {http://arxiv.org/abs/1603.01913},
	abstract = {This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations that link adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. By marginalizing over latent discourse relations, it also yields a language model that improves on a strong recurrent neural network baseline.},
	urldate = {2016-03-11},
	journal = {arXiv:1603.01913 [cs]},
	author = {Ji, Yangfeng and Haffari, Gholamreza and Eisenstein, Jacob},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01913},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BZ5X65N8/1603.html:text/html;Ji et al_2016_A Latent Variable Recurrent Neural Network for Discourse Relation Language.pdf:/home/user/Zotero/storage/S3VC6F8P/Ji et al_2016_A Latent Variable Recurrent Neural Network for Discourse Relation Language.pdf:application/pdf}
}

@article{holland-mini-review:-nodate,
	title = {Mini-review: {Prediction} errors, attention and associative learning},
	issn = {1074-7427},
	shorttitle = {Mini-review},
	url = {http://www.sciencedirect.com/science/article/pii/S1074742716000514},
	doi = {10.1016/j.nlm.2016.02.014},
	abstract = {Most modern theories of associative learning emphasize a critical role for prediction error (PE, the difference between received and expected events). One class of theories, exemplified by the Rescorla–Wagner (1972) model, asserts that PE determines the effectiveness of the reinforcer or unconditioned stimulus (US): surprising reinforcers are more effective than expected ones. A second class, represented by the Pearce–Hall (1980) model, argues that PE determines the associability of conditioned stimuli (CSs), the rate at which they may enter into new learning: the surprising delivery or omission of a reinforcer enhances subsequent processing of the CSs that were present when PE was induced. In this mini-review we describe evidence, mostly from our laboratory, for PE-induced changes in the associability of both CSs and USs, and the brain systems involved in the coding, storage and retrieval of these altered associability values. This evidence favors a number of modifications to behavioral models of how PE influences event processing, and suggests the involvement of widespread brain systems in animals’ responses to PE.},
	urldate = {2016-03-11},
	journal = {Neurobiology of Learning and Memory},
	author = {Holland, Peter C. and Schiffino, Felipe L.},
	keywords = {Attention, Associability, Associative learning, Pearce–Hall model, Prediction error},
	file = {Holland_Schiffino_Mini-review.pdf:/home/user/Zotero/storage/3SEXFP2N/Holland_Schiffino_Mini-review.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/AJ9AZN6C/S1074742716000514.html:text/html}
}

@article{gunturkun-cognition-nodate,
	title = {Cognition without {Cortex}},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000425},
	doi = {10.1016/j.tics.2016.02.001},
	abstract = {Assumptions on the neural basis of cognition usually focus on cortical mechanisms. Birds have no cortex, but recent studies in parrots and corvids show that their cognitive skills are on par with primates. These cognitive findings are accompanied by neurobiological discoveries that reveal avian and mammalian forebrains are homologous, and show similarities in connectivity and function down to the cellular level. But because birds have a large pallium, but no cortex, a specific cortical architecture cannot be a requirement for advanced cognitive skills. During the long parallel evolution of mammals and birds, several neural mechanisms for cognition and complex behaviors may have converged despite an overall forebrain organization that is otherwise vastly different.},
	urldate = {2016-03-11},
	journal = {Trends in Cognitive Sciences},
	author = {Güntürkün, Onur and Bugnyar, Thomas},
	keywords = {prefrontal cortex, birds, cognitive skills, evolution, pallium},
	file = {Güntürkün_Bugnyar_Cognition without Cortex.pdf:/home/user/Zotero/storage/2MF5M9A3/Güntürkün_Bugnyar_Cognition without Cortex.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CBACW4I4/S1364661316000425.html:text/html}
}

@article{love-cognitive-nodate,
	title = {Cognitive {Models} as {Bridge} between {Brain} and {Behavior}},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000474},
	doi = {10.1016/j.tics.2016.02.006},
	abstract = {How can disparate neural and behavioral measures be integrated? Turner and colleagues propose joint modeling as a solution. Joint modeling mutually constrains the interpretation of brain and behavioral measures by exploiting their covariation structure. Simultaneous estimation allows for more accurate prediction than would be possible by considering these measures in isolation.},
	urldate = {2016-03-11},
	journal = {Trends in Cognitive Sciences},
	author = {Love, Bradley C.},
	file = {Love_Cognitive Models as Bridge between Brain and Behavior.pdf:/home/user/Zotero/storage/WCAG4UU7/Love_Cognitive Models as Bridge between Brain and Behavior.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/U647FE4A/S1364661316000474.html:text/html}
}

@article{khomtchouk-zipfs-2016,
	title = {Zipf's law emerges asymptotically during phase transitions in communicative systems},
	url = {http://arxiv.org/abs/1603.03153},
	abstract = {Zipf's law predicts a power-law relationship between word rank and frequency in language communication systems, and is widely reported in texts yet remains enigmatic as to its origins. Computer simulations have shown that language communication systems emerge at an abrupt phase transition in the fidelity of mappings between symbols and objects. Since the phase transition approximates the Heaviside or step function, we show that Zipfian scaling emerges asymptotically at high rank based on the Laplace transform which yields \$(1/r)(1-e{\textasciicircum}{-r})\$, where \$r\$ denotes rank. We thereby demonstrate that Zipf's law gradually emerges from the moment of phase transition in communicative systems. We show that this power-law scaling behavior explains the emergence of natural languages at phase transitions. We find that the emergence of Zipf's law during language communication suggests that the use of rare words in a lexicon (i.e., high \$r\$) is critical for the construction of an effective communicative system at the phase transition.},
	urldate = {2016-03-11},
	journal = {arXiv:1603.03153 [physics]},
	author = {Khomtchouk, Bohdan B. and Wahlestedt, Claes},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.03153},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/8ZKV249I/1603.html:text/html;Khomtchouk_Wahlestedt_2016_Zipf's law emerges asymptotically during phase transitions in communicative.pdf:/home/user/Zotero/storage/8PKGZZJ8/Khomtchouk_Wahlestedt_2016_Zipf's law emerges asymptotically during phase transitions in communicative.pdf:application/pdf}
}

@article{vanbinst-symbolic-2016,
	title = {Symbolic {Numerical} {Magnitude} {Processing} {Is} as {Important} to {Arithmetic} as {Phonological} {Awareness} {Is} to {Reading}},
	volume = {11},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0151045},
	doi = {10.1371/journal.pone.0151045},
	abstract = {In this article, we tested, using a 1-year longitudinal design, whether symbolic numerical magnitude processing or children’s numerical representation of Arabic digits, is as important to arithmetic as phonological awareness is to reading. Children completed measures of symbolic comparison, phonological awareness, arithmetic, reading at the start of third grade and the latter two were retested at the start of fourth grade. Cross-sectional and longitudinal correlations indicated that symbolic comparison was a powerful domain-specific predictor of arithmetic and that phonological awareness was a unique predictor of reading. Crucially, the strength of these independent associations was not significantly different. This indicates that symbolic numerical magnitude processing is as important to arithmetic development as phonological awareness is to reading and suggests that symbolic numerical magnitude processing is a good candidate for screening children at risk for developing mathematical difficulties.},
	number = {3},
	urldate = {2016-03-11},
	journal = {PLOS ONE},
	author = {Vanbinst, Kiran and Ansari, Daniel and Ghesquière, Pol and Smedt, Bert De},
	month = mar,
	year = {2016},
	keywords = {language, Phonology, Arabs, Arithmetic, Children, Phonemes, Regression analysis, Subtraction},
	pages = {e0151045},
	file = {Snapshot:/home/user/Zotero/storage/VVM6FMCI/article.html:text/html;Vanbinst et al_2016_Symbolic Numerical Magnitude Processing Is as Important to Arithmetic as.pdf:/home/user/Zotero/storage/EBKT39DU/Vanbinst et al_2016_Symbolic Numerical Magnitude Processing Is as Important to Arithmetic as.pdf:application/pdf}
}

@article{starr-when-2016,
	title = {When {DNA} is lying},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6278/1133},
	doi = {10.1126/science.351.6278.1133},
	abstract = {Greg Hampikian, who holds joint appointments in biology and criminal justice at Boise State University and heads the Idaho Innocence Project, has helped free innocent people from prison for more than 20 years by exploiting the power of DNA forensics—or by exposing its pitfalls. DNA evidence is virtually unassailable, and it has helped exonerate hundreds of wrongly convicted people. But new techniques make it possible to detect DNA at levels hundreds or even thousands of times lower than 30 years ago. This heightened sensitivity can create false positives and land innocent people in jail.
DNA analysis has helped free thousands of wrongly convicted people. But sometimes DNA lands innocent people in prison, Greg Hampikian warns.
DNA analysis has helped free thousands of wrongly convicted people. But sometimes DNA lands innocent people in prison, Greg Hampikian warns.},
	language = {en},
	number = {6278},
	urldate = {2016-03-11},
	journal = {Science},
	author = {Starr, Douglas},
	month = mar,
	year = {2016},
	pmid = {26965602},
	pages = {1133--1136},
	file = {Snapshot:/home/user/Zotero/storage/7744VSM5/1133.html:text/html;Starr_2016_When DNA is lying.pdf:/home/user/Zotero/storage/DRBD7XQM/Starr_2016_When DNA is lying.pdf:application/pdf}
}

@article{cuevas-simple-2016,
	title = {Simple universal models capture all classical spin physics},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6278/1180},
	doi = {10.1126/science.aab3326},
	abstract = {One model to rule them all?
The idea of finding an all-encompassing model to describe the world around us has appealed to generations of scientists. Although we are very far from this ultimate goal, more modest steps can be taken if we focus on particular problems. De las Cuevas and Cubitt used concepts borrowed from computer science to show that all classical spin models (introduced initially to study magnetism) can be solved by tackling a slightly more complex, universal model (see the Perspective by Wehner). Thanks to these findings, it may be possible to physically simulate systems with complex interactions, using the well-understood two-dimensional Ising model with fields.
Science, this issue p. 1180; see also p. 1156
Spin models are used in many studies of complex systems because they exhibit rich macroscopic behavior despite their microscopic simplicity. Here, we prove that all the physics of every classical spin model is reproduced in the low-energy sector of certain “universal models,” with at most polynomial overhead. This holds for classical models with discrete or continuous degrees of freedom. We prove necessary and sufficient conditions for a spin model to be universal and show that one of the simplest and most widely studied spin models, the two-dimensional Ising model with fields, is universal. Our results may facilitate physical simulations of Hamiltonians with complex interactions.
The two-dimensional Ising model with fields can simulate all classical spin models. [Also see Perspective by Wehner]
The two-dimensional Ising model with fields can simulate all classical spin models. [Also see Perspective by Wehner]},
	language = {en},
	number = {6278},
	urldate = {2016-03-11},
	journal = {Science},
	author = {Cuevas, Gemma De las and Cubitt, Toby S.},
	month = mar,
	year = {2016},
	pmid = {26965624},
	pages = {1180--1183},
	file = {Cuevas_Cubitt_2016_Simple universal models capture all classical spin physics.pdf:/home/user/Zotero/storage/P2ZWIT7I/Cuevas_Cubitt_2016_Simple universal models capture all classical spin physics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/T4PW4QPS/1180.html:text/html}
}

@article{ma-end--end-2016,
	title = {End-to-end {Sequence} {Labeling} via {Bi}-directional {LSTM}-{CNNs}-{CRF}},
	url = {http://arxiv.org/abs/1603.01354},
	abstract = {State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.},
	urldate = {2016-03-11},
	journal = {arXiv:1603.01354 [cs, stat]},
	author = {Ma, Xuezhe and Hovy, Eduard},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01354},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4KGFCQEA/1603.html:text/html;Ma_Hovy_2016_End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.pdf:/home/user/Zotero/storage/23J3G453/Ma_Hovy_2016_End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.pdf:application/pdf}
}

@article{mcqueen-megaman:-2016,
	title = {megaman: {Manifold} {Learning} with {Millions} of points},
	shorttitle = {megaman},
	url = {http://arxiv.org/abs/1603.02763},
	abstract = {Manifold Learning is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus manifold learning algorithms are, at least in theory, most applicable to high-dimensional data and sample sizes to enable accurate estimation of the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator and the estimation of the embedding distortion by the Riemannian metric method. In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey --- consisting of 0.6 million samples in 3750-dimensions --- a task which has not previously been possible.},
	urldate = {2016-03-11},
	journal = {arXiv:1603.02763 [cs, stat]},
	author = {McQueen, James and Meila, Marina and VanderPlas, Jacob and Zhang, Zhongyue},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.02763},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Computational Geometry},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2D6FBHU7/1603.html:text/html;McQueen et al_2016_megaman.pdf:/home/user/Zotero/storage/D72SQ3MU/McQueen et al_2016_megaman.pdf:application/pdf}
}

@article{diekert-solutions-2016,
	title = {Solutions of {Word} {Equations} over {Partially} {Commutative} {Structures}},
	url = {http://arxiv.org/abs/1603.02966},
	abstract = {We give NSPACE(n log n) algorithms solving the following decision problems. Satisfiability: Is the given equation over a free partially commutative monoid with involution (resp. a free partially commutative group) solvable? Finiteness: Are there only finitely many solutions of such an equation? PSPACE algorithms with worse complexities for the first problem are known, but so far, a PSPACE algorithm for the second problem was out of reach. Our results are much stronger: Given such an equation, its solutions form an EDT0L language effectively representable in NSPACE(n log n). In particular, we give an effective description of the set of all solutions for equations with constraints in free partially commutative monoids and groups.},
	urldate = {2016-03-11},
	journal = {arXiv:1603.02966 [cs]},
	author = {Diekert, Volker and Jeż, Artur and Kufleitner, Manfred},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.02966},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, F.4.3, F.2.2, F.4.2},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/W8FAZWBQ/1603.html:text/html;Diekert et al_2016_Solutions of Word Equations over Partially Commutative Structures.pdf:/home/user/Zotero/storage/ITAQ66BR/Diekert et al_2016_Solutions of Word Equations over Partially Commutative Structures.pdf:application/pdf}
}

@article{baker-statisticians-2016,
	title = {Statisticians issue warning over misuse of {P} values},
	volume = {531},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/nature.2016.19503},
	doi = {10.1038/nature.2016.19503},
	number = {7593},
	urldate = {2016-03-11},
	journal = {Nature},
	author = {Baker, Monya},
	month = mar,
	year = {2016},
	pages = {151--151}
}

@article{kuzma-policy:-2016,
	title = {Policy: {Reboot} the debate on genetic engineering},
	volume = {531},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Policy},
	url = {http://www.nature.com/doifinder/10.1038/531165a},
	doi = {10.1038/531165a},
	number = {7593},
	urldate = {2016-03-11},
	journal = {Nature},
	author = {Kuzma, Jennifer},
	month = mar,
	year = {2016},
	pages = {165--167}
}

@article{sengupta-towards-2016,
	title = {Towards a {Neuronal} {Gauge} {Theory}},
	volume = {14},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002400},
	doi = {10.1371/journal.pbio.1002400},
	abstract = {This Essay presents a formalism that not only provides a quantitative framework for modelling neural activity but also shows that neuronal dynamics across scales are described by the same principle.},
	number = {3},
	urldate = {2016-03-11},
	journal = {PLOS Biol},
	author = {Sengupta, Biswa and Tozzi, Arturo and Cooray, Gerald K. and Douglas, Pamela K. and Friston, Karl J.},
	month = mar,
	year = {2016},
	keywords = {Attention, Probability distribution, Differential geometry, Free energy, Manifolds, Nervous system, Sensory perception, Symmetry},
	pages = {e1002400},
	file = {Sengupta et al_2016_Towards a Neuronal Gauge Theory.pdf:/home/user/Zotero/storage/V7S5JSPW/Sengupta et al_2016_Towards a Neuronal Gauge Theory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/S5XXNTJ7/article.html:text/html}
}

@article{cohen-parsing-2015,
	title = {Parsing {Linear} {Context}-{Free} {Rewriting} {Systems} with {Fast} {Matrix} {Multiplication}},
	url = {http://arxiv.org/abs/1504.08342},
	abstract = {We describe a matrix multiplication recognition algorithm for a subset of binary linear context-free rewriting systems (LCFRS) with running time \$O(n{\textasciicircum}{\omega d})\$ where \$M(m) = O(m{\textasciicircum}{\omega})\$ is the running time for \$m \times m\$ matrix multiplication and \$d\$ is the "contact rank" of the LCFRS -- the maximal number of combination and non-combination points that appear in the grammar rules. We also show that this algorithm can be used as a subroutine to get a recognition algorithm for general binary LCFRS with running time \$O(n{\textasciicircum}{\omega d + 1})\$. The currently best known \$\omega\$ is smaller than \$2.38\$. Our result provides another proof for the best known result for parsing mildly context sensitive formalisms such as combinatory categorial grammars, head grammars, linear indexed grammars, and tree adjoining grammars, which can be parsed in time \$O(n{\textasciicircum}{4.76})\$. It also shows that inversion transduction grammars can be parsed in time \$O(n{\textasciicircum}{5.76})\$. In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing.},
	urldate = {2016-03-11},
	journal = {arXiv:1504.08342 [cs]},
	author = {Cohen, Shay B. and Gildea, Daniel},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.08342},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C2N8N8EU/1504.html:text/html;Cohen_Gildea_2015_Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication.pdf:/home/user/Zotero/storage/RT9TNMSF/Cohen_Gildea_2015_Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication.pdf:application/pdf}
}

@article{osborne-encoding-2015,
	title = {Encoding {Prior} {Knowledge} with {Eigenword} {Embeddings}},
	url = {http://arxiv.org/abs/1509.01007},
	abstract = {Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.},
	urldate = {2016-03-11},
	journal = {arXiv:1509.01007 [cs]},
	author = {Osborne, Dominique and Narayan, Shashi and Cohen, Shay B.},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.01007},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BXC5DERH/1509.html:text/html;Osborne et al_2015_Encoding Prior Knowledge with Eigenword Embeddings.pdf:/home/user/Zotero/storage/I2299WE2/Osborne et al_2015_Encoding Prior Knowledge with Eigenword Embeddings.pdf:application/pdf}
}

@article{cui-lstm-2015,
	title = {{LSTM} {Neural} {Reordering} {Feature} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1512.00177},
	abstract = {Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.},
	urldate = {2016-03-11},
	journal = {arXiv:1512.00177 [cs]},
	author = {Cui, Yiming and Wang, Shijin and Li, Jianfeng and Wang, Yuguang},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00177},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/SDXFNPX5/1512.html:text/html}
}

@article{place-separating-2014-1,
	title = {Separating {Regular} {Languages} with {First}-{Order} {Logic}},
	url = {http://arxiv.org/abs/1402.3277},
	doi = {10.2168/LMCS-12(1:5)2016},
	abstract = {Given two languages, a separator is a third language that contains the first one and is disjoint from the second one. We investigate the following decision problem: given two regular input languages of finite words, decide whether there exists a first-order definable separator. We prove that in order to answer this question, sufficient information can be extracted from semigroups recognizing the input languages, using a fixpoint computation. This yields an EXPTIME algorithm for checking first-order separability. Moreover, the correctness proof of this algorithm yields a stronger result, namely a description of a possible separator. Finally, we generalize this technique to answer the same question for regular languages of infinite words.},
	urldate = {2016-03-11},
	journal = {arXiv:1402.3277 [cs]},
	author = {Place, Thomas and Zeitoun, Marc},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.3277},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IDKPBG7M/1402.html:text/html;Place_Zeitoun_2014_Separating Regular Languages with First-Order Logic.pdf:/home/user/Zotero/storage/QTFN2XPG/Place_Zeitoun_2014_Separating Regular Languages with First-Order Logic.pdf:application/pdf}
}

@article{townsend-pymanopt:-2016,
	title = {Pymanopt: {A} {Python} {Toolbox} for {Manifold} {Optimization} using {Automatic} {Differentiation}},
	shorttitle = {Pymanopt},
	url = {http://128.84.21.199/abs/1603.03236?context=cs.LG},
	urldate = {2016-03-11},
	author = {Townsend, James and Koep, Niklas and Weichwald, Sebastian},
	month = mar,
	year = {2016},
	file = {Snapshot:/home/user/Zotero/storage/IDI33DVN/1603.html:text/html;Townsend et al_2016_Pymanopt.pdf:/home/user/Zotero/storage/4C4XMTEX/Townsend et al_2016_Pymanopt.pdf:application/pdf}
}

@article{grunwald-game-2004,
	title = {Game theory, maximum entropy, minimum discrepancy and robust {Bayesian} decision theory},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1091626173},
	doi = {10.1214/009053604000000553},
	abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss. Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other. Although Topsøe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. We here generalize this theory to apply to arbitrary decision problems and loss functions. We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context. This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts. We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions. This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback–Leibler divergence (the “redundancy-capacity theorem” of information theory). For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions. We use this theory to introduce a new concept of “generalized exponential family” linked to the specific decision problem under consideration, and we demonstrate that this shares many of the properties of standard exponential families. Finally, we show that the existence of an equilibrium in our game can be rephrased in terms of a “Pythagorean property” of the related divergence, thus generalizing previously announced results for Kullback–Leibler and Bregman divergences.},
	number = {4},
	urldate = {2016-03-11},
	journal = {The Annals of Statistics},
	author = {Grünwald, Peter D. and Dawid, A. Philip},
	month = aug,
	year = {2004},
	mrnumber = {MR2089128},
	zmnumber = {1048.62008},
	keywords = {duality, Additive model, Bayes act, Bregman divergence, Brier score, convexity, equalizer rule, exponential family, Gamma-minimax, generalized exponential family, Kullback–Leibler divergence, logarithmic score, maximin, mean-value constraints, minimax, mutual information, Pythagorean property, redundancy-capacity theorem, relative entropy, saddle-point, scoring rule, specific entropy, uncertainty function, zero–one loss},
	pages = {1367--1433},
	file = {Snapshot:/home/user/Zotero/storage/9MJ4VXWR/1091626173.html:text/html}
}

@article{reichle-comparing-2000-1,
	title = {Comparing the {EZ} reader model to other models of eye movement control in reading},
	url = {http://cogprints.org/1169/},
	urldate = {2016-03-10},
	author = {Reichle, Rayner},
	year = {2000},
	file = {raynerbbs.pdf:/home/user/Zotero/storage/RSHQ4Z4V/raynerbbs.pdf:application/pdf}
}

@article{reichle-testing-2011-1,
	title = {Testing an assumption of the {E}-{Z} {Reader} model of eye-movement control during reading: {Using} event-related potentials to examine the familiarity check: {E}-{Z} {Reader} and event-related potentials},
	volume = {48},
	issn = {00485772},
	shorttitle = {Testing an assumption of the {E}-{Z} {Reader} model of eye-movement control during reading},
	url = {http://doi.wiley.com/10.1111/j.1469-8986.2011.01169.x},
	doi = {10.1111/j.1469-8986.2011.01169.x},
	language = {en},
	number = {7},
	urldate = {2016-03-10},
	journal = {Psychophysiology},
	author = {Reichle, Erik D. and Tokowicz, Natasha and Liu, Ying and Perfetti, Charles A.},
	month = jul,
	year = {2011},
	pages = {993--1003},
	file = {Testing an assumption of the EZ Reader model of eyemovement control during reading\: Using eventrelated potentials to examine the familiarity check - Reichle et al Psychophysiology.pdf:/home/user/Zotero/storage/4Q9EU45E/Reichle et al Psychophysiology.pdf:application/pdf}
}

@article{nilsson-computational-2012,
	title = {Computational {Models} of {Eye} {Movements} in {Reading}: {A} {Data}-{Driven} {Approach} to the {Eye}-{Mind} {Link}},
	shorttitle = {Computational {Models} of {Eye} {Movements} in {Reading}},
	url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:484385},
	urldate = {2016-03-10},
	author = {Nilsson, Mattias},
	year = {2012},
	file = {Computational Models of Eye Movements in Reading\: A Data-Driven Approach to the Eye-Mind Link - FULLTEXT01.pdf:/home/user/Zotero/storage/7JPZDR5F/FULLTEXT01.pdf:application/pdf}
}

@inproceedings{nilsson-learning-2009,
	title = {Learning where to look: {Modeling} eye movements in reading},
	shorttitle = {Learning where to look},
	url = {http://dl.acm.org/citation.cfm?id=1596392},
	urldate = {2016-03-10},
	booktitle = {Proceedings of the {Thirteenth} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Nilsson, Mattias and Nivre, Joakim},
	year = {2009},
	pages = {93--101},
	file = {Learning Where to Look\: Modeling Eye Movements in Reading - W09-1113:/home/user/Zotero/storage/VINC52K5/W09-1113.pdf:application/pdf}
}

@book{bottou-proceedings-2009,
	address = {Madison},
	title = {Proceedings / {Twenty}-{Sixth} {International} {Conference} on {Machine} {Learning}: [{Montréal}, {Canada}, {June} 14 - 18, 2009 ; proceedings]},
	isbn = {978-1-60558-516-1},
	shorttitle = {Proceedings / {Twenty}-{Sixth} {International} {Conference} on {Machine} {Learning}},
	language = {eng},
	editor = {Bottou, Léon and {International Machine Learning Society}},
	year = {2009},
	keywords = {Kongress, Maschinelles Lernen},
	file = {bengio11b.pdf:/home/user/Zotero/storage/F4N968VG/bengio11b.pdf:application/pdf}
}

@article{gu-traversing-2015,
	title = {Traversing knowledge graphs in vector space},
	url = {http://arxiv.org/abs/1506.01094},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1506.01094},
	author = {Gu, Kelvin and Miller, John and Liang, Percy},
	year = {2015},
	file = {guu2015traversing.pdf:/home/user/Zotero/storage/NH5QX6TM/guu2015traversing.pdf:application/pdf}
}

@inproceedings{berant-semantic-2014,
	title = {Semantic {Parsing} via {Paraphrasing}.},
	url = {http://www.anthology.aclweb.org/P/P14/P14-1133.pdf},
	urldate = {2016-03-10},
	booktitle = {{ACL} (1)},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2014},
	pages = {1415--1425},
	file = {berant14paraphrasing.pdf:/home/user/Zotero/storage/F64D3VD2/berant14paraphrasing.pdf:application/pdf}
}

@inproceedings{wang-estimating-2015,
	title = {Estimating {Mixture} {Models} via {Mixtures} of {Polynomials}},
	url = {http://papers.nips.cc/paper/5702-real-time-decoding-of-an-integrate-and-fire-encoder},
	urldate = {2016-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Sida and Chaganty, Arun Tejasvi and Liang, Percy S.},
	year = {2015},
	pages = {487--495},
	file = {Estimating Mixture Models via Mixtures of Polynomials - wang2015polynomial.pdf:/home/user/Zotero/storage/72FV3BKN/wang2015polynomial.pdf:application/pdf}
}

@article{luong-parsing-2013,
	title = {Parsing entire discourses as very long strings: {Capturing} topic continuity in grounded language learning},
	volume = {1},
	shorttitle = {Parsing entire discourses as very long strings},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/113},
	urldate = {2016-03-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Luong, Minh-Thang and Frank, Michael C. and Johnson, Mark},
	year = {2013},
	pages = {315--326},
	file = {tacl13_social.pdf:/home/user/Zotero/storage/KEJ2PGC2/tacl13_social.pdf:application/pdf}
}

@article{berant-imitation-2015,
	title = {Imitation {Learning} of {Agenda}-based {Semantic} {Parsers}},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/646},
	urldate = {2016-03-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2015},
	pages = {545--558},
	file = {agenda-tacl2015.pdf:/home/user/Zotero/storage/XCWBEKS7/agenda-tacl2015.pdf:application/pdf}
}

@inproceedings{steinhardt-learning-2015,
	title = {Learning with {Relaxed} {Supervision}},
	url = {http://papers.nips.cc/paper/5683-learning-with-relaxed-supervision},
	urldate = {2016-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Steinhardt, Jacob and Liang, Percy S.},
	year = {2015},
	pages = {2809--2817},
	file = {relaxed-nips2015.pdf:/home/user/Zotero/storage/VUF3TX5J/relaxed-nips2015.pdf:application/pdf}
}

@article{chaganty-estimating-nodate,
	title = {Estimating {Mixture} {Models} via {Mixtures} of {Polynomials}},
	url = {http://www-cs.stanford.edu/~pliang/papers/polynomial-nips2015.pdf},
	urldate = {2016-03-10},
	author = {Chaganty, Sida I. Wang Arun Tejasvi and Liang, Percy},
	file = {polynomial-nips2015.pdf:/home/user/Zotero/storage/AXANM24D/polynomial-nips2015.pdf:application/pdf}
}

@inproceedings{werling---job-2015,
	title = {On-the-job learning with bayesian decision theory},
	url = {http://papers.nips.cc/paper/5860-on-the-job-learning-with-bayesian-decision-theory},
	urldate = {2016-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Werling, Keenon and Chaganty, Arun Tejasvi and Liang, Percy S. and Manning, Christopher D.},
	year = {2015},
	pages = {3447--3455},
	file = {onthejob-nips2015.pdf:/home/user/Zotero/storage/G7E6PFKB/onthejob-nips2015.pdf:application/pdf}
}

@article{gu-traversing-2015-1,
	title = {Traversing knowledge graphs in vector space},
	url = {http://arxiv.org/abs/1506.01094},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1506.01094},
	author = {Gu, Kelvin and Miller, John and Liang, Percy},
	year = {2015},
	file = {1506.01094.pdf:/home/user/Zotero/storage/CFDWW8CJ/1506.01094.pdf:application/pdf}
}

@article{pasupat-compositional-2015,
	title = {Compositional semantic parsing on semi-structured tables},
	url = {http://arxiv.org/abs/1508.00305},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1508.00305},
	author = {Pasupat, Panupong and Liang, Percy},
	year = {2015},
	file = {compositional-acl2015.pdf:/home/user/Zotero/storage/4ZG24C9P/compositional-acl2015.pdf:application/pdf}
}

@article{misra-environment-driven-2015,
	title = {Environment-{Driven} {Lexicon} {Induction} for {High}-{Level} {Instructions}},
	url = {http://www.anthology.aclweb.org/P/P15/P15-1096.pdf},
	urldate = {2016-03-10},
	journal = {ACL (1)},
	author = {Misra, Dipendra K. and Tao, Kejia and Liang, Percy and Saxena, Ashutosh},
	year = {2015},
	pages = {992--1002},
	file = {environment-acl2015.pdf:/home/user/Zotero/storage/TENSXWA5/environment-acl2015.pdf:application/pdf}
}

@article{steinhardt-reified-2015,
	title = {Reified {Context} {Models}},
	url = {http://arxiv.org/abs/1502.06665},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1502.06665},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {Reified Context Models - 1502.06665.pdf:/home/user/Zotero/storage/XEI48VM5/1502.06665.pdf:application/pdf}
}

@article{steinhardt-learning-2015-1,
	title = {Learning fast-mixing models for structured prediction},
	url = {http://arxiv.org/abs/1502.06668},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1502.06668},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {Learning Fast-Mixing Models for Structured Prediction - 1502.06668.pdf:/home/user/Zotero/storage/AAHNHZND/1502.06668.pdf:application/pdf}
}

@inproceedings{steinhardt-adaptivity-2014,
	title = {Adaptivity and optimism: {An} improved exponentiated gradient algorithm},
	shorttitle = {Adaptivity and optimism},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2\_steinhardtb14},
	urldate = {2016-03-10},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2014},
	pages = {1593--1601},
	file = {Adaptivity and Optimism\: An Improved Exponentiated Gradient Algorithm - paper.pdf:/home/user/Zotero/storage/W5GVU9IB/paper.pdf:application/pdf}
}

@inproceedings{steinhardt-flexible-2012,
	title = {Flexible martingale priors for deep hierarchies},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2012\_SteinhardtG12.pdf},
	urldate = {2016-03-10},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Steinhardt, Jacob and Ghahramani, Zoubin},
	year = {2012},
	pages = {1108--1116},
	file = {paper.pdf:/home/user/Zotero/storage/J9EQA8P3/paper.pdf:application/pdf}
}

@inproceedings{shi-learning-2015,
	title = {Learning {Where} to {Sample} in {Structured} {Prediction}.},
	url = {http://www-cs.stanford.edu/~jsteinhardt/publications/adainfer/paper.pdf},
	urldate = {2016-03-10},
	booktitle = {{AISTATS}},
	author = {Shi, Tianlin and Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {sample-aistats2015.pdf:/home/user/Zotero/storage/MX9U4BPT/sample-aistats2015.pdf:application/pdf}
}

@article{kuleshov-tensor-2015,
	title = {Tensor factorization via matrix factorization},
	url = {http://arxiv.org/abs/1501.07320},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1501.07320},
	author = {Kuleshov, Volodymyr and Chaganty, Arun Tejasvi and Liang, Percy},
	year = {2015},
	file = {1501.07320.pdf:/home/user/Zotero/storage/W8I62A63/1501.07320.pdf:application/pdf}
}

@article{kuleshov-tensor-2015-1,
	title = {Tensor factorization via matrix factorization},
	url = {http://arxiv.org/abs/1501.07320},
	urldate = {2016-03-10},
	journal = {arXiv preprint arXiv:1501.07320},
	author = {Kuleshov, Volodymyr and Chaganty, Arun Tejasvi and Liang, Percy},
	year = {2015},
	file = {1501.07320.pdf:/home/user/Zotero/storage/Q8T2BPMF/1501.07320.pdf:application/pdf}
}

@article{liang-bringing-2015-1,
	title = {Bringing machine learning and compositional semantics together},
	volume = {1},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-linguist-030514-125312},
	number = {1},
	urldate = {2016-03-10},
	journal = {Annu. Rev. Linguist.},
	author = {Liang, Percy and Potts, Christopher},
	year = {2015},
	pages = {355--376},
	file = {liang-potts-semantics.pdf:/home/user/Zotero/storage/P8MFWWWZ/liang-potts-semantics.pdf:application/pdf}
}

@inproceedings{steinhardt-filtering-2014,
	title = {Filtering with abstract particles},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c1\_steinhardt14},
	urldate = {2016-03-10},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2014},
	pages = {727--735},
	file = {filtering-icml2014.pdf:/home/user/Zotero/storage/9EUJJZBT/filtering-icml2014.pdf:application/pdf}
}

@article{rayner-eye-2009-2,
	title = {Eye movements and attention in reading, scene perception, and visual search},
	volume = {62},
	issn = {1747-0226},
	doi = {10.1080/17470210902816461},
	abstract = {Eye movements are now widely used to investigate cognitive processes during reading, scene perception, and visual search. In this article, research on the following topics is reviewed with respect to reading: (a) the perceptual span (or span of effective vision), (b) preview benefit, (c) eye movement control, and (d) models of eye movements. Related issues with respect to eye movements during scene perception and visual search are also reviewed. It is argued that research on eye movements during reading has been somewhat advanced over research on eye movements in scene perception and visual search and that some of the paradigms developed to study reading should be more widely adopted in the study of scene perception and visual search. Research dealing with "real-world" tasks and research utilizing the visual-world paradigm are also briefly discussed.},
	language = {eng},
	number = {8},
	journal = {Quarterly Journal of Experimental Psychology (2006)},
	author = {Rayner, Keith},
	month = aug,
	year = {2009},
	pmid = {19449261},
	keywords = {Humans, Pattern Recognition, Visual, Eye Movements, Reading, Models, Psychological, Attention, visual perception, Fixation, Ocular, Neuropsychological Tests, Photic Stimulation, Reaction Time, Visual Fields},
	pages = {1457--1506}
}

@article{rayner-eye-2009-3,
	title = {Eye {Movements} in {Reading}: {Models} and {Data}},
	volume = {2},
	issn = {1995-8692},
	shorttitle = {Eye {Movements} in {Reading}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2906818/},
	abstract = {Models of eye movement control in reading and their impact on the field are discussed. Differences between the E-Z Reader model and the SWIFT model are reviewed, as are benchmark data that need to be accounted for by any model of eye movement control. Predictions made by the models and how models can sometimes account for counterintuitive findings are also discussed. Finally, the role of models and data in further understanding the reading process is considered.},
	number = {5},
	urldate = {2016-03-09},
	journal = {Journal of eye movement research},
	author = {Rayner, Keith},
	month = apr,
	year = {2009},
	pmid = {20664810},
	pmcid = {PMC2906818},
	pages = {1--10},
	file = {Rayner_2009_Eye Movements in Reading.pdf:/home/user/Zotero/storage/2WS957I8/Rayner_2009_Eye Movements in Reading.pdf:application/pdf}
}

@article{reichle-e-z-2003-1,
	title = {The {E}-{Z} reader model of eye-movement control in reading: comparisons to other models},
	volume = {26},
	issn = {0140-525X},
	shorttitle = {The {E}-{Z} reader model of eye-movement control in reading},
	abstract = {The E-Z Reader model (Reichle et al. 1998; 1999) provides a theoretical framework for understanding how word identification, visual processing, attention, and oculomotor control jointly determine when and where the eyes move during reading. In this article, we first review what is known about eye movements during reading. Then we provide an updated version of the model (E-Z Reader 7) and describe how it accounts for basic findings about eye movement control in reading. We then review several alternative models of eye movement control in reading, discussing both their core assumptions and their theoretical scope. On the basis of this discussion, we conclude that E-Z Reader provides the most comprehensive account of eye movement control during reading. Finally, we provide a brief overview of what is known about the neural systems that support the various components of reading, and suggest how the cognitive constructs of our model might map onto this neural architecture.},
	language = {eng},
	number = {4},
	journal = {The Behavioral and Brain Sciences},
	author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
	month = aug,
	year = {2003},
	pmid = {15067951},
	keywords = {Humans, Eye Movements, Reading, Models, Psychological, Fixation, Ocular, Models, Neurological, Oculomotor Muscles, Verbal Behavior, Vision, Binocular},
	pages = {445--476; discussion 477--526}
}

@misc{zamakhshari--mufassal-nodate,
	title = {Al-{Mufassal}},
	url = {https://ia700404.us.archive.org/28/items/almufaalopusder00algoog/almufaalopusder00algoog.pdf},
	urldate = {2016-02-22},
	author = {Zamakhshari},
	file = {almufaalopusder00algoog.pdf:/home/user/Zotero/storage/FMVHWHX2/almufaalopusder00algoog.pdf:application/pdf}
}

@article{belov-distributions-2011,
	title = {Distributions of the {Kullback}-{Leibler} divergence with applications},
	volume = {64},
	issn = {0007-1102},
	doi = {10.1348/000711010X522227},
	abstract = {The Kullback-Leibler divergence (KLD) is a widely used method for measuring the fit of two distributions. In general, the distribution of the KLD is unknown. Under reasonable assumptions, common in psychometrics, the distribution of the KLD is shown to be asymptotically distributed as a scaled (non-central) chi-square with one degree of freedom or a scaled (doubly non-central) F. Applications of the KLD for detecting heterogeneous response data are discussed with particular emphasis on test security.},
	language = {eng},
	number = {Pt 2},
	journal = {The British Journal of Mathematical and Statistical Psychology},
	author = {Belov, Dmitry I. and Armstrong, Ronald D.},
	month = may,
	year = {2011},
	pmid = {21492134},
	keywords = {Humans, Bayes Theorem, Chi-Square Distribution, Educational Measurement, Genetic Association Studies, Magnetic Resonance Imaging, Poisson Distribution, Probability Theory, Psychological Tests, Sample Size, Stochastic Processes},
	pages = {291--309}
}

@misc{noauthor-fixation-nodate,
	title = {Fixation location effects on fixation durations during reading: an inverted optimal viewing position effect},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698901001663},
	urldate = {2016-02-18},
	file = {Fixation location effects on fixation durations during reading\: an inverted optimal viewing position effect:/home/user/Zotero/storage/9JVVDAWE/S0042698901001663.html:text/html;PII\: S0042-6989(01)00166-3 - 1-s2.0-S0042698901001663-main.pdf:/home/user/Zotero/storage/88W2F2UC/1-s2.0-S0042698901001663-main.pdf:application/pdf}
}

@article{bennett-notes-2003,
	title = {Notes on {Landauer}'s principle, reversible computation, and {Maxwell}'s {Demon}},
	volume = {34},
	issn = {13552198},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S135521980300039X},
	doi = {10.1016/S1355-2198(03)00039-X},
	language = {en},
	number = {3},
	urldate = {2016-02-17},
	journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
	author = {Bennett, Charles H.},
	month = sep,
	year = {2003},
	pages = {501--510},
	file = {bennett03.pdf:/home/user/Zotero/storage/NDNP4447/bennett03.pdf:application/pdf}
}

@article{jakic-influence-2008,
	title = {The {Influence} {Of} {The} {Word} {Connection} {Type} {On} {The} {Facilitation} {Effect} {In} {The} {Lexical} {Decision} {Task}},
	url = {https://www.researchgate.net/profile/Victor\_Kuperman/publication/23997416\_Frequency\_distribution\_of\_uniphones\_diphones\_and\_triphones\_in\_spontaneous\_speech/links/0046352f9228dccfe4000000.pdf#page=95},
	urldate = {2016-02-17},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Jakić, Milena and Kostić, Aleksandar and Filipović-\DJur\d jević, Dušica},
	year = {2008},
	pages = {93},
	file = {Jakic_et_al.pdf:/home/user/Zotero/storage/VFAB6IJD/Jakic_et_al.pdf:application/pdf}
}

@article{blevins-entropy-based-2008,
	title = {An entropy-based measure of morphological information},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.3980&rep=rep1&type=pdf#page=19},
	urldate = {2016-02-17},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Blevins, James P. and Ackerman, Farrell and Buttery, Paula and Malouf, Robert},
	year = {2008},
	pages = {17},
	file = {Blevins_Ackerman (1).pdf:/home/user/Zotero/storage/2U6KEWNA/Blevins_Ackerman (1).pdf:application/pdf}
}

@article{johnson-evidence-2013,
	title = {Evidence for automatic accessing of constructional meaning: {Jabberwocky} sentences prime associated verbs},
	volume = {28},
	issn = {0169-0965, 1464-0732},
	shorttitle = {Evidence for automatic accessing of constructional meaning},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2012.717632},
	doi = {10.1080/01690965.2012.717632},
	language = {en},
	number = {10},
	urldate = {2016-02-17},
	journal = {Language and Cognitive Processes},
	author = {Johnson, Matt A. and Goldberg, Adele E.},
	month = dec,
	year = {2013},
	pages = {1439--1452},
	file = {13Jabberwocky-published.pdf:/home/user/Zotero/storage/JEGTDEP2/13Jabberwocky-published.pdf:application/pdf}
}

@article{zeschel-introduction-2008,
	title = {Introduction},
	volume = {19},
	issn = {0936-5907, 1613-3641},
	url = {http://www.degruyter.com/view/j/cogl.2008.19.issue-3/cogl.2008.013/cogl.2008.013.xml},
	doi = {10.1515/COGL.2008.013},
	number = {3},
	urldate = {2016-02-17},
	journal = {Cognitive Linguistics},
	author = {Zeschel, Arne},
	month = jan,
	year = {2008},
	file = {08AmbridgeGoldberg-islands.pdf:/home/user/Zotero/storage/S8MWUMXT/08AmbridgeGoldberg-islands.pdf:application/pdf}
}

@article{goldberg-corpus-2011,
	title = {Corpus evidence of the viability of statistical preemption},
	volume = {22},
	url = {http://www.degruyter.com/view/j/cogl.2011.22.issue-1/cogl.2011.006/cogl.2011.006.xml},
	number = {1},
	urldate = {2016-02-17},
	journal = {Cognitive Linguistics},
	author = {Goldberg, Adele E.},
	year = {2011},
	pages = {131--153},
	file = {11CogLingPreemption.pdf:/home/user/Zotero/storage/FEBZBW8E/11CogLingPreemption.pdf:application/pdf}
}

@article{rosseel-can-2015,
	title = {Can social psychological attitude measures be used to study language attitudes?-{A} case study exploring the {Personalized} {Implicit} {Association} {Test}},
	shorttitle = {Can social psychological attitude measures be used to study language attitudes?},
	url = {https://hsbiblio.uni-tuebingen.de/xmlui/handle/10900/67222},
	urldate = {2016-02-17},
	author = {Rosseel, Laura and Speelman, Dirk and Geeraerts, Dirk},
	year = {2015},
	file = {Rosseel_Speelman_Geeraerts.pdf:/home/user/Zotero/storage/AS7U2R8Q/Rosseel_Speelman_Geeraerts.pdf:application/pdf}
}

@inproceedings{willems-understanding-2016,
	title = {Understanding {PP} placement in written {Dutch}: a corpus-based multifactorial investigation of the principal syntactic, semantic and discursive determinants},
	shorttitle = {Understanding {PP} placement in written {Dutch}},
	url = {https://biblio.ugent.be/publication/7039502},
	urldate = {2016-02-17},
	booktitle = {6th {Conference} on {Quantitative} {Investigations} in {Theoretical} {Linguistics}},
	publisher = {Universität Tübingen},
	author = {Willems, Annelore and De Sutter, Gert},
	year = {2016},
	file = {Willems_DeSutter.pdf:/home/user/Zotero/storage/VPJ53TVN/Willems_DeSutter.pdf:application/pdf}
}

@article{goldberg-subtle-2015,
	title = {Subtle implicit language facts emerge from the functions of constructions},
	volume = {6},
	url = {https://adele.princeton.edu/files/2015/01/16Frontiers-UG-Goldberg-final.pdf},
	urldate = {2016-02-17},
	journal = {Frontiers in Psychology},
	author = {Goldberg, Adele Eva},
	year = {2015},
	pages = {2019},
	file = {16Frontiers-UG-Goldberg-final.pdf:/home/user/Zotero/storage/Q4G54KKB/16Frontiers-UG-Goldberg-final.pdf:application/pdf}
}

@inproceedings{liang-identifiability-2012,
	title = {Identifiability and unmixing of latent parse trees},
	url = {http://papers.nips.cc/paper/4655-identifiability-and-unmixing-of-latent-parse-trees},
	urldate = {2016-02-17},
	booktitle = {Advances in neural information processing systems},
	author = {Liang, Percy S. and Hsu, Daniel J. and Kakade, Sham M.},
	year = {2012},
	pages = {1511--1519},
	file = {identifiability-nips2012.pdf:/home/user/Zotero/storage/CGIFQDSQ/identifiability-nips2012.pdf:application/pdf}
}

@article{boyd-learning-2011,
	title = {Learning what not to say: {The} role of statistical preemption and categorization in a-adjective production},
	volume = {87},
	shorttitle = {Learning what not to say},
	url = {http://muse.jhu.edu/journals/language/v087/87.1.boyd.html},
	number = {1},
	urldate = {2016-02-17},
	journal = {Language},
	author = {Boyd, Jeremy K. and Goldberg, Adele E.},
	year = {2011},
	pages = {55--83},
	file = {BoydGoldberg2011-official1.pdf:/home/user/Zotero/storage/ZNDXAU4T/BoydGoldberg2011-official1.pdf:application/pdf}
}

@article{boyd-learning-2011-1,
	title = {Learning what not to say: {The} role of statistical preemption and categorization in a-adjective production},
	volume = {87},
	shorttitle = {Learning what not to say},
	url = {http://muse.jhu.edu/journals/language/v087/87.1.boyd.html},
	number = {1},
	urldate = {2016-02-17},
	journal = {Language},
	author = {Boyd, Jeremy K. and Goldberg, Adele E.},
	year = {2011},
	pages = {55--83},
	file = {BoydGoldberg2011-official.pdf:/home/user/Zotero/storage/67H7TD4Z/BoydGoldberg2011-official.pdf:application/pdf}
}

@inproceedings{gens-learning-2013,
	title = {Learning the structure of sum-product networks},
	url = {http://jmlr.org/proceedings/papers/v28/gens13.html},
	urldate = {2016-02-17},
	booktitle = {Proceedings of {The} 30th {International} {Conference} on {Machine} {Learning}},
	author = {Gens, Robert and Pedro, Domingos},
	year = {2013},
	pages = {873--880},
	file = {slspn.pdf:/home/user/Zotero/storage/QKJ7V98K/slspn.pdf:application/pdf}
}

@article{pijpops-argument-2015,
	title = {Argument alternations of the {Dutch} psych verbs},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67207},
	urldate = {2016-02-17},
	author = {Pijpops, Dirk and Speelman, Dirk},
	year = {2015},
	file = {Pijpops_Speelman.pdf:/home/user/Zotero/storage/JHPWKRVR/Pijpops_Speelman.pdf:application/pdf}
}

@misc{noauthor-[1511.06394]-nodate,
	title = {[1511.06394] {Geodesics} of learned representations},
	url = {http://arxiv.org/abs/1511.06394},
	urldate = {2016-02-17},
	file = {[1511.06394] Geodesics of learned representations:/home/user/Zotero/storage/7BUDEJ9W/1511.html:text/html}
}

@misc{noauthor-[1511.06499]-nodate,
	title = {[1511.06499] {Variational} {Gaussian} {Process}},
	url = {http://arxiv.org/abs/1511.06499},
	urldate = {2016-02-17},
	file = {[1511.06499] Variational Gaussian Process:/home/user/Zotero/storage/KEFK5FIR/1511.html:text/html}
}

@misc{noauthor-[1511.08228]-nodate,
	title = {[1511.08228] {Neural} {GPUs} {Learn} {Algorithms}},
	url = {http://arxiv.org/abs/1511.08228},
	urldate = {2016-02-17},
	file = {[1511.08228] Neural GPUs Learn Algorithms:/home/user/Zotero/storage/TGM3HVK8/1511.html:text/html}
}

@misc{noauthor-[1511.06747]-nodate,
	title = {[1511.06747] {Data}-{Dependent} {Path} {Normalization} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06747},
	urldate = {2016-02-17},
	file = {[1511.06747] Data-Dependent Path Normalization in Neural Networks:/home/user/Zotero/storage/ETKWZSK3/1511.html:text/html}
}

@misc{noauthor-[1511.06909]-nodate,
	title = {[1511.06909] {BlackOut}: {Speeding} up {Recurrent} {Neural} {Network} {Language} {Models} {With} {Very} {Large} {Vocabularies}},
	url = {http://arxiv.org/abs/1511.06909},
	urldate = {2016-02-17},
	file = {[1511.06909] BlackOut\: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies:/home/user/Zotero/storage/8QXRJB9V/1511.html:text/html}
}

@misc{noauthor-[1511.04561]-nodate,
	title = {[1511.04561] 8-{Bit} {Approximations} for {Parallelism} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1511.04561},
	urldate = {2016-02-17},
	file = {[1511.04561] 8-Bit Approximations for Parallelism in Deep Learning:/home/user/Zotero/storage/C8HIU6WV/1511.html:text/html}
}

@misc{noauthor-[1511.05493]-nodate,
	title = {[1511.05493] {Gated} {Graph} {Sequence} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	urldate = {2016-02-17},
	file = {[1511.05493] Gated Graph Sequence Neural Networks:/home/user/Zotero/storage/XMHZ9QV3/1511.html:text/html}
}

@misc{noauthor-[1511.07543]-nodate,
	title = {[1511.07543] {Convergent} {Learning}: {Do} different neural networks learn the same representations?},
	url = {http://arxiv.org/abs/1511.07543},
	urldate = {2016-02-17},
	file = {[1511.07543] Convergent Learning\: Do different neural networks learn the same representations?:/home/user/Zotero/storage/4WXX57X7/1511.html:text/html}
}

@misc{noauthor-[1511.06410]-nodate,
	title = {[1511.06410] {Better} {Computer} {Go} {Player} with {Neural} {Network} and {Long}-term {Prediction}},
	url = {http://arxiv.org/abs/1511.06410},
	urldate = {2016-02-17},
	file = {[1511.06410] Better Computer Go Player with Neural Network and Long-term Prediction:/home/user/Zotero/storage/AGGN49CI/1511.html:text/html}
}

@misc{noauthor-[1511.04143]-nodate,
	title = {[1511.04143] {Deep} {Reinforcement} {Learning} in {Parameterized} {Action} {Space}},
	url = {http://arxiv.org/abs/1511.04143},
	urldate = {2016-02-17},
	file = {[1511.04143] Deep Reinforcement Learning in Parameterized Action Space:/home/user/Zotero/storage/8UPZCP7X/1511.html:text/html}
}

@misc{noauthor-[1511.05176]-nodate,
	title = {[1511.05176] {MuProp}: {Unbiased} {Backpropagation} for {Stochastic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05176},
	urldate = {2016-02-17},
	file = {[1511.05176] MuProp\: Unbiased Backpropagation for Stochastic Neural Networks:/home/user/Zotero/storage/DAVZ295X/1511.html:text/html}
}

@misc{noauthor-[1511.04834]-nodate,
	title = {[1511.04834] {Neural} {Programmer}: {Inducing} {Latent} {Programs} with {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1511.04834},
	urldate = {2016-02-17},
	file = {[1511.04834] Neural Programmer\: Inducing Latent Programs with Gradient Descent:/home/user/Zotero/storage/CEW3JAEZ/1511.html:text/html}
}

@misc{noauthor-[1506.05011]-nodate,
	title = {[1506.05011] {Bayesian} representation learning with oracle constraints},
	url = {http://arxiv.org/abs/1506.05011},
	urldate = {2016-02-17},
	file = {[1506.05011] Bayesian representation learning with oracle constraints:/home/user/Zotero/storage/AEZGAPDX/1506.html:text/html}
}

@misc{noauthor-[1511.06067]-nodate,
	title = {[1511.06067] {Convolutional} neural networks with low-rank regularization},
	url = {http://arxiv.org/abs/1511.06067},
	urldate = {2016-02-17},
	file = {[1511.06067] Convolutional neural networks with low-rank regularization:/home/user/Zotero/storage/BCEPP7AS/1511.html:text/html}
}

@misc{noauthor-[1511.02793]-nodate,
	title = {[1511.02793] {Generating} {Images} from {Captions} with {Attention}},
	url = {http://arxiv.org/abs/1511.02793},
	urldate = {2016-02-17},
	file = {[1511.02793] Generating Images from Captions with Attention:/home/user/Zotero/storage/NBP2CPA2/1511.html:text/html}
}

@misc{noauthor-[1510.03009]-nodate,
	title = {[1510.03009] {Neural} {Networks} with {Few} {Multiplications}},
	url = {http://arxiv.org/abs/1510.03009},
	urldate = {2016-02-17},
	file = {[1510.03009] Neural Networks with Few Multiplications:/home/user/Zotero/storage/ZDRN5H9V/1510.html:text/html}
}

@misc{noauthor-[1510.00149]-nodate,
	title = {[1510.00149] {Deep} {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	url = {http://arxiv.org/abs/1510.00149},
	urldate = {2016-02-17},
	file = {[1510.00149] Deep Compression\: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding:/home/user/Zotero/storage/89MKAAD4/1510.html:text/html}
}

@misc{noauthor-[1509.00519]-nodate,
	title = {[1509.00519] {Importance} {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	urldate = {2016-02-17},
	file = {[1509.00519] Importance Weighted Autoencoders:/home/user/Zotero/storage/VIZTRQP8/1509.html:text/html}
}

@misc{noauthor-[1511.03677]-nodate,
	title = {[1511.03677] {Learning} to {Diagnose} with {LSTM} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.03677},
	urldate = {2016-02-17},
	file = {[1511.03677] Learning to Diagnose with LSTM Recurrent Neural Networks:/home/user/Zotero/storage/WRAZCEDT/1511.html:text/html}
}

@misc{noauthor-[1511.00830]-nodate,
	title = {[1511.00830] {The} {Variational} {Fair} {Autoencoder}},
	url = {http://arxiv.org/abs/1511.00830},
	urldate = {2016-02-17},
	file = {[1511.00830] The Variational Fair Autoencoder:/home/user/Zotero/storage/56VAXFPA/1511.html:text/html}
}

@article{mattos-recurrent-2015,
	title = {Recurrent {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1511.06644},
	abstract = {We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.},
	urldate = {2016-02-17},
	journal = {arXiv:1511.06644 [cs, stat]},
	author = {Mattos, César Lincoln C. and Dai, Zhenwen and Damianou, Andreas and Forth, Jeremy and Barreto, Guilherme A. and Lawrence, Neil D.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06644},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2XNFQE2M/1511.html:text/html;Mattos et al_2015_Recurrent Gaussian Processes.pdf:/home/user/Zotero/storage/6P9FPVUT/Mattos et al_2015_Recurrent Gaussian Processes.pdf:application/pdf}
}

@article{rojas-carulla-causal-2015-1,
	title = {Causal {Transfer} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1507.05333},
	abstract = {Methods of domain adaptation try to combine knowledge from several related domains (or tasks) to improve performance on a test domain. Inspired by causal methodology, we assume that the covariate shift assumption holds true for a subset of predictor variables: the conditional of the target variable given this subset of predictors is invariant over all tasks. We prove that in an adversarial setting using this subset for prediction is optimal if no examples from the test task are observed. For a specific scenario, in which tasks are drawn from a meta distribution, further optimality results are available. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
	urldate = {2016-02-17},
	journal = {arXiv:1507.05333 [stat]},
	author = {Rojas-Carulla, Mateo and Schölkopf, Bernhard and Turner, Richard and Peters, Jonas},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.05333},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/MRJXUZZP/1507.html:text/html;Rojas-Carulla et al_2015_Causal Transfer in Machine Learning.pdf:/home/user/Zotero/storage/KF5S2FWM/Rojas-Carulla et al_2015_Causal Transfer in Machine Learning.pdf:application/pdf}
}

@article{luong-multi-task-2015-2,
	title = {Multi-task {Sequence} to {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.06114},
	abstract = {Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three settings to multi-task sequence to sequence learning: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Additionally, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the context of multi-task learning: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.},
	urldate = {2016-02-17},
	journal = {arXiv:1511.06114 [cs, stat]},
	author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06114},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IHCE6WCJ/1511.html:text/html;Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:/home/user/Zotero/storage/W9R6TBKX/Luong et al_2015_Multi-task Sequence to Sequence Learning.pdf:application/pdf}
}

@article{feng-ensemble-2016,
	title = {Ensemble {Robustness} of {Deep} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1602.02389},
	abstract = {The question why deep learning algorithms perform so well in practice has puzzled machine learning theoreticians and practitioners alike. However, most of well-established approaches, such as hypothesis capacity, robustness or sparseness, have not provided complete explanations, due to the high complexity of the deep learning algorithms and their inherent randomness. In this work, we introduce a new approach -- ensemble robustness -- towards characterizing the generalization performance of generic deep learning algorithms. Ensemble robustness concerns robustness of the population of the hypotheses that may be output by a learning algorithm. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbation is bounded in average, or equivalently, the performance variance of the algorithm is small. Quantifying the ensemble robustness of various deep learning algorithms may be difficult analytically. However, extensive simulations for seven common deep learning algorithms for different network architectures provide supporting evidence for our claims. In addition, as an example for utilizing ensemble robustness, we propose a novel semi-supervised learning method that outperforms the state-of-the-art. Furthermore, our work explains the good performance of several published deep learning algorithms.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.02389 [cs, stat]},
	author = {Feng, Jiashi and Zahavy, Tom and Kang, Bingyi and Xu, Huan and Mannor, Shie},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02389},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NDCMFHUA/1602.html:text/html;Feng et al_2016_Ensemble Robustness of Deep Learning Algorithms.pdf:/home/user/Zotero/storage/4GQGTIIB/Feng et al_2016_Ensemble Robustness of Deep Learning Algorithms.pdf:application/pdf}
}

@article{xie-theory-2016,
	title = {A {Theory} of {Generative} {ConvNet}},
	url = {http://arxiv.org/abs/1602.03264},
	abstract = {The convolutional neural network (ConvNet or CNN) is a powerful discriminative learning machine. In this paper, we show that a generative random field model that we call generative ConvNet can be derived from the discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of exponential tilting of a reference distribution. Assuming re-lu non-linearity and Gaussian white noise reference distribution, we show that the generative ConvNet model contains a representational structure with multiple layers of binary activation variables. The model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables that reconstruct the mean of the Gaussian piece. The Langevin dynamics for synthesis is driven by the reconstruction error, and the corresponding gradient descent dynamics converges to a local energy minimum that is auto-encoding. As for learning, we show that the contrastive divergence learning tends to reconstruct the observed images. Finally, we show that the maximum likelihood learning algorithm can generate realistic natural images.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.03264 [cs, stat]},
	author = {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Ying Nian},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.03264},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/V5WNXE9U/1602.html:text/html;Xie et al_2016_A Theory of Generative ConvNet.pdf:/home/user/Zotero/storage/EV43XZT2/Xie et al_2016_A Theory of Generative ConvNet.pdf:application/pdf}
}

@article{loreto-emergence-2016,
	title = {On the emergence of syntactic structures: quantifying and modelling duality of patterning},
	shorttitle = {On the emergence of syntactic structures},
	url = {http://arxiv.org/abs/1602.03661},
	abstract = {The complex organization of syntax in hierarchical structures is one of the core design features of human language. Duality of patterning refers for instance to the organization of the meaningful elements in a language at two distinct levels: a combinatorial level where meaningless forms are combined into meaningful forms and a compositional level where meaningful forms are composed into larger lexical units. The question remains wide open regarding how such a structure could have emerged. Furthermore a clear mathematical framework to quantify this phenomenon is still lacking. The aim of this paper is that of addressing these two aspects in a self-consistent way. First, we introduce suitable measures to quantify the level of combinatoriality and compositionality in a language, and present a framework to estimate these observables in human natural languages. Second, we show that the theoretical predictions of a multi-agents modeling scheme, namely the Blending Game, are in surprisingly good agreement with empirical data. In the Blending Game a population of individuals plays language games aiming at success in communication. It is remarkable that the two sides of duality of patterning emerge simultaneously as a consequence of a pure cultural dynamics in a simulated environment that contains meaningful relations, provided a simple constraint on message transmission fidelity is also considered.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.03661 [physics]},
	author = {Loreto, Vittorio and Gravino, Pietro and Servedio, Vito D. P. and Tria, Francesca},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.03661},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BAQVDR68/1602.html:text/html;Loreto et al_2016_On the emergence of syntactic structures.pdf:/home/user/Zotero/storage/5KX4AFJS/Loreto et al_2016_On the emergence of syntactic structures.pdf:application/pdf}
}

@article{gal-theoretically-2015,
	title = {A {Theoretically} {Grounded} {Application} of {Dropout} in {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.05287},
	abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit. Dropout is a widely used tool for regularisation in deep models, but a long strand of empirical research has claimed that it cannot be applied between the recurrent connections of an RNN. The argument is that noise hinders the network's ability to model sequences and therefore dropout should be applied to the RNN's inputs and outputs alone. But without regularisation in recurrent layers, existing techniques overfit quickly. In this paper we make use of a recently developed theoretical framework casting dropout as approximate variational inference. Based on the framework we derive mathematically grounded tools to apply dropout within the recurrent layers of RNNs, eliminating model overfitting. We apply our new variational inference based dropout technique in LSTM and GRU networks, evaluating the technique empirically. We show that the new approach outperforms existing techniques on sentiment analysis and language modelling tasks, extending our arsenal of variational tools in deep learning.},
	urldate = {2016-02-17},
	journal = {arXiv:1512.05287 [stat]},
	author = {Gal, Yarin},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.05287},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6KVAQAP7/1512.html:text/html;Gal_2015_A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:/home/user/Zotero/storage/AMNEZFWU/Gal_2015_A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:application/pdf}
}

@article{jozefowicz-exploring-2016,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02410},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C8NQZK9Q/1602.html:text/html;Jozefowicz et al_2016_Exploring the Limits of Language Modeling.pdf:/home/user/Zotero/storage/ZFS87X45/Jozefowicz et al_2016_Exploring the Limits of Language Modeling.pdf:application/pdf}
}

@article{hwang-automatic-2015,
	title = {The {Automatic} {Statistician}: {A} {Relational} {Perspective}},
	shorttitle = {The {Automatic} {Statistician}},
	url = {http://arxiv.org/abs/1511.08343},
	abstract = {Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data.},
	urldate = {2016-02-17},
	journal = {arXiv:1511.08343 [cs, stat]},
	author = {Hwang, Yunseong and Tong, Anh and Choi, Jaesik},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08343},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9ZM9N7ZB/1511.html:text/html;Hwang et al_2015_The Automatic Statistician.pdf:/home/user/Zotero/storage/9JDZ4SZ8/Hwang et al_2015_The Automatic Statistician.pdf:application/pdf}
}

@article{yin-attention-based-2016,
	title = {Attention-{Based} {Convolutional} {Neural} {Network} for {Machine} {Comprehension}},
	url = {http://arxiv.org/abs/1602.04341},
	abstract = {Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.04341 [cs]},
	author = {Yin, Wenpeng and Ebert, Sebastian and Schütze, Hinrich},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.04341},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IV4JI66E/1602.html:text/html;Yin et al_2016_Attention-Based Convolutional Neural Network for Machine Comprehension.pdf:/home/user/Zotero/storage/2QRP3734/Yin et al_2016_Attention-Based Convolutional Neural Network for Machine Comprehension.pdf:application/pdf}
}

@article{ganardi-circuit-2016,
	title = {Circuit {Evaluation} for {Finite} {Semirings}},
	url = {http://arxiv.org/abs/1602.04560},
	abstract = {The computational complexity of the circuit evaluation problem for finite semirings is considered, where semirings are not assumed to have an additive or multiplicative identity. The following dichotomy is shown: If a finite semiring is such that (i) the multiplicative semigroup is solvable and (ii) it does not contain a subsemiring with an additive identity \$0\$ and a multiplicative identity \$1 \neq 0\$, then the circuit evaluation problem for the semiring is in \$\mathsf{DET} \subseteq \mathsf{NC}{\textasciicircum}2\$. In all other cases, the circuit evaluation problem is \$\mathsf{P}\$-complete.},
	urldate = {2016-02-17},
	journal = {arXiv:1602.04560 [cs]},
	author = {Ganardi, Moses and Hucke, Danny and König, Daniel and Lohrey, Markus},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.04560},
	keywords = {Computer Science - Computational Complexity, 68W30, 16Z05, 68W10},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/F3JPJBPQ/1602.html:text/html;Ganardi et al_2016_Circuit Evaluation for Finite Semirings.pdf:/home/user/Zotero/storage/EJCMG8DD/Ganardi et al_2016_Circuit Evaluation for Finite Semirings.pdf:application/pdf}
}

@article{casares-syntax-2015,
	title = {Syntax {Evolution}: {Problems} and {Recursion}},
	shorttitle = {Syntax {Evolution}},
	url = {http://arxiv.org/abs/1508.03040},
	abstract = {We are Turing complete, and natural language parsing is decidable, so our syntactic abilities are in excess to those needed to speak a natural language. This is an anomaly, because evolution would not keep an overqualified feature for long. We solve this anomaly by using a coincidence, both syntax and problem solving are computing, and a difference, Turing completeness is not a requirement of syntax, but of problem solving. Then computing should have been shaped by evolutionary requirements coming from both syntax and problem solving, but the last one, Turing completeness, only from problem solving. So we propose and analyze a hypothesis: syntax and problem solving co-evolved in humans towards Turing completeness. Finally, we argue that Turing completeness, also known as recursion, is our most singular feature.},
	urldate = {2016-02-17},
	journal = {arXiv:1508.03040 [cs]},
	author = {Casares, Ramón},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.03040},
	keywords = {Computer Science - Computation and Language, I.2.7, 91F20, 68T20, 68T50, 92D15, I.2.8},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/28MSQ7GI/1508.html:text/html;Casares_2015_Syntax Evolution.pdf:/home/user/Zotero/storage/2ZMXQQGJ/Casares_2015_Syntax Evolution.pdf:application/pdf}
}

@article{fijalkow-characterisation-2015,
	title = {Characterisation of an {Algebraic} {Algorithm} for {Probabilistic} {Automata} {Characterisation} of an {Algebraic} {Algorithm} for {Probabilistic} {Automata}},
	url = {http://arxiv.org/abs/1501.02997},
	abstract = {We consider the value 1 problem for probabilistic automata over finite words: it asks whether a given probabilistic automaton accepts words with probability arbitrarily close to 1. This problem is known to be undecidable. However, different algorithms have been proposed to partially solve it; it has been recently shown that the Markov Monoid algorithm, based on algebra, is the most correct algorithm so far. The first contribution of this paper is to give a characterisation of the Markov Monoid algorithm. The second contribution is to develop a profinite theory for probabilistic automata, called the prostochastic theory. This new framework gives a topological account of the value 1 problem, which in this context is cast as an emptiness problem. The above characterisation is reformulated using the prostochastic theory, allowing to give a modular proof.},
	urldate = {2016-02-17},
	journal = {arXiv:1501.02997 [cs]},
	author = {Fijalkow, Nathanaël},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02997},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UAK44NGW/1501.html:text/html;Fijalkow_2015_Characterisation of an Algebraic Algorithm for Probabilistic Automata.pdf:/home/user/Zotero/storage/R9Q95RX9/Fijalkow_2015_Characterisation of an Algebraic Algorithm for Probabilistic Automata.pdf:application/pdf}
}

@article{steinhardt-learning-2015-2,
	title = {Learning fast-mixing models for structured prediction},
	url = {http://arxiv.org/abs/1502.06668},
	urldate = {2016-02-17},
	journal = {arXiv preprint arXiv:1502.06668},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {1502.06668.pdf:/home/user/Zotero/storage/SNG9ZQSE/1502.06668.pdf:application/pdf}
}

@book{cohen-proceedings-2006,
	address = {New York, NY},
	title = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}: [{June} 25 - 29, 2006, {Pittsburgh}, {PA}, {USA}]},
	isbn = {978-1-59593-383-6},
	shorttitle = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}},
	language = {eng},
	publisher = {ACM},
	editor = {Cohen, William W. and Moore, Andrew and {International Conference on Machine Learning}},
	year = {2006},
	keywords = {Kongress, Maschinelles Lernen},
	file = {1511.08400v3.pdf:/home/user/Zotero/storage/BWTD3TTT/1511.08400v3.pdf:application/pdf}
}

@article{parr-proof-nodate,
	title = {Proof {Delivery} {Form} {Psychological} {Medicine}},
	author = {Parr, Mr SJ},
	file = {11JCLBoydGoldbergConservative-1.pdf:/home/user/Zotero/storage/ENDM5KRQ/11JCLBoydGoldbergConservative-1.pdf:application/pdf}
}

@article{kulpmann-argument-2015,
	title = {Argument {Omission} between {Valency} and {Construction}},
	url = {https://hsbiblio.uni-tuebingen.de/xmlui/handle/10900/67218},
	urldate = {2016-02-17},
	author = {Külpmann, Robert and Symanczyk Joppe, Vilma},
	year = {2015},
	file = {Külpmann_SymanczykJoppe.pdf:/home/user/Zotero/storage/ZCBAA7TF/Külpmann_SymanczykJoppe.pdf:application/pdf}
}

@article{allen-distinguishing-2012,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X12001599},
	number = {3},
	urldate = {2016-02-17},
	journal = {Brain and language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	year = {2012},
	pages = {174--182},
	file = {12MVPA-final.pdf:/home/user/Zotero/storage/RMB2MWHU/12MVPA-final.pdf:application/pdf}
}

@inproceedings{chaganty-estimating-2014,
	title = {Estimating latent-variable graphical models using moments and likelihoods},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2\_chaganty14},
	urldate = {2016-02-17},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Chaganty, Arun T. and Liang, Percy},
	year = {2014},
	pages = {1872--1880},
	file = {graphical-icml2014.pdf:/home/user/Zotero/storage/GUQD9MTT/graphical-icml2014.pdf:application/pdf}
}

@techreport{goodman-concepts-2014,
	title = {Concepts in a probabilistic language of thought},
	url = {http://dspace.mit.edu/handle/1721.1/100174},
	urldate = {2016-02-17},
	institution = {Center for Brains, Minds and Machines (CBMM)},
	author = {Goodman, Noah D. and Tenenbaum, Joshua B. and Gerstenberg, Tobias},
	year = {2014},
	file = {GoodmanEtAl2015-Chapter.pdf:/home/user/Zotero/storage/3ZASJTAA/GoodmanEtAl2015-Chapter.pdf:application/pdf}
}

@article{allen-distinguishing-2012-1,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	issn = {0093934X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0093934X12001599},
	doi = {10.1016/j.bandl.2012.08.005},
	language = {en},
	number = {3},
	urldate = {2016-02-17},
	journal = {Brain and Language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	month = dec,
	year = {2012},
	pages = {174--182},
	file = {BrainLanguagepaper.pdf:/home/user/Zotero/storage/TWWDNZCE/BrainLanguagepaper.pdf:application/pdf}
}

@article{robenalt-judgment-2015,
	title = {Judgment evidence for statistical preemption: {It} is relatively better to vanish than to disappear a rabbit, but a lifeguard can equally well backstroke or swim children to shore},
	volume = {26},
	shorttitle = {Judgment evidence for statistical preemption},
	url = {http://www.degruyter.com/view/j/cogl.2015.26.issue-3/cog-2015-0004/cog-2015-0004.xml},
	number = {3},
	urldate = {2016-02-17},
	journal = {Cognitive Linguistics},
	author = {Robenalt, Clarice and Goldberg, Adele E.},
	year = {2015},
	pages = {467--503},
	file = {CogLing15disappear-vanish-final-wappendix1.pdf:/home/user/Zotero/storage/WVMDUEAV/CogLing15disappear-vanish-final-wappendix1.pdf:application/pdf}
}

@article{wilson-deep-2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	urldate = {2016-02-17},
	journal = {arXiv preprint arXiv:1511.02222},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2015},
	file = {1511.02222v1.pdf:/home/user/Zotero/storage/33CDKZBQ/1511.02222v1.pdf:application/pdf}
}

@article{goldberg-argument-2013,
	title = {Argument structure constructions versus lexical rules or derivational verb templates},
	volume = {28},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/mila.12026/full},
	number = {4},
	urldate = {2016-02-17},
	journal = {Mind \& Language},
	author = {Goldberg, Adele E.},
	year = {2013},
	pages = {435--465},
	file = {13Mind-Language-asc-not-rules.pdf:/home/user/Zotero/storage/MH363ZIA/13Mind-Language-asc-not-rules.pdf:application/pdf}
}

@article{robenalt-nonnative-2016,
	title = {Nonnative {Speakers} {Do} {Not} {Take} {Competing} {Alternative} {Expressions} {Into} {Account} the {Way} {Native} {Speakers} {Do}},
	volume = {66},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/lang.12149/full},
	number = {1},
	urldate = {2016-02-17},
	journal = {Language Learning},
	author = {Robenalt, Clarice and Goldberg, Adele E.},
	year = {2016},
	pages = {60--93},
	file = {Robenalt_GoldbergLLPreemptionL2-.pdf:/home/user/Zotero/storage/TCWWRZ9S/Robenalt_GoldbergLLPreemptionL2-.pdf:application/pdf}
}

@article{berant-imitation-2015-1,
	title = {Imitation {Learning} of {Agenda}-based {Semantic} {Parsers}},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/646},
	urldate = {2016-02-17},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2015},
	pages = {545--558},
	file = {agenda-tacl2015.pdf:/home/user/Zotero/storage/2W8IH9AB/agenda-tacl2015.pdf:application/pdf}
}

@article{mcfarland-making-2013,
	title = {Making the {Connection}: {Social} {Bonding} in {Courtship} {Situations} $^{\textrm{1}}$},
	volume = {118},
	issn = {0002-9602, 1537-5390},
	shorttitle = {Making the {Connection}},
	url = {http://www.journals.uchicago.edu/doi/10.1086/670240},
	doi = {10.1086/670240},
	language = {en},
	number = {6},
	urldate = {2016-02-17},
	journal = {American Journal of Sociology},
	author = {McFarland, Daniel A. and Jurafsky, Dan and Rawlings, Craig},
	month = may,
	year = {2013},
	pages = {1596--1649},
	file = {mcfarlandjurafskyrawlings.pdf:/home/user/Zotero/storage/9VVK8C3I/mcfarlandjurafskyrawlings.pdf:application/pdf}
}

@article{johnson-neural-2016,
	title = {Neural systems involved in processing novel linguistic constructions and their visual referents},
	volume = {31},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1055280},
	number = {1},
	urldate = {2016-02-17},
	journal = {Language, Cognition and Neuroscience},
	author = {Johnson, Matthew A. and Turk-Browne, Nicholas B. and Goldberg, Adele E.},
	year = {2016},
	pages = {129--144},
	file = {15LCN-Prediction-Cx-Neuro.pdf:/home/user/Zotero/storage/MMCV2KXX/15LCN-Prediction-Cx-Neuro.pdf:application/pdf}
}

@inproceedings{gens-discriminative-2012,
	title = {Discriminative learning of sum-product networks},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2012\_1484.pdf},
	urldate = {2016-02-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gens, Robert and Domingos, Pedro},
	year = {2012},
	pages = {3248--3256},
	file = {dspn.pdf:/home/user/Zotero/storage/GFAMUH49/dspn.pdf:application/pdf}
}

@article{jager-factoring-2015,
	title = {Factoring lexical and phonetic phylogenetic characters from word lists},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67205},
	urldate = {2016-02-17},
	author = {Jäger, Gerhard and List, Johann-Mattis},
	year = {2015},
	file = {Jaeger_List.pdf:/home/user/Zotero/storage/IRM7UWV5/Jaeger_List.pdf:application/pdf}
}

@article{wonnacott-input-2012,
	title = {Input effects on the acquisition of a novel phrasal construction in 5year olds},
	volume = {66},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X11001148},
	doi = {10.1016/j.jml.2011.11.004},
	language = {en},
	number = {3},
	urldate = {2016-02-17},
	journal = {Journal of Memory and Language},
	author = {Wonnacott, Elizabeth and Boyd, Jeremy K. and Thomson, Jennifer and Goldberg, Adele E.},
	month = apr,
	year = {2012},
	pages = {458--478},
	file = {12WonnacottBoydGoldbergJML.pdf:/home/user/Zotero/storage/AHSS2KBU/12WonnacottBoydGoldbergJML.pdf:application/pdf}
}

@article{perek-generalizing-2015,
	title = {Generalizing beyond the input: {The} functions of the constructions matter},
	volume = {84},
	issn = {0749596X},
	shorttitle = {Generalizing beyond the input},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000601},
	doi = {10.1016/j.jml.2015.04.006},
	language = {en},
	urldate = {2016-02-17},
	journal = {Journal of Memory and Language},
	author = {Perek, Florent and Goldberg, Adele E.},
	month = oct,
	year = {2015},
	pages = {108--127},
	file = {15JMLPerek-Goldberg.pdf:/home/user/Zotero/storage/C8WNPEX8/15JMLPerek-Goldberg.pdf:application/pdf}
}

@article{goodman-probabilistic-2014-1,
	title = {Probabilistic semantics and pragmatics: {Uncertainty} in language and thought},
	shorttitle = {Probabilistic semantics and pragmatics},
	url = {http://cocolab.stanford.edu/papers/GoodmanLassiter2015-Chapter.pdf},
	urldate = {2016-02-17},
	journal = {Handbook of Contemporary Semantic Theory. Wiley-Blackwell},
	author = {Goodman, Noah D. and Lassiter, Daniel},
	year = {2014},
	file = {GoodmanLassiter2015-Chapter.pdf:/home/user/Zotero/storage/49TJE84C/GoodmanLassiter2015-Chapter.pdf:application/pdf}
}

@inproceedings{shi-learning-2015-1,
	title = {Learning {Where} to {Sample} in {Structured} {Prediction}.},
	url = {http://www-cs.stanford.edu/~jsteinhardt/publications/adainfer/paper.pdf},
	urldate = {2016-02-17},
	booktitle = {{AISTATS}},
	author = {Shi, Tianlin and Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {sample-aistats2015.pdf:/home/user/Zotero/storage/FFKDFMSF/sample-aistats2015.pdf:application/pdf}
}

@article{fengler-brain-2015-1,
	title = {Brain structural correlates of complex sentence comprehension in children},
	volume = {15},
	issn = {18789293},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1878929315000900},
	doi = {10.1016/j.dcn.2015.09.004},
	language = {en},
	urldate = {2016-02-17},
	journal = {Developmental Cognitive Neuroscience},
	author = {Fengler, Anja and Meyer, Lars and Friederici, Angela D.},
	month = oct,
	year = {2015},
	pages = {48--57},
	file = {1-s2.0-S1878929315000900-main.pdf:/home/user/Zotero/storage/BC34D9BF/1-s2.0-S1878929315000900-main.pdf:application/pdf}
}

@inproceedings{gens-deep-2014,
	title = {Deep symmetry networks},
	url = {http://papers.nips.cc/paper/5424-deep-symmetry-networks},
	urldate = {2016-02-17},
	booktitle = {Advances in neural information processing systems},
	author = {Gens, Robert and Domingos, Pedro M.},
	year = {2014},
	pages = {2537--2545},
	file = {dsn.pdf:/home/user/Zotero/storage/VQNB8PJ9/dsn.pdf:application/pdf}
}

@article{mansimov-generating-2015,
	title = {Generating {Images} from {Captions} with {Attention}},
	url = {http://arxiv.org/abs/1511.02793},
	urldate = {2016-02-17},
	journal = {arXiv preprint arXiv:1511.02793},
	author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.02793v1.pdf:/home/user/Zotero/storage/VUZ2SQQV/1511.02793v1.pdf:application/pdf}
}

@article{vandevoorde-distributional-2015,
	title = {Distributional and translational solutions to the visualization of semantic differences between translated and non-translated {Dutch}},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67190},
	urldate = {2016-02-17},
	author = {Vandevoorde, Lore and De Baets, Pauline and Lefever, Els and Plevoets, Koen and De Sutter, Gert},
	year = {2015},
	file = {Vandevoorde_DeBaets_Lefever_Plevoets_DeSutter.pdf:/home/user/Zotero/storage/ES3FRI5W/Vandevoorde_DeBaets_Lefever_Plevoets_DeSutter.pdf:application/pdf}
}

@article{kober-learning-2014,
	title = {Learning {Motor} {Skills}},
	volume = {97},
	url = {http://link.springer.com/content/pdf/10.1007/978-3-319-03194-1.pdf},
	urldate = {2016-02-17},
	journal = {Springer Tracts in Advanced Robotics},
	author = {Kober, Jens and Peters, Jan},
	year = {2014},
	file = {Kober_IJRR_2013.pdf:/home/user/Zotero/storage/N4R4H9SI/Kober_IJRR_2013.pdf:application/pdf}
}

@article{griffiths-phoneme-2015,
	title = {From {Phoneme} to {Morpheme}: {A} {Computational} {Model}},
	shorttitle = {From {Phoneme} to {Morpheme}},
	url = {https://hsbiblio.uni-tuebingen.de/xmlui/handle/10900/67219},
	urldate = {2016-02-17},
	author = {Griffiths, Sascha and Purver, Matthew and Wiggins, Geraint},
	year = {2015},
	file = {Griffiths.pdf:/home/user/Zotero/storage/88ZJK55M/Griffiths.pdf:application/pdf}
}

@article{cohen-products-2011,
	title = {Products of weighted logic programs},
	volume = {11},
	issn = {1471-0684, 1475-3081},
	url = {http://www.journals.cambridge.org/abstract\_S1471068410000529},
	doi = {10.1017/S1471068410000529},
	language = {en},
	number = {2-3},
	urldate = {2016-02-17},
	journal = {Theory and Practice of Logic Programming},
	author = {Cohen, Shay B. and Simmons, Robert J. and Smith, Noah A.},
	month = mar,
	year = {2011},
	pages = {263--296},
	file = {tplp11product.pdf:/home/user/Zotero/storage/K87FG3DB/tplp11product.pdf:application/pdf}
}

@inproceedings{bosch-definite-2011,
	title = {Definite reference is not always based on salience},
	url = {http://edoc.hu-berlin.de/docviews/abstract.php?id=38017},
	urldate = {2016-02-17},
	booktitle = {{QITL}-4-{Proceedings} of {Quantitative} {Investigations} in {Theoretical} {Linguistics} 4 ({QITL}-4)},
	publisher = {Humboldt-Universität zu Berlin},
	author = {Bosch, Peter and Alexeyenko, Sascha and Brukamp, Kirsten and Cieschinger, Maria and Deng, Xiaoye and König, Peter},
	year = {2011},
	file = {QITL4_Proceedings.pdf:/home/user/Zotero/storage/M4PXH64I/QITL4_Proceedings.pdf:application/pdf}
}

@article{neri-elementary-2015-1,
	title = {The {Elementary} {Operations} of {Human} {Vision} {Are} {Not} {Reducible} to {Template}-{Matching}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004499},
	doi = {10.1371/journal.pcbi.1004499},
	language = {en},
	number = {11},
	urldate = {2016-02-17},
	journal = {PLOS Computational Biology},
	author = {Neri, Peter},
	editor = {Baker, Daniel Hart},
	month = nov,
	year = {2015},
	pages = {e1004499},
	file = {journal.pcbi.1004499.pdf:/home/user/Zotero/storage/I8755ZV8/journal.pcbi.1004499.pdf:application/pdf}
}

@incollection{nareyek-choosing-2003,
	title = {Choosing search heuristics by non-stationary reinforcement learning},
	url = {http://link.springer.com/chapter/10.1007/978-1-4757-4137-7\_25},
	urldate = {2016-02-17},
	booktitle = {Metaheuristics: {Computer} decision-making},
	publisher = {Springer},
	author = {Nareyek, Alexander},
	year = {2003},
	pages = {523--544},
	file = {book2015oct.pdf:/home/user/Zotero/storage/I2DGG878/book2015oct.pdf:application/pdf}
}

@article{sharma-action-2015,
	title = {Action {Recognition} using {Visual} {Attention}},
	url = {http://arxiv.org/abs/1511.04119},
	urldate = {2016-02-17},
	journal = {arXiv preprint arXiv:1511.04119},
	author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.04119v1.pdf:/home/user/Zotero/storage/4JZQFK74/1511.04119v1.pdf:application/pdf}
}

@article{speelman-putting-2008,
	title = {Putting the (in) direct causation hypothesis to the test: a quantitative study of {Dutch} doen ‘make’and laten ‘let’},
	shorttitle = {Putting the (in) direct causation hypothesis to the test},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.2137&rep=rep1&type=pdf#page=64},
	urldate = {2016-02-17},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Speelman, Dirk and Geeraerts, Dirk},
	year = {2008},
	pages = {62},
	file = {Speelman_Geeraerts.pdf:/home/user/Zotero/storage/22N6REB4/Speelman_Geeraerts.pdf:application/pdf}
}

@article{vinyals-order-2015,
	title = {Order {Matters}: {Sequence} to sequence for sets},
	shorttitle = {Order {Matters}},
	url = {http://arxiv.org/abs/1511.06391},
	abstract = {Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.},
	urldate = {2016-02-02},
	journal = {arXiv:1511.06391 [cs, stat]},
	author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06391},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/I7CHXUT2/1511.html:text/html;Vinyals et al_2015_Order Matters.pdf:/home/user/Zotero/storage/TKHXC8RS/Vinyals et al_2015_Order Matters.pdf:application/pdf}
}

@article{shen-minimum-2015,
	title = {Minimum {Risk} {Training} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1512.02433},
	urldate = {2016-02-02},
	journal = {arXiv preprint arXiv:1512.02433},
	author = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
	year = {2015},
	file = {1512.02433v2.pdf:/home/user/Zotero/storage/INBFUXR2/1512.02433v2.pdf:application/pdf}
}

@article{tu-coverage-based-2016,
	title = {Coverage-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1601.04811},
	abstract = {Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.},
	urldate = {2016-02-02},
	journal = {arXiv:1601.04811 [cs]},
	author = {Tu, Zhaopeng and Lu, Zhengdong and Liu, Yang and Liu, Xiaohua and Li, Hang},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.04811},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/FPXEB6CZ/1601.html:text/html;Tu et al_2016_Coverage-based Neural Machine Translation.pdf:/home/user/Zotero/storage/3KNABR7V/Tu et al_2016_Coverage-based Neural Machine Translation.pdf:application/pdf}
}

@article{schrader-can-2016,
	title = {Can {Germany} engineer a coal exit?},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6272/430},
	doi = {10.1126/science.351.6272.430},
	abstract = {Germany's coal-fired power plants, which provide nearly half of the nation's electricity, are posing a major challenge to its plans for an Energiewende—a shift to an energy system that will cut national greenhouse gas emissions by 80\% to 95\% by 2050. Many experts say achieving that goal will require the nation to phase out its coal plants—but the question of how and when that might happen has been contentious. Now, in the wake of the Paris climate pact, the debate is heating up. A prominent think tank has released a report concluding that Germany could abandon coal by 2040, and Germany's environment minister says that she will soon unveil a plan for reaching the 2050 emissions goal. Many observers expect it to include a coal exit timeline. But Germany's vice chancellor, as well as major labor and industry groups, are pushing back, fearing job losses and energy market disruptions—and predicting that a solo German move could simply shift emissions to other nations.
Debate grows over whether nation could eliminate a key but dirty fuel by 2040.
Debate grows over whether nation could eliminate a key but dirty fuel by 2040.},
	language = {en},
	number = {6272},
	urldate = {2016-02-02},
	journal = {Science},
	author = {Schrader, Christopher},
	month = jan,
	year = {2016},
	pmid = {26823404},
	pages = {430--431},
	file = {Schrader_2016_Can Germany engineer a coal exit.pdf:/home/user/Zotero/storage/D3J9WV7C/Schrader_2016_Can Germany engineer a coal exit.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/J8DXSUGU/430.html:text/html}
}

@article{levine-sex-2016,
	title = {Sex differences in spatial cognition: advancing the conversation},
	copyright = {© 2016 Wiley Periodicals, Inc.},
	issn = {1939-5086},
	shorttitle = {Sex differences in spatial cognition},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.1380/abstract},
	doi = {10.1002/wcs.1380},
	abstract = {The existence of a sex difference in spatial thinking, notably on tasks involving mental rotation, has been a topic of considerable research and debate. We review this literature, with a particular focus on the development of this sex difference, and consider four key questions: (1) When does the sex difference emerge developmentally and does the magnitude of this difference change across development? (2) What are the biological and environmental factors that contribute to sex differences in spatial skill and how might they interact? (3) How malleable are spatial skills, and is the sex difference reduced as a result of training? and (4) Does ‘spatializing’ the curriculum raise the level of spatial thinking in all students and hold promise for increasing and diversifying the STEM pipeline? Throughout the review, we consider promising avenues for future research. For further resources related to this article, please visit the WIREs website.},
	language = {en},
	urldate = {2016-02-02},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Levine, Susan C. and Foley, Alana and Lourenco, Stella and Ehrlich, Stacy and Ratliff, Kristin},
	month = jan,
	year = {2016},
	pages = {n/a--n/a},
	file = {Snapshot:/home/user/Zotero/storage/PJIA5XEW/abstract.html:text/html}
}

@article{llewellyn-dream-2016,
	title = {Dream to {Predict}? {REM} {Dreaming} as {Prospective} {Coding}},
	shorttitle = {Dream to {Predict}?},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01961/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w5-2016},
	doi = {10.3389/fpsyg.2015.01961},
	abstract = {The dream as prediction seems inherently improbable. The bizarre occurrences in dreams never characterize everyday life. Dreams do not come true! But assuming that bizarreness negates expectations may rest on a misunderstanding of how the predictive brain works. In evolutionary terms, the ability to rapidly predict what sensory input implies—through expectations derived from discerning patterns in associated past experiences—would have enhanced fitness and survival. For example, food and water are essential for survival, associating past experiences (to identify location patterns) predicts where they can be found. Similarly, prediction may enable predator identification from what would have been only a fleeting and ambiguous stimulus—without prior expectations. To confront the many challenges associated with natural settings, visual perception is vital for humans (and most mammals) and often responses must be rapid. Predictive coding during wake may, therefore, be based on unconscious imagery so that visual perception is maintained and appropriate motor actions triggered quickly. Speed may also dictate the form of the imagery. Bizarreness, during REM dreaming, may result from a prospective code fusing phenomena with the same meaning—within a particular context. For example, if the context is possible predation, from the perspective of the prey two different predators can both mean the same (i.e., immediate danger) and require the same response (e.g., flight). Prospective coding may also prune redundancy from memories, to focus the image on the contextually-relevant elements only, thus, rendering the non-relevant phenomena indeterminate—another aspect of bizarreness. In sum, this paper offers an evolutionary take on REM dreaming as a form of prospective coding which identifies a probabilistic pattern in past events. This pattern is portrayed in an unconscious, associative, sensorimotor image which may support cognition in wake through being mobilized as a predictive code. A particular dream illustrates.},
	urldate = {2016-02-02},
	journal = {Cognitive Science},
	author = {Llewellyn, Sue},
	year = {2016},
	keywords = {prediction, pattern, prospective coding, REM dreaming, unconscious},
	pages = {1961},
	file = {Llewellyn_2016_Dream to Predict.pdf:/home/user/Zotero/storage/XSWHF4S6/Llewellyn_2016_Dream to Predict.pdf:application/pdf}
}

@article{santos-social-2016,
	title = {Social {Norms} of {Cooperation} in {Small}-{Scale} {Societies}},
	volume = {12},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1004709},
	doi = {10.1371/journal.pcbi.1004709},
	abstract = {Author Summary The prevalence of cooperation among human societies is a puzzle that has caught the eye of researchers from multiple fields. Why is that people are selfless and often incur costs to aid others? Reputations are intimately linked with the answer to this question, and so are the social norms that dictate what is reckoned as a good or a bad action. Here we present a mathematical framework to analyze the relationship between different social norms and the sustainability of cooperation, in populations of arbitrary sizes. Indeed, it is known that cooperation, norms, reciprocity and the art of managing reputations, are features that go along with humans from their pre-historic existence in small-scale societies to the contemporary times, when technology supports the interaction with a large number of people. We show that population size is relevant when evaluating the merits of each social norm and conclude that there is a social norm especially effective in leveraging cooperation in small populations. That simple norm dictates that only whoever cooperates with good individuals, and defects against bad ones, deserves a good reputation.},
	number = {1},
	urldate = {2016-02-02},
	journal = {PLoS Comput Biol},
	author = {Santos, Fernando P. and Santos, Francisco C. and Pacheco, Jorge M.},
	month = jan,
	year = {2016},
	pages = {e1004709},
	file = {Santos et al_2016_Social Norms of Cooperation in Small-Scale Societies.pdf:/home/user/Zotero/storage/QKGBRH9X/Santos et al_2016_Social Norms of Cooperation in Small-Scale Societies.pdf:application/pdf}
}

@article{wand-lipreading-2016,
	title = {Lipreading with {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1601.08188},
	abstract = {Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feed-forward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6\% using the end-to-end neural network-based solution (11.6\% improvement over the best feature-based solution evaluated).},
	urldate = {2016-02-02},
	journal = {arXiv:1601.08188 [cs]},
	author = {Wand, Michael and Koutník, Jan and Schmidhuber, Jürgen},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.08188},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RUAZFECR/1601.html:text/html;Wand et al_2016_Lipreading with Long Short-Term Memory.pdf:/home/user/Zotero/storage/6DH7X27F/Wand et al_2016_Lipreading with Long Short-Term Memory.pdf:application/pdf}
}

@article{buchanan-expressing-2016,
	title = {Expressing geometry},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/v12/n2/full/nphys3656.html?WT.ec\_id=NPHYS-201602&spMailingID=50604575&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=860218152&spReportId=ODYwMjE4MTUyS0},
	doi = {10.1038/nphys3656},
	language = {en},
	number = {2},
	urldate = {2016-02-02},
	journal = {Nature Physics},
	author = {Buchanan, Mark},
	month = feb,
	year = {2016},
	keywords = {Biological physics, Epigenetics analysis},
	pages = {108--108},
	file = {Buchanan_2016_Expressing geometry.pdf:/home/user/Zotero/storage/N46UXXDE/Buchanan_2016_Expressing geometry.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ERGMGCJF/nphys3656.html:text/html}
}

@article{mattos-recurrent-2015-1,
	title = {Recurrent {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1511.06644},
	abstract = {We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.},
	urldate = {2016-02-02},
	journal = {arXiv:1511.06644 [cs, stat]},
	author = {Mattos, César Lincoln C. and Dai, Zhenwen and Damianou, Andreas and Forth, Jeremy and Barreto, Guilherme A. and Lawrence, Neil D.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06644},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QCSRX9SK/1511.html:text/html;Mattos et al_2015_Recurrent Gaussian Processes.pdf:/home/user/Zotero/storage/CCCQ2WBQ/Mattos et al_2015_Recurrent Gaussian Processes.pdf:application/pdf}
}

@misc{noauthor-notitle-nodate-3,
	url = {http://jmlr.csail.mit.edu/papers/v3/blei03a.html},
	urldate = {2016-02-02},
	file = {:/home/user/Zotero/storage/JQPZAFJ9/blei03a.html:text/html}
}

@inproceedings{johnson-adaptor-2006,
	title = {Adaptor grammars: {A} framework for specifying compositional nonparametric {Bayesian} models},
	shorttitle = {Adaptor grammars},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2006\_64.pdf},
	urldate = {2016-02-02},
	booktitle = {Advances in neural information processing systems},
	author = {Johnson, Mark and Griffiths, Thomas L. and Goldwater, Sharon},
	year = {2006},
	pages = {641--648},
	file = {Adaptor Grammars\: A Framework for Specifying Compositional Nonparametric Bayesian Models - 3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.pdf:/home/user/Zotero/storage/S3U75QZX/3101-adaptor-grammars-a-framework-for-specifying-compositional-nonparametric-bayesian-models.pdf:application/pdf}
}

@article{goldwater-bayesian-2009,
	title = {A {Bayesian} framework for word segmentation: {Exploring} the effects of context},
	volume = {112},
	issn = {00100277},
	shorttitle = {A {Bayesian} framework for word segmentation},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027709000675},
	doi = {10.1016/j.cognition.2009.03.008},
	language = {en},
	number = {1},
	urldate = {2016-02-02},
	journal = {Cognition},
	author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark},
	month = jul,
	year = {2009},
	pages = {21--54},
	file = {A Bayesian framework for word segmentation\: Exploring the effects of context - wordseg3.pdf:/home/user/Zotero/storage/A3VTTFNZ/wordseg3.pdf:application/pdf}
}

@article{gibson-noisy-channel-2013,
	title = {A {Noisy}-{Channel} {Account} of {Crosslinguistic} {Word}-{Order} {Variation}},
	volume = {24},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/content/24/7/1079},
	doi = {10.1177/0956797612463705},
	abstract = {The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent. Recent work suggests that the SOV order may be the default in human language. Why, then, is SVO order so common? We hypothesize that SOV/SVO variation can be explained by language users’ sensitivity to the possibility of noise corrupting the linguistic signal. In particular, the noisy-channel hypothesis predicts a shift from the default SOV order to SVO order for semantically reversible events, for which potential ambiguity arises in SOV order because two plausible agents appear on the same side of the verb. We found support for this prediction in three languages (English, Japanese, and Korean) by using a gesture-production task, which reflects word-order preferences largely independent of native language. Other patterns of crosslinguistic variation (e.g., the prevalence of case marking in SOV languages and its relative absence in SVO languages) also straightforwardly follow from the noisy-channel hypothesis.},
	language = {en},
	number = {7},
	urldate = {2016-02-02},
	journal = {Psychological Science},
	author = {Gibson, Edward and Piantadosi, Steven T. and Brink, Kimberly and Bergen, Leon and Lim, Eunice and Saxe, Rebecca},
	month = jul,
	year = {2013},
	pmid = {23649563},
	keywords = {Linguistics, language, psycholinguistics, cognition(s)},
	pages = {1079--1088},
	file = {Gibson et al_2013_A Noisy-Channel Account of Crosslinguistic Word-Order Variation.pdf:/home/user/Zotero/storage/MWXE8HH7/Gibson et al_2013_A Noisy-Channel Account of Crosslinguistic Word-Order Variation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GKUNIZDI/1079.html:text/html}
}

@inproceedings{genzel-entropy-2002,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '02},
	title = {Entropy {Rate} {Constancy} in {Text}},
	url = {http://dx.doi.org/10.3115/1073083.1073117},
	doi = {10.3115/1073083.1073117},
	abstract = {We present a constancy rate principle governing language generation. We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number. We demonstrate that this is indeed the case by measuring entropy in three different ways. We also show that this effect has both lexical (which words are used) and non-lexical (how the words are used) causes.},
	urldate = {2016-01-28},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Genzel, Dmitriy and Charniak, Eugene},
	year = {2002},
	pages = {199--206},
	file = {Genzel_Charniak_2002_Entropy Rate Constancy in Text.pdf:/home/user/Zotero/storage/286EAPIX/Genzel_Charniak_2002_Entropy Rate Constancy in Text.pdf:application/pdf}
}

@book{carrell-interactive-1988,
	address = {Cambridge},
	title = {Interactive {Approaches} to {Second} {Language} {Reading}},
	isbn = {978-1-139-52451-3},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139524513},
	urldate = {2016-01-28},
	publisher = {Cambridge University Press},
	editor = {Carrell, Patricia L. and Devine, Joanne and Eskey, David E.},
	year = {1988},
	file = {CBO9781139524513A013:/home/user/Zotero/storage/3EVCHIMJ/CBO9781139524513A013.pdf:application/pdf}
}

@article{bergen-strategic-2015,
	title = {The strategic use of noise in pragmatic reasoning},
	volume = {7},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/tops.12144/full},
	number = {2},
	urldate = {2016-01-28},
	journal = {Topics in cognitive science},
	author = {Bergen, Leon and Goodman, Noah D.},
	year = {2015},
	pages = {336--350},
	file = {BergenGoodman2014.pdf:/home/user/Zotero/storage/H4TAAX8C/BergenGoodman2014.pdf:application/pdf}
}

@article{failing-reward-2016,
	title = {Reward alters the perception of time},
	volume = {148},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027715301190},
	doi = {10.1016/j.cognition.2015.12.005},
	abstract = {Recent findings indicate that monetary rewards have a powerful effect on cognitive performance. In order to maximize overall gain, the prospect of earning reward biases visual attention to specific locations or stimulus features improving perceptual sensitivity and processing. The question we addressed in this study is whether the prospect of reward also affects the subjective perception of time. Here, participants performed a prospective timing task using temporal oddballs. The results show that temporal oddballs, displayed for varying durations, presented in a sequence of standard stimuli were perceived to last longer when they signaled a relatively high reward compared to when they signaled no or low reward. When instead of the oddball the standards signaled reward, the perception of the temporal oddball remained unaffected. We argue that by signaling reward, a stimulus becomes subjectively more salient thereby modulating its attentional deployment and distorting how it is perceived in time.},
	urldate = {2016-01-27},
	journal = {Cognition},
	author = {Failing, Michel and Theeuwes, Jan},
	month = mar,
	year = {2016},
	keywords = {Attention, Reward, Subjective perception, Time perception},
	pages = {19--26},
	file = {Failing_Theeuwes_2016_Reward alters the perception of time.pdf:/home/user/Zotero/storage/2WXHSUUT/Failing_Theeuwes_2016_Reward alters the perception of time.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/HDX575EH/S0010027715301190.html:text/html}
}

@article{ozcaliskan-does-2016,
	title = {Does language shape silent gesture?},
	volume = {148},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027715301153},
	doi = {10.1016/j.cognition.2015.12.001},
	abstract = {Languages differ in how they organize events, particularly in the types of semantic elements they express and the arrangement of those elements within a sentence. Here we ask whether these cross-linguistic differences have an impact on how events are represented nonverbally; more specifically, on how events are represented in gestures produced without speech (silent gesture), compared to gestures produced with speech (co-speech gesture). We observed speech and gesture in 40 adult native speakers of English and Turkish (N = 20/per language) asked to describe physical motion events (e.g., running down a path)—a domain known to elicit distinct patterns of speech and co-speech gesture in English- and Turkish-speakers. Replicating previous work (Kita \&amp; Özyürek, 2003), we found an effect of language on gesture when it was produced with speech—co-speech gestures produced by English-speakers differed from co-speech gestures produced by Turkish-speakers. However, we found no effect of language on gesture when it was produced on its own—silent gestures produced by English-speakers were identical in how motion elements were packaged and ordered to silent gestures produced by Turkish-speakers. The findings provide evidence for a natural semantic organization that humans impose on motion events when they convey those events without language.},
	urldate = {2016-01-27},
	journal = {Cognition},
	author = {Özçalışkan, Şeyda and Lucero, Ché and Goldin-Meadow, Susan},
	month = mar,
	year = {2016},
	keywords = {Cross-linguistic differences, Gesture, Language and cognition, Motion events},
	pages = {10--18},
	file = {Özçalışkan et al_2016_Does language shape silent gesture.pdf:/home/user/Zotero/storage/Q4XJTJM8/Özçalışkan et al_2016_Does language shape silent gesture.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/B8PQMRND/S0010027715301153.html:text/html}
}

@article{spratling-review-nodate,
	title = {A review of predictive coding algorithms},
	issn = {0278-2626},
	url = {http://www.sciencedirect.com/science/article/pii/S027826261530035X},
	doi = {10.1016/j.bandc.2015.11.003},
	abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term “predictive coding”. This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
	urldate = {2016-01-27},
	journal = {Brain and Cognition},
	author = {Spratling, M. W.},
	keywords = {neural networks, Free energy, Cortex, Predictive coding, Retina, Signal processing},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/UTHXKGHP/S027826261530035X.html:text/html;Spratling_A review of predictive coding algorithms.pdf:/home/user/Zotero/storage/3386D4XG/Spratling_A review of predictive coding algorithms.pdf:application/pdf}
}

@article{rayner-models-2010-1,
	title = {Models of the {Reading} {Process}},
	volume = {1},
	issn = {1939-5078},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001687/},
	doi = {10.1002/wcs.68},
	abstract = {Reading is a complex skill involving the orchestration of a number of components. Researchers often talk about a “model of reading” when talking about only one aspect of the reading process (for example, models of word identification are often referred to as “models of reading”). Here, we review prominent models that are designed to account for (1) word identification, (2) syntactic parsing, (3) discourse representations, and (4) how certain aspects of language processing (e.g., word identification), in conjunction with other constraints (e g., limited visual acuity, saccadic error, etc.), guide readers’ eyes. Unfortunately, it is the case that these various models addressing specific aspects of the reading process seldom make contact with models dealing with other aspects of reading. Thus, for example, the models of word identification seldom make contact with models of eye movement control, and vice versa. While this may be unfortunate in some ways, it is quite understandable in other ways because reading itself is a very complex process. We discuss prototypical models of aspects of the reading process in the order mentioned above. We do not review all possible models, but rather focus on those we view as being representative and most highly recognized.},
	number = {6},
	urldate = {2016-01-26},
	journal = {Wiley interdisciplinary reviews. Cognitive science},
	author = {Rayner, Keith and Reichle, Erik D.},
	year = {2010},
	pmid = {21170142},
	pmcid = {PMC3001687},
	pages = {787--799}
}

@article{rayner-eye-2009-4,
	title = {Eye movements and non-canonical reading: {Comments} on {Kennedy} and {Pynte} (2008)},
	volume = {49},
	issn = {0042-6989},
	shorttitle = {Eye movements and non-canonical reading},
	url = {http://www.sciencedirect.com/science/article/pii/S004269890800535X},
	doi = {10.1016/j.visres.2008.10.013},
	abstract = {Kennedy and Pynte [Kennedy, A., \&amp; Pynte, J. (2008). The consequences of violations to reading order: An eye movement analysis. Vision Research, 48, 2309–2320] presented data that they suggested pose problems for models of eye movement control in reading in which words are encoded serially. They focus on situations in which pairs of words are fixated out of order (i.e., the first word is skipped and the second fixated prior to a regression back to the first word). We strongly disagree with their claims and contest their arguments. We argue that their data set was obtained selectively and the events they believe are problematic do not occur frequently during reading. Furthermore, we do not consider that Kennedy and Pynte’s arguments pose serious difficulties for serial models of reading such as E-Z Reader.},
	number = {17},
	urldate = {2016-01-26},
	journal = {Vision Research},
	author = {Rayner, Keith and Pollatsek, Alexander and Liversedge, Simon P. and Reichle, Erik D.},
	month = aug,
	year = {2009},
	keywords = {Reading, Eye movement control, Models of eye movement control in reading},
	pages = {2232--2236},
	file = {Rayner et al_2009_Eye movements and non-canonical reading.pdf:/home/user/Zotero/storage/QQBE6ETA/Rayner et al_2009_Eye movements and non-canonical reading.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/FHRU3TRX/S004269890800535X.html:text/html}
}

@article{mahowald-meta-analysis-nodate,
	title = {A meta-analysis of syntactic priming in language production},
	url = {http://web.mit.edu/kylemaho/www/assets/pdf/meta.pdf},
	urldate = {2016-01-26},
	author = {Mahowald, Kyle and James, Ariel and Futrell, Richard and Gibson, Edward},
	file = {A meta-analysis of syntactic priming in language production - meta.pdf:/home/user/Zotero/storage/BVV6P578/meta.pdf:application/pdf}
}

@article{futrell-experiments-nodate,
	title = {Experiments with {Generative} {Models} for {Dependency} {Tree} {Linearization}},
	url = {http://anthology.aclweb.org/D/D15/D15-1231.pdf},
	urldate = {2016-01-26},
	author = {Futrell, Richard and Gibson, Edward},
	file = {Experiments with Generative Models for Dependency Tree Linearization - D15-1231.pdf:/home/user/Zotero/storage/QU9SNKSG/D15-1231.pdf:application/pdf}
}

@article{futrell-cross-linguistic-2015,
	title = {Cross-linguistic gestures reflect typological universals: {A} subject-initial, verb-final bias in speakers of diverse languages},
	volume = {136},
	issn = {00100277},
	shorttitle = {Cross-linguistic gestures reflect typological universals},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S001002771400242X},
	doi = {10.1016/j.cognition.2014.11.022},
	language = {en},
	urldate = {2016-01-26},
	journal = {Cognition},
	author = {Futrell, Richard and Hickey, Tina and Lee, Aldrin and Lim, Eunice and Luchkina, Elena and Gibson, Edward},
	month = mar,
	year = {2015},
	pages = {215--221},
	file = {futrell2015crosslinguistic.pdf:/home/user/Zotero/storage/JUIXF9XH/futrell2015crosslinguistic.pdf:application/pdf}
}

@article{futrell-structured-nodate,
	title = {Structured {Featural} {Representations} in a {Generative} {Model} of {Phonotactics}},
	url = {http://web.mit.edu/timod/www/papers/phonotactics.pdf},
	urldate = {2016-01-26},
	author = {Futrell, Richard and Albright, Adam and Graff, Peter and O’Donnell, Timothy J.},
	file = {phonotactics.pdf:/home/user/Zotero/storage/WHTRS4XA/phonotactics.pdf:application/pdf}
}

@article{bergen-learnability-nodate,
	title = {A {Learnability} {Analysis} of {Argument} and {Modifier} {Structure}},
	url = {http://ling.auf.net/lingbuzz/002502/current.pdf},
	urldate = {2016-01-26},
	author = {Bergen, Leon and Gibson, Edward and O’Donnell, Timothy J.},
	file = {arg-mod.pdf:/home/user/Zotero/storage/DUFRRXX4/arg-mod.pdf:application/pdf}
}

@article{hartshorne-linking-nodate,
	title = {Linking {Meaning} to {Language}: {Verbs} of {Psychological} {State} and the {Linking} {Problem}},
	shorttitle = {Linking {Meaning} to {Language}},
	url = {http://www.joshuakhartshorne.org/papers/Linking\_Submitted\_WithFigures.pdf},
	urldate = {2016-01-26},
	author = {Hartshorne, Joshua K. and O'Donnell, Timothy J. and Sudo, Yasutada and Uruwashi, Miki and Snedeker, Jesse and Hartshorne, Joshua},
	file = {Microsoft Word - Linking2_submitted.docx - psych-verbs.pdf:/home/user/Zotero/storage/UEWJVPZK/psych-verbs.pdf:application/pdf}
}

@inproceedings{luong-evaluating-2015-1,
	title = {Evaluating {Models} of {Computation} and {Storage} in {Human} {Sentence} {Processing}},
	url = {http://www.aclweb.org/anthology/W/W15/W15-24.pdf#page=26},
	urldate = {2016-01-26},
	booktitle = {{CONFERENCE} {ON} {EMPIRICAL} {METHODS} {IN} {NATURAL} {LANGUAGE} {PROCESSING}},
	author = {Luong, Minh-Thang and O’Donnell, Timothy J. and Goodman, Noah D.},
	year = {2015},
	pages = {14},
	file = {Evaluating Models of Computation and Storage in Human Sentence Processing - reading-time.pdf:/home/user/Zotero/storage/VQ9PQGFN/reading-time.pdf:application/pdf}
}

@article{hartshorne-causes-2015-1,
	title = {The causes and consequences explicit in verbs},
	volume = {30},
	issn = {2327-3798, 2327-3801},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1008524},
	doi = {10.1080/23273798.2015.1008524},
	language = {en},
	number = {6},
	urldate = {2016-01-26},
	journal = {Language, Cognition and Neuroscience},
	author = {Hartshorne, Joshua K. and O'Donnell, Timothy J. and Tenenbaum, Joshua B.},
	month = jul,
	year = {2015},
	pages = {716--734},
	file = {The causes and consequences explicit in verbs - explicit-causality.pdf:/home/user/Zotero/storage/M2NT9DP2/explicit-causality.pdf:application/pdf}
}

@article{almoammer-grammatical-2013,
	title = {Grammatical morphology as a source of early number word meanings},
	volume = {110},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1313652110},
	doi = {10.1073/pnas.1313652110},
	language = {en},
	number = {46},
	urldate = {2016-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Almoammer, A. and Sullivan, J. and Donlan, C. and Marusic, F. and Zaucer, R. and O'Donnell, T. and Barner, D.},
	month = nov,
	year = {2013},
	pages = {18448--18453},
	file = {pnas201313652 18448..18453 - dual.pdf:/home/user/Zotero/storage/HNDZKTBA/dual.pdf:application/pdf}
}

@inproceedings{bergen-arguments-2013,
	title = {Arguments and {Modifiers} from the {Learner}'s {Perspective}.},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.8061&rep=rep1&type=pdf#page=163},
	urldate = {2016-01-26},
	booktitle = {{ACL} (2)},
	publisher = {Citeseer},
	author = {Bergen, Leon and Gibson, Edward and O'Donnell, Timothy J.},
	year = {2013},
	pages = {115--119},
	file = {Arguments and Modifiers from the Learner's Perspective - arg-mod-acl.pdf:/home/user/Zotero/storage/8AI8SX7C/arg-mod-acl.pdf:application/pdf}
}

@inproceedings{fullwood-learning-2013,
	title = {Learning non-concatenative morphology},
	url = {http://www.aclweb.org/website/old\_anthology/W/W13/W13-26.pdf#page=31},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	author = {Fullwood, Michelle A. and O’donnell, Timothy J.},
	year = {2013},
	pages = {21--27},
	file = {Learning non-concatenative morphology - non-concatenative.pdf:/home/user/Zotero/storage/JTQISA29/non-concatenative.pdf:application/pdf}
}

@article{kline-transitive-nodate,
	title = {Transitive and periphrastic sentences affect memory for simple causal scenes},
	url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2013/papers/0163/paper0163.pdf},
	urldate = {2016-01-26},
	author = {Kline, Melissa and Muentener, Paul and Schulz, Laura},
	file = {Microsoft Word - Cogsci Change Blindness 042513 12PM.doc - KlineMuentenerSchulz_CogSci2013.pdf:/home/user/Zotero/storage/XEDGEZU7/KlineMuentenerSchulz_CogSci2013.pdf:application/pdf}
}

@article{kline-factors-2010,
	title = {Factors facilitating implicit learning: the case of the {Sesotho} passive},
	volume = {17},
	shorttitle = {Factors facilitating implicit learning},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10489223.2010.509268},
	number = {4},
	urldate = {2016-01-26},
	journal = {Language Acquisition},
	author = {Kline, Melissa and Demuth, Katherine},
	year = {2010},
	pages = {220--234},
	file = {Structural priming, thematic roles, and the acquisition of the passive - KlineDemuth2010_preprint.pdf:/home/user/Zotero/storage/JM9XQ2BM/KlineDemuth2010_preprint.pdf:application/pdf}
}

@article{delong-pre-processing-2014,
	title = {Pre-{Processing} in {Sentence} {Comprehension}: {Sensitivity} to {Likely} {Upcoming} {Meaning} and {Structure}: {Pre}-{Processing} in {Sentence} {Comprehension}},
	volume = {8},
	issn = {1749818X},
	shorttitle = {Pre-{Processing} in {Sentence} {Comprehension}},
	url = {http://doi.wiley.com/10.1111/lnc3.12093},
	doi = {10.1111/lnc3.12093},
	language = {en},
	number = {12},
	urldate = {2016-01-26},
	journal = {Language and Linguistics Compass},
	author = {DeLong, Katherine A. and Troyer, Melissa and Kutas, Marta},
	month = dec,
	year = {2014},
	pages = {631--645},
	file = {Pre-Processing in Sentence Comprehension\: Sensitivity to Likely Upcoming Meaning and Structure - DeLong_Et_Al_2014.pdf:/home/user/Zotero/storage/KJZXURTQ/DeLong_Et_Al_2014.pdf:application/pdf}
}

@article{futrell-corpus-nodate,
	title = {A {Corpus} {Investigation} of {Syntactic} {Embedding} in {Piraha}},
	url = {http://tedlab.mit.edu/piraha/piraha\_plos2.pdf},
	urldate = {2016-01-26},
	journal = {Plos One},
	author = {Futrell, Richard and Stearns, Laura and Everett, Daniel L. and Piantadosi, Steven T. and Gibson, Edward},
	file = {piraha_plos2.pdf:/home/user/Zotero/storage/KMCZQ6KB/piraha_plos2.pdf:application/pdf}
}

@article{piantadosi-problems-2015,
	title = {Problems in the philosophy of mathematics: {A} view from cognitive science},
	shorttitle = {Problems in the philosophy of mathematics},
	url = {http://colala.bcs.rochester.edu/papers/piantadosi2015problems.pdf},
	urldate = {2016-01-26},
	author = {Piantadosi, Steven T.},
	year = {2015},
	file = {piantadosi2015problems.pdf:/home/user/Zotero/storage/XMBHBK27/piantadosi2015problems.pdf:application/pdf}
}

@article{mollica-towards-nodate,
	title = {Towards semantically rich and recursive word learning models},
	url = {http://colala.bcs.rochester.edu/papers/mollica2015towards.pdf},
	urldate = {2016-01-26},
	author = {Mollica, Francis and Piantadosi, Steven T.},
	file = {mollica2015towards.pdf:/home/user/Zotero/storage/R48PB4QD/mollica2015towards.pdf:application/pdf}
}

@article{mollica-perceptual-nodate,
	title = {The perceptual foundation of linguistic context},
	url = {http://colala.bcs.rochester.edu/papers/mollica2015perceptual.pdf},
	urldate = {2016-01-26},
	author = {Mollica, Francis and Piantadosi, Steven T. and Tanenhaus, Michael K.},
	file = {mollica2015perceptual.pdf:/home/user/Zotero/storage/SXESJ5XJ/mollica2015perceptual.pdf:application/pdf}
}

@article{hemmer-inferring-nodate,
	title = {Inferring the {Tsimane}’s use of color categories from recognition memory},
	url = {http://colala.bcs.rochester.edu/papers/hemmer2015inferring.pdf},
	urldate = {2016-01-26},
	author = {Hemmer, Pernille and Persaud, Kimele and Kidd, Celeste and Piantadosi, Steven},
	file = {hemmer2015inferring.pdf:/home/user/Zotero/storage/8NTTC457/hemmer2015inferring.pdf:application/pdf}
}

@article{cantlon-origins-2015,
	title = {The {Origins} of {Counting} {Algorithms}},
	volume = {26},
	issn = {0956-7976, 1467-9280},
	url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797615572907},
	doi = {10.1177/0956797615572907},
	language = {en},
	number = {6},
	urldate = {2016-01-26},
	journal = {Psychological Science},
	author = {Cantlon, J. F. and Piantadosi, S. T. and Ferrigno, S. and Hughes, K. D. and Barnard, A. M.},
	month = jun,
	year = {2015},
	pages = {853--865},
	file = {cantlon2015origins.pdf:/home/user/Zotero/storage/TXTJSWQI/cantlon2015origins.pdf:application/pdf}
}

@article{piantadosi-quantitative-2014,
	title = {Quantitative {Standards} for {Absolute} {Linguistic} {Universals}},
	volume = {38},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/cogs.12088},
	doi = {10.1111/cogs.12088},
	language = {en},
	number = {4},
	urldate = {2016-01-26},
	journal = {Cognitive Science},
	author = {Piantadosi, Steven T. and Gibson, Edward},
	month = may,
	year = {2014},
	pages = {736--756},
	file = {untitled - piantadosi2014quantitative.pdf:/home/user/Zotero/storage/E9N2U7JA/piantadosi2014quantitative.pdf:application/pdf}
}

@article{piantadosi-childrens-2014,
	title = {Children's learning of number words in an indigenous farming-foraging group},
	volume = {17},
	issn = {1363755X},
	url = {http://doi.wiley.com/10.1111/desc.12078},
	doi = {10.1111/desc.12078},
	language = {en},
	number = {4},
	urldate = {2016-01-26},
	journal = {Developmental Science},
	author = {Piantadosi, Steven T. and Jara-Ettinger, Julian and Gibson, Edward},
	month = jul,
	year = {2014},
	pages = {553--563},
	file = {REV_ISS_WEB_DESC_12078_17-4 553..563 - piantadosi2014childrens.pdf:/home/user/Zotero/storage/435AQ9RW/piantadosi2014childrens.pdf:application/pdf}
}

@article{piantadosi-zipfs-2014,
	title = {Zipf’s word frequency law in natural language: {A} critical review and future directions},
	volume = {21},
	shorttitle = {Zipf’s word frequency law in natural language},
	url = {http://link.springer.com/article/10.3758/s13423-014-0585-6},
	number = {5},
	urldate = {2016-01-26},
	journal = {Psychonomic bulletin \& review},
	author = {Piantadosi, Steven T.},
	year = {2014},
	pages = {1112--1130},
	file = {piantadosi2014zipfs.pdf:/home/user/Zotero/storage/TWPS6G7T/piantadosi2014zipfs.pdf:application/pdf}
}

@article{piantadosi-efficient-2014,
	title = {Efficient estimation of {Weber}’s {W}},
	url = {http://colala.bcs.rochester.edu/papers/piantadosi2014weber.pdf},
	urldate = {2016-01-26},
	author = {Piantadosi, Steven T.},
	year = {2014},
	file = {piantadosi2015efficient.pdf:/home/user/Zotero/storage/HTGC5AZ8/piantadosi2015efficient.pdf:application/pdf}
}

@article{piantadosi-rational-nodate,
	title = {A rational analysis of the approximate number system},
	url = {http://colala.bcs.rochester.edu/papers/piantadosi2014approximate.pdf},
	urldate = {2016-01-26},
	author = {Piantadosi, Steven T.},
	file = {piantadosi2014approximate.pdf:/home/user/Zotero/storage/P32MUTK7/piantadosi2014approximate.pdf:application/pdf}
}

@article{frank-measuring-2012,
	title = {Measuring the {Development} of {Social} {Attention} {Using} {Free}-{Viewing}: {DEVELOPMENT} {OF} {SOCIAL} {ATTENTION}},
	volume = {17},
	issn = {15250008},
	shorttitle = {Measuring the {Development} of {Social} {Attention} {Using} {Free}-{Viewing}},
	url = {http://doi.wiley.com/10.1111/j.1532-7078.2011.00086.x},
	doi = {10.1111/j.1532-7078.2011.00086.x},
	language = {en},
	number = {4},
	urldate = {2016-01-26},
	journal = {Infancy},
	author = {Frank, Michael C. and Vul, Edward and Saxe, Rebecca},
	month = jul,
	year = {2012},
	pages = {355--375},
	file = {INFA_086 355..375 - FVS-infancy2012.pdf:/home/user/Zotero/storage/WZUGHMTD/FVS-infancy2012.pdf:application/pdf}
}

@article{frank-representing-2012-1,
	title = {Representing exact number visually using mental abacus.},
	volume = {141},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024427},
	doi = {10.1037/a0024427},
	language = {en},
	number = {1},
	urldate = {2016-01-26},
	journal = {Journal of Experimental Psychology: General},
	author = {Frank, Michael C. and Barner, David},
	year = {2012},
	pages = {134--149},
	file = {FB-jepg2011.pdf:/home/user/Zotero/storage/G4ZJIVMC/FB-jepg2011.pdf:application/pdf}
}

@article{frank-using-2009,
	title = {Using speakers' referential intentions to model early cross-situational word learning},
	volume = {20},
	url = {http://pss.sagepub.com/content/20/5/578.short},
	number = {5},
	urldate = {2016-01-26},
	journal = {Psychological Science},
	author = {Frank, Michael C. and Goodman, Noah D. and Tenenbaum, Joshua B.},
	year = {2009},
	pages = {578--585},
	file = {Using Speakers' Referential Intentions to Model Early Cross-Situational Word Learning - FGT-psychscience2009.pdf:/home/user/Zotero/storage/26QJPZ9J/FGT-psychscience2009.pdf:application/pdf}
}

@article{oconnor-social-2014,
	title = {Social dialect and men's voice pitch influence women's mate preferences},
	volume = {35},
	issn = {1090-5138},
	url = {http://www.sciencedirect.com/science/article/pii/S109051381400052X},
	doi = {10.1016/j.evolhumbehav.2014.05.001},
	abstract = {Low male voice pitch may communicate potential benefits for offspring in the form of heritable health and/or dominance, whereas access to resources may be indicated by correlates of socioeconomic status, such as sociolinguistic features. Here, we examine if voice pitch and social dialect influence women's perceptions of men's socioeconomic status and attractiveness. In Study 1, women perceived lower pitched male voices as higher in socioeconomic status than higher pitched male voices. In Study 2, women independently perceived lower pitched voices and higher status sociolinguistic dialects as higher in socioeconomic status and attractiveness. We also found a significant interaction wherein women preferred lower pitched men's voices more often when dialects were lower in sociolinguistic status than when they were higher in sociolinguistic status. Women also perceived lower pitched voices as higher in socioeconomic status more often when dialects were higher in sociolinguistic status than when lower in sociolinguistic status. Finally, women's own self-rated socioeconomic status was positively related to their preferences for voices with higher status sociolinguistic dialects, but not to their preferences for voice pitch. Hence, women's preferences for traits associated with potentially biologically heritable benefits, such as low voice pitch, are moderated by the presence of traits associated with resource accrual, such as social dialect markers. However, women's preferences for language markers of resource accrual may be functionally independent from preferences for potential biological indicators of heritable benefits, such as voice pitch.},
	number = {5},
	urldate = {2016-01-26},
	journal = {Evolution and Human Behavior},
	author = {O'Connor, Jillian J. M. and Fraccaro, Paul J. and Pisanski, Katarzyna and Tigue, Cara C. and O'Donnell, Timothy J. and Feinberg, David R.},
	month = sep,
	year = {2014},
	keywords = {Attractiveness, Fundamental frequency, Linguistic variables, Masculinity, Social status},
	pages = {368--375},
	file = {O'Connor et al_2014_Social dialect and men's voice pitch influence women's mate preferences.pdf:/home/user/Zotero/storage/W5ZXNMMK/O'Connor et al_2014_Social dialect and men's voice pitch influence women's mate preferences.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/QEU2G6GD/S109051381400052X.html:text/html}
}

@misc{noauthor-productivity-nodate,
	title = {Productivity and {Reuse} in {Language}},
	url = {https://mitpress.mit.edu/books/productivity-and-reuse-language},
	abstract = {A proposal for a formal model, Fragment Grammars,  that treats productivity and reuse as the target of inference in a probabilistic framework.},
	urldate = {2016-01-26},
	journal = {MIT Press},
	file = {Snapshot:/home/user/Zotero/storage/JUGDWIIU/productivity-and-reuse-language.html:text/html}
}

@article{bottou-machine-2014,
	title = {From machine learning to machine reasoning},
	volume = {94},
	url = {http://link.springer.com/article/10.1007/s10994-013-5335-x},
	number = {2},
	urldate = {2016-01-26},
	journal = {Machine learning},
	author = {Bottou, Léon},
	year = {2014},
	pages = {133--149},
	file = {tr-2011-02-08.pdf:/home/user/Zotero/storage/QZW9ACUQ/tr-2011-02-08.pdf:application/pdf}
}

@inproceedings{bicknell-evidence-2013,
	title = {Evidence for cognitively controlled saccade targeting in reading},
	url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2013/papers/0063/paper0063.pdf},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the 35th annual conference of the {Cognitive} {Science} {Society}},
	author = {Bicknell, Klinton and Higgins, Emily and Levy, Roger and Rayner, Keith},
	year = {2013},
	pages = {197--202},
	file = {bicknell-higgins-levy-rayner-2013-cogsci.pdf:/home/user/Zotero/storage/Z7VNKE4S/bicknell-higgins-levy-rayner-2013-cogsci.pdf:application/pdf}
}

@article{wasow-processing-nodate,
	title = {Processing, {Prosody}, and {Optional} to},
	url = {http://web.stanford.edu/~wasow/AmherstPaperFinalV4.pdf},
	urldate = {2016-01-26},
	author = {Wasow, Thomas and Levy, Roger and Melnick, Robin and Zhu, Hanzhi and Juzek, Tom},
	file = {AmherstPaperFinalV3 - wasow-levy-melnick-zhu-juzek-optional-to-forthcoming.pdf:/home/user/Zotero/storage/PAZVE38B/wasow-levy-melnick-zhu-juzek-optional-to-forthcoming.pdf:application/pdf}
}

@inproceedings{ferreira-good-2009,
	title = {Good enough language processing: {A} satisficing approach},
	shorttitle = {Good enough language processing},
	url = {http://ferreiralab.faculty.ucdavis.edu/wp-content/uploads/sites/222/2015/05/Ferreira-et-al.-2009\_GoodEnoughProcessing\_Cog-Sci-Society.pdf},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the 31st {Annual} conference of the {Cognitive} {Science} {Society}. {Austin}: {Cognitive} {Science} {Society}},
	author = {Ferreira, Fernanda and Engelhardt, Paul E. and Jones, Manon W.},
	year = {2009},
	file = {Microsoft Word - CogSci paper-revised_FF_final.doc - paper75.pdf:/home/user/Zotero/storage/XAGCKFJJ/paper75.pdf:application/pdf}
}

@article{von-der-malsburg-scanpaths-2013,
	title = {Scanpaths reveal syntactic underspecification and reanalysis strategies},
	volume = {28},
	issn = {0169-0965, 1464-0732},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2012.728232},
	doi = {10.1080/01690965.2012.728232},
	language = {en},
	number = {10},
	urldate = {2016-01-26},
	journal = {Language and Cognitive Processes},
	author = {von der Malsburg, Titus and Vasishth, Shravan},
	month = dec,
	year = {2013},
	pages = {1545--1578},
	file = {doi\:10.1080/01690965.2012.728232 - MalsburgVasishthLCP2013.pdf:/home/user/Zotero/storage/3WGG6NZ3/MalsburgVasishthLCP2013.pdf:application/pdf}
}

@inproceedings{fossum-sequential-2012-1,
	title = {Sequential vs. hierarchical syntactic models of human incremental sentence processing},
	url = {http://dl.acm.org/citation.cfm?id=2390313},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the 3rd {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Fossum, Victoria and Levy, Roger},
	year = {2012},
	pages = {61--69},
	file = {Sequential vs. Hierarchical Syntactic Models of Human Incremental Sentence Processing - W12-1706:/home/user/Zotero/storage/BU3C6C9D/W12-1706.pdf:application/pdf}
}

@article{kamide-integration-2003,
	title = {Integration of syntactic and semantic information in predictive processing: {Cross}-linguistic evidence from {German} and {English}},
	volume = {32},
	shorttitle = {Integration of syntactic and semantic information in predictive processing},
	url = {http://link.springer.com/article/10.1023/A:1021933015362},
	number = {1},
	urldate = {2016-01-26},
	journal = {Journal of psycholinguistic research},
	author = {Kamide, Yuki and Scheepers, Christoph and Altmann, Gerry TM},
	year = {2003},
	pages = {37--55},
	file = {PR3201_457647 - art%3A10.1023%2FA%3A1021933015362.pdf:/home/user/Zotero/storage/92WSBZTD/art%3A10.1023%2FA%3A1021933015362.pdf:application/pdf}
}

@article{koornneef-use-2006,
	title = {On the use of verb-based implicit causality in sentence comprehension: {Evidence} from self-paced reading and eye tracking},
	volume = {54},
	issn = {0749596X},
	shorttitle = {On the use of verb-based implicit causality in sentence comprehension},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X05001464},
	doi = {10.1016/j.jml.2005.12.003},
	language = {en},
	number = {4},
	urldate = {2016-01-26},
	journal = {Journal of Memory and Language},
	author = {Koornneef, A and Vanberkum, J},
	month = may,
	year = {2006},
	pages = {445--465},
	file = {doi\:10.1016/j.jml.2005.12.003 - Koornneef_2006_ontheuse.pdf:/home/user/Zotero/storage/R4VDB7VK/Koornneef_2006_ontheuse.pdf:application/pdf}
}

@article{onnis-human-2006,
	title = {Human {Language} {Processing}: {Connectionist} {Models}'},
	shorttitle = {Human {Language} {Processing}},
	url = {http://cnl.psych.cornell.edu/pubs/2006-occ-Encyc-LandL.pdf},
	urldate = {2016-01-26},
	journal = {Encyclopedia of Language \& Linguistics (Second Edition). Oxford: Elsevier},
	author = {Onnis, Luca and Christiansen, Morten and Chater, Nick},
	year = {2006},
	pages = {401--409},
	file = {Lali_h3 401..409 - 2006-occ-Encyc-LandL.pdf:/home/user/Zotero/storage/7FFFFIQS/2006-occ-Encyc-LandL.pdf:application/pdf}
}

@article{mahowald-snap-nodate,
	title = {{SNAP} {Judgments}: {A} {Small} {N} {Acceptability} {Paradigm} ({SNAP}) for {Linguistic} {Acceptability} {Judgments}},
	shorttitle = {{SNAP} {Judgments}},
	url = {http://web.mit.edu/kylemaho/www/SNAP.pdf},
	urldate = {2016-01-26},
	author = {Mahowald, Kyle and Graff, Peter and Hartman, Jeremy and Gibson, Edward},
	file = {Microsoft Word - SNAP_revision1_submit.docx - SNAP.pdf:/home/user/Zotero/storage/W6WFACKA/SNAP.pdf:application/pdf}
}

@article{futrell-large-scale-2015,
	title = {Large-scale evidence of dependency length minimization in 37 languages},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1502134112},
	doi = {10.1073/pnas.1502134112},
	language = {en},
	number = {33},
	urldate = {2016-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
	month = aug,
	year = {2015},
	pages = {10336--10341},
	file = {dep.pdf:/home/user/Zotero/storage/WEAANQUC/dep.pdf:application/pdf}
}

@article{gibson-pragmatic-2015,
	title = {A {Pragmatic} {Account} of {Complexity} in {Definite} {Antecedent}-{Contained}-{Deletion} {Relative} {Clauses}},
	volume = {32},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/cgi/doi/10.1093/jos/ffu006},
	doi = {10.1093/jos/ffu006},
	language = {en},
	number = {4},
	urldate = {2016-01-26},
	journal = {Journal of Semantics},
	author = {Gibson, E. and Jacobson, P. and Graff, P. and Mahowald, K. and Fedorenko, E. and Piantadosi, S. T.},
	month = nov,
	year = {2015},
	pages = {579--618},
	file = {OP-SEMA140013 1..40 - gibsonetal_jsemantics.pdf:/home/user/Zotero/storage/BZ92KRQS/gibsonetal_jsemantics.pdf:application/pdf}
}

@article{mahowald-short-2013,
	title = {Short, frequent words are more likely to appear genetically related by chance},
	volume = {110},
	url = {http://www.pnas.org/content/110/35/E3253.short},
	number = {35},
	urldate = {2016-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mahowald, Kyle and Gibson, Edward},
	year = {2013},
	pages = {E3253--E3253},
	file = {pnas201308822 1..1 - mahowald_pnas.pdf:/home/user/Zotero/storage/FJJZ8FP2/mahowald_pnas.pdf:application/pdf}
}

@article{mahowald-info/information-2013,
	title = {Info/information theory: {Speakers} choose shorter words in predictive contexts},
	volume = {126},
	issn = {00100277},
	shorttitle = {Info/information theory},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027712002107},
	doi = {10.1016/j.cognition.2012.09.010},
	language = {en},
	number = {2},
	urldate = {2016-01-26},
	journal = {Cognition},
	author = {Mahowald, Kyle and Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward},
	month = feb,
	year = {2013},
	pages = {313--318},
	file = {Info/information theory\: Speakers choose shorter words in predictive contexts - mahowald_info.pdf:/home/user/Zotero/storage/WIVTBV8N/mahowald_info.pdf:application/pdf}
}

@inproceedings{futrell-quantifying-2015,
	title = {Quantifying word order freedom in dependency corpora},
	url = {http://www.aclweb.org/anthology/W15-21#page=101},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the {Third} {International} {Conference} on {Dependency} {Linguistics} ({Depling} 2015)},
	author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
	year = {2015},
	pages = {91--100},
	file = {Quantifying Word Order Freedom in Dependency Corpora - depling-conf.pdf:/home/user/Zotero/storage/W3UNRRZZ/depling-conf.pdf:application/pdf}
}

@misc{noauthor-use-nodate,
	title = {On the use of verb-based implicit causality in sentence comprehension: {Evidence} from self-paced reading and eye tracking — {Max} {Planck} {Institute} for {Psycholinguistics}},
	url = {http://www.mpi.nl/publications/escidoc-59343/@@popup},
	urldate = {2016-01-26},
	file = {On the use of verb-based implicit causality in sentence comprehension\: Evidence from self-paced reading and eye tracking — Max Planck Institute for Psycholinguistics:/home/user/Zotero/storage/WXF5KFBF/@@popup.html:text/html}
}

@article{bankova-graded-2016,
	title = {Graded {Entailment} for {Compositional} {Distributional} {Semantics}},
	url = {http://arxiv.org/abs/1601.04908},
	abstract = {The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categorical compositional distributional semantics. Taking advantage of the abstract categorical framework allows us to vary our choice of model. This enables the introduction of a notion of entailment, exploiting ideas from the categorical semantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce a novel robust graded order capturing the entailment strength between concepts. This graded measure emerges from a general framework for approximate entailment, induced by any commutative monoid. Quantum logic embeds in our graded order. Our main theorem shows that entailment strength lifts compositionally to the sentence level, giving a lower bound on sentence entailment. We describe the essential properties of graded entailment such as continuity, and provide a procedure for calculating entailment strength.},
	urldate = {2016-01-26},
	journal = {arXiv:1601.04908 [quant-ph]},
	author = {Bankova, Desislava and Coecke, Bob and Lewis, Martha and Marsden, Daniel},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.04908},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Mathematics - Category Theory, Quantum Physics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XG5BFZ4M/1601.html:text/html;Bankova et al_2016_Graded Entailment for Compositional Distributional Semantics.pdf:/home/user/Zotero/storage/HZD478DZ/Bankova et al_2016_Graded Entailment for Compositional Distributional Semantics.pdf:application/pdf}
}

@article{lewis-concept-2016,
	title = {Concept {Generation} in {Language} {Evolution}},
	url = {http://arxiv.org/abs/1601.06732},
	abstract = {This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation.},
	urldate = {2016-01-26},
	journal = {arXiv:1601.06732 [cs]},
	author = {Lewis, Martha and Lawry, Jonathan},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06732},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/H3AA4WBH/1601.html:text/html;Lewis_Lawry_2016_Concept Generation in Language Evolution.pdf:/home/user/Zotero/storage/BI933ZF4/Lewis_Lawry_2016_Concept Generation in Language Evolution.pdf:application/pdf}
}

@article{cheng-long-2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	abstract = {Teaching machines to process text with psycholinguistics insights is a challenging task. We propose an attentive machine reader that reads text from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long Short-Term Memory architecture, which, different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference.},
	urldate = {2016-01-26},
	journal = {arXiv:1601.06733 [cs]},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06733},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/475BP3W5/1601.html:text/html;Cheng et al_2016_Long Short-Term Memory-Networks for Machine Reading.pdf:/home/user/Zotero/storage/K2KK2UIV/Cheng et al_2016_Long Short-Term Memory-Networks for Machine Reading.pdf:application/pdf}
}

@article{hwang-character-level-2016,
	title = {Character-{Level} {Incremental} {Speech} {Recognition} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1601.06581},
	abstract = {In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90\% on the Wall Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284 training set.},
	urldate = {2016-01-26},
	journal = {arXiv:1601.06581 [cs]},
	author = {Hwang, Kyuyeon and Sung, Wonyong},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06581},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/N6RKIK8V/1601.html:text/html;Hwang_Sung_2016_Character-Level Incremental Speech Recognition with Recurrent Neural Networks.pdf:/home/user/Zotero/storage/87UTA2VI/Hwang_Sung_2016_Character-Level Incremental Speech Recognition with Recurrent Neural Networks.pdf:application/pdf}
}

@article{nguyen-kernel-2016,
	title = {A {Kernel} {Independence} {Test} for {Geographical} {Language} {Variation}},
	url = {http://arxiv.org/abs/1601.06579},
	abstract = {Quantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation. However, existing approaches have important drawbacks. First, they make unjustified assumptions about the nature of spatial variation: some assume that the geographical distribution of linguistic variables is Gaussian, while others assume that linguistic variation is aligned to pre-defined geopolitical units such as states or counties. Second, they are not applicable to all types of linguistic data: some approaches apply only to frequencies, others to boolean indicators of whether a linguistic variable is present. We present a new method for measuring geographical language variation, which solves both of these problems. Our approach builds on reproducing kernel Hilbert space (RKHS) representations for nonparametric statistics, and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real datasets: a corpus of Dutch tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in North American newspapers. Our proposed test is shown to support robust inferences across a broad range of scenarios and types of data.},
	urldate = {2016-01-26},
	journal = {arXiv:1601.06579 [cs]},
	author = {Nguyen, Dong and Eisenstein, Jacob},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06579},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/W52VVGZE/1601.html:text/html;Nguyen_Eisenstein_2016_A Kernel Independence Test for Geographical Language Variation.pdf:/home/user/Zotero/storage/8E3ZNEG5/Nguyen_Eisenstein_2016_A Kernel Independence Test for Geographical Language Variation.pdf:application/pdf}
}

@article{denton-exploiting-2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	url = {http://arxiv.org/abs/1404.0736},
	abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1\% of the original model.},
	urldate = {2016-01-26},
	journal = {arXiv:1404.0736 [cs]},
	author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.0736},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/JGTKTHMX/1404.html:text/html;Denton et al_2014_Exploiting Linear Structure Within Convolutional Networks for Efficient.pdf:/home/user/Zotero/storage/5AMM86B3/Denton et al_2014_Exploiting Linear Structure Within Convolutional Networks for Efficient.pdf:application/pdf}
}

@article{denil-predicting-2013,
	title = {Predicting {Parameters} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1306.0543},
	abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
	urldate = {2016-01-26},
	journal = {arXiv:1306.0543 [cs, stat]},
	author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and de Freitas, Nando},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.0543},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QUPZ38MU/1306.html:text/html;Denil et al_2013_Predicting Parameters in Deep Learning.pdf:/home/user/Zotero/storage/H2RGKDG7/Denil et al_2013_Predicting Parameters in Deep Learning.pdf:application/pdf}
}

@article{brakel-strong-2009,
	title = {Strong systematicity in sentence processing by simple recurrent networks},
	url = {http://141.14.165.6/CogSci09/papers/344/paper344.pdf},
	urldate = {2016-01-26},
	journal = {Cogsci 2009 proceedings. Austin, TX. Web site: http://csjarchive. cogsci. rpi. edu/proceedings/2009/papers/344/index. html},
	author = {Brakel, Philémon and Frank, Stefan L.},
	year = {2009},
	file = {paper344.pdf:/home/user/Zotero/storage/AI9VJ7JS/paper344.pdf:application/pdf}
}

@article{frank-cross-linguistic-2015-1,
	title = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}: {Working}-{Memory} {Constraints} or {Language} {Statistics}?},
	shorttitle = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12247/full},
	urldate = {2016-01-26},
	journal = {Cognitive science},
	author = {Frank, Stefan L. and Trompenaars, Thijs and Vasishth, Shravan},
	year = {2015},
	file = {() - GrammaticalityIllusion.pdf:/home/user/Zotero/storage/JW42N2GF/GrammaticalityIllusion.pdf:application/pdf}
}

@article{frank-cross-linguistic-2015-2,
	title = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}: {Working}-{Memory} {Constraints} or {Language} {Statistics}?},
	shorttitle = {Cross-{Linguistic} {Differences} in {Processing} {Double}-{Embedded} {Relative} {Clauses}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12247/full},
	urldate = {2016-01-26},
	journal = {Cognitive science},
	author = {Frank, Stefan L. and Trompenaars, Thijs and Vasishth, Shravan},
	year = {2015},
	file = {() - GrammaticalityIllusion.pdf:/home/user/Zotero/storage/U2USKAXX/GrammaticalityIllusion.pdf:application/pdf}
}

@inproceedings{frank-modelling-2014-1,
	title = {Modelling reading times in bilingual sentence comprehension},
	url = {http://iworx6.webxtra.net/~stefanfr/pubs/bilingual\_models.pdf},
	urldate = {2016-01-26},
	booktitle = {Proceedings of the 36th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {Frank, Stefan L.},
	year = {2014},
	pages = {1860--1861},
	file = {bilingual_models.pdf:/home/user/Zotero/storage/B25I895B/bilingual_models.pdf:application/pdf}
}

@article{brown-hierarchical-2015,
	title = {A {Hierarchical} {Generative} {Framework} of {Language} {Processing}: {Linking} {Language} {Perception}, {Interpretation}, and {Production} {Abnormalities} in {Schizophrenia}},
	shorttitle = {A {Hierarchical} {Generative} {Framework} of {Language} {Processing}},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2015.00643/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Neuroscience-w4-2016},
	doi = {10.3389/fnhum.2015.00643},
	abstract = {Language and thought dysfunction are central to the schizophrenia syndrome. They are evident in the major symptoms of psychosis itself, particularly as disorganized language output (positive thought disorder) and auditory verbal hallucinations (AVHs), and they also manifest as abnormalities in both high-level semantic and contextual processing and low-level perception. However, the literatures characterizing these abnormalities have largely been separate and have sometimes provided mutually exclusive accounts of aberrant language in schizophrenia. In this review, we propose that recent generative probabilistic frameworks of language processing can provide crucial insights that link these four lines of research. We first outline neural and cognitive evidence that real-time language comprehension and production normally involve internal generative circuits that propagate probabilistic predictions to perceptual cortices — predictions that are incrementally updated based on prediction error signals as new inputs are encountered. We then explain how disruptions to these circuits may compromise communicative abilities in schizophrenia by reducing the efficiency and robustness of both high-level language processing and low-level speech perception. We also argue that such disruptions may contribute to the phenomenology of thought-disordered speech and false perceptual inferences in the language system (i.e., AVHs). This perspective suggests a number of productive avenues for future research that may elucidate not only the mechanisms of language abnormalities in schizophrenia, but also promising directions for cognitive rehabilitation.},
	urldate = {2016-01-25},
	journal = {Frontiers in Human Neuroscience},
	author = {Brown, Meredith and Kuperberg, Gina R.},
	year = {2015},
	keywords = {language, speech perception, Schizophrenia, auditory verbal hallucinations, generative models, thought disorder},
	pages = {643}
}

@article{omrak-genomic-2016,
	title = {Genomic {Evidence} {Establishes} {Anatolia} as the {Source} of the {European} {Neolithic} {Gene} {Pool}},
	volume = {26},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S096098221501516X},
	doi = {10.1016/j.cub.2015.12.019},
	abstract = {Summary
Anatolia and the Near East have long been recognized as the epicenter of the Neolithic expansion through archaeological evidence. Recent archaeogenetic studies on Neolithic European human remains have shown that the Neolithic expansion in Europe was driven westward and northward by migration from a supposed Near Eastern origin [1–5]. However, this expansion and the establishment of numerous culture complexes in the Aegean and Balkans did not occur until 8,500 before present (BP), over 2,000 years after the initial settlements in the Neolithic core area [6–9]. We present ancient genome-wide sequence data from 6,700-year-old human remains excavated from a Neolithic context in Kumtepe, located in northwestern Anatolia near the well-known (and younger) site Troy [10]. Kumtepe is one of the settlements that emerged around 7,000 BP, after the initial expansion wave brought Neolithic practices to Europe. We show that this individual displays genetic similarities to the early European Neolithic gene pool and modern-day Sardinians, as well as a genetic affinity to modern-day populations from the Near East and the Caucasus. Furthermore, modern-day Anatolians carry signatures of several admixture events from different populations that have diluted this early Neolithic farmer component, explaining why modern-day Sardinian populations, instead of modern-day Anatolian populations, are genetically more similar to the people that drove the Neolithic expansion into Europe. Anatolia’s central geographic location appears to have served as a connecting point, allowing a complex contact network with other areas of the Near East and Europe throughout, and after, the Neolithic.},
	number = {2},
	urldate = {2016-01-25},
	journal = {Current Biology},
	author = {Omrak, Ayça and Günther, Torsten and Valdiosera, Cristina and Svensson, Emma M. and Malmström, Helena and Kiesewetter, Henrike and Aylward, William and Storå, Jan and Jakobsson, Mattias and Götherström, Anders},
	month = jan,
	year = {2016},
	pages = {270--275},
	file = {Omrak et al_2016_Genomic Evidence Establishes Anatolia as the Source of the European Neolithic.pdf:/home/user/Zotero/storage/DPM5GU7V/Omrak et al_2016_Genomic Evidence Establishes Anatolia as the Source of the European Neolithic.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/87RCH4PZ/S096098221501516X.html:text/html}
}

@article{yin-neural-2015,
	title = {Neural {Enquirer}: {Learning} to {Query} {Tables} with {Natural} {Language}},
	shorttitle = {Neural {Enquirer}},
	url = {http://arxiv.org/abs/1512.00965},
	abstract = {We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully "neuralized": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.},
	urldate = {2016-01-25},
	journal = {arXiv:1512.00965 [cs]},
	author = {Yin, Pengcheng and Lu, Zhengdong and Li, Hang and Kao, Ben},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00965},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ECCSKB8V/1512.html:text/html;Yin et al_2015_Neural Enquirer.pdf:/home/user/Zotero/storage/ZA8M8BSB/Yin et al_2015_Neural Enquirer.pdf:application/pdf}
}

@article{kerr-generalized-2000,
	title = {Generalized phase space version of {Langevin} equations and associated {Fokker}-{Planck} equations},
	volume = {15},
	issn = {1434-6028},
	url = {http://link.springer.com/10.1007/s100510051129},
	doi = {10.1007/s100510051129},
	language = {en},
	number = {2},
	urldate = {2016-01-25},
	journal = {The European Physical Journal B},
	author = {Kerr, W.C. and Graham, A.J.},
	month = may,
	year = {2000},
	pages = {305--311}
}

@article{shannon-mathematical-2001,
	title = {A mathematical theory of communication},
	volume = {5},
	url = {http://dl.acm.org/citation.cfm?id=584093},
	number = {1},
	urldate = {2016-01-25},
	journal = {ACM SIGMOBILE Mobile Computing and Communications Review},
	author = {Shannon, Claude Elwood},
	year = {2001},
	pages = {3--55},
	file = {shannon1948.dvi - 1a60974412770d1dfd200d28597f3c03.pdf:/home/user/Zotero/storage/56FB29F3/1a60974412770d1dfd200d28597f3c03.pdf:application/pdf}
}

@article{grainger-vision-nodate,
	title = {A {Vision} of {Reading}},
	volume = {0},
	issn = {1364-6613},
	url = {http://www.cell.com/article/S1364661315003137/abstract},
	doi = {10.1016/j.tics.2015.12.008},
	abstract = {Different fields of research within the cognitive sciences have investigated basic processes in reading, but progress has been hampered by limited cross-fertilization. We propose a theoretical framework aimed at facilitating integration of findings obtained via these different approaches with respect to the impact of visual factors on reading. We describe a specialized system for parallel letter processing that assigns letter identities to different locations along the horizontal meridian within the limits imposed by visual acuity and crowding. Spatial attention is used to set up this system during reading development, and difficulty in doing so has repercussions in terms of efficient translation of the orthographic code into its phonological counterpart, and fast access to semantics from print., Processing of orthographic information begins with scale-invariant gaze-centered letter detectors that conjunctively encode letter identity and letter location. Visual acuity, crowding, and spatial attention conjointly determine activity in these gaze-centered letter detectors., Location-invariant orthographic processing involves the computation of orthographic chunks and orthographic features. Orthographic chunks encode highly co-occurring letter combinations using precise information about letter order. Orthographic features encode diagnostic information with respect to word identity using more flexible letter position information., Orthographic processing operates in parallel across multiple words and is pooled into a single processing channel, hence allowing orthographic overlap across neighboring words in a sentence to exert a mutually facilitatory influence.},
	language = {English},
	number = {0},
	urldate = {2016-01-25},
	journal = {Trends in Cognitive Sciences},
	author = {Grainger, Jonathan and Dufau, Stéphane and Ziegler, Johannes C.},
	keywords = {Eye Movements, Reading, orthographic processing, dyslexia, letter visibility},
	file = {Grainger et al_A Vision of Reading.pdf:/home/user/Zotero/storage/4APCD7XX/Grainger et al_A Vision of Reading.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/F4HUW9B7/S1364-6613(15)00313-7.html:text/html}
}

@article{ambati-incremental-nodate,
	title = {An {Incremental} {Algorithm} for {Transition}-based {CCG} {Parsing}},
	url = {http://www.aclweb.org/anthology/N/N15/N15-1006.pdf},
	urldate = {2016-01-25},
	author = {Ambati, Bharat Ram and Deoskar, Tejaswini and Johnson, Mark and Steedman, Mark},
	file = {An Incremental Algorithm for Transition-based CCG Parsing - N15-1006:/home/user/Zotero/storage/WCEG2QH8/N15-1006.pdf:application/pdf}
}

@article{reddy-large-scale-2014,
	title = {Large-scale semantic parsing without question-answer pairs},
	volume = {2},
	url = {http://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/398},
	urldate = {2016-01-25},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Reddy, Siva and Lapata, Mirella and Steedman, Mark},
	year = {2014},
	pages = {377--392},
	file = {Q14-1030:/home/user/Zotero/storage/PAQEGNI3/Q14-1030.pdf:application/pdf}
}

@article{giraudo-operads-2014,
	title = {Operads, quasiorders, and regular languages},
	url = {http://arxiv.org/abs/1401.2010},
	abstract = {We generalize the construction of multitildes in the aim to provide multitilde operators for regular languages. We show that the underliying algebraic structure involves the action of some operads. An operad is an algebraic structure that mimics the composition of the functions. The involved operads are described in terms of combinatorial objects. These operads are obtained from more primitive objects, namely precompositions, whose algebraic counter-parts are investigated. One of these operads acts faithfully on languages in the sense that two different operators act in two different ways.},
	urldate = {2016-01-25},
	journal = {arXiv:1401.2010 [cs, math]},
	author = {Giraudo, Samuele and Luque, Jean-Gabriel and Mignot, Ludovic and Nicart, Florent},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.2010},
	keywords = {Computer Science - Formal Languages and Automata Theory, Mathematics - Combinatorics, 68Q70, 68Q45, 18D50},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XJRDBT9W/1401.html:text/html;Giraudo et al_2014_Operads, quasiorders, and regular languages.pdf:/home/user/Zotero/storage/AP2ZWPFV/Giraudo et al_2014_Operads, quasiorders, and regular languages.pdf:application/pdf}
}

@article{vincent-lamarre-latent-2014,
	title = {The {Latent} {Structure} of {Dictionaries}},
	url = {http://arxiv.org/abs/1411.0129},
	abstract = {How many words (and which ones) are sufficient to define all other words? When dictionaries are analyzed as directed graphs with links from defining words to defined words, they reveal a latent structure. Recursively removing all words that are reachable by definition but that do not define any further words reduces the dictionary to a Kernel of about 10\%. This is still not the smallest number of words that can define all the rest. About 75\% of the Kernel turns out to be its Core, a Strongly Connected Subset of words with a definitional path to and from any pair of its words and no word's definition depending on a word outside the set. But the Core cannot define all the rest of the dictionary. The 25\% of the Kernel surrounding the Core consists of small strongly connected subsets of words: the Satellites. The size of the smallest set of words that can define all the rest (the graph's Minimum Feedback Vertex Set or MinSet) is about 1\% of the dictionary, 15\% of the Kernel, and half-Core, half-Satellite. But every dictionary has a huge number of MinSets. The Core words are learned earlier, more frequent, and less concrete than the Satellites, which in turn are learned earlier and more frequent but more concrete than the rest of the Dictionary. In principle, only one MinSet's words would need to be grounded through the sensorimotor capacity to recognize and categorize their referents. In a dual-code sensorimotor-symbolic model of the mental lexicon, the symbolic code could do all the rest via re-combinatory definition.},
	urldate = {2016-01-25},
	journal = {arXiv:1411.0129 [cs]},
	author = {Vincent-Lamarre, Philippe and Massé, Alexandre Blondin and Lopes, Marcos and Lord, Mélanie and Marcotte, Odile and Harnad, Stevan},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.0129},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2FH2PWU7/1411.html:text/html;Vincent-Lamarre et al_2014_The Latent Structure of Dictionaries.pdf:/home/user/Zotero/storage/ZZQRP9NU/Vincent-Lamarre et al_2014_The Latent Structure of Dictionaries.pdf:application/pdf}
}

@article{narayan-paraphrase-2016,
	title = {Paraphrase {Generation} from {Latent}-{Variable} {PCFGs} for {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/1601.06068},
	abstract = {One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries -- there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser significantly improves over strong baselines.},
	urldate = {2016-01-25},
	journal = {arXiv:1601.06068 [cs]},
	author = {Narayan, Shashi and Reddy, Siva and Cohen, Shay B.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06068},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1601.06068 PDF:/home/user/Zotero/storage/VFW2Z6N2/Narayan et al. - 2016 - Paraphrase Generation from Latent-Variable PCFGs f.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/TCA6A4X9/1601.html:text/html}
}

@article{zhang-fitting-2011,
	title = {On {Fitting} {Generalized} {Linear} {Mixed}-effects {Models} for {Binary} {Responses} using {Different} {Statistical} {Packages}},
	volume = {30},
	issn = {0277-6715},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175267/},
	doi = {10.1002/sim.4265},
	abstract = {The generalized linear mixed-effects model (GLMM) is a popular paradigm to extend models for cross-sectional data to a longitudinal setting. When applied to modeling binary responses, different software packages and even different procedures within a package may give quite different results. In this report, we describe the statistical approaches that underlie these different procedures and discuss their strengths and weaknesses when applied to fit correlated binary responses. We then illustrate these considerations by applying these procedures implemented in some popular software packages to simulated and real study data. Our simulation results indicate a lack of reliability for most of the procedures considered, which carries significant implications for applying such popular software packages in practice.},
	number = {20},
	urldate = {2016-02-12},
	journal = {Statistics in medicine},
	author = {Zhang, Hui and Lu, Naiji and Feng, Changyong and Thurston, Sally W. and Xia, Yinglin and Tu, Xin M.},
	month = sep,
	year = {2011},
	pmid = {21671252},
	pmcid = {PMC3175267},
	pages = {2562--2572},
	file = {Zhang et al_2011_On Fitting Generalized Linear Mixed-effects Models for Binary Responses using.pdf:/home/user/Zotero/storage/WDW8QHF6/Zhang et al_2011_On Fitting Generalized Linear Mixed-effects Models for Binary Responses using.pdf:application/pdf}
}

@article{shapiro-analysis-1965,
	title = {An {Analysis} of {Variance} {Test} for {Normality} ({Complete} {Samples})},
	volume = {52},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2333709},
	doi = {10.2307/2333709},
	number = {3/4},
	urldate = {2016-02-12},
	journal = {Biometrika},
	author = {Shapiro, S. S. and Wilk, M. B.},
	year = {1965},
	pages = {591--611},
	file = {Shapiro_Wilk_1965_An Analysis of Variance Test for Normality (Complete Samples).pdf:/home/user/Zotero/storage/2PNSQBJS/Shapiro_Wilk_1965_An Analysis of Variance Test for Normality (Complete Samples).pdf:application/pdf}
}

@article{schaul-no-2012,
	title = {No more pesky learning rates},
	url = {http://arxiv.org/abs/1206.1106},
	urldate = {2016-02-12},
	journal = {arXiv preprint arXiv:1206.1106},
	author = {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
	year = {2012},
	file = {1206.1106.pdf:/home/user/Zotero/storage/NU9I7B8M/1206.1106.pdf:application/pdf}
}

@article{lovell-simple-2008,
	title = {A {Simple} {Proof} of the {FWL} {Theorem}},
	volume = {39},
	issn = {0022-0485, 2152-4068},
	url = {http://www.tandfonline.com/doi/abs/10.3200/JECE.39.1.88-91},
	doi = {10.3200/JECE.39.1.88-91},
	language = {en},
	number = {1},
	urldate = {2016-02-12},
	journal = {The Journal of Economic Education},
	author = {Lovell, Michael C.},
	month = jan,
	year = {2008},
	pages = {88--91},
	file = {HLDR00108-13-Lovell.tex - JECE.39.1.88-91:/home/user/Zotero/storage/XD98EF33/JECE.39.1.pdf:application/pdf}
}

@article{latour-reassembling-2005,
	title = {Reassembling the social-an introduction to actor-network-theory},
	volume = {1},
	url = {http://adsabs.harvard.edu/abs/2005reso.book.....L%EF%BF%BD%C3%9C},
	urldate = {2016-02-09},
	journal = {Reassembling the Social-An Introduction to Actor-Network-Theory, by Bruno Latour, pp. 316. Foreword by Bruno Latour. Oxford University Press, Sep 2005. ISBN-10: 0199256047. ISBN-13: 9780199256044},
	author = {Latour, Bruno},
	year = {2005},
	file = {paris-princeton.pdf:/home/user/Zotero/storage/NN4G4EFW/paris-princeton.pdf:application/pdf}
}

@techreport{cardaliaguet-notes-2010,
	title = {Notes on mean field games},
	url = {http://www.math.unipd.it/~dottmath/corsi2013/lecturenotes/Cardaliaguet/Cardaliaguet\_MFG\_2012.pdf},
	urldate = {2016-02-09},
	institution = {Technical report},
	author = {Cardaliaguet, Pierre},
	year = {2010},
	file = {MFG100629.pdf:/home/user/Zotero/storage/ZEMZ5HBV/MFG100629.pdf:application/pdf}
}

@article{latour-reassembling-2005-1,
	title = {Reassembling the social-an introduction to actor-network-theory},
	volume = {1},
	url = {http://adsabs.harvard.edu/abs/2005reso.book.....L%EF%BF%BD%C3%9C},
	urldate = {2016-02-09},
	journal = {Reassembling the Social-An Introduction to Actor-Network-Theory, by Bruno Latour, pp. 316. Foreword by Bruno Latour. Oxford University Press, Sep 2005. ISBN-10: 0199256047. ISBN-13: 9780199256044},
	author = {Latour, Bruno},
	year = {2005},
	file = {paris-princeton.pdf:/home/user/Zotero/storage/VBRUCGNM/paris-princeton.pdf:application/pdf}
}

@incollection{caines-mean-2013,
	title = {Mean {Field} {Games}},
	copyright = {©2014 Springer-Verlag London},
	isbn = {978-1-4471-5102-9},
	url = {http://link.springer.com/referenceworkentry/10.1007/978-1-4471-5102-9\_30-1},
	language = {en},
	urldate = {2016-02-09},
	booktitle = {Encyclopedia of {Systems} and {Control}},
	publisher = {Springer London},
	author = {Caines, Peter E.},
	editor = {Baillieul, John and Samad, Tariq},
	year = {2013},
	doi = {10.1007/978-1-4471-5102-9\_30-1},
	keywords = {Artificial Intelligence (incl. Robotics), Control, Industrial and Production Engineering, Industrial Chemistry/Chemical Engineering, Optimization, Systems Theory, Control},
	pages = {1--6},
	file = {Caines_2013_Mean Field Games.pdf:/home/user/Zotero/storage/X3JCSX6E/Caines_2013_Mean Field Games.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/P5N56SBE/10.html:text/html}
}

@misc{noauthor-large-nodate,
	title = {Large investor trading impacts on volatility},
	url = {http://www.sciencedirect.com/science/article/pii/S0294144906000412},
	urldate = {2016-02-09},
	file = {Large investor trading impacts on volatility:/home/user/Zotero/storage/SCZB8T94/S0294144906000412.html:text/html}
}

@article{jost-modeling-2014,
	title = {Modeling epigenome folding: formation and dynamics of topologically associated chromatin domains},
	volume = {42},
	issn = {0305-1048},
	shorttitle = {Modeling epigenome folding},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4150797/},
	doi = {10.1093/nar/gku698},
	abstract = {Genomes of eukaryotes are partitioned into domains of functionally distinct chromatin states. These domains are stably inherited across many cell generations and can be remodeled in response to developmental and external cues, hence contributing to the robustness and plasticity of expression patterns and cell phenotypes. Remarkably, recent studies indicate that these 1D epigenomic domains tend to fold into 3D topologically associated domains forming specialized nuclear chromatin compartments. However, the general mechanisms behind such compartmentalization including the contribution of epigenetic regulation remain unclear. Here, we address the question of the coupling between chromatin folding and epigenome. Using polymer physics, we analyze the properties of a block copolymer model that accounts for local epigenomic information. Considering copolymers build from the epigenomic landscape of Drosophila, we observe a very good agreement with the folding patterns observed in chromosome conformation capture experiments. Moreover, this model provides a physical basis for the existence of multistability in epigenome folding at sub-chromosomal scale. We show how experiments are fully consistent with multistable conformations where topologically associated domains of the same epigenomic state interact dynamically with each other. Our approach provides a general framework to improve our understanding of chromatin folding during cell cycle and differentiation and its relation to epigenetics.},
	number = {15},
	urldate = {2016-02-09},
	journal = {Nucleic Acids Research},
	author = {Jost, Daniel and Carrivain, Pascal and Cavalli, Giacomo and Vaillant, Cédric},
	month = sep,
	year = {2014},
	pmid = {25092923},
	pmcid = {PMC4150797},
	pages = {9553--9561}
}

@article{florian-jaeger-redundancy-2010-1,
	title = {Redundancy and reduction: {Speakers} manage syntactic information density},
	volume = {61},
	issn = {00100285},
	shorttitle = {Redundancy and reduction},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010028510000083},
	doi = {10.1016/j.cogpsych.2010.02.002},
	language = {en},
	number = {1},
	urldate = {2016-02-09},
	journal = {Cognitive Psychology},
	author = {Florian Jaeger, T.},
	month = aug,
	year = {2010},
	pages = {23--62},
	file = {Redundancy and reduction\: Speakers manage syntactic information density - 1-s2.0-S0010028510000083-main.pdf:/home/user/Zotero/storage/IAVF5VMB/1-s2.0-S0010028510000083-main.pdf:application/pdf}
}

@book{lehmann-theory-1998,
	address = {New York},
	edition = {2nd ed},
	series = {Springer texts in statistics},
	title = {Theory of point estimation},
	isbn = {978-0-387-98502-2},
	publisher = {Springer},
	author = {Lehmann, E. L. and Casella, George},
	year = {1998},
	keywords = {Fix-point estimation},
	file = {E.L.Lehmann, G.Casella - Theory of point estimation. 2nd edition.pdf:/home/user/Zotero/storage/6IDRVVVZ/E.L.Lehmann, G.Casella - Theory of point estimation. 2nd edition.pdf:application/pdf}
}

@article{yulia-not-nodate,
	title = {Not {All} {Contexts} {Are} {Created} {Equal}: {Better} {Word} {Representations} with {Variable} {Attention}},
	shorttitle = {Not {All} {Contexts} {Are} {Created} {Equal}},
	url = {http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP161.pdf},
	urldate = {2016-02-09},
	author = {Yulia, Wang Ling Lin Chu-Cheng and Amir, Tsvetkov Silvio and Alan, Ramón Fernandez Astudillo Chris Dyer and Trancoso, W. Black Isabel},
	file = {emnlp2015-2.pdf:/home/user/Zotero/storage/GNZMM3IN/emnlp2015-2.pdf:application/pdf}
}

@article{veness-compress-2014,
	title = {Compress and control},
	url = {http://arxiv.org/abs/1411.5326},
	urldate = {2016-02-09},
	journal = {arXiv preprint arXiv:1411.5326},
	author = {Veness, Joel and Bellemare, Marc G. and Hutter, Marcus and Chua, Alvin and Desjardins, Guillaume},
	year = {2014},
	file = {1411.5326v1.pdf:/home/user/Zotero/storage/4EVR3ZRG/1411.5326v1.pdf:application/pdf}
}

@inproceedings{demberg-computational-2009,
	title = {A computational model of prediction in human parsing: {Unifying} locality and surprisal effects},
	shorttitle = {A computational model of prediction in human parsing},
	url = {http://www.coli.uni-saarland.de/~vera/demberg\_keller\_cogsci\_2009.pdf},
	urldate = {2016-02-09},
	booktitle = {Proceedings of the 29th meeting of the {Cognitive} {Science} {Society} ({CogSci}-09)},
	author = {Demberg, Vera and Keller, Frank},
	year = {2009},
	file = {cogsci09b.pdf:/home/user/Zotero/storage/H9NKDF89/cogsci09b.pdf:application/pdf}
}

@article{levy-expectation-based-2008-2,
	title = {Expectation-based syntactic comprehension},
	volume = {106},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027707001436},
	doi = {10.1016/j.cognition.2007.05.006},
	language = {en},
	number = {3},
	urldate = {2016-02-09},
	journal = {Cognition},
	author = {Levy, Roger},
	month = mar,
	year = {2008},
	pages = {1126--1177},
	file = {doi\:10.1016/j.cognition.2007.05.006 - 1-s2.0-S0010027707001436-main.pdf:/home/user/Zotero/storage/N7KJC3ZG/1-s2.0-S0010027707001436-main.pdf:application/pdf}
}

@article{wurm-what-2014,
	title = {What residualizing predictors in regression analyses does (and what it does not do)},
	volume = {72},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X13001368},
	doi = {10.1016/j.jml.2013.12.003},
	abstract = {Psycholinguists are making increasing use of regression analyses and mixed-effects modeling. In an attempt to deal with concerns about collinearity, a number of researchers orthogonalize predictor variables by residualizing (i.e., by regressing one predictor onto another, and using the residuals as a stand-in for the original predictor). In the current study, the effects of residualizing predictor variables are demonstrated and discussed using ordinary least-squares regression and mixed-effects models. Some of these effects are almost certainly not what the researcher intended and are probably highly undesirable. Most importantly, what residualizing does not do is change the result for the residualized variable, which many researchers probably will find surprising. Further, some analyses with residualized variables cannot be meaningfully interpreted. Hence, residualizing is not a useful remedy for collinearity.},
	urldate = {2016-02-09},
	journal = {Journal of Memory and Language},
	author = {Wurm, Lee H. and Fisicaro, Sebastiano A.},
	month = apr,
	year = {2014},
	keywords = {Regression analysis, Collinearity, Mixed-effects modeling, Orthogonalization, Residualization, Statistical analysis},
	pages = {37--48},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/MJC72WIH/S0749596X13001368.html:text/html;Wurm_Fisicaro_2014_What residualizing predictors in regression analyses does (and what it does not.pdf:/home/user/Zotero/storage/USUZ25XV/Wurm_Fisicaro_2014_What residualizing predictors in regression analyses does (and what it does not.pdf:application/pdf}
}

@article{hawking-breakdown-1976,
	title = {Breakdown of predictability in gravitational collapse},
	volume = {14},
	url = {http://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.2460},
	number = {10},
	urldate = {2016-02-03},
	journal = {Physical Review D},
	author = {Hawking, Stephen W.},
	year = {1976},
	pages = {2460},
	file = {Breakdown of predictability in gravitational collapse - PhysRevD.14.2460:/home/user/Zotero/storage/DHF3AB9N/PhysRevD.14.pdf:application/pdf}
}

@article{almheiri-black-2013,
	title = {Black holes: complementarity or firewalls?},
	volume = {2013},
	shorttitle = {Black holes},
	url = {http://link.springer.com/article/10.1007/JHEP02(2013)062},
	number = {2},
	urldate = {2016-02-03},
	journal = {Journal of High Energy Physics},
	author = {Almheiri, Ahmed and Marolf, Donald and Polchinski, Joseph and Sully, James},
	year = {2013},
	pages = {1--20},
	file = {1207.3123v4.pdf:/home/user/Zotero/storage/8BI43P4J/1207.3123v4.pdf:application/pdf}
}

@article{mathur-information-2009,
	title = {The information paradox: a pedagogical introduction},
	volume = {26},
	shorttitle = {The information paradox},
	url = {http://iopscience.iop.org/article/10.1088/0264-9381/26/22/224001/meta},
	number = {22},
	urldate = {2016-02-03},
	journal = {Classical and Quantum Gravity},
	author = {Mathur, Samir D.},
	year = {2009},
	pages = {224001},
	file = {() - 0909.1038v2.pdf:/home/user/Zotero/storage/3RRVDCRP/0909.1038v2.pdf:application/pdf}
}

@article{brunel-mutual-1998,
	title = {Mutual {Information}, {Fisher} {Information}, and {Population} {Coding}},
	volume = {10},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/089976698300017115},
	doi = {10.1162/089976698300017115},
	abstract = {In the context of parameter estimation and model selection, it is only quite recently that a direct link between the Fisher information and information-theoretic quantities has been exhibited. We give an interpretation of this link within the standard framework of information theory. We show that in the context of population coding, the mutual information between the activity of a large array of neurons and a stimulus to which the neurons are tuned is naturally related to the Fisher information. In the light of this result, we consider the optimization of the tuning curves parameters in the case of neurons responding to a stimulus represented by an angular variable.},
	number = {7},
	urldate = {2016-02-02},
	journal = {Neural Computation},
	author = {Brunel, Nicolas and Nadal, Jean-Pierre},
	month = oct,
	year = {1998},
	pages = {1731--1757},
	file = {Brunel_Nadal_1998_Mutual Information, Fisher Information, and Population Coding.pdf:/home/user/Zotero/storage/6DFUZBSQ/Brunel_Nadal_1998_Mutual Information, Fisher Information, and Population Coding.pdf:application/pdf;Neural Computation Snapshot:/home/user/Zotero/storage/FFXR7KP6/089976698300017115.html:text/html}
}

@article{arora-practical-2012,
	title = {A practical algorithm for topic modeling with provable guarantees},
	url = {http://arxiv.org/abs/1212.4777},
	urldate = {2016-02-02},
	journal = {arXiv preprint arXiv:1212.4777},
	author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
	year = {2012},
	file = {1212.4777.pdf:/home/user/Zotero/storage/Z6TP87Q3/1212.4777.pdf:application/pdf}
}

@article{pascanu-revisiting-2013,
	title = {Revisiting {Natural} {Gradient} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1301.3584},
	abstract = {We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
	urldate = {2016-02-02},
	journal = {arXiv:1301.3584 [cs]},
	author = {Pascanu, Razvan and Bengio, Yoshua},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3584},
	keywords = {Computer Science - Learning, Computer Science - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2JE6RS5R/1301.html:text/html;Pascanu_Bengio_2013_Revisiting Natural Gradient for Deep Networks.pdf:/home/user/Zotero/storage/NZK6C3KG/Pascanu_Bengio_2013_Revisiting Natural Gradient for Deep Networks.pdf:application/pdf}
}

@article{rische-regularization-2016,
	title = {Regularization of languages by adults and children: {A} mathematical framework},
	volume = {84},
	issn = {0010-0285},
	shorttitle = {Regularization of languages by adults and children},
	url = {http://www.sciencedirect.com/science/article/pii/S001002851500081X},
	doi = {10.1016/j.cogpsych.2015.10.001},
	abstract = {The fascinating ability of humans to modify the linguistic input and “create” a language has been widely discussed. In the work of Newport and colleagues, it has been demonstrated that both children and adults have some ability to process inconsistent linguistic input and “improve” it by making it more consistent. In Hudson Kam and Newport (2009), artificial miniature language acquisition from an inconsistent source was studied. It was shown that (i) children are better at language regularization than adults and that (ii) adults can also regularize, depending on the structure of the input. In this paper we create a learning algorithm of the reinforcement-learning type, which exhibits patterns reported in Hudson Kam and Newport (2009) and suggests a way to explain them. It turns out that in order to capture the differences between children’s and adults’ learning patterns, we need to introduce a certain asymmetry in the learning algorithm. Namely, we have to assume that the reaction of the learners differs depending on whether or not the source’s input coincides with the learner’s internal hypothesis. We interpret this result in the context of a different reaction of children and adults to implicit, expectation-based evidence, positive or negative. We propose that a possible mechanism that contributes to the children’s ability to regularize an inconsistent input is related to their heightened sensitivity to positive evidence rather than the (implicit) negative evidence. In our model, regularization comes naturally as a consequence of a stronger reaction of the children to evidence supporting their preferred hypothesis. In adults, their ability to adequately process implicit negative evidence prevents them from regularizing the inconsistent input, resulting in a weaker degree of regularization.},
	urldate = {2016-02-02},
	journal = {Cognitive Psychology},
	author = {Rische, Jacquelyn L. and Komarova, Natalia L.},
	month = feb,
	year = {2016},
	keywords = {mathematical modeling, Frequency boosting, Frequency matching, Reinforcement algorithms},
	pages = {1--30},
	file = {Rische_Komarova_2016_Regularization of languages by adults and children.pdf:/home/user/Zotero/storage/ZDCDU52M/Rische_Komarova_2016_Regularization of languages by adults and children.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/F3EEUHFG/S001002851500081X.html:text/html}
}

@article{pennisi-female-2016,
	title = {Female organs revealed as weapons in sexual arms race},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6270/214},
	doi = {10.1126/science.351.6270.214},
	abstract = {For decades biologists have marveled at the diversity of penises across the animal kingdom. But in anatomy, as in other fields, females are getting more credit. At a recent symposium ostensibly about the male organ, the complexity of some female genitalia and their role in shaping phallic diversity stole the show. Researchers studying whales, snakes, and other animals are finding that female sex organs have some of the same baroque complexity seen in males. They now see females as active participants in an arms race, likely evolving more complex genitalia to control mating and to create barriers against forced matings—which in turn leads to male countermeasures. They called for more intense looks at how the sex organs interact during copulation and proposed new methods for measuring these organs.
Researchers broaden their focus beyond male anatomy.
Researchers broaden their focus beyond male anatomy.},
	language = {en},
	number = {6270},
	urldate = {2016-02-02},
	journal = {Science},
	author = {Pennisi, Elizabeth},
	month = jan,
	year = {2016},
	pmid = {26816357},
	pages = {214--215},
	file = {Pennisi_2016_Female organs revealed as weapons in sexual arms race.pdf:/home/user/Zotero/storage/ZFACBUXV/Pennisi_2016_Female organs revealed as weapons in sexual arms race.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TK2RDBPG/214.html:text/html}
}

@article{schmitz-boolean-1998,
	title = {The {Boolean} {Hierarchy} over {Level} 1/2 of the {Straubing}-{Therien} {Hierarchy}},
	url = {http://arxiv.org/abs/cs/9809118},
	abstract = {For some fixed alphabet A, a language L of A* is in the class L(1/2) of the Straubing-Therien hierarchy if and only if it can be expressed as a finite union of languages A*aA*bA*...A*cA*, where a,b,...,c are letters. The class L(1) is defined as the boolean closure of L(1/2). It is known that the classes L(1/2) and L(1) are decidable. We give a membership criterion for the single classes of the boolean hierarchy over L(1/2). From this criterion we can conclude that this boolean hierarchy is proper and that its classes are decidable.In finite model theory the latter implies the decidability of the classes of the boolean hierarchy over the class Sigma(1) of the FO({\textless})-logic. Moreover we prove a ``forbidden-pattern'' characterization of L(1) of the type: L is in L(1) if and only if a certain pattern does not appear in the transition graph of a deterministic finite automaton accepting L. We discuss complexity theoretical consequences of our results.},
	urldate = {2016-02-02},
	journal = {arXiv:cs/9809118},
	author = {Schmitz, Heinz and Wagner, Klaus W.},
	month = sep,
	year = {1998},
	note = {arXiv: cs/9809118},
	keywords = {Computer Science - Formal Languages and Automata Theory, F.4.3, Computer Science - Computational Complexity, F.1.3},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QENSTK2Z/9809118.html:text/html;Schmitz_Wagner_1998_The Boolean Hierarchy over Level 1-2 of the Straubing-Therien Hierarchy.pdf:/home/user/Zotero/storage/59RC4CG8/Schmitz_Wagner_1998_The Boolean Hierarchy over Level 1-2 of the Straubing-Therien Hierarchy.pdf:application/pdf}
}

@article{xie-scale-2015,
	title = {Scale {Up} {Nonlinear} {Component} {Analysis} with {Doubly} {Stochastic} {Gradients}},
	url = {http://arxiv.org/abs/1504.03655},
	abstract = {Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate \$\tilde{O}(1/t)\$ to the global optimum, even for the top \$k\$ eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.},
	urldate = {2016-02-02},
	journal = {arXiv:1504.03655 [cs]},
	author = {Xie, Bo and Liang, Yingyu and Song, Le},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.03655},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BVERGB3P/1504.html:text/html;Xie et al_2015_Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients.pdf:/home/user/Zotero/storage/X5RG2VT9/Xie et al_2015_Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients.pdf:application/pdf}
}

@article{awasthi-provably-2015,
	title = {On some provably correct cases of variational inference for topic models},
	url = {http://arxiv.org/abs/1503.06567},
	abstract = {Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It's closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. More specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012c). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003). It is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient. While our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest.},
	urldate = {2016-02-02},
	journal = {arXiv:1503.06567 [cs, stat]},
	author = {Awasthi, Pranjal and Risteski, Andrej},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.06567},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UPE89WPB/1503.html:text/html;Awasthi_Risteski_2015_On some provably correct cases of variational inference for topic models.pdf:/home/user/Zotero/storage/Z95DRHJT/Awasthi_Risteski_2015_On some provably correct cases of variational inference for topic models.pdf:application/pdf}
}

@article{arora-simple-2015,
	title = {Simple, {Efficient}, and {Neural} {Algorithms} for {Sparse} {Coding}},
	url = {http://arxiv.org/abs/1503.00778},
	abstract = {Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Re- cent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
	urldate = {2016-02-02},
	journal = {arXiv:1503.00778 [cs, stat]},
	author = {Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.00778},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {Arora et al_2015_Simple, Efficient, and Neural Algorithms for Sparse Coding.pdf:/home/user/Zotero/storage/SWCUI5T2/Arora et al_2015_Simple, Efficient, and Neural Algorithms for Sparse Coding.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/7TK9B32S/1503.html:text/html}
}

@article{arora-rand-walk:-2015,
	title = {{RAND}-{WALK}: {A} {Latent} {Variable} {Model} {Approach} to {Word} {Embeddings}},
	shorttitle = {{RAND}-{WALK}},
	url = {http://arxiv.org/abs/1502.03520},
	abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods including Vector Space Methods (VSMs) such as Latent Semantic Analysis (LSA), generative text models such as topic models, matrix factorization, neural nets, and energy-based models. Many of these use nonlinear operations on co-occurrence statistics, such as computing Pairwise Mutual Information (PMI). Some use hand-tuned hyperparameters and term reweighting. Often a generative model can help provide theoretical insight into such modeling choices, but there appears to be no such model to explain the above nonlinear models. For example, we know of no generative model for which the correct solution is the usual (dimension-restricted) PMI model. This paper gives a new generative model, a dynamic version of the loglinear topic model of Mnih and Hinton (2007),, as well as a pair of training objectives called RAND-WALK to compute word embeddings. The methodological novelty is to use the prior to compute closed form expressions for word statistics. These provide an explanation for the PMI model and other recent models, as well as hyperparameter choices. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are spatially isotropic. The model also helps explain why linear algebraic structure arises in low-dimensional semantic embeddings. Such structure has been used to solve analogy tasks by Mikolov et al. (2013a) and many subsequent papers. This theoretical explanation is to give an improved analogy solving method that improves success rates on analogy solving by a few percent.},
	urldate = {2016-02-02},
	journal = {arXiv:1502.03520 [cs, stat]},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03520},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Arora et al_2015_RAND-WALK.pdf:/home/user/Zotero/storage/5HT5UCUH/Arora et al_2015_RAND-WALK.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/EEVJBS9C/1502.html:text/html}
}

@article{arora-why-2015,
	title = {Why are deep nets reversible: {A} simple theory, with implications for training},
	shorttitle = {Why are deep nets reversible},
	url = {http://arxiv.org/abs/1511.05653},
	abstract = {Generative models for deep learning are promising both to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. However, there is no accompanying "proof of correctness" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is \$A\$ then the reverse transformation is \$A{\textasciicircum}T\$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.) (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. The generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels and include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.},
	urldate = {2016-02-02},
	journal = {arXiv:1511.05653 [cs]},
	author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05653},
	keywords = {Computer Science - Learning},
	file = {Arora et al_2015_Why are deep nets reversible.pdf:/home/user/Zotero/storage/VSWAFWNT/Arora et al_2015_Why are deep nets reversible.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/U9SZWEJ5/1511.html:text/html}
}

@article{arora-linear-2016,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	url = {http://arxiv.org/abs/1601.03764},
	abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding. The success of the method ---which applies to several embedding methods including word2vec--- is mathematically explained using the random walk on discourses model (Arora et al., 2015). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 "discourse atoms" that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy.},
	urldate = {2016-02-02},
	journal = {arXiv:1601.03764 [cs, stat]},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.03764},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Arora et al_2016_Linear Algebraic Structure of Word Senses, with Applications to Polysemy.pdf:/home/user/Zotero/storage/B7ZWQDWE/Arora et al_2016_Linear Algebraic Structure of Word Senses, with Applications to Polysemy.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/UG4IJDR4/1601.html:text/html}
}

@article{kuperberg-separate-2016,
	title = {Separate streams or probabilistic inference? {What} the {N}400 can tell us about the comprehension of events},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Separate streams or probabilistic inference?},
	url = {http://dx.doi.org/10.1080/23273798.2015.1130233},
	doi = {10.1080/23273798.2015.1130233},
	abstract = {Since the early 2000s, several event-related potential studies have challenged the assumption that we always use syntactic contextual information to influence semantic processing of incoming words, as reflected by the N400 component. One approach for explaining these findings is to posit distinct semantic and syntactic processing mechanisms, each with distinct time courses. While this approach can explain specific datasets, it cannot account for the wider body of findings. I propose an alternative explanation: a dynamic generative framework in which our goal is to infer the underlying event that best explains the set of inputs encountered at any given time. Within this framework, combinations of semantic and syntactic cues with varying reliabilities are used as evidence to weight probabilistic hypotheses about this event. I further argue that the computational principles of this framework can be extended to understand how we infer situation models during discourse comprehension, and intended messages during spoken communication.},
	number = {0},
	urldate = {2016-02-02},
	journal = {Language, Cognition and Neuroscience},
	author = {Kuperberg, Gina R.},
	month = jan,
	year = {2016},
	pages = {1--15},
	file = {Kuperberg_2016_Separate streams or probabilistic inference.pdf:/home/user/Zotero/storage/97VXP3MW/Kuperberg_2016_Separate streams or probabilistic inference.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/P7E6U796/23273798.2015.html:text/html}
}

@article{westo-memory-2015,
	title = {Memory {Stacking} in {Hierarchical} {Networks}},
	volume = {28},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/NECO\_a\_00803},
	doi = {10.1162/NECO\_a\_00803},
	abstract = {Robust representations of sounds with a complex spectrotemporal structure are thought to emerge in hierarchically organized auditory cortex, but the computational advantage of this hierarchy remains unknown. Here, we used computational models to study how such hierarchical structures affect temporal binding in neural networks. We equipped individual units in different types of feedforward networks with local memory mechanisms storing recent inputs and observed how this affected the ability of the networks to process stimuli context dependently. Our findings illustrate that these local memories stack up in hierarchical structures and hence allow network units to exhibit selectivity to spectral sequences longer than the time spans of the local memories. We also illustrate that short-term synaptic plasticity is a potential local memory mechanism within the auditory cortex, and we show that it can bring robustness to context dependence against variation in the temporal rate of stimuli, while introducing nonlinearities to response profiles that are not well captured by standard linear spectrotemporal receptive field models. The results therefore indicate that short-term synaptic plasticity might provide hierarchically structured auditory cortex with computational capabilities important for robust representations of spectrotemporal patterns.},
	number = {2},
	urldate = {2016-02-02},
	journal = {Neural Computation},
	author = {Westö, Johan and May, Patrick J. C. and Tiitinen, Hannu},
	month = dec,
	year = {2015},
	pages = {327--353},
	file = {Snapshot:/home/user/Zotero/storage/2RGQB7W7/NECO_a_00803.html:text/html}
}

@article{wei-mutual-2015,
	title = {Mutual {Information}, {Fisher} {Information}, and {Efficient} {Coding}},
	volume = {28},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/NECO\_a\_00804},
	doi = {10.1162/NECO\_a\_00804},
	abstract = {Fisher information is generally believed to represent a lower bound on mutual information (Brunel \& Nadal, 1998), a result that is frequently used in the assessment of neural coding efficiency. However, we demonstrate that the relation between these two quantities is more nuanced than previously thought. For example, we find that in the small noise regime, Fisher information actually provides an upper bound on mutual information. Generally our results show that it is more appropriate to consider Fisher information as an approximation rather than a bound on mutual information. We analytically derive the correspondence between the two quantities and the conditions under which the approximation is good. Our results have implications for neural coding theories and the link between neural population coding and psychophysically measurable behavior. Specifically, they allow us to formulate the efficient coding problem of maximizing mutual information between a stimulus variable and the response of a neural population in terms of Fisher information. We derive a signature of efficient coding expressed as the correspondence between the population Fisher information and the distribution of the stimulus variable. The signature is more general than previously proposed solutions that rely on specific assumptions about the neural tuning characteristics. We demonstrate that it can explain measured tuning characteristics of cortical neural populations that do not agree with previous models of efficient coding.},
	number = {2},
	urldate = {2016-02-02},
	journal = {Neural Computation},
	author = {Wei, Xue-Xin and Stocker, Alan A.},
	month = dec,
	year = {2015},
	pages = {305--326},
	file = {Snapshot:/home/user/Zotero/storage/W6V9RZF7/NECO_a_00804.html:text/html}
}

@article{charnavel-anaphor-2016,
	title = {Anaphor {Binding}: {What} {French} {Inanimate} {Anaphors} {Show}},
	volume = {47},
	issn = {0024-3892},
	shorttitle = {Anaphor {Binding}},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00204},
	doi = {10.1162/LING\_a\_00204},
	abstract = {Owing to different ideas about what counts as an anaphor subject to Condition A, two influential but superficially incompatible versions of Condition A of binding theory have coexisted: Chomsky’s (1986) version, and versions of predicate-based binding theories defended by Pollard and Sag (1992) and Reinhart and Reuland (1993) and modified in various ways since ( Pollard 2005, Reuland 2011). Using inanimate anaphors to independently control for sensitivity to Condition A without the confound of logophoricity, we show that Condition A must be checked at the syntax-interpretation interface and that Chomsky’s (1986) version (an anaphor must be bound within the smallest complete functional complex containing it and a possible binder) is nearly correct, with one amendment: a tensed TP boundary is opaque to the search for an antecedent. Given these results, we argue that Condition A should be reduced to phase theory and we outline how this can be done.},
	number = {1},
	urldate = {2016-02-02},
	journal = {Linguistic Inquiry},
	author = {Charnavel, Isabelle and Sportiche, Dominique},
	month = jan,
	year = {2016},
	pages = {35--87},
	file = {Snapshot:/home/user/Zotero/storage/83XBRBVF/LING_a_00204.html:text/html}
}

@article{gao-interpreting-2016,
	title = {Interpreting the {Dependence} of {Mutation} {Rates} on {Age} and {Time}},
	volume = {14},
	url = {http://dx.doi.org/10.1371/journal.pbio.1002355},
	doi = {10.1371/journal.pbio.1002355},
	abstract = {Modeling how the source of mutations relates to their rate of accumulation with age, sex, and number of cell divisions helps to explain perplexing observations about germline and somatic mutations.},
	number = {1},
	urldate = {2016-02-02},
	journal = {PLoS Biol},
	author = {Gao, Ziyue and Wyman, Minyoung J. and Sella, Guy and Przeworski, Molly},
	month = jan,
	year = {2016},
	pages = {e1002355},
	file = {Gao et al_2016_Interpreting the Dependence of Mutation Rates on Age and Time.pdf:/home/user/Zotero/storage/SCPAJH2I/Gao et al_2016_Interpreting the Dependence of Mutation Rates on Age and Time.pdf:application/pdf}
}

@article{bankova-graded-2016-1,
	title = {Graded {Entailment} for {Compositional} {Distributional} {Semantics}},
	url = {http://arxiv.org/abs/1601.04908},
	abstract = {The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categorical compositional distributional semantics. Taking advantage of the abstract categorical framework allows us to vary our choice of model. This enables the introduction of a notion of entailment, exploiting ideas from the categorical semantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce a novel robust graded order capturing the entailment strength between concepts. This graded measure emerges from a general framework for approximate entailment, induced by any commutative monoid. Quantum logic embeds in our graded order. Our main theorem shows that entailment strength lifts compositionally to the sentence level, giving a lower bound on sentence entailment. We describe the essential properties of graded entailment such as continuity, and provide a procedure for calculating entailment strength.},
	urldate = {2016-02-02},
	journal = {arXiv:1601.04908 [quant-ph]},
	author = {Bankova, Desislava and Coecke, Bob and Lewis, Martha and Marsden, Daniel},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.04908},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Mathematics - Category Theory, Quantum Physics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TR87IXP6/1601.html:text/html;Bankova et al_2016_Graded Entailment for Compositional Distributional Semantics.pdf:/home/user/Zotero/storage/JJH96ME8/Bankova et al_2016_Graded Entailment for Compositional Distributional Semantics.pdf:application/pdf}
}

@article{humphreys-similarity-based-nodate,
	title = {Similarity-based competition in relative clause production and comprehension},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X15001618},
	doi = {10.1016/j.jml.2015.12.007},
	abstract = {This work investigates the role of semantic similarity in sentence production and comprehension. Previous research suggests that animacy and conceptual similarity of the noun concepts within complex descriptive phrases modulate structural preferences in production, and processing cost in comprehension. For example, animate-head phrases such as the girl that the boy is pulling are rare in production and more difficult to understand in comprehension. In contrast, phrases with passive clauses such as the girl being pulled by the boy are commonly produced and more easily understood, as are inanimate-head structures such as the truck the boy is pulling. In three picture-based studies, we examined the mechanisms underlying semantic similarity effects in producing and comprehending these phrases. Study 1 investigated structural preferences in production, whereas Study 2 investigated processing cost in comprehension. Study 3 used eye-tracking to examine the time-course of production processes. The results showed that semantic similarity elicited competition during phrase planning, influenced the choice of syntactic structure in production, and engendered comprehension difficulty in animate-head active configurations. Structural preferences, fixation probabilities reflecting production planning processes and comprehension cost significantly correlated with measures of conceptual similarity across the three studies. We argue that similarity-based competition modulates sentence production and comprehension processes when verbs are planned or interpreted, i.e., when event-based semantic or syntactic roles are determined. In addition to task-specific processes, we suggest that a similar and shared semantic competition mechanism underlies both production and comprehension, a view consistent with existing evidence for common brain regions recruited in both tasks.},
	urldate = {2016-02-02},
	journal = {Journal of Memory and Language},
	author = {Humphreys, Gina F. and Mirković, Jelena and Gennari, Silvia P.},
	keywords = {Sentence comprehension, similarity-based interference, Semantic interference, Sentence planning, Sentence production, Similarity-based competition},
	file = {Humphreys et al_Similarity-based competition in relative clause production and comprehension.pdf:/home/user/Zotero/storage/EAXEJWPP/Humphreys et al_Similarity-based competition in relative clause production and comprehension.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/2FIESR5C/S0749596X15001618.html:text/html}
}

@article{ghosh-learning-2015,
	title = {Learning {Representations} of {Affect} from {Speech}},
	url = {http://arxiv.org/abs/1511.04747},
	abstract = {There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition.},
	urldate = {2016-02-02},
	journal = {arXiv:1511.04747 [cs]},
	author = {Ghosh, Sayan and Laksana, Eugene and Morency, Louis-Philippe and Scherer, Stefan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04747},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GW3SCDMJ/1511.html:text/html;Ghosh et al_2015_Learning Representations of Affect from Speech.pdf:/home/user/Zotero/storage/2QE6G7DR/Ghosh et al_2015_Learning Representations of Affect from Speech.pdf:application/pdf}
}

@article{palangi-deep-2015,
	title = {Deep {Sentence} {Embedding} {Using} {Long} {Short}-{Term} {Memory} {Networks}: {Analysis} and {Application} to {Information} {Retrieval}},
	shorttitle = {Deep {Sentence} {Embedding} {Using} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1502.06922},
	abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.},
	urldate = {2016-02-02},
	journal = {arXiv:1502.06922 [cs]},
	author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.06922},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2XC8RD8P/1502.html:text/html;Palangi et al_2015_Deep Sentence Embedding Using Long Short-Term Memory Networks.pdf:/home/user/Zotero/storage/75WSH8E7/Palangi et al_2015_Deep Sentence Embedding Using Long Short-Term Memory Networks.pdf:application/pdf}
}

@article{ithapu-interplay-2015,
	title = {On the interplay of network structure and gradient convergence in deep learning},
	url = {http://arxiv.org/abs/1511.05297},
	abstract = {The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-\`a-vis their interaction with parameter convergence rates. We present a framework to address these questions based on the backpropagation convergence for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon the results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experiments we conducted as well as independent empirical observations reported by other groups in recent papers.},
	urldate = {2016-02-02},
	journal = {arXiv:1511.05297 [cs, stat]},
	author = {Ithapu, Vamsi K. and Ravi, Sathya and Singh, Vikas},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05297},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/DTZPN3I8/1511.html:text/html;Ithapu et al_2015_On the interplay of network structure and gradient convergence in deep learning.pdf:/home/user/Zotero/storage/MB8KMHR3/Ithapu et al_2015_On the interplay of network structure and gradient convergence in deep learning.pdf:application/pdf}
}

@article{serban-building-2015-1,
	title = {Building {End}-{To}-{End} {Dialogue} {Systems} {Using} {Generative} {Hierarchical} {Neural} {Network} {Models}},
	url = {http://arxiv.org/abs/1507.04808},
	abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
	urldate = {2016-04-09},
	journal = {arXiv:1507.04808 [cs]},
	author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.04808},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.7, I.5.1},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/MRGRUUGG/1507.html:text/html;Serban et al_2015_Building End-To-End Dialogue Systems Using Generative Hierarchical Neural.pdf:/home/user/Zotero/storage/86RUMU87/Serban et al_2015_Building End-To-End Dialogue Systems Using Generative Hierarchical Neural.pdf:application/pdf}
}

@book{calzolari-proceedings-2012,
	title = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}-2012), {Istanbul}, {Turkey}, {May} 23-25, 2012},
	isbn = {978-2-9517408-7-7},
	publisher = {European Language Resources Association (ELRA)},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Dogan, Mehmet Ugur and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios},
	year = {2012}
}

@inproceedings{petrov-universal-2012,
	title = {A {Universal} {Part}-of-{Speech} {Tagset}},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/summaries/274.html},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}-2012), {Istanbul}, {Turkey}, {May} 23-25, 2012},
	author = {Petrov, Slav and Das, Dipanjan and McDonald, Ryan T.},
	year = {2012},
	pages = {2089--2096},
	file = {universal.pdf:/home/user/Zotero/storage/BD4MQSKQ/universal.pdf:application/pdf}
}

@article{sturm-deciding-2015,
	title = {Deciding {First}-{Order} {Satisfiability} when {Universal} and {Existential} {Variables} are {Separated}},
	url = {http://arxiv.org/abs/1511.08999},
	abstract = {We introduce a new decidable fragment of first-order logic with equality which strictly generalizes two already well-known ones --- the Bernays--Sch\"onfinkel--Ramsey (BSR) Fragment and the Monadic Fragment. The defining principle is the syntactic separation of universally quantified variables from existentially quantified ones at the level of atoms. Thus, our classification neither rests on restrictions of quantifier prefixes (as in the BSR case) nor on restrictions on the arity of predicate symbols (as in the monadic case). We demonstrate that the new fragment exhibits the finite model property and derive a non-elementary upper bound on the computing time required for deciding satisfiability in the new fragment. For the subfragment of prenex sentences with the quantifier prefix \$\exists{\textasciicircum}* \forall{\textasciicircum}* \exists{\textasciicircum}*\$ the satisfiability problem is shown to be complete for NEXPTIME. Finally, we discuss how automated reasoning procedures can take advantage of our results.},
	urldate = {2016-04-07},
	journal = {arXiv:1511.08999 [cs]},
	author = {Sturm, Thomas and Voigt, Marco and Weidenbach, Christoph},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08999},
	keywords = {Computer Science - Logic in Computer Science, F.4.1},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VXW9DJER/1511.html:text/html;Sturm et al_2015_Deciding First-Order Satisfiability when Universal and Existential Variables.pdf:/home/user/Zotero/storage/HGVWJEJV/Sturm et al_2015_Deciding First-Order Satisfiability when Universal and Existential Variables.pdf:application/pdf}
}

@article{golub-character-level-2016,
	title = {Character-{Level} {Question} {Answering} with {Attention}},
	url = {http://arxiv.org/abs/1604.00727},
	abstract = {We show that an encoder-decoder framework can be successfully applied to question-answering with a structured knowledge base. In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates. We use our model for single-relation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset, where we improve state-of-the-art accuracy by 2\% for both Freebase2M and Freebase5M subsets proposed. Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18\% of the entities seen during training when testing.},
	urldate = {2016-04-07},
	journal = {arXiv:1604.00727 [cs]},
	author = {Golub, David and He, Xiaodong},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00727},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/U3UEFV3S/1604.html:text/html;Golub_He_2016_Character-Level Question Answering with Attention.pdf:/home/user/Zotero/storage/94UVTKMW/Golub_He_2016_Character-Level Question Answering with Attention.pdf:application/pdf}
}

@article{vera-modeling-2016-1,
	title = {Modeling self-organization of vocabularies under phonological similarity effects},
	url = {http://arxiv.org/abs/1603.05354},
	abstract = {This work develops a computational model (by Automata Networks) of phonological similarity effects involved in the formation of word-meaning associations on artificial populations of speakers. Classical studies show that in recalling experiments memory performance was impaired for phonologically similar words versus dissimilar ones. Here, the individuals confound phonologically similar words according to a predefined parameter. The main hypothesis is that there is a critical range of the parameter, and with this, of working-memory mechanisms, which implies drastic changes in the final consensus of the entire population. Theoretical results present proofs of convergence for a particular case of the model within a worst-case complexity framework. Computer simulations describe the evolution of an energy function that measures the amount of local agreement between individuals. The main finding is the appearance of sudden changes in the energy function at critical parameters.},
	urldate = {2016-04-07},
	journal = {arXiv:1603.05354 [physics]},
	author = {Vera, Javier},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05354},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VPM754WX/1603.html:text/html;Vera_2016_Modeling self-organization of vocabularies under phonological similarity effects.pdf:/home/user/Zotero/storage/S3PENFTJ/Vera_2016_Modeling self-organization of vocabularies under phonological similarity effects.pdf:application/pdf}
}

@article{lassiter-graded-2014,
	title = {Graded modality},
	url = {http://web.stanford.edu/~danlass/Lassiter-graded-modality-draft.pdf},
	urldate = {2016-04-07},
	journal = {Companion to Semantics, Wiley},
	author = {Lassiter, Daniel},
	year = {2014},
	file = {Lassiter-graded-modality-draft.pdf:/home/user/Zotero/storage/GZFV2DCU/Lassiter-graded-modality-draft.pdf:application/pdf}
}

@article{lassiter-conditional-nodate,
	title = {Conditional antecedents provide no evidence for a grammatical theory of scalar implicature},
	url = {http://web.stanford.edu/~danlass/Lassiter-SI-conditionals-draft.pdf},
	urldate = {2016-04-07},
	author = {Lassiter, Daniel},
	file = {Lassiter-SI-conditionals-draft.pdf:/home/user/Zotero/storage/WJNSVTZK/Lassiter-SI-conditionals-draft.pdf:application/pdf}
}

@article{lassiter-communicating-nodate,
	title = {Communicating with {Epistemic} {Modals} in {Stochastic} λ-{Calculus}},
	url = {http://web.stanford.edu/~danlass/Lassiter-Goodman-epistemic-modals-communication.pdf},
	urldate = {2016-04-07},
	journal = {Manuscript, Stanford University},
	author = {Lassiter, Daniel and Goodman, Noah D.},
	file = {Lassiter-Goodman-epistemic-modals-communication.pdf:/home/user/Zotero/storage/GXG3VFTW/Lassiter-Goodman-epistemic-modals-communication.pdf:application/pdf}
}

@article{lassiter-graded-nodate,
	title = {{GRADED} {MODALITY}: {QUALITATIVE} {AND} {QUANTITATIVE} {PERSPECTIVES}},
	shorttitle = {{GRADED} {MODALITY}},
	url = {http://web.stanford.edu/~danlass/Lassiter-book-draft.pdf},
	urldate = {2016-04-07},
	author = {Lassiter, Daniel},
	file = {Lassiter-book-draft.pdf:/home/user/Zotero/storage/VHJPP7DX/Lassiter-book-draft.pdf:application/pdf}
}

@article{buchanan-apr-2010,
	title = {On {Apr} 28, 2010, at 4: 46 {AM},{\textless} lktyler@ csl. psychol. cam. ac. uk{\textgreater} wrote:{\textgreater} 28-{Apr}-2010},
	shorttitle = {On {Apr} 28, 2010, at 4},
	url = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Boston-Hale-Vasishth-KlieglInPress.pdf},
	urldate = {2016-04-07},
	author = {Buchanan, Kirsten},
	year = {2010},
	file = {Boston2010LCP.pdf:/home/user/Zotero/storage/BBJV5T6E/Boston2010LCP.pdf:application/pdf}
}

@article{boston-parsing-2008,
	title = {Parsing costs as predictors of reading difficulty: {An} evaluation using the {Potsdam} {Sentence} {Corpus}},
	shorttitle = {Parsing costs as predictors of reading difficulty},
	url = {http://openscience.uni-leipzig.de/index.php/mr2/article/view/62},
	number = {1},
	urldate = {2016-04-07},
	journal = {The Mind Research Repository (beta)},
	author = {Boston, Marisa and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
	year = {2008},
	file = {boston-2008-jemr.pdf:/home/user/Zotero/storage/ZI487FWS/boston-2008-jemr.pdf:application/pdf}
}

@inproceedings{hale-pcfgs-2006,
	title = {{PCFGs} with syntactic and prosodic indicators of speech repairs},
	url = {http://dl.acm.org/citation.cfm?id=1220196},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computational} {Linguistics} and the 44th annual meeting of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hale, John and Shafran, Izhak and Yung, Lisa and Dorr, Bonnie and Harper, Mary and Krasnyanskaya, Anna and Lease, Matthew and Liu, Yang and Roark, Brian and Snover, Matthew and {others}},
	year = {2006},
	pages = {161--168},
	file = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics - P06-1021.pdf:/home/user/Zotero/storage/2RADE8KV/P06-1021.pdf:application/pdf}
}

@article{kim-acceptability-2011,
	title = {The acceptability cline in {VP} ellipsis},
	volume = {14},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9612.2011.00160.x/full},
	number = {4},
	urldate = {2016-04-07},
	journal = {Syntax},
	author = {Kim, Christina S. and Kobele, Gregory M. and Runner, Jeffrey T. and Hale, John T.},
	year = {2011},
	pages = {318--354},
	file = {2010-KimEtAl10.pdf:/home/user/Zotero/storage/PFJ2U9WH/2010-KimEtAl10.pdf:application/pdf}
}

@article{chakarov-debugging-2016,
	title = {Debugging {Machine} {Learning} {Tasks}},
	url = {http://arxiv.org/abs/1603.07292},
	urldate = {2016-04-07},
	journal = {arXiv preprint arXiv:1603.07292},
	author = {Chakarov, Aleksandar and Nori, Aditya and Rajamani, Sriram and Sen, Shayak and Vijaykeerthy, Deepak},
	year = {2016},
	file = {1603.07292v1.pdf:/home/user/Zotero/storage/98IGCVIC/1603.07292v1.pdf:application/pdf}
}

@article{kehler-evaluating-nodate,
	title = {Evaluating an {Expectation}-{Driven} {QUD} {Model} of {Discourse} {Interpretation}},
	url = {http://www.lel.ed.ac.uk/~hrohde/papers/KehlerRohde.2016.pdf},
	urldate = {2016-04-07},
	author = {Kehler, Andrew and Rohde, Hannah},
	file = {KehlerRohdeResubmittedRound2Final - KehlerRohde.2016.pdf:/home/user/Zotero/storage/J6Q48878/KehlerRohde.2016.pdf:application/pdf}
}

@article{narasimhan-improving-2016,
	title = {Improving {Information} {Extraction} by {Acquiring} {External} {Evidence} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1603.07954},
	urldate = {2016-04-07},
	journal = {arXiv preprint arXiv:1603.07954},
	author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
	year = {2016},
	file = {1603.07954v1.pdf:/home/user/Zotero/storage/QH55PJAZ/1603.07954v1.pdf:application/pdf}
}

@inproceedings{stanley-efficient-2002,
	title = {Efficient evolution of neural network topologies},
	volume = {2},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1004508},
	urldate = {2016-04-07},
	booktitle = {Evolutionary {Computation}, 2002. {CEC}'02. {Proceedings} of the 2002 {Congress} on},
	publisher = {IEEE},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	year = {2002},
	pages = {1757--1762},
	file = {mycec-expand3.dvi - stanley.cec02.pdf:/home/user/Zotero/storage/K5BU8M75/stanley.cec02.pdf:application/pdf}
}

@article{gao-degrees-2016,
	title = {Degrees of {Freedom} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1603.09260},
	urldate = {2016-04-07},
	journal = {arXiv preprint arXiv:1603.09260},
	author = {Gao, Tianxiang and Jojic, Vladimir},
	year = {2016},
	file = {1603.09260v1.pdf:/home/user/Zotero/storage/9WJWXBNH/1603.09260v1.pdf:application/pdf}
}

@article{gibson-linguistic-1998-1,
	title = {Linguistic complexity: {Locality} of syntactic dependencies},
	volume = {68},
	shorttitle = {Linguistic complexity},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027798000341},
	number = {1},
	urldate = {2016-04-07},
	journal = {Cognition},
	author = {Gibson, Edward},
	year = {1998},
	pages = {1--76},
	file = {PII\: S0010-0277(98)00034-1 - CG195gibson.pdf:/home/user/Zotero/storage/ZNMFQBNP/CG195gibson.pdf:application/pdf}
}

@inproceedings{bicknell-why-2011,
	title = {Why readers regress to previous words: {A} statistical analysis},
	shorttitle = {Why readers regress to previous words},
	url = {https://mindmodeling.org/cogsci2011/papers/0210/paper0210.pdf},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 33rd annual meeting of the {Cognitive} {Science} {Society}},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2011},
	pages = {931--936},
	file = {bicknell-levy-2011-cogsci.pdf:/home/user/Zotero/storage/AU6PKZ8C/bicknell-levy-2011-cogsci.pdf:application/pdf}
}

@inproceedings{bicknell-rational-2010,
	title = {Rational eye movements in reading combining uncertainty about previous words with contextual probability},
	url = {https://mindmodeling.org/cogsci2010/papers/0337/paper0337.pdf},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 32nd annual conference of the cognitive science society},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2010},
	pages = {1142--1147},
	file = {bicknell-levy-2010-cogsci.pdf:/home/user/Zotero/storage/ADBI7E6D/bicknell-levy-2010-cogsci.pdf:application/pdf}
}

@inproceedings{bicknell-rational-2010-1,
	title = {A rational model of eye movement control in reading},
	url = {http://dl.acm.org/citation.cfm?id=1858800},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 48th annual meeting of the association for computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2010},
	pages = {1168--1178},
	file = {A Rational Model of Eye Movement Control in Reading - bicknell-levy-2010-acl.pdf:/home/user/Zotero/storage/XFUGIIIT/bicknell-levy-2010-acl.pdf:application/pdf}
}

@article{levy-eye-2009,
	title = {Eye movement evidence that readers maintain and act on uncertainty about past linguistic input},
	volume = {106},
	url = {https://www.pnas.org/content/106/50/21086.full},
	number = {50},
	urldate = {2016-04-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
	year = {2009},
	pages = {21086--21090},
	file = {levy-etal-2009-pnas.pdf:/home/user/Zotero/storage/7IBIGZG8/levy-etal-2009-pnas.pdf:application/pdf}
}

@inproceedings{bicknell-correcting-2009,
	title = {Correcting the incorrect: {Local} coherence effects modeled with prior belief update},
	volume = {35},
	shorttitle = {Correcting the incorrect},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/BLS/article/viewFile/3594/3299},
	urldate = {2016-04-07},
	booktitle = {Annual {Meeting} of the {Berkeley} {Linguistics} {Society}},
	author = {Bicknell, Klinton and Levy, Roger and Demberg, Vera},
	year = {2009},
	pages = {13--24},
	file = {bicknell-levy-demberg-2009-bls.pdf:/home/user/Zotero/storage/IU8SFIF7/bicknell-levy-demberg-2009-bls.pdf:application/pdf}
}

@inproceedings{bicknell-model-2009,
	title = {A model of local coherence effects in human sentence processing as consequences of updates from bottom-up prior to posterior beliefs},
	url = {http://dl.acm.org/citation.cfm?id=1620851},
	urldate = {2016-04-07},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2009},
	pages = {665--673},
	file = {A model of local coherence effects in human sentence processing as consequences of updates from bottom-up prior to posterior beliefs - bicknell-levy-2009-naacl.pdf:/home/user/Zotero/storage/H94GJ77G/bicknell-levy-2009-naacl.pdf:application/pdf}
}

@article{smith-effect-2013,
	title = {The effect of word predictability on reading time is logarithmic},
	volume = {128},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027713000413},
	doi = {10.1016/j.cognition.2013.02.013},
	language = {en},
	number = {3},
	urldate = {2016-04-07},
	journal = {Cognition},
	author = {Smith, Nathaniel J. and Levy, Roger},
	month = sep,
	year = {2013},
	pages = {302--319},
	file = {The effect of word predictability on reading time is logarithmic - smith-levy-2013-cognition.pdf:/home/user/Zotero/storage/MF4DRTNT/smith-levy-2013-cognition.pdf:application/pdf}
}

@inproceedings{smith-fixation-2010,
	title = {Fixation durations in first-pass reading reflect uncertainty about word identity},
	url = {http://vorpus.org/papers/smith-levy-2010-cogsci.pdf},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 32nd annual meeting of the cognitive science society},
	author = {Smith, Nathaniel J. and Levy, Roger},
	year = {2010},
	file = {smith-levy-2010-cogsci.pdf:/home/user/Zotero/storage/S4Z7KEAZ/smith-levy-2010-cogsci.pdf:application/pdf}
}

@inproceedings{smith-optimal-2008,
	title = {Optimal processing times in reading: a formal model and empirical investigation},
	shorttitle = {Optimal processing times in reading},
	url = {http://idiom.ucsd.edu/~rlevy/papers/smith-levy-2008-cogsci.pdf},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 30th annual conference of the cognitive science society},
	author = {Smith, Nathaniel J. and Levy, Roger},
	year = {2008},
	pages = {595--600},
	file = {smith-levy-2008-cogsci.pdf:/home/user/Zotero/storage/F6CFDSEC/smith-levy-2008-cogsci.pdf:application/pdf}
}

@article{wittenberg-if-2015,
	title = {If you want a quick kiss, make it count: {How} choice of syntactic construction affects event construal},
	shorttitle = {If you want a quick kiss, make it count},
	url = {https://www.researchgate.net/profile/Eva\_Wittenberg/publication/282777410\_If\_you\_want\_a\_quick\_hug\_make\_it\_count\_How\_grammar\_affects\_estimated\_event\_durations/links/561bf48508aea803672434c1.pdf},
	urldate = {2016-04-07},
	journal = {Manuscript, UC San Diego},
	author = {Wittenberg, Eva and Levy, Roger},
	year = {2015},
	file = {wittenberg-levy-QuickKiss-ms-2015-08-26.pdf:/home/user/Zotero/storage/DVIGMV8P/wittenberg-levy-QuickKiss-ms-2015-08-26.pdf:application/pdf}
}

@inproceedings{doyle-nonparametric-2014,
	title = {Nonparametric {Learning} of {Phonological} {Constraints} in {Optimality} {Theory}.},
	url = {http://idiom.ucsd.edu/~rlevy/papers/doyle-bicknell-levy-2014-acl.pdf},
	urldate = {2016-04-07},
	booktitle = {{ACL} (1)},
	author = {Doyle, Gabriel and Bicknell, Klinton and Levy, Roger},
	year = {2014},
	pages = {1094--1103},
	file = {Nonparametric Learning of Phonological Constraints in Optimality Theory - doyle-bicknell-levy-2014-acl.pdf:/home/user/Zotero/storage/QU5TA5GX/doyle-bicknell-levy-2014-acl.pdf:application/pdf}
}

@article{levy-using-2014,
	title = {Using {R} formulae to test for main effects in the presence of higher-order interactions},
	url = {http://arxiv.org/abs/1405.2094},
	urldate = {2016-04-07},
	journal = {arXiv preprint arXiv:1405.2094},
	author = {Levy, Roger},
	year = {2014},
	file = {() - levy-2014-mainEffects-v1.pdf:/home/user/Zotero/storage/S3UGKXXW/levy-2014-mainEffects-v1.pdf:application/pdf}
}

@inproceedings{pajak-model-2013,
	title = {A model of generalization in distributional learning of phonetic categories},
	url = {http://www.anthology.aclweb.org/W/W13/W13-26.pdf#page=21},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 4th workshop on cognitive modeling and computational linguistics},
	author = {Pajak, Bozena and Bicknell, Klinton and Levy, Roger},
	year = {2013},
	pages = {11--20},
	file = {A model of generalization in distributional learning of phonetic categories - pajak-bicknell-levy-2013-cmcl.pdf:/home/user/Zotero/storage/VBJHWEQ5/pajak-bicknell-levy-2013-cmcl.pdf:application/pdf}
}

@inproceedings{bicknell-why-2012,
	title = {Why long words take longer to read: the role of uncertainty about word length},
	shorttitle = {Why long words take longer to read},
	url = {http://dl.acm.org/citation.cfm?id=2390309},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 3rd {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2012},
	pages = {21--30},
	file = {Why long words take longer to read\: the role of uncertainty about word length - bicknell-levy-2012-cmcl.pdf:/home/user/Zotero/storage/62CTA8T6/bicknell-levy-2012-cmcl.pdf:application/pdf}
}

@inproceedings{bicknell-word-2012,
	title = {Word predictability and frequency effects in a rational model of reading},
	url = {http://idiom.ucsd.edu/~rlevy/papers/bicknell-levy-2012-cogsci.pdf},
	urldate = {2016-04-07},
	booktitle = {Proceedings of the 34th {Annual} {Conference} of the {Cognitive} {Science} {Society}. {Austin}, {TX}: {Cognitive} {Science} {Society}},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2012},
	file = {bicknell-levy-2012-cogsci.pdf:/home/user/Zotero/storage/JJIBHSG2/bicknell-levy-2012-cogsci.pdf:application/pdf}
}

@article{bicknell-utility-2012,
	title = {The utility of modelling word identification from visual input within models of eye movements in reading},
	volume = {20},
	issn = {1350-6285, 1464-0716},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13506285.2012.668144},
	doi = {10.1080/13506285.2012.668144},
	language = {en},
	number = {4-5},
	urldate = {2016-04-07},
	journal = {Visual Cognition},
	author = {Bicknell, Klinton and Levy, Roger},
	month = apr,
	year = {2012},
	pages = {422--456},
	file = {doi\:10.1080/13506285.2012.668144 - bicknell-levy-2012-viscog.pdf:/home/user/Zotero/storage/I3P6X7SX/bicknell-levy-2012-viscog.pdf:application/pdf}
}

@inproceedings{smith-learning-2013,
	title = {Learning and using language via recursive pragmatic reasoning about other agents},
	url = {http://papers.nips.cc/paper/4929-learning-and-using-language-via-recursive-pragmatic-reasoning-about-other},
	urldate = {2016-04-07},
	booktitle = {Advances in neural information processing systems},
	author = {Smith, Nathaniel J. and Goodman, Noah and Frank, Michael},
	year = {2013},
	pages = {3039--3047},
	file = {smith-goodman-frank-2013-pragmatics-learning.pdf:/home/user/Zotero/storage/A85H8ZAV/smith-goodman-frank-2013-pragmatics-learning.pdf:application/pdf}
}

@article{kutas-look-2011,
	title = {A look around at what lies ahead: {Prediction} and predictability in language processing},
	shorttitle = {A look around at what lies ahead},
	url = {https://books.google.com/books?hl=en&lr=&id=RNzTF7Gyn7IC&oi=fnd&pg=PA190&dq=%22chances+of+mispredicting+are%22+%22from+theoretical+models+that%22+%22explore+possible+consequences+of+a%22+%22to+move+beyond+the+debate+of%22+%22syntactically+congruent%22+%22raced+past+the+barn+fell%E2%80%9D+(Bever,+1970).%22+&ots=bSEdm3iFlW&sig=owe2aFE1-Yf1y8MQz4KUcUtohvM},
	urldate = {2016-04-07},
	journal = {Predictions in the brain: Using our past to generate a future},
	author = {Kutas, Marta and DeLong, Katherine A. and Smith, Nathaniel J.},
	year = {2011},
	pages = {190--207},
	file = {15_Bar_Chapter_15.indd - kutas-delong-smith-2011-chapter.pdf:/home/user/Zotero/storage/QE7FXUUI/kutas-delong-smith-2011-chapter.pdf:application/pdf}
}

@article{bergen-pragmatic-2014,
	title = {Pragmatic reasoning through semantic inference},
	url = {http://web.mit.edu/bergen/www/papers/BergenLevyGoodman2015.pdf},
	urldate = {2016-04-07},
	journal = {Unpublished manuscript},
	author = {Bergen, Leon and Levy, Roger and Goodman, Noah D.},
	year = {2014},
	file = {BergenLevyGoodman2015-ms.pdf:/home/user/Zotero/storage/8CCRIEQF/BergenLevyGoodman2015-ms.pdf:application/pdf}
}

@inproceedings{hahn-null-2011,
	title = {Null {Conjuncts} and {Bound} {Pronouns} in {Arabic}},
	booktitle = {The {Proceedings} of the 18th {International} {Conference} on {Head}-{Driven} {Phrase} {Structure} {Grammar}},
	author = {Hahn, Michael},
	year = {2011}
}

@inproceedings{hahn-arabic-2012,
	address = {Chungnam National University Daejeon},
	title = {Arabic {Relativization} {Patterns}: {A} {Unified} {HPSG} {Analysis}},
	url = {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2012/hahn.pdf},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Head}-{Driven} {Phrase} {Structure} {Grammar}},
	author = {Hahn, Michael},
	year = {2012}
}

@article{mcdonald-eye-2003,
	title = {Eye movements reveal the on-line computation of lexical probabilities during reading},
	volume = {14},
	issn = {0956-7976},
	abstract = {Skilled readers are able to derive meaning from a stream of visual input with remarkable efficiency. In this article, we present the first evidence that statistical information latent in the linguistic environment can contribute to an account of reading behavior. In two eye-tracking studies, we demonstrate that the transitional probabilities between words have a measurable influence on fixation durations, and using a simple Bayesian statistical model, we show that lexical probabilities derived by combining transitional probability with the prior probability of a word's occurrence provide the most parsimonious account of the eye movement data. We suggest that the brain is able to draw upon statistical information in order to rapidly estimate the lexical probabilities of upcoming words: a computationally inexpensive mechanism that may underlie proficient reading.},
	language = {eng},
	number = {6},
	journal = {Psychological Science},
	author = {McDonald, Scott A. and Shillcock, Richard C.},
	month = nov,
	year = {2003},
	pmid = {14629701},
	keywords = {Humans, Eye Movements, Reading, Fixation, Ocular, Bayes Theorem, Vocabulary},
	pages = {648--652}
}

@article{mcdonald-low-level-2003,
	title = {Low-level predictive inference in reading: the influence of transitional probabilities on eye movements},
	volume = {43},
	issn = {0042-6989},
	shorttitle = {Low-level predictive inference in reading},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698903002372},
	doi = {10.1016/S0042-6989(03)00237-2},
	abstract = {We report the results of an investigation into the ability of transitional probability (word-to-word contingency statistics) to account for reading behaviour. Using a corpus of eye movements recorded during the reading of newspaper text, we demonstrate both the forward [P(n{\textbar}n−1)] and backward [P(n{\textbar}n+1)] transitional probability measures to be predictive of first fixation and gaze durations: the higher the transitional probability, the shorter the fixation time. Initial fixation position was also affected by the forward measure; we observed a small rightward shift for words that were highly predictable from the preceding word. Although transitional probability is sensitive to word class, with function words being generally more predictable from their context than content words, the measures accounted equally well for the data for both classes.},
	number = {16},
	urldate = {2016-04-06},
	journal = {Vision Research},
	author = {McDonald, Scott A. and Shillcock, Richard C.},
	month = jul,
	year = {2003},
	keywords = {Eye Movements, Reading, Fixation duration, Fixation location},
	pages = {1735--1751},
	file = {McDonald_Shillcock_2003_Low-level predictive inference in reading.pdf:/home/user/Zotero/storage/9KTQVBZS/McDonald_Shillcock_2003_Low-level predictive inference in reading.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/BVCD99GK/S0042698903002372.html:text/html}
}

@article{reichle-toward-1998,
	title = {Toward a model of eye movement control in reading},
	volume = {105},
	issn = {0033-295X},
	abstract = {The authors present several versions of a general model, titled the E-Z Reader model, of eye movement control in reading. The major goal of the modeling is to relate cognitive processing (specifically aspects of lexical access) to eye movements in reading. The earliest and simplest versions of the model (E-Z Readers 1 and 2) merely attempt to explain the total time spent on a word before moving forward (the gaze duration) and the probability of fixating a word; later versions (E-Z Readers 3-5) also attempt to explain the durations of individual fixations on individual words and the number of fixations on individual words. The final version (E-Z Reader 5) appears to be psychologically plausible and gives a good account of many phenomena in reading. It is also a good tool for analyzing eye movement data in reading. Limitations of the model and directions for future research are also discussed.},
	language = {eng},
	number = {1},
	journal = {Psychological Review},
	author = {Reichle, E. D. and Pollatsek, A. and Fisher, D. L. and Rayner, K.},
	month = jan,
	year = {1998},
	pmid = {9450374},
	keywords = {Humans, psycholinguistics, Eye Movements, Reading, Models, Psychological, Attention, Cognition},
	pages = {125--157}
}

@article{rayner-models-2010-2,
	title = {Models of the {Reading} {Process}},
	volume = {1},
	issn = {1939-5078},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001687/},
	doi = {10.1002/wcs.68},
	abstract = {Reading is a complex skill involving the orchestration of a number of components. Researchers often talk about a “model of reading” when talking about only one aspect of the reading process (for example, models of word identification are often referred to as “models of reading”). Here, we review prominent models that are designed to account for (1) word identification, (2) syntactic parsing, (3) discourse representations, and (4) how certain aspects of language processing (e.g., word identification), in conjunction with other constraints (e g., limited visual acuity, saccadic error, etc.), guide readers’ eyes. Unfortunately, it is the case that these various models addressing specific aspects of the reading process seldom make contact with models dealing with other aspects of reading. Thus, for example, the models of word identification seldom make contact with models of eye movement control, and vice versa. While this may be unfortunate in some ways, it is quite understandable in other ways because reading itself is a very complex process. We discuss prototypical models of aspects of the reading process in the order mentioned above. We do not review all possible models, but rather focus on those we view as being representative and most highly recognized.},
	number = {6},
	urldate = {2016-04-05},
	journal = {Wiley interdisciplinary reviews. Cognitive science},
	author = {Rayner, Keith and Reichle, Erik D.},
	year = {2010},
	pmid = {21170142},
	pmcid = {PMC3001687},
	pages = {787--799}
}

@inproceedings{matthies-blinkers-2013-1,
	title = {With {Blinkers} on: {Robust} {Prediction} of {Eye} {Movements} across {Readers}.},
	shorttitle = {With {Blinkers} on},
	url = {http://www.aclweb.org/website/old\_anthology/D/D13/D13-1075.pdf},
	urldate = {2016-04-04},
	booktitle = {{EMNLP}},
	author = {Matthies, Franz and Søgaard, Anders},
	year = {2013},
	pages = {803--807},
	file = {With Blinkers on\: Robust Prediction of Eye Movements across Readers - D13-1075:/home/user/Zotero/storage/HZEKXRZX/D13-1075.pdf:application/pdf}
}

@article{simner-synaesthesia-2011,
	series = {From {Dreams} to {Psychosis}: {A} {European} {Science} {Foundation} {Exploratory} {Workshop}},
	title = {Synaesthesia in a logographic language: {The} colouring of {Chinese} characters and {Pinyin}/{Bopomo} spellings},
	volume = {20},
	issn = {1053-8100},
	shorttitle = {Synaesthesia in a logographic language},
	url = {http://www.sciencedirect.com/science/article/pii/S1053810011001395},
	doi = {10.1016/j.concog.2011.05.006},
	abstract = {Studies of linguistic synaesthesias in English have shown a range of fine-grained language mechanisms governing the associations between colours on the one hand, and graphemes, phonemes and words on the other. However, virtually nothing is known about how synaesthetic colouring might operate in non-alphabetic systems. The current study shows how synaesthetic speakers of Mandarin Chinese come to colour the logographic units of their language. Both native and non-native Chinese speakers experienced synaesthetic colours for characters, and for words spelled in the Chinese spelling systems of Pinyin and Bopomo. We assessed the influences of lexical tone and Pinyin/Bopomo spelling and showed that synaesthetic colours are assigned to Chinese words in a non-random fashion. Our data show that Chinese-speaking synaesthetes with very different native languages can exhibit both differences and similarities in the ways in which they come to colour their Chinese words.},
	number = {4},
	urldate = {2016-04-01},
	journal = {Consciousness and Cognition},
	author = {Simner, Julia and Hung, Wan-Yu and Shillcock, Richard},
	month = dec,
	year = {2011},
	keywords = {Chinese, Character, Colour, Logographic, Synaesthesia, Synesthesia},
	pages = {1376--1392},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/836UN7DI/S1053810011001395.html:text/html;Simner et al_2011_Synaesthesia in a logographic language.pdf:/home/user/Zotero/storage/QWNVV64H/Simner et al_2011_Synaesthesia in a logographic language.pdf:application/pdf}
}

@article{obregon-foveational-2012,
	title = {Foveational complexity in single word identification: {Contralateral} visual pathways are advantaged over ipsilateral pathways},
	volume = {50},
	issn = {0028-3932},
	shorttitle = {Foveational complexity in single word identification},
	url = {http://www.sciencedirect.com/science/article/pii/S0028393212003818},
	doi = {10.1016/j.neuropsychologia.2012.09.009},
	abstract = {Recognition of a single word is an elemental task in innumerable cognitive psychology experiments, but involves unexpected complexity. We test a controversial claim that the human fovea is vertically divided, with each half projecting to either the contralateral or ipsilateral hemisphere, thereby influencing foveal word recognition. We report a novel haploscope task: the two halves of a four-letter word were briefly presented to the two eyes in a Both condition ( st {\textbar} ep ) ( st {\textbar} ep ) , a Contralateral condition ( st {\textbar} \_ \_ ) ( \_ \_ {\textbar} ep ) , or an Ipsilateral condition ( \_ \_ {\textbar} ep ) ( st {\textbar} \_ \_ ) , all yielding the same single word percept (step). The Both condition yielded superior perceptual recognition, followed by the contralateral projection, then the ipsilateral projection. These results demonstrate that the structure of the fovea influences even the recognition of short, foveally presented words. Projecting different parts of the same word to different hemispheres involves unforeseen complexities and opportunities for optimizing hemispheric coordination.},
	number = {14},
	urldate = {2016-04-01},
	journal = {Neuropsychologia},
	author = {Obregón, Mateo and Shillcock, Richard},
	month = dec,
	year = {2012},
	keywords = {Foveal splitting, Haploscope, Hemispheric processing, Stereoscope, Word perception},
	pages = {3279--3283},
	file = {Obregón_Shillcock_2012_Foveational complexity in single word identification.pdf:/home/user/Zotero/storage/89BK5R8G/Obregón_Shillcock_2012_Foveational complexity in single word identification.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/HS8VNC7S/S0028393212003818.html:text/html}
}

@article{dare-serial-2013,
	title = {Serial and parallel processing in reading: {Investigating} the effects of parafoveal orthographic information on nonisolated word recognition},
	volume = {66},
	issn = {1747-0218},
	shorttitle = {Serial and parallel processing in reading},
	url = {http://dx.doi.org/10.1080/17470218.2012.703212},
	doi = {10.1080/17470218.2012.703212},
	abstract = {We present a novel lexical decision task and three boundary paradigm eye-tracking experiments that clarify the picture of parallel processing in word recognition in context. First, we show that lexical decision is facilitated by associated letter information to the left and right of the word, with no apparent hemispheric specificity. Second, we show that parafoveal preview of a repeat of word n at word n + 1 facilitates reading of word n relative to a control condition with an unrelated word at word n + 1. Third, using a version of the boundary paradigm that allowed for a regressive eye movement, we show no parafoveal “postview” effect on reading word n of repeating word n at word n – 1. Fourth, we repeat the second experiment but compare the effects of parafoveal previews consisting of a repeated word n with a transposed central bigram (e.g., caot for coat) and a substituted central bigram (e.g., ceit for coat), showing the latter to have a deleterious effect on processing word n, thereby demonstrating that the parafoveal preview effect is at least orthographic and not purely visual.},
	number = {3},
	urldate = {2016-04-01},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Dare, Natasha and Shillcock, Richard},
	month = mar,
	year = {2013},
	pmid = {22950804},
	pages = {487--504},
	file = {Dare_Shillcock_2013_Serial and parallel processing in reading.pdf:/home/user/Zotero/storage/9JQEAAMU/Dare_Shillcock_2013_Serial and parallel processing in reading.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/X5FUNHB4/17470218.2012.html:text/html}
}

@article{kreiner-binocular-2008,
	title = {Binocular fixations: {What} happens when each eye looks at a different word?},
	volume = {43},
	issn = {0020-7594},
	shorttitle = {Binocular fixations},
	url = {http://www.research.ed.ac.uk/portal/en/publications/binocular-fixations-what-happens-when-each-eye-looks-at-a-different-word(90077b98-91e7-4657-b130-21251d7c8761).html},
	abstract = {Description},
	language = {English},
	number = {3-4},
	urldate = {2016-04-01},
	journal = {International Journal of Psychology},
	author = {Kreiner, Hamutal and Shillcock, Richard},
	year = {2008},
	file = {Snapshot:/home/user/Zotero/storage/VQF3WII2/binocular-fixations-what-happens-when-each-eye-looks-at-a-different-word(90077b98-91e7-4657-b13.html:text/html}
}

@article{shillcock-binocular-2010,
	title = {Binocular foveation in reading},
	volume = {72},
	issn = {1943-3921, 1943-393X},
	url = {http://www.springerlink.com/index/10.3758/BF03196694},
	doi = {10.3758/BF03196694},
	language = {en},
	number = {8},
	urldate = {2016-04-01},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Shillcock, Richard and Roberts, Matthew and Kreiner, Hamutal and Obregón, Mateo},
	month = nov,
	year = {2010},
	pages = {2184--2203}
}

@article{shillcock-multiple-2011,
	title = {Multiple models and multiple perspectives approaches in modelling eye-movements in reading},
	volume = {4},
	issn = {1995-8692},
	url = {http://www.research.ed.ac.uk/portal/en/publications/multiple-models-and-multiple-perspectives-approaches-in-modelling-eyemovements-in-reading(c43c4df2-131d-4982-897d-0d7383a8a67c).html},
	abstract = {Description},
	language = {English},
	number = {3},
	urldate = {2016-04-01},
	journal = {Journal of Eye Movement Research},
	author = {Shillcock, Richard},
	year = {2011},
	file = {Snapshot:/home/user/Zotero/storage/AJ6HARA5/multiple-models-and-multiple-perspectives-approaches-in-modelling-eyemovements-in-reading(c43c4.html:text/html}
}

@misc{noauthor-concrete-nodate,
	title = {The {Concrete} {Universal} and {Cognitive} {Science} - {Springer}},
	url = {http://link.springer.com/article/10.1007%2Fs10516-013-9210-y},
	urldate = {2016-04-01},
	file = {The Concrete Universal and Cognitive Science - Springer:/home/user/Zotero/storage/6JE4TJRR/10.html:text/html}
}

@article{nuthmann-binocular-2014,
	title = {A binocular moving window technique to study the roles of the two eyes in reading},
	volume = {22},
	issn = {1350-6285, 1464-0716},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13506285.2013.876480},
	doi = {10.1080/13506285.2013.876480},
	language = {en},
	number = {3-4},
	urldate = {2016-04-01},
	journal = {Visual Cognition},
	author = {Nuthmann, Antje and Beveridge, Madeleine E. L. and Shillcock, Richard C.},
	month = apr,
	year = {2014},
	pages = {259--282}
}

@article{hung-synaesthesia-2014,
	title = {Synaesthesia in {Chinese} characters: {The} role of radical function and position},
	volume = {24},
	issn = {10538100},
	shorttitle = {Synaesthesia in {Chinese} characters},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1053810013001682},
	doi = {10.1016/j.concog.2013.12.009},
	language = {en},
	urldate = {2016-04-01},
	journal = {Consciousness and Cognition},
	author = {Hung, Wan-Yu and Simner, Julia and Shillcock, Richard and Eagleman, David M.},
	month = feb,
	year = {2014},
	pages = {38--48}
}

@inproceedings{cruickshank-predicting-2015,
	title = {Predicting actions using an adaptive probabilistic model of human decision behaviours},
	url = {http://www.research.ed.ac.uk/portal/en/publications/predicting-actions-using-an-adaptive-probabilistic-model-of-human-decision-behaviours(c8a52bac-5aa6-4341-b5aa-f558f24a8c16).html},
	abstract = {Description},
	language = {English},
	urldate = {2016-04-01},
	author = {Cruickshank, Anthony and Shillcock, Richard and Ramamoorthy, Ram},
	month = jun,
	year = {2015},
	file = {Cruickshank et al_2015_Predicting actions using an adaptive probabilistic model of human decision.pdf:/home/user/Zotero/storage/R7ESQV4U/Cruickshank et al_2015_Predicting actions using an adaptive probabilistic model of human decision.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/KC6DZS57/predicting-actions-using-an-adaptive-probabilistic-model-of-human-decision-behaviours(c8a52bac-.html:text/html}
}

@article{monaghan-how-2014,
	title = {How arbitrary is language?},
	volume = {369},
	issn = {0962-8436},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123678/},
	doi = {10.1098/rstb.2013.0299},
	abstract = {It is a long established convention that the relationship between sounds and meanings of words is essentially arbitrary—typically the sound of a word gives no hint of its meaning. However, there are numerous reported instances of systematic sound–meaning mappings in language, and this systematicity has been claimed to be important for early language development. In a large-scale corpus analysis of English, we show that sound–meaning mappings are more systematic than would be expected by chance. Furthermore, this systematicity is more pronounced for words involved in the early stages of language acquisition and reduces in later vocabulary development. We propose that the vocabulary is structured to enable systematicity in early language learning to promote language acquisition, while also incorporating arbitrariness for later language in order to facilitate communicative expressivity and efficiency.},
	number = {1651},
	urldate = {2016-04-01},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Monaghan, Padraic and Shillcock, Richard C. and Christiansen, Morten H. and Kirby, Simon},
	month = sep,
	year = {2014},
	pmid = {25092667},
	pmcid = {PMC4123678},
	file = {Monaghan et al_2014_How arbitrary is language.pdf:/home/user/Zotero/storage/WXXGFITA/Monaghan et al_2014_How arbitrary is language.pdf:application/pdf}
}

@article{monaghan-how-2014-1,
	title = {How arbitrary is language?},
	volume = {369},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0299},
	doi = {10.1098/rstb.2013.0299},
	language = {en},
	number = {1651},
	urldate = {2016-04-01},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Monaghan, P. and Shillcock, R. C. and Christiansen, M. H. and Kirby, S.},
	month = aug,
	year = {2014},
	pages = {20130299--20130299}
}

@article{suster-bilingual-2016,
	title = {Bilingual {Learning} of {Multi}-sense {Embeddings} with {Discrete} {Autoencoders}},
	url = {http://arxiv.org/abs/1603.09128},
	abstract = {We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.},
	urldate = {2016-03-31},
	journal = {arXiv:1603.09128 [cs, stat]},
	author = {Šuster, Simon and Titov, Ivan and van Noord, Gertjan},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.09128},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/EZBMAF56/1603.html:text/html;Šuster et al_2016_Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders.pdf:/home/user/Zotero/storage/CQP5NGHE/Šuster et al_2016_Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders.pdf:application/pdf}
}

@article{gutmann-noise-contrastive-2012,
	title = {Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
	volume = {13},
	url = {http://dl.acm.org/citation.cfm?id=2188396},
	number = {1},
	urldate = {2016-03-31},
	journal = {The Journal of Machine Learning Research},
	author = {Gutmann, Michael U. and Hyvärinen, Aapo},
	year = {2012},
	pages = {307--361},
	file = {gutmann12a.dvi - Gutmann12JMLR.pdf:/home/user/Zotero/storage/NPWQU2VQ/Gutmann12JMLR.pdf:application/pdf}
}

@article{nowak-error-1999,
	title = {An error limit for the evolution of language},
	volume = {266},
	url = {http://rspb.royalsocietypublishing.org/content/266/1433/2131.short},
	number = {1433},
	urldate = {2016-03-31},
	journal = {Proceedings of the Royal Society of London B: Biological Sciences},
	author = {Nowak, Martin A. and Krakauer, David C. and Dress, Andreas},
	year = {1999},
	pages = {2131--2136},
	file = {procroysoc99a_0.pdf:/home/user/Zotero/storage/FX4WVBWD/procroysoc99a_0.pdf:application/pdf}
}

@article{nowak-evolutionary-2000,
	title = {Evolutionary biology of language},
	volume = {355},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2000.0723},
	doi = {10.1098/rstb.2000.0723},
	language = {en},
	number = {1403},
	urldate = {2016-03-31},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Nowak, M. A.},
	month = nov,
	year = {2000},
	pages = {1615--1622},
	file = {Evolutionary biology of language - philtransroysoc00c_0.pdf:/home/user/Zotero/storage/G2EH8FF6/philtransroysoc00c_0.pdf:application/pdf}
}

@article{plotkin-language-2000,
	title = {Language {Evolution} and {Information} {Theory}},
	volume = {205},
	issn = {00225193},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022519300920538},
	doi = {10.1006/jtbi.2000.2053},
	language = {en},
	number = {1},
	urldate = {2016-03-31},
	journal = {Journal of Theoretical Biology},
	author = {Plotkin, Joshua B and Nowak, Martin A},
	month = jul,
	year = {2000},
	pages = {147--159},
	file = {Language Evolution and Information Theory - jtb00c.pdf:/home/user/Zotero/storage/MTWQPC3P/jtb00c.pdf:application/pdf}
}

@article{komarova-natural-2001,
	title = {Natural selection of the critical period for language acquisition},
	volume = {268},
	issn = {0962-8452, 1471-2954},
	url = {http://rspb.royalsocietypublishing.org/cgi/doi/10.1098/rspb.2001.1629},
	doi = {10.1098/rspb.2001.1629},
	language = {en},
	number = {1472},
	urldate = {2016-03-31},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Komarova, N. L. and Nowak, M. A.},
	month = jun,
	year = {2001},
	pages = {1189--1196},
	file = {Natural selection of the critical period for language acquisition - procroysoc01a_0.pdf:/home/user/Zotero/storage/CXSP6QFP/procroysoc01a_0.pdf:application/pdf}
}

@article{nowak-towards-2001,
	title = {Towards an evolutionary theory of language},
	volume = {5},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661300016831},
	number = {7},
	urldate = {2016-03-31},
	journal = {Trends in cognitive sciences},
	author = {Nowak, Martin A. and Komarova, Natalia L.},
	year = {2001},
	pages = {288--295},
	file = {tcs01_0.pdf:/home/user/Zotero/storage/T2NUNTCZ/tcs01_0.pdf:application/pdf}
}

@article{nowak-computational-2002,
	title = {Computational and evolutionary aspects of language},
	volume = {417},
	url = {http://www.nature.com/nature/journal/v417/n6889/abs/nature00771.html},
	number = {6889},
	urldate = {2016-03-31},
	journal = {Nature},
	author = {Nowak, Martin A. and Komarova, Natalia L. and Niyogi, Partha},
	year = {2002},
	pages = {611--617},
	file = {47284 611..617 - nature02a_0.pdf:/home/user/Zotero/storage/7T33TIX3/nature02a_0.pdf:application/pdf}
}

@article{komarova-language-2003,
	title = {Language {Dynamics} in {Finite} {Populations}},
	volume = {221},
	issn = {00225193},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022519303931997},
	doi = {10.1006/jtbi.2003.3199},
	language = {en},
	number = {3},
	urldate = {2016-03-31},
	journal = {Journal of Theoretical Biology},
	author = {Komarova, Natalia L. and Nowak, Martin A.},
	month = apr,
	year = {2003},
	pages = {445--457},
	file = {Language Dynamics in Finite Populations - jtb03a_0_0.pdf:/home/user/Zotero/storage/WBDMSHFW/jtb03a_0_0.pdf:application/pdf}
}

@article{matsen-winstay-2004,
	title = {Win–stay, lose–shift in language learning from peers},
	volume = {101},
	url = {https://www.pnas.org/content/101/52/18053.full},
	number = {52},
	urldate = {2016-03-31},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Matsen, Frederick A. and Nowak, Martin A.},
	year = {2004},
	pages = {18053--18057},
	file = {pnas04_0.pdf:/home/user/Zotero/storage/INIZQ7BH/pnas04_0.pdf:application/pdf}
}

@article{mitchener-chaos-2004,
	title = {Chaos and language},
	volume = {271},
	issn = {0962-8452, 1471-2954},
	url = {http://rspb.royalsocietypublishing.org/cgi/doi/10.1098/rspb.2003.2643},
	doi = {10.1098/rspb.2003.2643},
	language = {en},
	number = {1540},
	urldate = {2016-03-31},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Mitchener, W. G. and Nowak, M. A.},
	month = apr,
	year = {2004},
	pages = {701--704},
	file = {procroysoc04_0.pdf:/home/user/Zotero/storage/FMK3G57S/procroysoc04_0.pdf:application/pdf}
}

@article{lieberman-quantifying-2007,
	title = {Quantifying the evolutionary dynamics of language},
	volume = {449},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/nature06137},
	doi = {10.1038/nature06137},
	number = {7163},
	urldate = {2016-03-31},
	journal = {Nature},
	author = {Lieberman, Erez and Michel, Jean-Baptiste and Jackson, Joe and Tang, Tina and Nowak, Martin A.},
	month = oct,
	year = {2007},
	pages = {713--716},
	file = {11.10 cover UK copy - nature07_0.pdf:/home/user/Zotero/storage/E5P7IE68/nature07_0.pdf:application/pdf}
}

@article{hoffman-cooperate-2015,
	title = {Cooperate without looking: {Why} we care what people think and not just what they do},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Cooperate without looking},
	url = {http://www.pnas.org/content/112/6/1727},
	doi = {10.1073/pnas.1417904112},
	abstract = {Evolutionary game theory typically focuses on actions but ignores motives. Here, we introduce a model that takes into account the motive behind the action. A crucial question is why do we trust people more who cooperate without calculating the costs? We propose a game theory model to explain this phenomenon. One player has the option to “look” at the costs of cooperation, and the other player chooses whether to continue the interaction. If it is occasionally very costly for player 1 to cooperate, but defection is harmful for player 2, then cooperation without looking is a subgame perfect equilibrium. This behavior also emerges in population-based processes of learning or evolution. Our theory illuminates a number of key phenomena of human interactions: authentic altruism, why people cooperate intuitively, one-shot cooperation, why friends do not keep track of favors, why we admire principled people, Kant’s second formulation of the Categorical Imperative, taboos, and love.},
	language = {en},
	number = {6},
	urldate = {2016-03-31},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hoffman, Moshe and Yoeli, Erez and Nowak, Martin A.},
	month = feb,
	year = {2015},
	pmid = {25624473},
	keywords = {game theory, cooperation, emotion, evolution, motive},
	pages = {1727--1732},
	file = {Hoffman et al_2015_Cooperate without looking.pdf:/home/user/Zotero/storage/KVDJ7PWB/Hoffman et al_2015_Cooperate without looking.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9ESBP4IE/1727.html:text/html}
}

@misc{noauthor-spatial-nodate,
	title = {A spatial model predicts that dispersal and cell turnover limit intratumour heterogeneity : {Nature} : {Nature} {Publishing} {Group}},
	url = {http://www.nature.com/nature/journal/v525/n7568/full/nature14971.html},
	urldate = {2016-03-31},
	file = {A spatial model predicts that dispersal and cell turnover limit intratumour heterogeneity \: Nature \: Nature Publishing Group:/home/user/Zotero/storage/MEXJWKIU/nature14971.html:text/html}
}

@article{nowak-evolution-1999,
	title = {The evolution of language},
	volume = {96},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/96/14/8028},
	doi = {10.1073/pnas.96.14.8028},
	abstract = {The emergence of language was a defining moment in the evolution of modern humans. It was an innovation that changed radically the character of human society. Here, we provide an approach to language evolution based on evolutionary game theory. We explore the ways in which protolanguages can evolve in a nonlinguistic society and how specific signals can become associated with specific objects. We assume that early in the evolution of language, errors in signaling and perception would be common. We model the probability of misunderstanding a signal and show that this limits the number of objects that can be described by a protolanguage. This “error limit” is not overcome by employing more sounds but by combining a small set of more easily distinguishable sounds into words. The process of “word formation” enables a language to encode an essentially unlimited number of objects. Next, we analyze how words can be combined into sentences and specify the conditions for the evolution of very simple grammatical rules. We argue that grammar originated as a simplified rule system that evolved by natural selection to reduce mistakes in communication. Our theory provides a systematic approach for thinking about the origin and evolution of human language.},
	language = {en},
	number = {14},
	urldate = {2016-03-31},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nowak, Martin A. and Krakauer, David C.},
	month = jul,
	year = {1999},
	pmid = {10393942},
	pages = {8028--8033},
	file = {Nowak_Krakauer_1999_The evolution of language.pdf:/home/user/Zotero/storage/UT7IZ2RU/Nowak_Krakauer_1999_The evolution of language.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VM2TMEC6/8028.html:text/html}
}

@inproceedings{hyvarinen-estimation-2005,
	title = {Estimation of non-normalized statistical models by score matching},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/Hyvarinen05.pdf},
	urldate = {2016-03-31},
	booktitle = {Journal of {Machine} {Learning} {Research}},
	author = {Hyvärinen, Aapo},
	year = {2005},
	pages = {695--709},
	file = {JMLR05.pdf:/home/user/Zotero/storage/VSNGDBRX/JMLR05.pdf:application/pdf}
}

@book{bishop-deep-2006,
	address = {New York},
	series = {Information science and statistics},
	title = {deep and tractable density estimator},
	isbn = {978-0-387-31073-2},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
	file = {dnade.pdf:/home/user/Zotero/storage/SNAG7MSD/dnade.pdf:application/pdf}
}

@article{tsuchiya-no-report-2015,
	title = {No-{Report} {Paradigms}: {Extracting} the {True} {Neural} {Correlates} of {Consciousness}},
	volume = {19},
	issn = {1879-307X},
	shorttitle = {No-{Report} {Paradigms}},
	doi = {10.1016/j.tics.2015.10.002},
	abstract = {The goal of consciousness research is to reveal the neural basis of phenomenal experience. To study phenomenology, experimenters seem obliged to ask reports from the subjects to ascertain what they experience. However, we argue that the requirement of reports has biased the search for the neural correlates of consciousness over the past decades. More recent studies attempt to dissociate neural activity that gives rise to consciousness from the activity that enables the report; in particular, no-report paradigms have been utilized to study conscious experience in the full absence of any report. We discuss the advantages and disadvantages of report-based and no-report paradigms, and ask how these jointly bring us closer to understanding the true neural basis of consciousness.},
	language = {eng},
	number = {12},
	journal = {Trends in Cognitive Sciences},
	author = {Tsuchiya, Naotsugu and Wilke, Melanie and Frässle, Stefan and Lamme, Victor A. F.},
	month = dec,
	year = {2015},
	pmid = {26585549},
	keywords = {Attention, Consciousness, access, awareness, introspection, report},
	pages = {757--770}
}

@inproceedings{chen-modeling-2014,
	title = {Modeling sentence processing difficulty with a conditional probability calculator},
	url = {https://mindmodeling.org/cogsci2014/papers/322/paper322.pdf},
	urldate = {2016-03-31},
	booktitle = {Proceedings of the 36th {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Chen, Zhong and Hunter, Tim and Yun, Jiwon and Hale, John},
	year = {2014},
	pages = {1856--1857},
	file = {paper322.pdf:/home/user/Zotero/storage/8XMNGFKK/paper322.pdf:application/pdf}
}

@misc{john-hale-what-nodate,
	title = {what a rational parser would do},
	url = {https://courses.cit.cornell.edu/jth99/whatrational-ms.pdf},
	urldate = {2016-03-24},
	author = {{John Hale}},
	file = {whatrational-ms.pdf:/home/user/Zotero/storage/5SQM23MW/whatrational-ms.pdf:application/pdf}
}

@book{nowak-evolutionary-nodate,
	title = {Evolutionary},
	author = {Nowak},
	file = {[Martin_A._Nowak]_Evolutionary_Dynamics_Exploring(BookZZ.org).djvu:/home/user/Zotero/storage/2A7BCU4V/[Martin_A._Nowak]_Evolutionary_Dynamics_Exploring(BookZZ.org).djvu:image/vnd.djvu}
}

@article{yuan-influence-2016,
	title = {On the {Influence} of {Momentum} {Acceleration} on {Online} {Learning}},
	url = {http://arxiv.org/abs/1603.04136},
	abstract = {The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size case and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the stochastic (online) setting when adaptation becomes necessary and when the true gradient vectors are not known beforehand. The analysis also suggests a method to retain some of the advantages of the momentum construction by employing a decaying momentum parameter, as opposed to a decaying step-size. In this way, the enhanced convergence rate during the initial stages of adaptation is preserved without the often-observed degradation in MSD performance.},
	urldate = {2016-03-30},
	journal = {arXiv:1603.04136 [cs, math, stat]},
	author = {Yuan, Kun and Ying, Bicheng and Sayed, Ali H.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04136},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/DK2D82DT/1603.html:text/html;Yuan et al_2016_On the Influence of Momentum Acceleration on Online Learning.pdf:/home/user/Zotero/storage/6F9575XC/Yuan et al_2016_On the Influence of Momentum Acceleration on Online Learning.pdf:application/pdf}
}

@article{lahiri-universal-2016,
	title = {A universal tradeoff between power, precision and speed in physical communication},
	url = {http://arxiv.org/abs/1603.07758},
	abstract = {Maximizing the speed and precision of communication while minimizing power dissipation is a fundamental engineering design goal. Also, biological systems achieve remarkable speed, precision and power efficiency using poorly understood physical design principles. Powerful theories like information theory and thermodynamics do not provide general limits on power, precision and speed. Here we go beyond these classical theories to prove that the product of precision and speed is universally bounded by power dissipation in any physical communication channel whose dynamics is faster than that of the signal. Moreover, our derivation involves a novel connection between friction and information geometry. These results may yield insight into both the engineering design of communication devices and the structure and function of biological signaling systems.},
	urldate = {2016-03-30},
	journal = {arXiv:1603.07758 [cond-mat, physics:physics, q-bio, stat]},
	author = {Lahiri, Subhaneil and Sohl-Dickstein, Jascha and Ganguli, Surya},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07758},
	keywords = {Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Physics - Biological Physics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RQS84CUI/1603.html:text/html;Lahiri et al_2016_A universal tradeoff between power, precision and speed in physical.pdf:/home/user/Zotero/storage/PB8235UI/Lahiri et al_2016_A universal tradeoff between power, precision and speed in physical.pdf:application/pdf}
}

@article{meyer-same-nodate,
	title = {Same, different, or closely related: {What} is the relationship between language production and comprehension?},
	issn = {0749-596X},
	shorttitle = {Same, different, or closely related},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X16000243},
	doi = {10.1016/j.jml.2016.03.002},
	urldate = {2016-03-30},
	journal = {Journal of Memory and Language},
	author = {Meyer, Antje S. and Huettig, Falk and Levelt, Willem J. M.},
	file = {Meyer et al_Same, different, or closely related.pdf:/home/user/Zotero/storage/XR6DDGVF/Meyer et al_Same, different, or closely related.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/9WUUUZMG/S0749596X16000243.html:text/html}
}

@article{huang-independence-nodate,
	title = {The independence of syntactic processing in {Mandarin}: {Evidence} from structural priming},
	issn = {0749-596X},
	shorttitle = {The independence of syntactic processing in {Mandarin}},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X16000255},
	doi = {10.1016/j.jml.2016.02.005},
	abstract = {Although it is generally accepted that syntactic information is processed independently of semantic information in languages such as English, there is less agreement about whether the same is true in languages such as Mandarin that have fewer reliable cues to syntactic structure. We report five experiments that used a structural priming paradigm to investigate the independence of syntactic processing in Mandarin. In a recognition memory task, Mandarin native speakers described ditransitive events after repeating prime sentences with a double object (DO) or prepositional object (PO) structure. Participants tended to repeat syntactic structure across prime and target sentences. Critically, this tendency occurred whether or not semantic features (animacy of the recipient) were also repeated across sentences, both when the verb was repeated and when it was not. We conclude that Mandarin speakers compute independent syntactic representations during language processing.},
	urldate = {2016-03-30},
	journal = {Journal of Memory and Language},
	author = {Huang, Jian and Pickering, Martin J. and Yang, Juanhua and Wang, Suiping and Branigan, Holly P.},
	keywords = {Syntax, Chinese, structural priming, Animacy, Mandarin},
	file = {Huang et al_The independence of syntactic processing in Mandarin.pdf:/home/user/Zotero/storage/ZDFAEB2T/Huang et al_The independence of syntactic processing in Mandarin.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/Q5QKH53D/S0749596X16000255.html:text/html}
}

@article{jones-reading-2016,
	title = {Do reading and spelling share a lexicon?},
	volume = {86},
	issn = {0010-0285},
	url = {http://www.sciencedirect.com/science/article/pii/S0010028516000153},
	doi = {10.1016/j.cogpsych.2016.02.003},
	abstract = {In the reading and spelling literature, an ongoing debate concerns whether reading and spelling share a single orthographic lexicon or rely upon independent lexica. Available evidence tends to support a single lexicon account over an independent lexica account, but evidence is mixed and open to alternative explanation. In the current work, we propose another, largely ignored account – separate-but-shared lexica – according to which reading and spelling have separate orthographic lexica, but information can be shared between them. We report three experiments designed to competitively evaluate these three theoretical accounts. In each experiment, participants learned new words via reading training and/or spelling training. The key manipulation concerned the amount of reading versus spelling practice a given item received. Following training, we assessed both response time and accuracy on final outcome measures of reading and spelling. According to the independent lexica account, final performance in one modality will not be influenced by the level of practice in the other modality. According to the single lexicon account, final performance will depend on the overall amount of practice regardless of modality. According to the separate-but-shared account, final performance will be influenced by the level of practice in both modalities but will benefit more from same-modality practice. Results support the separate-but-shared account, indicating that reading and spelling rely upon separate lexica, but information can be shared between them.},
	urldate = {2016-03-30},
	journal = {Cognitive Psychology},
	author = {Jones, Angela C. and Rawson, Katherine A.},
	month = may,
	year = {2016},
	keywords = {Reading, spelling, Lexicon},
	pages = {152--184},
	file = {Jones_Rawson_2016_Do reading and spelling share a lexicon.pdf:/home/user/Zotero/storage/GRPETJAN/Jones_Rawson_2016_Do reading and spelling share a lexicon.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/AKCVH7NW/S0010028516000153.html:text/html}
}

@article{love-cognitive-2016,
	title = {Cognitive {Models} as {Bridge} between {Brain} and {Behavior}},
	volume = {20},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000474},
	doi = {10.1016/j.tics.2016.02.006},
	abstract = {How can disparate neural and behavioral measures be integrated? Turner and colleagues propose joint modeling as a solution. Joint modeling mutually constrains the interpretation of brain and behavioral measures by exploiting their covariation structure. Simultaneous estimation allows for more accurate prediction than would be possible by considering these measures in isolation.},
	number = {4},
	urldate = {2016-03-30},
	journal = {Trends in Cognitive Sciences},
	author = {Love, Bradley C.},
	month = apr,
	year = {2016},
	pages = {247--248},
	file = {Love_2016_Cognitive Models as Bridge between Brain and Behavior.pdf:/home/user/Zotero/storage/69AMQFV6/Love_2016_Cognitive Models as Bridge between Brain and Behavior.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CE6BS8GS/S1364661316000474.html:text/html}
}

@article{shannon-is-2016,
	title = {Is {Birdsong} {More} {Like} {Speech} or {Music}?},
	volume = {20},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000450},
	doi = {10.1016/j.tics.2016.02.004},
	abstract = {Music and speech share many acoustic cues but not all are equally important. For example, harmonic pitch is essential for music but not for speech. When birds communicate is their song more like speech or music? A new study contrasting pitch and spectral patterns shows that birds perceive their song more like humans perceive speech.},
	number = {4},
	urldate = {2016-03-30},
	journal = {Trends in Cognitive Sciences},
	author = {Shannon, Robert V.},
	month = apr,
	year = {2016},
	keywords = {music, harmonic pitch, spectral envelope, speech, temporal envelope},
	pages = {245--247},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/F3GDN7U8/S1364661316000450.html:text/html;Shannon_2016_Is Birdsong More Like Speech or Music.pdf:/home/user/Zotero/storage/SA3BK544/Shannon_2016_Is Birdsong More Like Speech or Music.pdf:application/pdf}
}

@article{overgaard-can-2016,
	title = {Can {No}-{Report} {Paradigms} {Extract} {True} {Correlates} of {Consciousness}?},
	volume = {20},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S136466131600022X},
	doi = {10.1016/j.tics.2016.01.004},
	number = {4},
	urldate = {2016-03-30},
	journal = {Trends in Cognitive Sciences},
	author = {Overgaard, Morten and Fazekas, Peter},
	month = apr,
	year = {2016},
	keywords = {Consciousness, introspection, report, NCC},
	pages = {241--242},
	file = {Overgaard_Fazekas_2016_Can No-Report Paradigms Extract True Correlates of Consciousness.pdf:/home/user/Zotero/storage/HKPRRRA3/Overgaard_Fazekas_2016_Can No-Report Paradigms Extract True Correlates of Consciousness.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/NCZC7N36/S136466131600022X.html:text/html}
}

@misc{noauthor-neural-nodate-1,
	title = {Neural {Networks}, {Manifolds}, and {Topology} -- colah's blog},
	url = {http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/},
	urldate = {2016-03-29},
	file = {Neural Networks, Manifolds, and Topology -- colah's blog:/home/user/Zotero/storage/AHSD9VX2/2014-03-NN-Manifolds-Topology.html:text/html}
}

@article{macklin-cordes-high-definition-2015,
	title = {High-{Definition} {Phonotactics} {Reflect} {Linguistic} {Pasts}},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67189},
	urldate = {2016-04-10},
	author = {Macklin-Cordes, Jayden L. and Round, Erich R.},
	year = {2015},
	file = {M-Cordes_Round.pdf:/home/user/Zotero/storage/NFQNT4WE/M-Cordes_Round.pdf:application/pdf}
}

@inproceedings{bosch-definite-2011-1,
	title = {Definite reference is not always based on salience},
	url = {http://edoc.hu-berlin.de/docviews/abstract.php?id=38017},
	urldate = {2016-04-10},
	booktitle = {{QITL}-4-{Proceedings} of {Quantitative} {Investigations} in {Theoretical} {Linguistics} 4 ({QITL}-4)},
	publisher = {Humboldt-Universität zu Berlin},
	author = {Bosch, Peter and Alexeyenko, Sascha and Brukamp, Kirsten and Cieschinger, Maria and Deng, Xiaoye and König, Peter},
	year = {2011},
	file = {QITL4_Proceedings.pdf:/home/user/Zotero/storage/7R7QCGI5/QITL4_Proceedings.pdf:application/pdf}
}

@article{neri-elementary-2015-2,
	title = {The {Elementary} {Operations} of {Human} {Vision} {Are} {Not} {Reducible} to {Template}-{Matching}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004499},
	doi = {10.1371/journal.pcbi.1004499},
	language = {en},
	number = {11},
	urldate = {2016-04-10},
	journal = {PLOS Computational Biology},
	author = {Neri, Peter},
	editor = {Baker, Daniel Hart},
	month = nov,
	year = {2015},
	pages = {e1004499},
	file = {journal.pcbi.1004499.pdf:/home/user/Zotero/storage/Q8T2AHVW/journal.pcbi.1004499.pdf:application/pdf}
}

@incollection{nareyek-choosing-2003-1,
	title = {Choosing search heuristics by non-stationary reinforcement learning},
	url = {http://link.springer.com/chapter/10.1007/978-1-4757-4137-7\_25},
	urldate = {2016-04-10},
	booktitle = {Metaheuristics: {Computer} decision-making},
	publisher = {Springer},
	author = {Nareyek, Alexander},
	year = {2003},
	pages = {523--544},
	file = {book2015oct.pdf:/home/user/Zotero/storage/BUNJ9XI8/book2015oct.pdf:application/pdf}
}

@book{tono-frequency-2013,
	address = {London ; New York},
	series = {Routledge frequency dictionaries},
	title = {A frequency dictionary of {Japanese}: core vocabulary for learners},
	isbn = {978-0-415-61012-4 978-0-415-61013-1 978-0-415-60104-7},
	shorttitle = {A frequency dictionary of {Japanese}},
	language = {eng jap},
	publisher = {Routledge},
	author = {Tono, Yukio and Makawa, Kikuo and Yamazaki, Makoto},
	year = {2013},
	keywords = {Word frequency, English, Japanese language},
	file = {[Yukio_Tono,_Makoto_Yamazaki,_Kikuo_Maekawa]_A_Fre(BookZZ.org).pdf:/home/user/Zotero/storage/48ANUFVI/[Yukio_Tono,_Makoto_Yamazaki,_Kikuo_Maekawa]_A_Fre(BookZZ.org).pdf:application/pdf}
}

@article{sharma-action-2015-1,
	title = {Action {Recognition} using {Visual} {Attention}},
	url = {http://arxiv.org/abs/1511.04119},
	urldate = {2016-04-10},
	journal = {arXiv preprint arXiv:1511.04119},
	author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.04119v1.pdf:/home/user/Zotero/storage/HJXUXGQ3/1511.04119v1.pdf:application/pdf}
}

@article{freeman-more-nodate,
	title = {More {Than} {Meets} the {Eye}: {Split}-{Second} {Social} {Perception}},
	issn = {1364-6613},
	shorttitle = {More {Than} {Meets} the {Eye}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000504},
	doi = {10.1016/j.tics.2016.03.003},
	abstract = {Recent research suggests that visual perception of social categories is shaped not only by facial features but also by higher-order social cognitive processes (e.g., stereotypes, attitudes, goals). Building on neural computational models of social perception, we outline a perspective of how multiple bottom-up visual cues are flexibly integrated with a range of top-down processes to form perceptions, and we identify a set of key brain regions involved. During this integration, ‘hidden’ social category activations are often triggered which temporarily impact perception without manifesting in explicit perceptual judgments. Importantly, these hidden impacts and other aspects of the perceptual process predict downstream social consequences – from politicians’ electoral success to several evaluative biases – independently of the outcomes of that process.},
	urldate = {2016-04-10},
	journal = {Trends in Cognitive Sciences},
	author = {Freeman, Jonathan B. and Johnson, Kerri L.},
	keywords = {neuroimaging, computational models, face processing, person perception, stereotypes, top-down effects},
	file = {Freeman_Johnson_More Than Meets the Eye.pdf:/home/user/Zotero/storage/WJUZ5UHT/Freeman_Johnson_More Than Meets the Eye.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/8NPVBJ7T/S1364661316000504.html:text/html}
}

@article{martin-what-nodate,
	title = {What {Cognitive} {Representations} {Support} {Primate} {Theory} of {Mind}?},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316000656},
	doi = {10.1016/j.tics.2016.03.005},
	abstract = {Much recent work has examined the evolutionary origins of human mental state representations. This work has yielded strikingly consistent results: primates show a sophisticated ability to track the current and past perceptions of others, but they fail to represent the beliefs of others. We offer a new account of the nuanced performance of primates in theory of mind (ToM) tasks. We argue that primates form awareness relations tracking the aspects of reality that other agents are aware of. We contend that these awareness relations allow primates to make accurate predictions in social situations, but that this capacity falls short of our human-like representational ToM. We end by explaining how this new account makes important new empirical predictions about primate ToM.},
	urldate = {2016-04-10},
	journal = {Trends in Cognitive Sciences},
	author = {Martin, Alia and Santos, Laurie R.},
	keywords = {Cognitive development, social cognition, comparative cognition, infant cognition, mental states, nonhuman primates, theory of mind},
	file = {Martin_Santos_What Cognitive Representations Support Primate Theory of Mind.pdf:/home/user/Zotero/storage/QNJPBRCA/Martin_Santos_What Cognitive Representations Support Primate Theory of Mind.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/J6XN6JIS/S1364661316000656.html:text/html}
}

@article{stella-mental-2016,
	title = {Mental {Lexicon} {Growth} {Modelling} {Reveals} the {Multiplexity} of the {English} {Language}},
	volume = {644},
	url = {http://arxiv.org/abs/1604.01243},
	doi = {10.1007/978-3-319-30569-1\_20},
	abstract = {In this work we extend previous analyses of linguistic networks by adopting a multi-layer network framework for modelling the human mental lexicon, i.e. an abstract mental repository where words and concepts are stored together with their linguistic patterns. Across a three-layer linguistic multiplex, we model English words as nodes and connect them according to (i) phonological similarities, (ii) synonym relationships and (iii) free word associations. Our main aim is to exploit this multi-layered structure to explore the influence of phonological and semantic relationships on lexicon assembly over time. We propose a model of lexicon growth which is driven by the phonological layer: words are suggested according to different orderings of insertion (e.g. shorter word length, highest frequency, semantic multiplex features) and accepted or rejected subject to constraints. We then measure times of network assembly and compare these to empirical data about the age of acquisition of words. In agreement with empirical studies in psycholinguistics, our results provide quantitative evidence for the hypothesis that word acquisition is driven by features at multiple levels of organisation within language.},
	urldate = {2016-04-10},
	journal = {arXiv:1604.01243 [physics]},
	author = {Stella, Massimo and Brede, Markus},
	year = {2016},
	note = {arXiv: 1604.01243},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks, Physics - Physics and Society},
	pages = {267--279},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GHX2MN4J/1604.html:text/html;Stella_Brede_2016_Mental Lexicon Growth Modelling Reveals the Multiplexity of the English Language.pdf:/home/user/Zotero/storage/KNRV8W3C/Stella_Brede_2016_Mental Lexicon Growth Modelling Reveals the Multiplexity of the English Language.pdf:application/pdf}
}

@article{kawaguchi-bayesian-2016,
	title = {Bayesian {Optimization} with {Exponential} {Convergence}},
	url = {http://arxiv.org/abs/1604.01348},
	abstract = {This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.},
	urldate = {2016-04-10},
	journal = {arXiv:1604.01348 [cs, stat]},
	author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.01348},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KDRVF8CR/1604.html:text/html;Kawaguchi et al_2016_Bayesian Optimization with Exponential Convergence.pdf:/home/user/Zotero/storage/46II25SP/Kawaguchi et al_2016_Bayesian Optimization with Exponential Convergence.pdf:application/pdf}
}

@article{ortiz-martinez-online-2016,
	title = {Online {Learning} for {Statistical} {Machine} {Translation}},
	volume = {42},
	issn = {0891-2017},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/COLI\_a\_00244},
	doi = {10.1162/COLI\_a\_00244},
	abstract = {We present online learning techniques for statistical machine translation (SMT). The availability of large training data sets that grow constantly over time is becoming more and more frequent in the field of SMT—for example, in the context of translation agencies or the daily translation of government proceedings. When new knowledge is to be incorporated in the SMT models, the use of batch learning techniques require very time-consuming estimation processes over the whole training set that may take days or weeks to be executed. By means of the application of online learning, new training samples can be processed individually in real time. For this purpose, we define a state-of-the-art SMT model composed of a set of submodels, as well as a set of incremental update rules for each of these submodels. To test our techniques, we have studied two well-known SMT applications that can be used in translation agencies: post-editing and interactive machine translation. In both scenarios, the SMT system collaborates with the user to generate high-quality translations. These user-validated translations can be used to extend the SMT models by means of online learning. Empirical results in the two scenarios under consideration show the great impact of frequent updates in the system performance. The time cost of such updates was also measured, comparing the efficiency of a batch learning SMT system with that of an online learning system, showing that online learning is able to work in real time whereas the time cost of batch retraining soon becomes infeasible. Empirical results also showed that the performance of online learning is comparable to that of batch learning. Moreover, the proposed techniques were able to learn from previously estimated models or from scratch. We also propose two new measures to predict the effectiveness of online learning in SMT tasks. The translation system with online learning capabilities presented here is implemented in the open-source Thot toolkit for SMT.},
	number = {1},
	urldate = {2016-04-10},
	journal = {Computational Linguistics},
	author = {Ortiz-Martínez, Daniel},
	month = feb,
	year = {2016},
	pages = {121--161},
	file = {Snapshot:/home/user/Zotero/storage/MAKXDUSR/COLI_a_00244.html:text/html}
}

@article{neubig-optimization-2016,
	title = {Optimization for {Statistical} {Machine} {Translation}: {A} {Survey}},
	volume = {42},
	issn = {0891-2017},
	shorttitle = {Optimization for {Statistical} {Machine} {Translation}},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/COLI\_a\_00241},
	doi = {10.1162/COLI\_a\_00241},
	abstract = {In statistical machine translation (SMT), the optimization of the system parameters to maximize translation accuracy is now a fundamental part of virtually all modern systems. In this article, we survey 12 years of research on optimization for SMT, from the seminal work on discriminative models (Och and Ney 2002) and minimum error rate training (Och 2003), to the most recent advances. Starting with a brief introduction to the fundamentals of SMT systems, we follow by covering a wide variety of optimization algorithms for use in both batch and online optimization. Specifically, we discuss losses based on direct error minimization, maximum likelihood, maximum margin, risk minimization, ranking, and more, along with the appropriate methods for minimizing these losses. We also cover recent topics, including large-scale optimization, nonlinear models, domain-dependent optimization, and the effect of MT evaluation measures or search on optimization. Finally, we discuss the current state of affairs in MT optimization, and point out some unresolved problems that will likely be the target of further research in optimization for MT.},
	number = {1},
	urldate = {2016-04-10},
	journal = {Computational Linguistics},
	author = {Neubig, Graham and Watanabe, Taro},
	month = feb,
	year = {2016},
	pages = {1--54},
	file = {Snapshot:/home/user/Zotero/storage/92GVTGXU/COLI_a_00241.html:text/html}
}

@article{lee-neural-2016,
	title = {Neural {Correlates} of {Predictive} {Saccades}},
	issn = {0898-929X},
	url = {http://dx.doi.org/10.1162/jocn\_a\_00968},
	doi = {10.1162/jocn\_a\_00968},
	abstract = {Every day we generate motor responses that are timed with external cues. This phenomenon of sensorimotor synchronization has been simplified and studied extensively using finger tapping sequences that are executed in synchrony with auditory stimuli. The predictive saccade paradigm closely resembles the finger tapping task. In this paradigm, participants follow a visual target that “steps” between two fixed locations on a visual screen at predictable ISIs. Eventually, the time from target appearance to saccade initiation (i.e., saccadic RT) becomes predictive with values nearing 0 msec. Unlike the finger tapping literature, neural control of predictive behavior described within the eye movement literature has not been well established and is inconsistent, especially between neuroimaging and patient lesion studies. To resolve these discrepancies, we used fMRI to investigate the neural correlates of predictive saccades by contrasting brain areas involved with behavior generated from the predictive saccade task with behavior generated from a reactive saccade task (saccades are generated toward targets that are unpredictably timed). We observed striking differences in neural recruitment between reactive and predictive conditions: Reactive saccades recruited oculomotor structures, as predicted, whereas predictive saccades recruited brain structures that support timing in motor responses, such as the Crus I of the Cerebellum, and structures commonly associated with the default mode network. Therefore, our results were more consistent with those found in the finger tapping literature.},
	urldate = {2016-04-10},
	journal = {Journal of Cognitive Neuroscience},
	author = {Lee, Stephen M. and Peltsch, Alicia and Kilmade, Maureen and Brien, Donald C. and Coe, Brian C. and Johnsrude, Ingrid S. and Munoz, Douglas P.},
	month = apr,
	year = {2016},
	pages = {1--18},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/386TA4BN/jocn_a_00968.html:text/html}
}

@article{jocham-reward-guided-2016-1,
	title = {Reward-{Guided} {Learning} with and without {Causal} {Attribution}},
	volume = {90},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001112/abstract},
	doi = {10.1016/j.neuron.2016.02.018},
	language = {English},
	number = {1},
	urldate = {2016-04-10},
	journal = {Neuron},
	author = {Jocham, Gerhard and Brodersen, Kay H. and Constantinescu, Alexandra O. and Kahn, Martin C. and Ianni, Angela M. and Walton, Mark E. and Rushworth, Matthew F. S. and Behrens, Timothy E. J.},
	month = apr,
	year = {2016},
	pages = {177--190},
	file = {Jocham et al_2016_Reward-Guided Learning with and without Causal Attribution.pdf:/home/user/Zotero/storage/4EZRNBVP/Jocham et al_2016_Reward-Guided Learning with and without Causal Attribution.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZGP5Z3NB/S0896-6273(16)00111-2.html:text/html}
}

@article{rajan-recurrent-2016-1,
	title = {Recurrent {Network} {Models} of {Sequence} {Generation} and {Memory}},
	volume = {90},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316001021/abstract},
	doi = {10.1016/j.neuron.2016.02.009},
	abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memory-guided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
	language = {English},
	number = {1},
	urldate = {2016-04-10},
	journal = {Neuron},
	author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
	month = apr,
	year = {2016},
	pages = {128--142},
	file = {Rajan et al_2016_Recurrent Network Models of Sequence Generation and Memory.pdf:/home/user/Zotero/storage/EFZS4TJX/Rajan et al_2016_Recurrent Network Models of Sequence Generation and Memory.pdf:application/pdf}
}

@inproceedings{lassiter-weakness-2014,
	title = {The weakness of must: {In} defense of a mantra},
	volume = {24},
	shorttitle = {The weakness of must},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/viewFile/24.597/2727},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Lassiter, Daniel},
	year = {2014},
	pages = {597--618},
	file = {2727.pdf:/home/user/Zotero/storage/SVXA6BT3/2727.pdf:application/pdf}
}

@inproceedings{lassiter-context-2013,
	title = {Context, scale structure, and statistics in the interpretation of positive-form adjectives},
	volume = {23},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/2658},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Lassiter, Daniel and Goodman, Noah D.},
	year = {2013},
	pages = {587--610},
	file = {2404.pdf:/home/user/Zotero/storage/XQXW8SGA/2404.pdf:application/pdf}
}

@inproceedings{lassiter-context-2013-1,
	title = {Context, scale structure, and statistics in the interpretation of positive-form adjectives},
	volume = {23},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/2658},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Lassiter, Daniel and Goodman, Noah D.},
	year = {2013},
	pages = {587--610},
	file = {2404.pdf:/home/user/Zotero/storage/GJRB4TFB/2404.pdf:application/pdf}
}

@inproceedings{lassiter-weakness-2014-1,
	title = {The weakness of must: {In} defense of a mantra},
	volume = {24},
	shorttitle = {The weakness of must},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/viewFile/24.597/2727},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Lassiter, Daniel},
	year = {2014},
	pages = {597--618},
	file = {2727.pdf:/home/user/Zotero/storage/IDVXI56R/2727.pdf:application/pdf}
}

@article{franke-what-nodate,
	title = {What does the crowd believe? {A} hierarchical approach to estimating subjective beliefs from empirical data},
	shorttitle = {What does the crowd believe?},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/FrankeDablander\_2016\_What\_does\_the\_crowd\_believe.pdf},
	urldate = {2016-03-24},
	author = {Franke, Michael and Dablander, Fabian and Schöller, Anthea and Bennett, Erin and Degen, Judith and Tessler, Michael Henry and Kao, Justine and Goodman, Noah D.},
	file = {FrankeDablander_2016_What_does_the_crowd_believe.pdf:/home/user/Zotero/storage/E8JB4BF5/FrankeDablander_2016_What_does_the_crowd_believe.pdf:application/pdf}
}

@article{herbstritt-definitely-nodate,
	title = {Definitely maybe and possibly even probably: efficient communication of higher-order uncertainty},
	shorttitle = {Definitely maybe and possibly even probably},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/HerbstrittFranke\_2016\_Definitely\_maybe\_and\_possibly\_even\_probably.pdf},
	urldate = {2016-03-24},
	author = {Herbstritt, Michele and Franke, Michael},
	file = {HerbstrittFranke_2016_Definitely_maybe_and_possibly_even_probably.pdf:/home/user/Zotero/storage/I3KCV4S8/HerbstrittFranke_2016_Definitely_maybe_and_possibly_even_probably.pdf:application/pdf}
}

@article{brochhagen-learning-nodate,
	title = {Learning biases may prevent lexicalization of pragmatic inferences: a case study combining iterated ({Bayesian}) learning and functional selection},
	shorttitle = {Learning biases may prevent lexicalization of pragmatic inferences},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/BrochhagenFranke\_2016\_Learning\_biases\_may\_prevent\_lexicalization\_of\_pragmatic\_inferences.pdf},
	urldate = {2016-03-24},
	author = {Brochhagen, Thomas and Franke, Michael and van Rooij, Robert},
	file = {BrochhagenFranke_2016_Learning_biases_may_prevent_lexicalization_of_pragmatic_inferences.pdf:/home/user/Zotero/storage/QJ3WB2B5/BrochhagenFranke_2016_Learning_biases_may_prevent_lexicalization_of_pragmatic_inferences.pdf:application/pdf}
}

@article{galeazzi-smart-2015-1,
	title = {Smart {Transformations}: {The} {Evolution} of {Choice} {Principles}},
	shorttitle = {Smart {Transformations}},
	url = {http://arxiv.org/abs/1505.07054},
	urldate = {2016-03-24},
	journal = {arXiv preprint arXiv:1505.07054},
	author = {Galeazzi, Paolo and Franke, Michael},
	year = {2015},
	file = {() - 1505.07054v1.pdf:/home/user/Zotero/storage/3N6U5XE8/1505.07054v1.pdf:application/pdf}
}

@article{franke-probabilistic-2015,
	title = {Probabilistic pragmatics, or why {Bayes}’ rule is probably important for pragmatics},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/FrankeJager\_2015\_Probabilistic%20pragmatics,%20or%20why%20Bayes%27%20rule%20is%20probably%20important%20for%20pragmatics.pdf},
	urldate = {2016-03-24},
	journal = {Ms., Universität Tübingen},
	author = {Franke, Michael and Jäger, Gerhard},
	year = {2015},
	file = {FrankeJager_2015_Probabilistic pragmatics, or why Bayes' rule is probably important for pragmatics.pdf:/home/user/Zotero/storage/99M9XA7F/FrankeJager_2015_Probabilistic pragmatics, or why Bayes' rule is probably important for pragmati.pdf:application/pdf}
}

@article{franke-reasoning-2015,
	title = {Reasoning in reference games},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/FrankeDegen\_2015\_Reasoning%20in%20Reference%20Games%20Individual-%20vs.~Population-Level%20Probabilistic%20Modelinga.pdf},
	urldate = {2016-03-24},
	journal = {manuscript, Tübingen \& Stanford},
	author = {Franke, Michael and Degen, Judith},
	year = {2015}
}

@article{franke-task-2016,
	title = {Task types, link functions \& probabilistic modeling in experimental pragmatics},
	url = {http://www.xprag.de/wp-content/uploads/2015/08/TiXPrag-preproc.pdf#page=65},
	urldate = {2016-03-24},
	journal = {Pre-proceedings of Trends in Experimental Pragmatics},
	author = {Franke, Michael},
	year = {2016},
	pages = {60},
	file = {Task types, link functions & probabilistic modeling in experimental pragmatics - Franke_2016_Task types, link functions & probabilistic modeling in experimental.pdf:/home/user/Zotero/storage/QPT7INKS/Franke_2016_Task types, link functions & probabilistic modeling in experimental.pdf:application/pdf}
}

@inproceedings{scholler-semantic-2015,
	title = {Semantic values as latent parameters: {Surprising} few \& many},
	volume = {25},
	shorttitle = {Semantic values as latent parameters},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/25.143},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Schöller, Anthea and Franke, Michael},
	year = {2015},
	pages = {143--162},
	file = {Semantic values as latent parameters\: surprising few & many - SchoellerFranke2015_salt25.pdf:/home/user/Zotero/storage/E34597BX/SchoellerFranke2015_salt25.pdf:application/pdf}
}

@article{franke-evolution-nodate-1,
	title = {The evolution of compositionality and proto-syntax in signaling games},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/Franke\_2014\_The%20Evolution%20of%20Compositionality%20and%20Proto-Syntax%20in%20Signaling%20Games.pdf},
	urldate = {2016-03-24},
	author = {Franke, Michael},
	file = {Franke_2014_The Evolution of Compositionality and Proto-Syntax in Signaling Games.pdf:/home/user/Zotero/storage/6ATVMKTH/Franke_2014_The Evolution of Compositionality and Proto-Syntax in Signaling Games.pdf:application/pdf}
}

@incollection{qing-variations-2015,
	title = {Variations on a {Bayesian} theme: {Comparing} {Bayesian} models of referential reasoning},
	shorttitle = {Variations on a {Bayesian} theme},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-17064-0\_9},
	urldate = {2016-03-24},
	booktitle = {Bayesian {Natural} {Language} {Semantics} and {Pragmatics}},
	publisher = {Springer},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2015},
	pages = {201--220},
	file = {QingFranke_2013_Variations_on_Bayes.pdf:/home/user/Zotero/storage/XWPEXJ88/QingFranke_2013_Variations_on_Bayes.pdf:application/pdf}
}

@incollection{franke-strategies-2015,
	title = {Strategies of persuasion, manipulation and propaganda: psychological and social aspects},
	shorttitle = {Strategies of persuasion, manipulation and propaganda},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-48540-8\_8},
	urldate = {2016-03-24},
	booktitle = {Models of {Strategic} {Reasoning}},
	publisher = {Springer},
	author = {Franke, Michael and Van Rooij, Robert},
	year = {2015},
	pages = {255--291},
	file = {Franke-vanRooij_Opinion-Dynamics.pdf:/home/user/Zotero/storage/376QUNQU/Franke-vanRooij_Opinion-Dynamics.pdf:application/pdf}
}

@inproceedings{qing-gradable-2014-1,
	title = {Gradable adjectives, vagueness, and optimal language use: {A} speaker-oriented model},
	volume = {24},
	shorttitle = {Gradable adjectives, vagueness, and optimal language use},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/SALT/article/view/24.23},
	urldate = {2016-03-24},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2014},
	pages = {23--41},
	file = {Gradable adjectives, vagueness, and optimal language use\: A speaker-oriented model - QingFranke_2014_Gradable_AdjectivesSALT:/home/user/Zotero/storage/CNW9PR9Z/QingFranke_2014_Gradable_AdjectivesSALT.pdf:application/pdf}
}

@incollection{franke-pragmatic-2014,
	title = {Pragmatic back-and-forth reasoning},
	url = {http://link.springer.com/chapter/10.1057/9781137333285\_7},
	urldate = {2016-03-24},
	booktitle = {Pragmatics, {Semantics} and the {Case} of {Scalar} {Implicatures}},
	publisher = {Springer},
	author = {Franke, Michael and Jäger, Gerhard},
	year = {2014},
	pages = {170--200},
	file = {FrankeJager_2013_Pragmatic_Back-and-Forth Reasoning.pdf:/home/user/Zotero/storage/GB4U9VMJ/FrankeJager_2013_Pragmatic_Back-and-Forth Reasoning.pdf:application/pdf}
}

@inproceedings{franke-typical-2014-1,
	title = {Typical use of quantifiers: {A} probabilistic speaker model},
	shorttitle = {Typical use of quantifiers},
	url = {https://www.researchgate.net/profile/Michael\_Franke2/publication/267034467\_Typical\_use\_of\_quantifiers\_A\_probabilistic\_model/links/54425e8f0cf2e6f0c0f92f23.pdf},
	urldate = {2016-03-24},
	booktitle = {Proceedings of the 36th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {Franke, Michael},
	year = {2014},
	pages = {487--492},
	file = {Franke_2014_Typical use of quantifiers A probabilistic speaker model.pdf:/home/user/Zotero/storage/XJ8VF94A/Franke_2014_Typical use of quantifiers A probabilistic speaker model.pdf:application/pdf}
}

@inproceedings{qing-meaning-2014-1,
	title = {Meaning and use of gradable adjectives: {Formal} modeling meets empirical data},
	shorttitle = {Meaning and use of gradable adjectives},
	url = {https://mindmodeling.org/cogsci2014/papers/213/paper213.pdf},
	urldate = {2016-03-24},
	booktitle = {Proceedings of {CogSci}},
	author = {Qing, Ciyang and Franke, Michael},
	year = {2014},
	file = {QingFranke_2014_Meaning and Use of Gradable Adjectives Formal Modeling Meets Empirical.pdf:/home/user/Zotero/storage/I3ZH7GKZ/QingFranke_2014_Meaning and Use of Gradable Adjectives Formal Modeling Meets Empirical.pdf:application/pdf}
}

@inproceedings{franke-evolution-2014,
	title = {On the evolution of choice principles},
	volume = {1208},
	url = {http://ceur-ws.org/Vol-1208/paper3.pdf},
	urldate = {2016-03-24},
	booktitle = {Proceedings of the second workshop reasoning about other minds: {Logical} and cognitive perspectives, co-located with advances in modal logic, groningen, {CEUR} workshop proceedings},
	author = {Franke, Michael and Galeazzi, Paolo},
	year = {2014},
	pages = {11--15},
	file = {paper3.pdf:/home/user/Zotero/storage/CRSASWD5/paper3.pdf:application/pdf}
}

@article{franke-creative-2014,
	title = {Creative compositionality from reinforcement learning in signaling games},
	volume = {10},
	url = {https://books.google.com/books?hl=en&lr=&id=LRC3CgAAQBAJ&oi=fnd&pg=PA82&dq=%22term+language+to+a+compositional+language+(Jackendoff,+1999).+A%22+%22also+combine+signals+into+sequences+with+novel+meanings+(c.f.%22+%22possible,+it+is+unsatisfactory+to+simply+point+to+a+potential+evolutionary%22+&ots=XFV8oN5qq7&sig=Px8PZ5LxArJ\_XwX5v\_PaKSmYV-k},
	urldate = {2016-03-24},
	journal = {Erica A. Cartmill, Sean Roberts, Heidi Lyn and Hannah Cornish/The Evolution of Language: Proceedings of EvoLang},
	author = {Franke, Michael},
	year = {2014},
	pages = {82--89},
	file = {Franke_2014_Creative Compositionality from Reinforcement Learning in Signaling Games.pdf:/home/user/Zotero/storage/J9QJF9AM/Franke_2014_Creative Compositionality from Reinforcement Learning in Signaling Games.pdf:application/pdf}
}

@inproceedings{degen-cost-based-2013-2,
	title = {Cost-based pragmatic inference about referential expressions},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/DegenFranke\_2013\_Cost-Based-Inference.pdf},
	urldate = {2016-03-24},
	booktitle = {Proceedings of the 35th annual conference of the cognitive science society},
	author = {Degen, Judith and Franke, Michael and Jäger, Gerhard},
	year = {2013},
	pages = {376--381},
	file = {DegenFranke_2013_Cost-Based-Inference.pdf:/home/user/Zotero/storage/DGVGF5CK/DegenFranke_2013_Cost-Based-Inference.pdf:application/pdf}
}

@article{lee-how-2011,
	title = {How cognitive modeling can benefit from hierarchical {Bayesian} models},
	volume = {55},
	issn = {00222496},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022249610001148},
	doi = {10.1016/j.jmp.2010.08.013},
	language = {en},
	number = {1},
	urldate = {2016-03-24},
	journal = {Journal of Mathematical Psychology},
	author = {Lee, Michael D.},
	month = feb,
	year = {2011},
	pages = {1--7},
	file = {How cognitive modeling can benefit from hierarchical Bayesian models - Lee2011.pdf:/home/user/Zotero/storage/RA8ARXPI/Lee2011.pdf:application/pdf}
}

@article{lee-three-2008,
	title = {Three case studies in the {Bayesian} analysis of cognitive models},
	volume = {15},
	issn = {1069-9384, 1531-5320},
	url = {http://www.springerlink.com/index/10.3758/PBR.15.1.1},
	doi = {10.3758/PBR.15.1.1},
	language = {en},
	number = {1},
	urldate = {2016-03-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Lee, M. D.},
	month = feb,
	year = {2008},
	pages = {1--15},
	file = {Lee2008.pdf:/home/user/Zotero/storage/DMX45VRP/Lee2008.pdf:application/pdf}
}

@article{vandekerckhove-model-2014,
	title = {Model comparison and the principle of parsimony},
	url = {http://escholarship.org/uc/item/9j47k5q9.pdf},
	urldate = {2016-03-24},
	author = {Vandekerckhove, Joachim and Matzke, Dora and Wagenmakers, Eric-Jan},
	year = {2014},
	file = {chapter14-R1.dvi - VandekerckhoveEtAlinpress.pdf:/home/user/Zotero/storage/FFGIXEXZ/VandekerckhoveEtAlinpress.pdf:application/pdf}
}

@article{degen-processing-2015,
	title = {Processing {Scalar} {Implicature}: {A} {Constraint}-{Based} {Approach}},
	volume = {39},
	issn = {03640213},
	shorttitle = {Processing {Scalar} {Implicature}},
	url = {http://doi.wiley.com/10.1111/cogs.12171},
	doi = {10.1111/cogs.12171},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {Cognitive Science},
	author = {Degen, Judith and Tanenhaus, Michael K.},
	month = may,
	year = {2015},
	pages = {667--710},
	file = {untitled - DegenTanenhaus2014.pdf:/home/user/Zotero/storage/RV4UXQJF/DegenTanenhaus2014.pdf:application/pdf}
}

@article{bob-truth-nodate,
	title = {Truth and typicality in the interpretation of quantifiers},
	url = {https://www.researchgate.net/profile/Bob\_Van\_Tiel/publication/262373297\_Truth\_and\_typicality\_in\_the\_interpretation\_of\_quantifiers/links/00b7d538f22b2142ee000000.pdf},
	urldate = {2016-03-24},
	author = {Bob, V. A. N.},
	file = {vanTiel&Geurts.pdf:/home/user/Zotero/storage/AKD28WPS/vanTiel&Geurts.pdf:application/pdf}
}

@article{gronau-bayesian-nodate,
	title = {Bayesian {Mixture} {Modeling} of {Significant} {P} {Values}: {A} {Meta}-{Analytic} {Method} to {Estimate} the {Degree} of {Contamination} from {E}0},
	shorttitle = {Bayesian {Mixture} {Modeling} of {Significant} {P} {Values}},
	url = {http://www.ejwagenmakers.com/submitted/GronauMixtureModel.pdf},
	urldate = {2016-03-24},
	author = {Gronau, Quentin Frederik and Duizer, Monique and Bakker, Marjan and Wagenmakers, Eric-Jan},
	file = {GronauMixtureModel.pdf:/home/user/Zotero/storage/RZF2PK4W/GronauMixtureModel.pdf:application/pdf}
}

@article{gershman-tutorial-2012,
	title = {A tutorial on {Bayesian} nonparametric models},
	volume = {56},
	issn = {00222496},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S002224961100071X},
	doi = {10.1016/j.jmp.2011.08.004},
	language = {en},
	number = {1},
	urldate = {2016-03-24},
	journal = {Journal of Mathematical Psychology},
	author = {Gershman, Samuel J. and Blei, David M.},
	month = feb,
	year = {2012},
	pages = {1--12},
	file = {A tutorial on Bayesian nonparametric models - GershmanBlei2012.pdf:/home/user/Zotero/storage/RF8JUJ79/GershmanBlei2012.pdf:application/pdf}
}

@article{wagner-brian-2013-1,
	title = {{BRIAN} {SKYRMS} {Signals}: {Evolution}, {Learning}, and {Information}},
	volume = {64},
	issn = {0007-0882, 1464-3537},
	shorttitle = {{BRIAN} {SKYRMS} {Signals}},
	url = {http://bjps.oxfordjournals.org/content/64/4/883},
	doi = {10.1093/bjps/axt004},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {The British Journal for the Philosophy of Science},
	author = {Wagner, Elliott O. and Franke, Michael},
	month = dec,
	year = {2013},
	pages = {883--887},
	file = {Snapshot:/home/user/Zotero/storage/NNXAX4MX/883.html:text/html;Wagner_Franke_2013_BRIAN SKYRMS Signals.pdf:/home/user/Zotero/storage/MJ333VH8/Wagner_Franke_2013_BRIAN SKYRMS Signals.pdf:application/pdf}
}

@article{franke-game-2013,
	title = {Game {Theoretic} {Pragmatics}},
	volume = {8},
	copyright = {© 2013 The Author. Philosophy Compass © 2013 Blackwell Publishing Ltd},
	issn = {1747-9991},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/phc3.12015/abstract},
	doi = {10.1111/phc3.12015},
	abstract = {Game theoretic pragmatics is a small but growing part of formal pragmatics, the linguistic subfield studying language use. The general logic of a game theoretic explanation of a pragmatic phenomenon is this: (i) the conversational context is modelled as a game between speaker and hearer; (ii) an adequate solution concept then selects the to-be-explained behavior in the game model. For such an explanation to be convincing, both components, game model and solution concept, should be formulated and scrutinized as explicitly as possible. The article demonstrates this by a concise overview of both evolutionary and non-evolutionary approaches to game theoretic pragmatics, arguing for the use of agent-based micro-dynamics within evolutionary, and for the use of epistemic game theory within non-evolutionary approaches.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Philosophy Compass},
	author = {Franke, Michael},
	month = mar,
	year = {2013},
	pages = {269--284},
	file = {Franke_2013_Game Theoretic Pragmatics.pdf:/home/user/Zotero/storage/S8J6FPHJ/Franke_2013_Game Theoretic Pragmatics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/E9MU8JDN/abstract.html:text/html}
}

@article{franke-pragmatic-2013-2,
	title = {Pragmatic {Reasoning} {About} {Unawareness}},
	volume = {79},
	issn = {0165-0106, 1572-8420},
	url = {http://link.springer.com/article/10.1007/s10670-013-9464-1},
	doi = {10.1007/s10670-013-9464-1},
	abstract = {Language use and interpretation is heavily contingent on context. But human interlocutors need not always agree what the actual context is. In game theoretic approaches to language use and interpretation, interlocutors’ beliefs about the context are the players’ beliefs about the game that they are playing. Together this entails that we need to consider cases in which interlocutors have different subjective conceptualizations of the game they are in. This paper therefore extends iterated best response reasoning, as an established model for pragmatic reasoning, to games with unawareness. This extension not only leads to more plausible context models for many communicative situations, but also to improved predictions for otherwise problematic cases and an extension of the scope of pragmatic phenomena that can be captured by game theoretic analysis.},
	language = {en},
	number = {4},
	urldate = {2016-03-24},
	journal = {Erkenntnis},
	author = {Franke, Michael},
	month = mar,
	year = {2013},
	keywords = {Philosophy, Logic, Epistemology, Ethics, Ontology},
	pages = {729--767},
	file = {Franke_2013_Pragmatic Reasoning About Unawareness.pdf:/home/user/Zotero/storage/NBBDDD9E/Franke_2013_Pragmatic Reasoning About Unawareness.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/T58I27GC/10.html:text/html}
}

@article{franke-admissibility-2014-2,
	title = {On admissibility in game theoretic pragmatics},
	volume = {37},
	issn = {0165-0157, 1573-0549},
	url = {http://link.springer.com/article/10.1007/s10988-014-9148-6},
	doi = {10.1007/s10988-014-9148-6},
	abstract = {In a recent contribution in this journal, Sascia Pavan proposed a new game theoretic approach to explain generalized conversational implicatures in terms of general principles of rational behavior. His approach is based on refining Nash equilibrium by a procedure called iterated admissibility. I would like to strengthen Pavan’s case by sketching an epistemic interpretation of iterated admissibility, so as to further our understanding of why iterated admissibility might be a good approximation of pragmatic reasoning. But the explicit epistemic view taken here also points to some shortcomings of his approach in comparison to rivaling accounts.},
	language = {en},
	number = {3},
	urldate = {2016-03-24},
	journal = {Linguistics and Philosophy},
	author = {Franke, Michael},
	month = jul,
	year = {2014},
	keywords = {Semantics, Philosophy of Language},
	pages = {249--256},
	file = {Franke_2014_On admissibility in game theoretic pragmatics.pdf:/home/user/Zotero/storage/XA9SXICG/Franke_2014_On admissibility in game theoretic pragmatics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IJTNV4BS/10.html:text/html}
}

@article{franke-game-2014,
	title = {Game {Theory} and the {Evolution} of {Meaning}},
	volume = {8},
	copyright = {Language and Linguistics Compass © 2014 John Wiley \& Sons Ltd},
	issn = {1749-818X},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/lnc3.12086/abstract},
	doi = {10.1111/lnc3.12086},
	abstract = {Evolutionary game theory is a general, but mathematically precise framework for modeling the competition between and fitness-based selection of different types of behavior. We review recent applications of this framework to account for the evolution of behavior that lends meaning to ostensible acts and signs.},
	language = {en},
	number = {9},
	urldate = {2016-03-24},
	journal = {Language and Linguistics Compass},
	author = {Franke, Michael and Wagner, Elliott O.},
	month = sep,
	year = {2014},
	pages = {359--372},
	file = {Franke_Wagner_2014_Game Theory and the Evolution of Meaning.pdf:/home/user/Zotero/storage/983DSMPN/Franke_Wagner_2014_Game Theory and the Evolution of Meaning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CDSQRGJV/abstract.html:text/html}
}

@incollection{van-rooij-optimality-theoretic-2015,
	edition = {Winter 2015},
	title = {Optimality-{Theoretic} and {Game}-{Theoretic} {Approaches} to {Implicature}},
	url = {http://plato.stanford.edu/archives/win2015/entries/implicature-optimality-games/},
	abstract = {Linguistic pragmatics studies the context-dependent use and interpretation ofexpressions.  Perhaps the most important notion in pragmatics is Grice's(1967) conversational implicature. It is based on theinsight that by means of general principles of rational cooperative behaviorwe can communicate more with the use of a sentence than theconventional semantic meaning associated with it. Grice has argued,for instance, that the exclusive interpretation of‘or’—according to which we infer from ‘John or Marycame’ that John and Mary didn't come both—is not due to thesemantic meaning of ‘or’ but should be accounted for by a theoryof conversational implicature. In this particular example,—a typicalexample of a so-called Quantity implicature—the hearer's implication istaken to follow from the fact that the speaker could have used a contrasting,and informatively stronger expression, but chose not to. Other implicaturesmay follow from what the  hearer thinks that the speaker takes to be normalstates of affairs, i.e., stereotypical interpretations. For both types ofimplicatures, the hearer's (pragmatic) interpretation of an expressioninvolves what he takes to be the speaker's reason for using this expression.But obviously, this speaker's reason must involve assumptions about thehearer's reasoning as well., In this entry we will discuss formal accounts of conversational implicaturesthat explicitly take into account the interactive reasoning of speaker andhearer (e.g., what speaker and hearer believe about each other, the relevantaspects of the context of utterance etc.) and that aim to reductively explainconversational implicature as the result of goal-oriented, economicallyoptimized language use.},
	urldate = {2016-03-24},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	author = {van Rooij, Robert and Franke, Michael},
	editor = {Zalta, Edward N.},
	year = {2015},
	keywords = {game theory, implicature, defaults in semantics and pragmatics, Grice, Paul, logic: and games, pragmatics},
	file = {SEP - Snapshot:/home/user/Zotero/storage/EH2GEQGP/implicature-optimality-games.html:text/html}
}

@article{blei-variational-2016-1,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	urldate = {2016-03-22},
	journal = {arXiv preprint arXiv:1601.00670},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	year = {2016},
	file = {1601.00670v2.pdf:/home/user/Zotero/storage/4KBREW3Q/1601.00670v2.pdf:application/pdf}
}

@article{sedghi-training-2016,
	title = {Training {Input}-{Output} {Recurrent} {Neural} {Networks} through {Spectral} {Methods}},
	url = {http://arxiv.org/abs/1603.00954},
	urldate = {2016-03-22},
	journal = {arXiv preprint arXiv:1603.00954},
	author = {Sedghi, Hanie and Anandkumar, Anima},
	year = {2016},
	file = {() - 1603.00954v2.pdf:/home/user/Zotero/storage/EESDUTWE/1603.00954v2.pdf:application/pdf}
}

@article{brusini-erp-nodate,
	title = {{ERP} evidence for on-line syntactic computations in 2-year-olds},
	issn = {1878-9293},
	url = {http://www.sciencedirect.com/science/article/pii/S187892931530044X},
	doi = {10.1016/j.dcn.2016.02.009},
	abstract = {Syntax allows human beings to build an infinite number of sentences from a finite number of words. How this unique, productive power of human language unfolds over the course of language development is still hotly debated. When they listen to sentences comprising newly-learned words, do children generalize from their knowledge of the legal combinations of word categories or do they instead rely on strings of words stored in memory to detect syntactic errors? Using novel words taught in the lab, we recorded Evoked Response Potentials (ERPs) in two-year-olds and adults listening to grammatical and ungrammatical sentences containing syntactic contexts that had not been used during training. In toddlers, the ungrammatical use of words, even when they have been just learned, induced an early left anterior negativity (surfacing 100-400 ms after target word onset) followed by a late posterior positivity (surfacing 700-900 ms after target word onset) that was not observed in grammatical sentences. This late effect was remarkably similar to the P600 displayed by adults, suggesting that toddlers and adults perform similar syntactic computations. Our results thus show that toddlers build on-line expectations regarding the syntactic category of upcoming words in a sentence.},
	urldate = {2016-03-22},
	journal = {Developmental Cognitive Neuroscience},
	author = {Brusini, Perrine and Dehaene-Lambertz, Ghislaine and Dutat, Michel and Goffinet, François and Christophe, Anne},
	keywords = {language acquisition, Evoked Potentials, Syntactic Processing, Toddlers},
	file = {Brusini et al_ERP evidence for on-line syntactic computations in 2-year-olds.pdf:/home/user/Zotero/storage/9GTXUMXD/Brusini et al_ERP evidence for on-line syntactic computations in 2-year-olds.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/D3RG4EEH/S187892931530044X.html:text/html}
}

@article{roon-perceiving-nodate,
	title = {Perceiving while producing: {Modeling} the dynamics of phonological planning},
	issn = {0749-596X},
	shorttitle = {Perceiving while producing},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X16000073},
	doi = {10.1016/j.jml.2016.01.005},
	abstract = {We offer a dynamical model of phonological planning that provides a formal instantiation of how the speech production and perception systems interact during online processing. The model is developed on the basis of evidence from an experimental task that requires concurrent use of both systems, the so-called response–distractor task in which speakers hear distractor syllables while they are preparing to produce required responses. The model formalizes how ongoing response planning is affected by perception and accounts for a range of results reported across previous studies. It does so by explicitly addressing the setting of parameter values in representations. The key unit of the model is that of the dynamic field, a distribution of activation over the range of values associated with each representational parameter. The setting of parameter values takes place by the attainment of a stable distribution of activation over the entire field, stable in the sense that it persists even after the response cue in the above experiments has been removed. This and other properties of representations that have been taken as axiomatic in previous work are derived by the dynamics of the proposed model.},
	urldate = {2016-03-22},
	journal = {Journal of Memory and Language},
	author = {Roon, Kevin D. and Gafos, Adamantios I.},
	keywords = {Computational Modeling, speech perception, speech production, Phonological planning, Response time modulation},
	file = {Roon_Gafos_Perceiving while producing.pdf:/home/user/Zotero/storage/8BQRIKEB/Roon_Gafos_Perceiving while producing.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/T5XXI4XM/S0749596X16000073.html:text/html}
}

@article{atsma-causal-2016,
	title = {Causal {Inference} for {Spatial} {Constancy} across {Saccades}},
	volume = {12},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004766},
	doi = {10.1371/journal.pcbi.1004766},
	abstract = {Author Summary   During saccadic eye movements, the image on our retinas is, contrary to subjective experience, highly unstable. This study examines how the brain distinguishes the image perturbations caused by saccades and those due to changes in the visual scene. We first show that participants made severe errors in judging the presaccadic location of an object that shifts during a saccade. We then show that these observations can be modeled based on causal inference principles, evaluating whether presaccadic and postsaccadic object percepts derive from a single stable object or not. On a single trial level, this evaluation is not “either/or” but a probability that also determines the weight by which pre- and postsaccadic signals are separated and integrated in judging object locations across saccades.},
	number = {3},
	urldate = {2016-03-22},
	journal = {PLOS Comput Biol},
	author = {Atsma, Jeroen and Maij, Femke and Koppen, Mathieu and Irwin, David E. and Medendorp, W. Pieter},
	month = mar,
	year = {2016},
	keywords = {memory, Eye Movements, vision, Eyes, Probability distribution, Sensory perception, Retina, Anisotropy},
	pages = {e1004766},
	file = {Atsma et al_2016_Causal Inference for Spatial Constancy across Saccades.pdf:/home/user/Zotero/storage/V7SMTMEK/Atsma et al_2016_Causal Inference for Spatial Constancy across Saccades.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/H3G92C54/article.html:text/html}
}

@article{chung-character-level-2016,
	title = {A {Character}-level {Decoder} without {Explicit} {Segmentation} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1603.06147},
	abstract = {The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.},
	urldate = {2016-03-22},
	journal = {arXiv:1603.06147 [cs]},
	author = {Chung, Junyoung and Cho, Kyunghyun and Bengio, Yoshua},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.06147},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4VEPUZSU/1603.html:text/html;Chung et al_2016_A Character-level Decoder without Explicit Segmentation for Neural Machine.pdf:/home/user/Zotero/storage/BZ77P7KV/Chung et al_2016_A Character-level Decoder without Explicit Segmentation for Neural Machine.pdf:application/pdf}
}

@article{andor-globally-2016,
	title = {Globally {Normalized} {Transition}-{Based} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1603.06042},
	abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
	urldate = {2016-03-22},
	journal = {arXiv:1603.06042 [cs]},
	author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.06042},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {Andor et al_2016_Globally Normalized Transition-Based Neural Networks.pdf:/home/user/Zotero/storage/QUCXJBHG/Andor et al_2016_Globally Normalized Transition-Based Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/8ES5GRDM/1603.html:text/html}
}

@article{bowman-fast-2016,
	title = {A {Fast} {Unified} {Model} for {Parsing} and {Sentence} {Understanding}},
	url = {http://arxiv.org/abs/1603.06021},
	abstract = {Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.},
	urldate = {2016-03-22},
	journal = {arXiv:1603.06021 [cs]},
	author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.06021},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/EW5NEINC/1603.html:text/html;Bowman et al_2016_A Fast Unified Model for Parsing and Sentence Understanding.pdf:/home/user/Zotero/storage/AIEQCF74/Bowman et al_2016_A Fast Unified Model for Parsing and Sentence Understanding.pdf:application/pdf}
}

@article{abel-exploratory-2016,
	title = {Exploratory {Gradient} {Boosting} for {Reinforcement} {Learning} in {Complex} {Domains}},
	url = {http://arxiv.org/abs/1603.04119},
	urldate = {2016-03-22},
	journal = {arXiv preprint arXiv:1603.04119},
	author = {Abel, David and Agarwal, Alekh and Diaz, Fernando and Krishnamurthy, Akshay and Schapire, Robert E.},
	year = {2016},
	file = {1603.04119v1.pdf:/home/user/Zotero/storage/7NKNFRZG/1603.04119v1.pdf:application/pdf}
}

@article{yuan-influence-2016-1,
	title = {On the {Influence} of {Momentum} {Acceleration} on {Online} {Learning}},
	url = {http://arxiv.org/abs/1603.04136},
	urldate = {2016-03-22},
	journal = {arXiv preprint arXiv:1603.04136},
	author = {Yuan, Kun and Ying, Bicheng and Sayed, Ali H.},
	year = {2016},
	file = {() - 1603.04136v1.pdf:/home/user/Zotero/storage/5HGI4QER/1603.04136v1.pdf:application/pdf}
}

@article{cheng-syntax-aware-2015,
	title = {Syntax-{Aware} {Multi}-{Sense} {Word} {Embeddings} for {Deep} {Compositional} {Models} of {Meaning}},
	url = {http://arxiv.org/abs/1508.02354},
	urldate = {2016-03-22},
	journal = {arXiv preprint arXiv:1508.02354},
	author = {Cheng, Jianpeng and Kartsaklis, Dimitri},
	year = {2015},
	file = {Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning - D15-1177.pdf:/home/user/Zotero/storage/B5N6SBEH/D15-1177.pdf:application/pdf}
}

@inproceedings{das-paraphrase-2009,
	title = {Paraphrase identification as probabilistic quasi-synchronous recognition},
	url = {http://dl.acm.org/citation.cfm?id=1687944},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 1-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Das, Dipanjan and Smith, Noah A.},
	year = {2009},
	pages = {468--476},
	file = {Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition - P09-1053.pdf:/home/user/Zotero/storage/D5BRGXPK/P09-1053.pdf:application/pdf}
}

@inproceedings{he-multi-perspective-2015,
	title = {Multi-perspective sentence similarity modeling with convolutional neural networks},
	url = {http://www.anthology.aclweb.org/D/D15/D15-1181.pdf},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {He, Hua and Gimpel, Kevin and Lin, Jimmy},
	year = {2015},
	pages = {1576--1586},
	file = {Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks - D15-1181.pdf:/home/user/Zotero/storage/A86BQRWK/D15-1181.pdf:application/pdf}
}

@article{kamper-unsupervised-nodate,
	title = {Unsupervised {Word} {Segmentation} and {Lexicon} {Discovery} {Using} {Acoustic} {Word} {Embeddings}},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=7378940},
	urldate = {2016-03-22},
	author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
	file = {taslp16-unsupTIDigits.pdf:/home/user/Zotero/storage/WJBIGRX6/taslp16-unsupTIDigits.pdf:application/pdf}
}

@inproceedings{kamper-unsupervised-2015,
	title = {Unsupervised neural network based feature extraction using weak top-down constraints},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=7179087},
	urldate = {2016-03-22},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2015 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Kamper, Herman and Elsner, Micha and Jansen, Aren and Goldwater, Sharon},
	year = {2015},
	pages = {5818--5822},
	file = {icassp15-correspondenceAE.pdf:/home/user/Zotero/storage/NH255UW6/icassp15-correspondenceAE.pdf:application/pdf}
}

@inproceedings{kamper-fully-2015,
	title = {Fully unsupervised small-vocabulary speech recognition using a segmental {Bayesian} model},
	url = {http://www.kamperh.com/papers/kamper+jansen+goldwater\_interspeech2015.pdf},
	urldate = {2016-03-22},
	booktitle = {Proc. {Interspeech}},
	author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
	year = {2015},
	file = {interspeech15-unsupTIDigits.pdf:/home/user/Zotero/storage/Q7VCDN3I/interspeech15-unsupTIDigits.pdf:application/pdf}
}

@article{pate-talkers-2015,
	title = {Talkers account for listener and channel characteristics to communicate efficiently},
	volume = {78},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X14001259},
	urldate = {2016-03-22},
	journal = {Journal of Memory and Language},
	author = {Pate, John K. and Goldwater, Sharon},
	year = {2015},
	pages = {1--17},
	file = {jml15-predictability.pdf:/home/user/Zotero/storage/35ARZMPG/jml15-predictability.pdf:application/pdf}
}

@inproceedings{renshaw-comparison-2015,
	title = {A comparison of neural network methods for unsupervised representation learning on the {Zero} {Resource} {Speech} {Challenge}},
	url = {http://www.kamperh.com/papers/renshaw+kamper+jansen+goldwater\_interspeech2015.pdf},
	urldate = {2016-03-22},
	booktitle = {Proc. {Interspeech}},
	author = {Renshaw, Daniel and Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
	year = {2015},
	file = {A Comparison of Neural Network Methods for Unsupervised Representation Learning on the Zero Resource Speech Challenge - interspeech15-NNMethods.pdf:/home/user/Zotero/storage/TSZS977N/interspeech15-NNMethods.pdf:application/pdf}
}

@inproceedings{frank-weak-2014,
	title = {Weak semantic context helps phonetic learning in a model of infant language acquisition.},
	url = {http://www.research.ed.ac.uk/portal/files/14970263/acl14\_TLDmodel.pdf},
	urldate = {2016-03-22},
	booktitle = {{ACL} (1)},
	author = {Frank, Stella and Feldman, Naomi and Goldwater, Sharon},
	year = {2014},
	pages = {1073--1083},
	file = {acl14-TLDmodel.pdf:/home/user/Zotero/storage/FSUIFH9F/acl14-TLDmodel.pdf:application/pdf}
}

@inproceedings{kamper-unsupervised-2014,
	title = {Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=7078557},
	urldate = {2016-03-22},
	booktitle = {Spoken {Language} {Technology} {Workshop} ({SLT}), 2014 {IEEE}},
	publisher = {IEEE},
	author = {Kamper, Herman and Jansen, Aren and King, Simon and Goldwater, Sharon},
	year = {2014},
	pages = {100--105},
	file = {slt14-lexcluster.pdf:/home/user/Zotero/storage/WCZSFZDA/slt14-lexcluster.pdf:application/pdf}
}

@inproceedings{elsner-joint-2013,
	title = {A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
	url = {http://ling.umd.edu/~nhf/papers/SegmentationLexicalPhonetic.pdf},
	urldate = {2016-03-22},
	booktitle = {Proceedings of {EMNLP}},
	author = {Elsner, Micha and Goldwater, Sharon and Feldman, Naomi and Wood, Frank},
	year = {2013},
	pages = {42--54},
	file = {emnlp13-SegLexPhon.pdf:/home/user/Zotero/storage/55GGUEXJ/emnlp13-SegLexPhon.pdf:application/pdf}
}

@inproceedings{frank-exploring-2013,
	title = {Exploring the {Utility} of {Joint} {Morphological} and {Syntactic} {Learning} from {Child}-directed {Speech}.},
	url = {http://homepages.inf.ed.ac.uk/keller/papers/emnlp13b.pdf},
	urldate = {2016-03-22},
	booktitle = {{EMNLP}},
	author = {Frank, Stella and Keller, Frank and Goldwater, Sharon},
	year = {2013},
	pages = {30--41},
	file = {emnlp13-MorphSynCDS.pdf:/home/user/Zotero/storage/65M9ZNBN/emnlp13-MorphSynCDS.pdf:application/pdf}
}

@inproceedings{jones-modeling-2013,
	title = {Modeling graph languages with grammars extracted via tree decompositions},
	url = {http://www.usfx.bo/nueva/vicerrectorado/citas/TECNOLOGICAS\_20/Electronica/61.pdf#page=66},
	urldate = {2016-03-22},
	booktitle = {Proc. {FSMNLP}},
	author = {Jones, Bevan Keeley and Goldwater, Sharon and Johnson, Mark},
	year = {2013},
	pages = {54--62},
	file = {C\:/Users/Bevan/Documents/research/writings/papers/hrg-lang-mod/hrg-lang-mod.dvi - fsmnlp13-graphgrammars.pdf:/home/user/Zotero/storage/JTRCSEJZ/fsmnlp13-graphgrammars.pdf:application/pdf}
}

@article{jansen-summary-2013,
	title = {A summary of the 2012 {JHU} {CLSP} workshop on zero resource speech technologies and models of early language acquisition},
	url = {http://repository.cmu.edu/lti/94/},
	urldate = {2016-03-22},
	author = {Jansen, Aren and Dupoux, Emmanuel and Goldwater, Sharon and Johnson, Mark and Khudanpur, Sanjeev and Church, Kenneth and Feldman, Naomi and Hermansky, Hynek and Metze, Florian and Rose, Richard and {others}},
	year = {2013},
	file = {icassp2013a.dvi - icassp13-jhuSummary.pdf:/home/user/Zotero/storage/JRK4STHV/icassp13-jhuSummary.pdf:application/pdf}
}

@inproceedings{christodoulopoulos-turning-2012,
	title = {Turning the pipeline into a loop: {Iterated} unsupervised dependency parsing and {PoS} induction},
	shorttitle = {Turning the pipeline into a loop},
	url = {http://dl.acm.org/citation.cfm?id=2390441},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the {NAACL}-{HLT} {Workshop} on the {Induction} of {Linguistic} {Structure}},
	publisher = {Association for Computational Linguistics},
	author = {Christodoulopoulos, Christos and Goldwater, Sharon and Steedman, Mark},
	year = {2012},
	pages = {96--99},
	file = {Turning the pipeline into a loop\: Iterated unsupervised dependency parsing and PoS induction - naacl12wkshp_iterated.pdf:/home/user/Zotero/storage/CAB45FMJ/naacl12wkshp_iterated.pdf:application/pdf}
}

@inproceedings{elsner-bootstrapping-2012,
	title = {Bootstrapping a unified model of lexical and phonetic acquisition},
	url = {http://dl.acm.org/citation.cfm?id=2390551},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the 50th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Long} {Papers}-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Elsner, Micha and Goldwater, Sharon and Eisenstein, Jacob},
	year = {2012},
	pages = {184--193},
	file = {acl12-bootstrapping.pdf:/home/user/Zotero/storage/9M5KUHT9/acl12-bootstrapping.pdf:application/pdf}
}

@inproceedings{jones-semantic-2012,
	title = {Semantic parsing with bayesian tree transducers},
	url = {http://dl.acm.org/citation.cfm?id=2390593},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the 50th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Long} {Papers}-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Jones, Bevan Keeley and Johnson, Mark and Goldwater, Sharon},
	year = {2012},
	pages = {488--496},
	file = {C\:/Users/Bevan/Documents/research/writings/papers/tst-hybridtree/acl2012/wordOrderAndTreeTransducers.dvi - acl12-transducers.pdf:/home/user/Zotero/storage/EUS67GNM/acl12-transducers.pdf:application/pdf}
}

@inproceedings{kwiatkowski-probabilistic-2012,
	title = {A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings},
	url = {http://dl.acm.org/citation.cfm?id=2380848},
	urldate = {2016-03-22},
	booktitle = {Proceedings of the 13th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Goldwater, Sharon and Zettlemoyer, Luke and Steedman, Mark},
	year = {2012},
	pages = {234--244},
	file = {eacl12-ccgAcq.pdf:/home/user/Zotero/storage/ZQB44R47/eacl12-ccgAcq.pdf:application/pdf}
}

@article{barker-quantumstatistical-1979,
	title = {A quantum‐statistical {Monte} {Carlo} method; path integrals with boundary conditions},
	volume = {70},
	issn = {0021-9606, 1089-7690},
	url = {http://scitation.aip.org/content/aip/journal/jcp/70/6/10.1063/1.437829},
	doi = {10.1063/1.437829},
	abstract = {A new Monte Carlo method for problems in quantum‐statistical mechanics is described. The method is based on the use of iterated short‐time Green’s functions, for which ’’image’’ approximations are used. It is similar to the use of Feynman or Wiener path integrals but with a modification to take account of hard‐core boundary conditions. It is applied to two one‐dimensional test problems: that of a single particle in a hard‐walled box and that of two hard particles in a hard‐walled box. For these test problems, the results are in excellent agreement with exact quantum‐mechanical results both at high temperatures (near the classical limit) and at very low temperatures such that essentially only the ground state is occupied. Generalizations to three‐dimensional systems, to many‐body systems, and to more realistic potentials are discussed briefly.},
	number = {6},
	urldate = {2016-03-21},
	journal = {The Journal of Chemical Physics},
	author = {Barker, J. A.},
	month = mar,
	year = {1979},
	keywords = {Boundary value problems, Green's function methods, Ground states, Monte Carlo methods},
	pages = {2914--2918},
	file = {Barker_1979_A quantum‐statistical Monte Carlo method\; path integrals with boundary.pdf:/home/user/Zotero/storage/I7P3FMHR/Barker_1979_A quantum‐statistical Monte Carlo method\; path integrals with boundary.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/8HIVWW9P/1.html:text/html}
}

@article{glaesemann-path-2003,
	title = {A path integral approach to molecular thermochemistry},
	volume = {118},
	issn = {0021-9606, 1089-7690},
	url = {http://scitation.aip.org/content/aip/journal/jcp/118/4/10.1063/1.1529682},
	doi = {10.1063/1.1529682},
	abstract = {The calculation of thermochemical data requires accurate molecular energies. When such high accuracy is needed, often a method such as G1, G2, or G3 is used to calculate the energy. These methods rely upon the standard harmonic normal modeanalysis to calculate the vibrational and rotational contributions to the energy. We present a method for going beyond the harmonic analysis, which uses path integral Monte Carlo to calculate the vibrational and rotational contributions. Anharmonic effects are found to be as large as 2.5 kcal/mol for the molecules studied. Analytical methods for determining an optimal path discretization are presented. A novel potential energy caching scheme, which greatly improves computational efficiency, is also presented.},
	number = {4},
	urldate = {2016-03-21},
	journal = {The Journal of Chemical Physics},
	author = {Glaesemann, Kurt R. and Fried, Laurence E.},
	month = jan,
	year = {2003},
	keywords = {Monte Carlo methods, Chemical thermodynamics, Normal modes, Vibration rotation analysis},
	pages = {1596--1603},
	file = {Glaesemann_Fried_2003_A path integral approach to molecular thermochemistry.pdf:/home/user/Zotero/storage/NGS4824B/Glaesemann_Fried_2003_A path integral approach to molecular thermochemistry.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GAIEERX7/1.html:text/html}
}

@article{wang-path-1997,
	title = {Path integral grand canonical {Monte} {Carlo}},
	volume = {107},
	issn = {0021-9606, 1089-7690},
	url = {http://scitation.aip.org/content/aip/journal/jcp/107/13/10.1063/1.474874},
	doi = {10.1063/1.474874},
	abstract = {We derive the real-space path integral formulations of Widom’s test particle method and grand canonical Monte Carlo (GCMC). We apply these simulation methods to hydrogen and neon at temperatures ranging from 30 to 120 K. In addition, in order to explore configuration space both efficiently and ergodically, our implementation involves multiple time step hybrid Monte Carlo. Agreement between experiment and simulation for chemical potentials (Widom) and densities (GCMC) is very good over the entire temperature range, even when quantum effects are large.},
	number = {13},
	urldate = {2016-03-21},
	journal = {The Journal of Chemical Physics},
	author = {Wang, Qinyu and Johnson, J. Karl and Broughton, Jeremy Q.},
	month = oct,
	year = {1997},
	keywords = {Monte Carlo methods, Chemical potential, Quantum effects, Sum over histories quantization},
	pages = {5108--5117},
	file = {Snapshot:/home/user/Zotero/storage/XFJRXWDB/1.html:text/html;Wang et al_1997_Path integral grand canonical Monte Carlo.pdf:/home/user/Zotero/storage/WU3JWBVS/Wang et al_1997_Path integral grand canonical Monte Carlo.pdf:application/pdf}
}

@article{anderson-quantum-1976,
	title = {Quantum chemistry by random walk. {H} 2P, {H}+3D3h1A′1, {H}23{Σ}+u, {H}41{Σ}+g, {Be} 1S},
	volume = {65},
	issn = {0021-9606, 1089-7690},
	url = {http://scitation.aip.org/content/aip/journal/jcp/65/10/10.1063/1.432868},
	doi = {10.1063/1.432868},
	abstract = {The random‐walk method of solving the Schrödinger equation for molecular wavefunctions is extended to incorporate the effects of electron spin in several one‐ to four‐electron systems. Improved calculation procedures reduce computation requirements for high accuracy by a factor of about 10. Results are given for the systems H 2 P, H+ 3 D 3h 1 A′1, H2 3Σ+ u , linear equidistant H4 1Σ+ g , and Be 1 S.},
	number = {10},
	urldate = {2016-03-21},
	journal = {The Journal of Chemical Physics},
	author = {Anderson, James B.},
	month = nov,
	year = {1976},
	keywords = {Random walks, Wave equations, Wave functions},
	pages = {4121--4127},
	file = {Anderson_1976_Quantum chemistry by random walk.pdf:/home/user/Zotero/storage/IA2RX97K/Anderson_1976_Quantum chemistry by random walk.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XRPMTETP/1.html:text/html}
}

@article{grimm-monte-carlo-1971,
	title = {Monte-{Carlo} solution of {Schrödinger}'s equation},
	volume = {7},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/0021999171900544},
	doi = {10.1016/0021-9991(71)90054-4},
	abstract = {A new Monte-Carlo method is presented for the calculation of the ground-state wavefunction and energy value of the many-body Schrödinger equation. Several refinements to the iterative scheme, including the use of variational wavefunctions to improve the energy estimate and a variance reducing technique, are also discussed. The method allows for a straightforward treatment of repulsive potentials. It is applied to several problems including the three-nucleon problem with simple two-body forces.},
	number = {1},
	urldate = {2016-03-21},
	journal = {Journal of Computational Physics},
	author = {Grimm, R. C and Storer, R. G},
	month = feb,
	year = {1971},
	pages = {134--156},
	file = {Grimm_Storer_1971_Monte-Carlo solution of Schrödinger's equation.pdf:/home/user/Zotero/storage/P8DZEFRP/Grimm_Storer_1971_Monte-Carlo solution of Schrödinger's equation.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/J3EBZ2DI/0021999171900544.html:text/html}
}

@article{anderson-randomwalk-1975,
	title = {A random‐walk simulation of the {Schrödinger} equation: {H}+3},
	volume = {63},
	issn = {0021-9606, 1089-7690},
	shorttitle = {A random‐walk simulation of the {Schrödinger} equation},
	url = {http://scitation.aip.org/content/aip/journal/jcp/63/4/10.1063/1.431514},
	doi = {10.1063/1.431514},
	abstract = {A simple random‐walk method for obtaining a b i n i t i o solutions of the Schrödinger equation is examined in its application to the case of the molecular ion H+ 3 in the equilateral triangle configuration with side length R=1.66 bohr. The method, which is based on the similarity of the Schrödinger equation and the diffusion equation, involves the random movement of imaginary particles (psips) in electron configuration space subject to a variable chance of multiplication or disappearance. The computation requirements for high accuracy in determining energies of H+ 3 are greater than those of existing LCAO–MO–SCF–CI methods. For more complex molecular systems the method may be competitive.},
	number = {4},
	urldate = {2016-03-21},
	journal = {The Journal of Chemical Physics},
	author = {Anderson, James B.},
	month = aug,
	year = {1975},
	keywords = {Diffusion},
	pages = {1499--1503},
	file = {Anderson_1975_A random‐walk simulation of the Schrödinger equation.pdf:/home/user/Zotero/storage/DGEB59ES/Anderson_1975_A random‐walk simulation of the Schrödinger equation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/WRTA572V/1.html:text/html}
}

@article{frermann-bayesian-2016,
	title = {A {Bayesian} {Model} of {Diachronic} {Meaning} {Change}},
	volume = {4},
	copyright = {Copyright (c) 2016 Association for Computational Linguistics},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/796},
	abstract = {Word meanings change over time and an automated procedure for extracting 
this information from text would be useful for historical exploratory 
studies, information retrieval or question answering. We present a 
dynamic Bayesian model of diachronic meaning change, which infers 
temporal word representations as a set of senses and their prevalence. 
Unlike previous work, we explicitly model language change as a smooth, 
gradual process. We experimentally show that this modeling decision is 
beneficial: our model performs competitively on meaning change detection 
tasks whilst inducing discernible word senses and their development over 
time. Application of our model to the SemEval-2015 temporal 
classification benchmark datasets further reveals that it performs on 
par with highly optimized task-specific systems.},
	language = {en},
	number = {0},
	urldate = {2016-03-18},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Frermann, Lea and Lapata, Mirella},
	month = feb,
	year = {2016},
	pages = {31--45},
	file = {Frermann_Lapata_2016_A Bayesian Model of Diachronic Meaning Change.pdf:/home/user/Zotero/storage/I9BA5XB4/Frermann_Lapata_2016_A Bayesian Model of Diachronic Meaning Change.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZFDQQW3X/796.html:text/html}
}

@article{rezakhanlou-lectures-2012,
	title = {Lectures on {Random} {Matrices}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.940&rep=rep1&type=pdf},
	urldate = {2016-03-18},
	journal = {Notes for a UC Berkeley topics course},
	author = {Rezakhanlou, Fraydoun},
	year = {2012},
	file = {randommatrix.pdf:/home/user/Zotero/storage/CPQRZ8XI/randommatrix.pdf:application/pdf}
}

@inproceedings{huang-neurons-2014,
	title = {Neurons as {Monte} {Carlo} {Samplers}: {Bayesian}￼ {Inference} and {Learning} in {Spiking} {Networks}},
	shorttitle = {Neurons as {Monte} {Carlo} {Samplers}},
	url = {http://papers.nips.cc/paper/5273-neurons-as-monte-carlo-samplers-bayesian-inference-and-learning-in-spiking-networks},
	urldate = {2016-03-18},
	booktitle = {Advances in neural information processing systems},
	author = {Huang, Yanping and Rao, Rajesh P.},
	year = {2014},
	pages = {1943--1951},
	file = {Neurons as Monte Carlo Samplers\: Bayesian ￼Inference and Learning in Spiking Networks - 5273-neurons-as-monte-carlo-samplers-bayesian-inference-and-learning-in-spiking-networks.pdf:/home/user/Zotero/storage/J2PMDCHS/5273-neurons-as-monte-carlo-samplers-bayesian-inference-and-learning-in-spiking-networks.pdf:application/pdf}
}

@article{bahdanau-end--end-2015-1,
	title = {End-to-end attention-based large vocabulary speech recognition},
	url = {http://arxiv.org/abs/1508.04395},
	urldate = {2016-03-18},
	journal = {arXiv preprint arXiv:1508.04395},
	author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
	year = {2015},
	file = {1508.04395v2.pdf:/home/user/Zotero/storage/RRKS8QDZ/1508.04395v2.pdf:application/pdf}
}

@article{agarwal-second-2016,
	title = {Second {Order} {Stochastic} {Optimization} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1602.03943},
	urldate = {2016-03-18},
	journal = {arXiv preprint arXiv:1602.03943},
	author = {Agarwal, Naman and Bullins, Brian and Hazan, Elad},
	year = {2016},
	file = {1602.03943v3.pdf:/home/user/Zotero/storage/6U73XSVE/1602.03943v3.pdf:application/pdf}
}

@phdthesis{mazzaro-experimental-2011,
	title = {Experimental approaches to sound variation: {A} sociophonetic study of labial and velar fricatives and approximants in {Argentine} {Spanish}},
	shorttitle = {Experimental approaches to sound variation},
	url = {https://tspace.library.utoronto.ca/handle/1807/31859},
	urldate = {2016-03-18},
	school = {University of Toronto},
	author = {Mazzaro, Natalia},
	year = {2011},
	file = {Mazzaro_Natalia_201111_PhD_thesis - Mazzaro_Natalia_201111_PhD_thesis.pdf:/home/user/Zotero/storage/CK3C8CVV/Mazzaro_Natalia_201111_PhD_thesis.pdf:application/pdf}
}

@article{thomas-sociophonetic-2002,
	title = {Sociophonetic applications of speech perception experiments},
	volume = {77},
	url = {http://muse.jhu.edu/journals/american\_speech/v077/77.2thomas.html},
	number = {2},
	urldate = {2016-03-18},
	journal = {American Speech},
	author = {Thomas, Erik R.},
	year = {2002},
	pages = {115--147},
	file = {ASp77.2.1Thomas.art - Thomas.pdf:/home/user/Zotero/storage/8GSTVQ2H/Thomas.pdf:application/pdf}
}

@article{hay-sociophonetics-2007,
	title = {Sociophonetics},
	volume = {36},
	issn = {0084-6570, 1545-4290},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev.anthro.34.081804.120633},
	doi = {10.1146/annurev.anthro.34.081804.120633},
	language = {en},
	number = {1},
	urldate = {2016-03-18},
	journal = {Annual Review of Anthropology},
	author = {Hay, Jennifer and Drager, Katie},
	month = sep,
	year = {2007},
	pages = {89--103},
	file = {Sociophonetics - annurev.anthro.34.081804.120633.pdf:/home/user/Zotero/storage/24VICFA9/annurev.anthro.34.081804.120633.pdf:application/pdf}
}

@article{foulkes-social-2006,
	title = {The social life of phonetics and phonology},
	volume = {34},
	issn = {00954470},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0095447005000525},
	doi = {10.1016/j.wocn.2005.08.002},
	language = {en},
	number = {4},
	urldate = {2016-03-18},
	journal = {Journal of Phonetics},
	author = {Foulkes, Paul and Docherty, Gerard},
	month = oct,
	year = {2006},
	pages = {409--438},
	file = {doi\:10.1016/j.wocn.2005.08.002 - Foulkes.Docherty.socl.life.phontc.phonolJPhon2006.pdf:/home/user/Zotero/storage/MNJBS3R2/Foulkes.Docherty.socl.life.phontc.phonolJPhon2006.pdf:application/pdf}
}

@inproceedings{desjardins-natural-2015,
	title = {Natural neural networks},
	url = {http://papers.nips.cc/paper/5953-natural-neural-networks},
	urldate = {2016-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and {others}},
	year = {2015},
	pages = {2062--2070},
	file = {1507.00210v1.pdf:/home/user/Zotero/storage/7ESGTZUM/1507.00210v1.pdf:application/pdf}
}

@inproceedings{heess-learning-2015,
	title = {Learning continuous control policies by stochastic value gradients},
	url = {http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients},
	urldate = {2016-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Tim and Erez, Tom and Tassa, Yuval},
	year = {2015},
	pages = {2926--2934},
	file = {1510.09142v1.pdf:/home/user/Zotero/storage/RT36MEBX/1510.09142v1.pdf:application/pdf}
}

@article{schwarz-presupposition-2016,
	title = {Presupposition {Projection} in {Online} {Processing}},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/lookup/doi/10.1093/jos/ffw005},
	doi = {10.1093/jos/ffw005},
	language = {en},
	urldate = {2016-03-18},
	journal = {Journal of Semantics},
	author = {Schwarz, Florian and Tiemann, Sonja},
	month = mar,
	year = {2016},
	pages = {ffw005},
	file = {Presupposition Projection in Online Processing - jos.ffw005.full.pdf:/home/user/Zotero/storage/N7AKHRGC/jos.ffw005.full.pdf:application/pdf}
}

@article{lake-human-level-2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	url = {http://science.sciencemag.org/content/350/6266/1332.short},
	number = {6266},
	urldate = {2016-03-18},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	year = {2015},
	pages = {1332--1338},
	file = {1332 1332..1338 - Science-2015-Lake-1332-8.pdf:/home/user/Zotero/storage/JHRABF5U/Science-2015-Lake-1332-8.pdf:application/pdf}
}

@article{roberts-stochastic-2016,
	title = {A stochastic neuronal model predicts random search behaviors at multiple spatial scales in {C}. elegans},
	volume = {5},
	copyright = {© 2016, Roberts et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/5/e12572v2},
	doi = {10.7554/eLife.12572},
	abstract = {An animal’s ability to rapidly and efficiently locate new sources of food in its environment can mean the difference between life and death. As a result, animals have evolved foraging strategies that are adapted to the distribution and detectability of food sources. Organisms ranging from bacteria to humans use one such strategy, called random search, to locate food that cannot be detected at a distance and that is randomly distributed in their surroundings. The biological mechanisms that underpin random search are relatively well understood in single-cell organisms such as bacteria, but this information tells us little about the mechanisms that are used by animals, which use their nervous system to control their foraging behavior. Roberts et al. have now investigated the biological basis for random search behavior in a tiny roundworm called Caenorhabditis elegans. This worm forages for pockets of bacteria in decaying plant matter and has a simple and well-understood nervous system. Roberts et al. used information on how the cells in this worm’s nervous system connect together into so-called “neural circuits” to generate a mathematical model of random searching. The model revealed that the worm’s neural circuitry for random searching can be understood in terms of two groups of neuron-like components that switch randomly between “ON” and “OFF” states. While one group promotes forward movement, the other promotes backward movement, which is associated with a change in search direction. These two groups inhibit each other so that only one group usually is active at a given time. By adjusting this model to reproduce the behavioral records of real worms searching for food, Roberts et al. could predict the key neuronal connections involved. These predictions were then confirmed by taking electrical recordings from neurons. The model could also account for the unexpected behavioral effects that are seen when a neuron in one of these groups was destroyed or altered by a genetic mutation. These findings thus reveal a biological mechanism for random search behavior in worms that might operate in other animals as well. The findings might also provide future insight into the neural circuits involved in sleep and wakefulness in mammals, which is organized in a similar way.},
	language = {en},
	urldate = {2016-03-18},
	journal = {eLife},
	author = {Roberts, William M. and Augustine, Steven B. and Lawton, Kristy J. and Lindsay, Theodore H. and Thiele, Tod R. and Izquierdo, Eduardo J. and Faumont, Serge and Lindsay, Rebecca A. and Britton, Matthew Cale and Pokala, Navin and Bargmann, Cornelia I. and Lockery, Shawn R.},
	month = jan,
	year = {2016},
	keywords = {\textit{C. elegans}, hidden markov model, locomotion, spatial orientation, stochastic neural network},
	pages = {e12572},
	file = {Roberts et al_2016_A stochastic neuronal model predicts random search behaviors at multiple.pdf:/home/user/Zotero/storage/637JRBJ7/Roberts et al_2016_A stochastic neuronal model predicts random search behaviors at multiple.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7HW8DGX6/e12572v2.html:text/html}
}

@article{blythe-word-2016,
	title = {Word learning under infinite uncertainty},
	volume = {151},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716300452},
	doi = {10.1016/j.cognition.2016.02.017},
	abstract = {Language learners must learn the meanings of many thousands of words, despite those words occurring in complex environments in which infinitely many meanings might be inferred by the learner as a word’s true meaning. This problem of infinite referential uncertainty is often attributed to Willard Van Orman Quine. We provide a mathematical formalisation of an ideal cross-situational learner attempting to learn under infinite referential uncertainty, and identify conditions under which word learning is possible. As Quine’s intuitions suggest, learning under infinite uncertainty is in fact possible, provided that learners have some means of ranking candidate word meanings in terms of their plausibility; furthermore, our analysis shows that this ranking could in fact be exceedingly weak, implying that constraints which allow learners to infer the plausibility of candidate word meanings could themselves be weak. This approach lifts the burden of explanation from ‘smart’ word learning constraints in learners, and suggests a programme of research into weak, unreliable, probabilistic constraints on the inference of word meaning in real word learners.},
	urldate = {2016-04-29},
	journal = {Cognition},
	author = {Blythe, Richard A. and Smith, Andrew D. M. and Smith, Kenny},
	month = jun,
	year = {2016},
	keywords = {word learning, Cross-situational learning, Quine’s Problem},
	pages = {18--27},
	file = {Blythe et al_2016_Word learning under infinite uncertainty.pdf:/home/user/Zotero/storage/XQQGM8DA/Blythe et al_2016_Word learning under infinite uncertainty.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/G9ADV59P/S0010027716300452.html:text/html}
}

@article{calandra-manifold-2014-1,
	title = {Manifold {Gaussian} {Processes} for {Regression}},
	url = {http://arxiv.org/abs/1402.5876},
	abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
	urldate = {2016-04-29},
	journal = {arXiv:1402.5876 [cs, stat]},
	author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.5876},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BJFWGDID/1402.html:text/html;Calandra et al_2014_Manifold Gaussian Processes for Regression.pdf:/home/user/Zotero/storage/HGZSDSK5/Calandra et al_2014_Manifold Gaussian Processes for Regression.pdf:application/pdf}
}

@article{navawongse-drosophila-2016,
	title = {Drosophila learn efficient paths to a food source},
	volume = {131},
	issn = {1074-7427},
	url = {http://www.sciencedirect.com/science/article/pii/S1074742716300181},
	doi = {10.1016/j.nlm.2016.03.019},
	abstract = {Elucidating the genetic, and neuronal bases for learned behavior is a central problem in neuroscience. A leading system for neurogenetic discovery is the vinegar fly Drosophila melanogaster; fly memory research has identified genes and circuits that mediate aversive and appetitive learning. However, methods to study adaptive food-seeking behavior in this animal have lagged decades behind rodent feeding analysis, largely due to the challenges presented by their small scale. There is currently no method to dynamically control flies’ access to food. In rodents, protocols that use dynamic food delivery are a central element of experimental paradigms that date back to the influential work of Skinner. This method is still commonly used in the analysis of learning, memory, addiction, feeding, and many other subjects in experimental psychology. The difficulty of microscale food delivery means this is not a technique used in fly behavior. In the present manuscript we describe a microfluidic chip integrated with machine vision and automation to dynamically control defined liquid food presentations and sensory stimuli. Strikingly, repeated presentations of food at a fixed location produced improvements in path efficiency during food approach. This shows that improved path choice is a learned behavior. Active control of food availability using this microfluidic system is a valuable addition to the methods currently available for the analysis of learned feeding behavior in flies.},
	urldate = {2016-04-29},
	journal = {Neurobiology of Learning and Memory},
	author = {Navawongse, Rapeechai and Choudhury, Deepak and Raczkowska, Marlena and Stewart, James Charles and Lim, Terrence and Rahman, Mashiur and Toh, Alicia Guek Geok and Wang, Zhiping and Claridge-Chang, Adam},
	month = may,
	year = {2016},
	keywords = {Behavior, Drosophila, Drug screening, Feeding task, Flies, Learning assay, Skinner box},
	pages = {176--181},
	file = {Navawongse et al_2016_Drosophila learn efficient paths to a food source.pdf:/home/user/Zotero/storage/HW5I8IZ2/Navawongse et al_2016_Drosophila learn efficient paths to a food source.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/IKCPRQPJ/S1074742716300181.html:text/html}
}

@article{brand-evolution:-2016,
	title = {Evolution: {On} the {Origin} of {Symmetry}, {Synapsis}, and {Species}},
	volume = {26},
	issn = {0960-9822},
	shorttitle = {Evolution},
	url = {http://www.cell.com/article/S0960982216301907/abstract},
	doi = {10.1016/j.cub.2016.03.014},
	language = {English},
	number = {8},
	urldate = {2016-04-29},
	journal = {Current Biology},
	author = {Brand, Cara L. and Presgraves, Daven C.},
	month = apr,
	year = {2016},
	pages = {R325--R328},
	file = {Brand_Presgraves_2016_Evolution.pdf:/home/user/Zotero/storage/VJXCCZPS/Brand_Presgraves_2016_Evolution.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/P56UQ9KX/S0960-9822(16)30190-7.html:text/html}
}

@article{gross-planet-2016,
	title = {A planet with two billion cars},
	volume = {26},
	issn = {0960-9822},
	url = {http://www.cell.com/article/S0960982216303414/abstract},
	doi = {10.1016/j.cub.2016.04.019},
	language = {English},
	number = {8},
	urldate = {2016-04-29},
	journal = {Current Biology},
	author = {Gross, Michael},
	month = apr,
	year = {2016},
	pages = {R307--R310},
	file = {Gross_2016_A planet with two billion cars.pdf:/home/user/Zotero/storage/M5Z7NJGT/Gross_2016_A planet with two billion cars.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/W78I56R7/S0960-9822(16)30341-4.html:text/html}
}

@article{kim-learning-2015,
	title = {Learning to {Select} {Pre}-{Trained} {Deep} {Representations} with {Bayesian} {Evidence} {Framework}},
	url = {http://arxiv.org/abs/1506.02565},
	abstract = {We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.},
	urldate = {2016-04-29},
	journal = {arXiv:1506.02565 [cs, stat]},
	author = {Kim, Yong-Deok and Jang, Taewoong and Han, Bohyung and Choi, Seungjin},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02565},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/I59RB25U/1506.html:text/html;Kim et al_2015_Learning to Select Pre-Trained Deep Representations with Bayesian Evidence.pdf:/home/user/Zotero/storage/M837Q78I/Kim et al_2015_Learning to Select Pre-Trained Deep Representations with Bayesian Evidence.pdf:application/pdf}
}

@article{jacquemard-fo2<+1-2016-1,
	title = {{FO}2({\textless},+1,{\textasciitilde}) on data trees, data tree automata and branching vector addition systems},
	url = {http://arxiv.org/abs/1601.01579},
	abstract = {A data tree is an unranked ordered tree where each node carries a label from a finite alphabet and a datum from some infinite domain. We consider the two variable first order logic FO2({\textless},+1,{\textasciitilde}) over data trees. Here +1 refers to the child and the next sibling relations while {\textless} refers to the descendant and following sibling relations. Moreover, {\textasciitilde} is a binary predicate testing data equality. We exhibit an automata model, denoted DAD\# that is more expressive than FO2({\textless},+1,{\textasciitilde}) but such that emptiness of DAD\# and satisfiability of FO2({\textless},+1,{\textasciitilde}) are inter-reducible. This is proved via a model of counter tree automata, denoted EBVASS, that extends Branching Vector Addition Systems with States (BVASS) with extra features for merging counters. We show that, as decision problems, reachability for EBVASS, satisfiability of FO2({\textless},+1,{\textasciitilde}) and emptiness of DAD\# are equivalent.},
	urldate = {2016-04-29},
	journal = {arXiv:1601.01579 [cs]},
	author = {Jacquemard, Florent and Segoufin, Luc and Dimino, Jerémie},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01579},
	keywords = {Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/V49AW2A3/1601.html:text/html;Jacquemard et al_2016_FO2(,+1,~) on data trees, data tree automata and branching vector addition.pdf:/home/user/Zotero/storage/UW3768VJ/Jacquemard et al_2016_FO2(,+1,~) on data trees, data tree automata and branching vector addition.pdf:application/pdf}
}

@article{mcqueen-when-2016,
	title = {When brain regions talk to each other during speech processing, what are they talking about? {Commentary} on {Gow} and {Olson} (2015)},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {When brain regions talk to each other during speech processing, what are they talking about?},
	url = {http://dx.doi.org/10.1080/23273798.2016.1154975},
	doi = {10.1080/23273798.2016.1154975},
	abstract = {This commentary on Gow and Olson [2015. Sentential influences on acoustic-phonetic processing: A Granger causality analysis of multimodal imaging data. Language, Cognition and Neuroscience. Advance online publication. doi:10.1080/23273798.2015.1029498] questions in three ways their conclusion that speech perception is based on interactive processing. First, it is not clear that the data presented by Gow and Olson reflect normal speech recognition. Second, Gow and Olson's conclusion depends on still-debated assumptions about the functions performed by specific brain regions. Third, the results are compatible with feedforward models of speech perception and appear inconsistent with models in which there are online interactions about phonological content. We suggest that progress in the neuroscience of speech perception requires the generation of testable hypotheses about the function(s) performed by inter-regional connections.},
	number = {0},
	urldate = {2016-04-29},
	journal = {Language, Cognition and Neuroscience},
	author = {McQueen, James M. and Eisner, Frank and Norris, Dennis},
	month = apr,
	year = {2016},
	pages = {1--4},
	file = {McQueen et al_2016_When brain regions talk to each other during speech processing, what are they.pdf:/home/user/Zotero/storage/IJEHI26I/McQueen et al_2016_When brain regions talk to each other during speech processing, what are they.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/PIUBBPR9/23273798.2016.html:text/html}
}

@article{kwisthout-be-nodate,
	title = {To be precise, the details don’t matter: {On} predictive processing, precision, and level of detail of predictions},
	issn = {0278-2626},
	shorttitle = {To be precise, the details don’t matter},
	url = {http://www.sciencedirect.com/science/article/pii/S0278262616300161},
	doi = {10.1016/j.bandc.2016.02.008},
	abstract = {Many theoretical and empirical contributions to the Predictive Processing account emphasize the important role of precision modulation of prediction errors. Recently it has been proposed that the causal models used in human predictive processing are best formally modeled by categorical probability distributions. Crucially, such distributions assume a well-defined, discrete state space. In this paper we explore the consequences of this formalization. In particular we argue that the level of detail of generative models and predictions modulates prediction error. We show that both increasing the level of detail of the generative models and decreasing the level of detail of the predictions can be suitable mechanisms for lowering prediction errors. Both increase precision, yet come at the price of lowering the amount of information that can be gained by correct predictions. Our theoretical result establishes a key open empirical question to address: How does the brain optimize the trade-off between high precision and information gain when making its predictions?},
	urldate = {2016-04-29},
	journal = {Brain and Cognition},
	author = {Kwisthout, Johan and Bekkering, Harold and van Rooij, Iris},
	keywords = {Causal Bayesian networks, Formal modeling, Level of detail, Precision, Predictive processing, Structured representations},
	file = {Kwisthout et al_To be precise, the details don’t matter.pdf:/home/user/Zotero/storage/V74XA9S5/Kwisthout et al_To be precise, the details don’t matter.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/4V2MM5FA/S0278262616300161.html:text/html}
}

@article{fleischer-operations-2016-1,
	title = {Operations on {Weakly} {Recognizing} {Morphisms}},
	url = {http://arxiv.org/abs/1603.05376},
	abstract = {Weakly recognizing morphisms from free semigroups onto finite semigroups are a classical way for defining the class of omega-regular languages, i.e., a set of infinite words is weakly recognizable by such a morphism if and only if it is accepted by some B\"uchi automaton. We consider the descriptional complexity of various constructions for weakly recognizing morphisms. This includes the conversion from and to B\"uchi automata, the conversion into strongly recognizing morphisms, and complementation. For some problems, we are able to give more precise bounds in the case of binary alphabets or simple semigroups.},
	urldate = {2016-04-29},
	journal = {arXiv:1603.05376 [cs]},
	author = {Fleischer, Lukas and Kufleitner, Manfred},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.05376},
	keywords = {Computer Science - Formal Languages and Automata Theory, F.4.3, F.2.2},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AXPRRIPC/1603.html:text/html;Fleischer_Kufleitner_2016_Operations on Weakly Recognizing Morphisms.pdf:/home/user/Zotero/storage/ZAKPK6DK/Fleischer_Kufleitner_2016_Operations on Weakly Recognizing Morphisms.pdf:application/pdf}
}

@article{asgari-comparing-2016,
	title = {Comparing {Fifty} {Natural} {Languages} and {Twelve} {Genetic} {Languages} {Using} {Word} {Embedding} {Language} {Divergence} ({WELD}) as a {Quantitative} {Measure} of {Language} {Distance}},
	url = {http://arxiv.org/abs/1604.08561},
	abstract = {We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is defined as divergence between unified similarity distribution of words between languages. Using such a measure, we perform language comparison for fifty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward defining a quantitative measure of similarity between languages, with applications in languages classification, genre identification, dialect identification, and evaluation of translations.},
	urldate = {2016-04-29},
	journal = {arXiv:1604.08561 [cs]},
	author = {Asgari, Ehsaneddin and Mofrad, Mohammad R. K.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08561},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RR8AEIF8/1604.html:text/html;Asgari_Mofrad_2016_Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word.pdf:/home/user/Zotero/storage/RBHK2VVM/Asgari_Mofrad_2016_Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word.pdf:application/pdf}
}

@article{harsha-polynomial-2016,
	title = {On {Polynomial} {Approximations} to \${{AC}}{\textasciicircum}0\$},
	url = {http://arxiv.org/abs/1604.08121},
	abstract = {We make progress on some questions related to polynomial approximations of \${\rm AC}{\textasciicircum}0\$. It is known, by works of Tarui (Theoret. Comput. Sci. 1993) and Beigel, Reingold, and Spielman (Proc. \$6\$th CCC, 1991), that any \${\rm AC}{\textasciicircum}0\$ circuit of size \$s\$ and depth \$d\$ has an \$\varepsilon\$-error probabilistic polynomial over the reals of degree \$(\log (s/\varepsilon)){\textasciicircum}{O(d)}\$. We improve this upper bound to \$(\log s){\textasciicircum}{O(d)}\cdot \log(1/\varepsilon)\$, which is much better for small values of \$\varepsilon\$. We give an application of this result by using it to resolve a question posed by Tal (ECCC 2014): we show that \$(\log s){\textasciicircum}{O(d)}\cdot \log(1/\varepsilon)\$-wise independence fools \${\rm AC}{\textasciicircum}0\$, improving on Tal's strengthening of Braverman's theorem (J. ACM, 2010) that \$(\log (s/\varepsilon)){\textasciicircum}{O(d)}\$-wise independence fools \${\rm AC}{\textasciicircum}0\$. Up to the constant implicit in the \$O(d)\$, our result is tight. As far as we know, this is the first PRG construction for \${\rm AC}{\textasciicircum}0\$ that achieves optimal dependence on the error \$\varepsilon\$. We also prove lower bounds on the best polynomial approximations to \${\rm AC}{\textasciicircum}0\$. We show that any polynomial approximating the \${\rm OR}\$ function on \$n\$ bits to a small constant error must have degree at least \$\widetilde{\Omega}(\sqrt{\log n})\$. This result improves exponentially on a recent lower bound demonstrated by Meka, Nguyen, and Vu (arXiv 2015).},
	urldate = {2016-04-28},
	journal = {arXiv:1604.08121 [cs]},
	author = {Harsha, Prahladh and Srinivasan, Srikanth},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08121},
	keywords = {Computer Science - Computational Complexity},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Z4XKRPAX/1604.html:text/html;Harsha_Srinivasan_2016_On Polynomial Approximations to \$ AC ^0\$.pdf:/home/user/Zotero/storage/Z2GFXKNM/Harsha_Srinivasan_2016_On Polynomial Approximations to \$ AC ^0\$.pdf:application/pdf}
}

@article{kemp-learning-2007,
	title = {Learning overhypotheses with hierarchical {Bayesian} models},
	volume = {10},
	issn = {1363-755X, 1467-7687},
	url = {http://doi.wiley.com/10.1111/j.1467-7687.2007.00585.x},
	doi = {10.1111/j.1467-7687.2007.00585.x},
	language = {en},
	number = {3},
	urldate = {2016-04-26},
	journal = {Developmental Science},
	author = {Kemp, Charles and Perfors, Amy and Tenenbaum, Joshua B.},
	month = may,
	year = {2007},
	pages = {307--321},
	file = {KempPTDevSci.pdf:/home/user/Zotero/storage/3M8QWEPS/KempPTDevSci.pdf:application/pdf}
}

@inproceedings{sukhbaatar-end--end-2015-1,
	title = {End-to-end memory networks},
	url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks},
	urldate = {2016-04-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and {others}},
	year = {2015},
	pages = {2431--2439},
	file = {1503.08895v4.pdf:/home/user/Zotero/storage/3PKNFJD2/1503.08895v4.pdf:application/pdf}
}

@article{marcolli-syntactic-2016,
	title = {Syntactic {Parameters} and a {Coding} {Theory} {Perspective} on {Entropy} and {Complexity} of {Language} {Families}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/18/4/110},
	doi = {10.3390/e18040110},
	abstract = {We present a simple computational approach to assigning a measure of complexity and information/entropy to families of natural languages, based on syntactic parameters and the theory of error correcting codes. We associate to each language a binary string of syntactic parameters and to a language family a binary code, with code words the binary string associated to each language. We then evaluate the code parameters (rate and relative minimum distance) and the position of the parameters with respect to the asymptotic bound of error correcting codes and the Gilbert–Varshamov bound. These bounds are, respectively, related to the Kolmogorov complexity and the Shannon entropy of the code and this gives us a computationally simple way to obtain estimates on the complexity and information, not of individual languages but of language families. This notion of complexity is related, from the linguistic point of view to the degree of variability of syntactic parameter across languages belonging to the same (historical) family.},
	language = {en},
	number = {4},
	urldate = {2016-04-26},
	journal = {Entropy},
	author = {Marcolli, Matilde},
	month = apr,
	year = {2016},
	keywords = {Syntax, asymptotic bound, error-correcting codes, Gilbert–Varshamov bound, Kolmogorov complexity, principles and parameters, Shannon entropy},
	pages = {110},
	file = {Marcolli_2016_Syntactic Parameters and a Coding Theory Perspective on Entropy and Complexity.pdf:/home/user/Zotero/storage/WJW57C87/Marcolli_2016_Syntactic Parameters and a Coding Theory Perspective on Entropy and Complexity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/6GSEAMAM/110.html:text/html}
}

@article{tran-recurrent-2016,
	title = {Recurrent {Memory} {Networks} for {Language} {Modeling}},
	url = {http://arxiv.org/abs/1601.01272},
	abstract = {Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2\% accuracy, surpassing the previous state-of-the-art by a large margin.},
	urldate = {2016-04-26},
	journal = {arXiv:1601.01272 [cs]},
	author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01272},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/HKEPUNAZ/1601.html:text/html;Tran et al_2016_Recurrent Memory Networks for Language Modeling.pdf:/home/user/Zotero/storage/P73N7UND/Tran et al_2016_Recurrent Memory Networks for Language Modeling.pdf:application/pdf}
}

@article{cheng-agreement-based-2015-1,
	title = {Agreement-based {Joint} {Training} for {Bidirectional} {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1512.04650},
	abstract = {The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.},
	urldate = {2016-04-26},
	journal = {arXiv:1512.04650 [cs]},
	author = {Cheng, Yong and Shen, Shiqi and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.04650},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/QCR5THW2/1512.html:text/html;Cheng et al_2015_Agreement-based Joint Training for Bidirectional Attention-based Neural Machine.pdf:/home/user/Zotero/storage/9ZNR6K3H/Cheng et al_2015_Agreement-based Joint Training for Bidirectional Attention-based Neural Machine.pdf:application/pdf}
}

@article{pinker-past-2002,
	title = {The past and future of the past tense},
	volume = {6},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661302019903},
	doi = {10.1016/S1364-6613(02)01990-3},
	abstract = {What is the interaction between storage and computation in language processing? What is the psychological status of grammatical rules? What are the relative strengths of connectionist and symbolic models of cognition? How are the components of language implemented in the brain? The English past tense has served as an arena for debates on these issues. We defend the theory that irregular past-tense forms are stored in the lexicon, a division of declarative memory, whereas regular forms can be computed by a concatenation rule, which requires the procedural system. Irregulars have the psychological, linguistic and neuropsychological signatures of lexical memory, whereas regulars often have the signatures of grammatical processing. Furthermore, because regular inflection is rule-driven, speakers can apply it whenever memory fails.},
	number = {11},
	urldate = {2016-04-25},
	journal = {Trends in Cognitive Sciences},
	author = {Pinker, Steven and Ullman, Michael T.},
	month = nov,
	year = {2002},
	keywords = {declarative, Inflectional Morphology, Langauge Processing, Past Tense, procedural, rules},
	pages = {456--463},
	file = {Pinker_Ullman_2002_The past and future of the past tense.pdf:/home/user/Zotero/storage/M2UHRRET/Pinker_Ullman_2002_The past and future of the past tense.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DRIG6KZD/S1364661302019903.html:text/html}
}

@article{perfors-tutorial-2011,
	series = {Probabilistic models of cognitive development},
	title = {A tutorial introduction to {Bayesian} models of cognitive development},
	volume = {120},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S001002771000291X},
	doi = {10.1016/j.cognition.2010.11.015},
	abstract = {We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.},
	number = {3},
	urldate = {2016-04-25},
	journal = {Cognition},
	author = {Perfors, Amy and Tenenbaum, Joshua B. and Griffiths, Thomas L. and Xu, Fei},
	month = sep,
	year = {2011},
	keywords = {Cognitive development, Bayesian models},
	pages = {302--321},
	file = {Perfors et al_2011_A tutorial introduction to Bayesian models of cognitive development.pdf:/home/user/Zotero/storage/NSD6N2RD/Perfors et al_2011_A tutorial introduction to Bayesian models of cognitive development.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/2QVP6BRN/S001002771000291X.html:text/html}
}

@article{perfors-tutorial-2011-1,
	series = {Probabilistic models of cognitive development},
	title = {A tutorial introduction to {Bayesian} models of cognitive development},
	volume = {120},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S001002771000291X},
	doi = {10.1016/j.cognition.2010.11.015},
	abstract = {We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.},
	number = {3},
	urldate = {2016-04-25},
	journal = {Cognition},
	author = {Perfors, Amy and Tenenbaum, Joshua B. and Griffiths, Thomas L. and Xu, Fei},
	month = sep,
	year = {2011},
	keywords = {Cognitive development, Bayesian models},
	pages = {302--321},
	file = {Perfors et al_2011_A tutorial introduction to Bayesian models of cognitive development.pdf:/home/user/Zotero/storage/RGX5N6TW/Perfors et al_2011_A tutorial introduction to Bayesian models of cognitive development.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/CM4SMUT4/S001002771000291X.html:text/html}
}

@article{hoffman-stochastic-2013,
	title = {Stochastic variational inference},
	volume = {14},
	url = {http://dl.acm.org/citation.cfm?id=2502622},
	number = {1},
	urldate = {2016-04-25},
	journal = {The Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347},
	file = {hoffman13a.dvi - HoffmanBleiWangPaisley2013.pdf:/home/user/Zotero/storage/AJCPMMIH/HoffmanBleiWangPaisley2013.pdf:application/pdf}
}

@inproceedings{demberg-syntactic-2012,
	title = {Syntactic surprisal affects spoken word duration in conversational contexts},
	url = {http://dl.acm.org/citation.cfm?id=2390992},
	urldate = {2016-04-25},
	booktitle = {Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning},
	publisher = {Association for Computational Linguistics},
	author = {Demberg, Vera and Sayeed, Asad B. and Gorinski, Philip J. and Engonopoulos, Nikolaos},
	year = {2012},
	pages = {356--367},
	file = {Syntactic Surprisal Affects Spoken Word Duration in Conversational Contexts - D12-1033:/home/user/Zotero/storage/B6GEJQ7P/D12-1033.pdf:application/pdf}
}

@article{gorinski-movie-nodate,
	title = {Movie {Script} {Summarization} as {Graph}-based {Scene} {Extraction}},
	url = {http://www.aclweb.org/anthology/N/N15/N15-1113.pdf},
	urldate = {2016-04-25},
	author = {Gorinski, Philip John and Lapata, Mirella},
	file = {Movie Script Summarization as Graph-based Scene Extraction - N15-1113:/home/user/Zotero/storage/985RTM4R/N15-1113.pdf:application/pdf}
}

@article{lu-learning-2016,
	title = {Learning {Compact} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1604.02594},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1604.02594},
	author = {Lu, Zhiyun and Sindhwani, Vikas and Sainath, Tara N.},
	year = {2016},
	file = {() - 1604.02594v1:/home/user/Zotero/storage/V44ZWZZ5/1604.pdf:application/pdf}
}

@article{lou-communicating-2015,
	title = {Communicating with sentences: {A} multi-word naming game model},
	shorttitle = {Communicating with sentences},
	url = {http://arxiv.org/abs/1512.08347},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1512.08347},
	author = {Lou, Yang and Chen, Guanrong and Hu, Jianwei},
	year = {2015},
	file = {1512.08347v3:/home/user/Zotero/storage/WJTKGFAQ/1512.pdf:application/pdf}
}

@article{xu-show-2015-1,
	title = {Show, attend and tell: {Neural} image caption generation with visual attention},
	shorttitle = {Show, attend and tell},
	url = {http://arxiv.org/abs/1502.03044},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1502.03044},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	year = {2015},
	file = {1502.03044v2.pdf:/home/user/Zotero/storage/B5JEDPM8/1502.03044v2.pdf:application/pdf}
}

@inproceedings{ji-discriminative-2013,
	title = {Discriminative {Improvements} to {Distributional} {Sentence} {Similarity}.},
	url = {http://jiyfeng.github.io/papers/ji-emnlp-2013.pdf},
	urldate = {2016-04-25},
	booktitle = {{EMNLP}},
	author = {Ji, Yangfeng and Eisenstein, Jacob},
	year = {2013},
	pages = {891--896},
	file = {ji-emnlp-2013.pdf:/home/user/Zotero/storage/7TWZG6BI/ji-emnlp-2013.pdf:application/pdf}
}

@article{wen-network-based-2016,
	title = {A {Network}-based {End}-to-{End} {Trainable} {Task}-oriented {Dialogue} {System}},
	url = {http://arxiv.org/abs/1604.04562},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1604.04562},
	author = {Wen, Tsung-Hsien and Gasic, Milica and Mrksic, Nikola and Rojas-Barahona, Lina M. and Su, Pei-Hao and Ultes, Stefan and Vandyke, David and Young, Steve},
	year = {2016},
	file = {1604.04562v1:/home/user/Zotero/storage/BT69GVPX/1604.pdf:application/pdf}
}

@article{faruqui-non-distributional-2015,
	title = {Non-distributional word vector representations},
	url = {http://arxiv.org/abs/1506.05230},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1506.05230},
	author = {Faruqui, Manaal and Dyer, Chris},
	year = {2015},
	file = {Non-distributional Word Vector Representations - P15-2076:/home/user/Zotero/storage/TPIS3E8F/P15-2076.pdf:application/pdf}
}

@inproceedings{hazan-direct-2010,
	title = {Direct loss minimization for structured prediction},
	url = {http://papers.nips.cc/paper/4069-direct-loss-minimization-for-structured-prediction},
	urldate = {2016-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hazan, Tamir and Keshet, Joseph and McAllester, David A.},
	year = {2010},
	pages = {1594--1602},
	file = {Direct Loss Minimization for Structured Prediction - 23317d7dc87b6e640da6ce04cbd25da5afb4.pdf:/home/user/Zotero/storage/TPC35MJJ/23317d7dc87b6e640da6ce04cbd25da5afb4.pdf:application/pdf}
}

@article{lahiri-universal-2016-1,
	title = {A universal tradeoff between power, precision and speed in physical communication},
	url = {http://arxiv.org/abs/1603.07758},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1603.07758},
	author = {Lahiri, Subhaneil and Sohl-Dickstein, Jascha and Ganguli, Surya},
	year = {2016},
	file = {1603.07758.pdf:/home/user/Zotero/storage/PHPGXQSP/1603.07758.pdf:application/pdf}
}

@article{saxe-exact-2013,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	urldate = {2016-04-25},
	journal = {arXiv preprint arXiv:1312.6120},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	year = {2013},
	file = {DynamLearn.pdf:/home/user/Zotero/storage/JU38XZKM/DynamLearn.pdf:application/pdf}
}

@inproceedings{piech-deep-2015,
	title = {Deep {Knowledge} {Tracing}},
	url = {http://papers.nips.cc/paper/5654-deep-knowledge-tracing},
	urldate = {2016-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J. and Sohl-Dickstein, Jascha},
	year = {2015},
	pages = {505--513},
	file = {Deep Knowledge Tracing - DeepKnowledgeTracing.pdf:/home/user/Zotero/storage/AEFI6HFJ/DeepKnowledgeTracing.pdf:application/pdf}
}

@article{gao-simplicity-2015,
	title = {On simplicity and complexity in the brave new world of large-scale neuroscience},
	volume = {32},
	issn = {09594388},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438815000768},
	doi = {10.1016/j.conb.2015.04.003},
	language = {en},
	urldate = {2016-04-25},
	journal = {Current Opinion in Neurobiology},
	author = {Gao, Peiran and Ganguli, Surya},
	month = jun,
	year = {2015},
	pages = {148--155},
	file = {On simplicity and complexity in the brave new world of large-scale neuroscience - 15.BraveNewWorld.pdf:/home/user/Zotero/storage/WDDUKB52/15.BraveNewWorld.pdf:application/pdf}
}

@article{giret-evidence-2014,
	title = {Evidence for a causal inverse model in an avian cortico-basal ganglia circuit},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1317087111},
	doi = {10.1073/pnas.1317087111},
	language = {en},
	number = {16},
	urldate = {2016-04-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Giret, N. and Kornfeld, J. and Ganguli, S. and Hahnloser, R. H. R.},
	month = apr,
	year = {2014},
	pages = {6063--6068},
	file = {CausalInv.14.PNAS.pdf:/home/user/Zotero/storage/VN38AX7F/CausalInv.14.PNAS.pdf:application/pdf}
}

@inproceedings{hazan-direct-2010-1,
	title = {Direct loss minimization for structured prediction},
	url = {http://papers.nips.cc/paper/4069-direct-loss-minimization-for-structured-prediction},
	urldate = {2016-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hazan, Tamir and Keshet, Joseph and McAllester, David A.},
	year = {2010},
	pages = {1594--1602},
	file = {Direct Loss Minimization for Structured Prediction - 23317d7dc87b6e640da6ce04cbd25da5afb4.pdf:/home/user/Zotero/storage/KHU9FFMW/23317d7dc87b6e640da6ce04cbd25da5afb4.pdf:application/pdf}
}

@article{yamins-performance-optimized-2014,
	title = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1403112111},
	doi = {10.1073/pnas.1403112111},
	language = {en},
	number = {23},
	urldate = {2016-04-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yamins, D. L. K. and Hong, H. and Cadieu, C. F. and Solomon, E. A. and Seibert, D. and DiCarlo, J. J.},
	month = jun,
	year = {2014},
	pages = {8619--8624},
	file = {92787.pdf:/home/user/Zotero/storage/2T44AD8R/92787.pdf:application/pdf}
}

@article{amelunxen-living-2014,
	title = {Living on the edge: {Phase} transitions in convex programs with random data},
	shorttitle = {Living on the edge},
	url = {http://imaiai.oxfordjournals.org/content/early/2014/06/28/imaiai.iau005.abstract},
	urldate = {2016-04-25},
	journal = {Information and Inference},
	author = {Amelunxen, Dennis and Lotz, Martin and McCoy, Michael B. and Tropp, Joel A.},
	year = {2014},
	pages = {iau005},
	file = {Living on the edge\: Phase transitions in convex programs with random data - 1303.6672.pdf:/home/user/Zotero/storage/GZNQE6HV/1303.6672.pdf:application/pdf}
}

@inproceedings{lucas-superspace-2012,
	title = {Superspace extrapolation reveals inductive biases in function learning},
	url = {http://repository.cmu.edu/psychology/958/?utm\_source=repository.cmu.edu%2Fpsychology%2F958&utm\_medium=PDF&utm\_campaign=PDFCoverPages},
	urldate = {2016-04-25},
	publisher = {Cognitive Science Society},
	author = {Lucas, Christopher G. and Sterling, Douglas and Kemp, Charles},
	year = {2012},
	file = {lucasSuperspaceCogsci.pdf:/home/user/Zotero/storage/VAPDW63Z/lucasSuperspaceCogsci.pdf:application/pdf}
}

@inproceedings{lucas-unified-2012,
	title = {A unified theory of counterfactual reasoning},
	url = {http://repository.cmu.edu/psychology/957/},
	urldate = {2016-04-25},
	publisher = {Cognitive Science Society},
	author = {Lucas, Christopher G. and Kemp, Charles},
	year = {2012},
	file = {lucask_counterfactualReasoningCogsci.pdf:/home/user/Zotero/storage/DWJ4VKFH/lucask_counterfactualReasoningCogsci.pdf:application/pdf}
}

@article{bes-non-bayesian-2012,
	title = {Non-{Bayesian} {Inference}: {Causal} {Structure} {Trumps} {Correlation}: {Cognitive} {Science}},
	volume = {36},
	issn = {03640213},
	shorttitle = {Non-{Bayesian} {Inference}},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2012.01262.x},
	doi = {10.1111/j.1551-6709.2012.01262.x},
	language = {en},
	number = {7},
	urldate = {2016-04-25},
	journal = {Cognitive Science},
	author = {Bes, Bénédicte and Sloman, Steven and Lucas, Christopher G. and Raufaste, Éric},
	month = sep,
	year = {2012},
	pages = {1178--1203},
	file = {besSlomanLucasRaufaste2012.pdf:/home/user/Zotero/storage/GH7NNAHG/besSlomanLucasRaufaste2012.pdf:application/pdf}
}

@inproceedings{jern-evaluating-2011,
	title = {Evaluating the inverse decision-making approach to preference learning.},
	url = {http://papers.nips.cc/paper/4372-evaluating-the-inverse-decision-making-approach-to-preference-learning-spotlight.pdf},
	urldate = {2016-04-25},
	booktitle = {{NIPS}},
	author = {Jern, Alan and Lucas, Christopher G. and Kemp, Charles},
	year = {2011},
	pages = {2276--2284},
	file = {preferencelearninNips11.pdf:/home/user/Zotero/storage/ST9B7R35/preferencelearninNips11.pdf:application/pdf}
}

@inproceedings{waisman-bayesian-2011,
	title = {A {Bayesian} model of navigation in squirrels},
	url = {http://jacobs.berkeley.edu/wp-content/uploads/2015/10/CogSci-2011-Waisman.pdf},
	urldate = {2016-04-25},
	booktitle = {Proceedings of the 33rd {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {Waisman, Anna S. and Lucas, Christopher G. and Griffiths, Thomas L. and Jacobs, Lucia F.},
	year = {2011},
	file = {bayesianSquirrelsCogsci11.pdf:/home/user/Zotero/storage/3W99ZKUP/bayesianSquirrelsCogsci11.pdf:application/pdf}
}

@article{lucas-learning-2009,
	title = {Learning the {Form} of {Causal} {Relationships} {Using} {Hierarchical} {Bayesian} {Models}},
	volume = {34},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2009.01058.x},
	doi = {10.1111/j.1551-6709.2009.01058.x},
	language = {en},
	number = {1},
	urldate = {2016-04-25},
	journal = {Cognitive Science},
	author = {Lucas, Christopher G. and Griffiths, Thomas L.},
	month = jul,
	year = {2009},
	pages = {113--147},
	file = {lucasFuncformHierBayes.pdf:/home/user/Zotero/storage/8GJ8BVZU/lucasFuncformHierBayes.pdf:application/pdf}
}

@article{kushnir-inferring-2009,
	title = {Inferring {Hidden} {Causal} {Structure}},
	volume = {34},
	issn = {03640213, 15516709},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2009.01072.x},
	doi = {10.1111/j.1551-6709.2009.01072.x},
	language = {en},
	number = {1},
	urldate = {2016-04-25},
	journal = {Cognitive Science},
	author = {Kushnir, Tamar and Gopnik, Alison and Lucas, Chris and Schulz, Laura},
	month = oct,
	year = {2009},
	pages = {148--160},
	file = {kushnirGopnikLucasSchulz2010.pdf:/home/user/Zotero/storage/6HZ2W6NS/kushnirGopnikLucasSchulz2010.pdf:application/pdf}
}

@inproceedings{lucas-developmental-2010,
	title = {Developmental differences in learning the forms of causal relationships},
	url = {http://homepages.inf.ed.ac.uk/clucas2/docs/lucasFuncformdevCogsci.pdf},
	urldate = {2016-04-25},
	booktitle = {Proceedings of the 32nd annual conference of the cognitive science society},
	author = {Lucas, Chris and Gopnik, Alison and Griffiths, Thomas L.},
	year = {2010},
	pages = {2852--2857},
	file = {lucasFuncformdevCogsci.pdf:/home/user/Zotero/storage/MBBSBNKE/lucasFuncformdevCogsci.pdf:application/pdf}
}

@article{lucas-when-2014,
	title = {When children are better (or at least more open-minded) learners than adults: {Developmental} differences in learning the forms of causal relationships},
	volume = {131},
	shorttitle = {When children are better (or at least more open-minded) learners than adults},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027713002540},
	number = {2},
	urldate = {2016-04-25},
	journal = {Cognition},
	author = {Lucas, Christopher G. and Bridgers, Sophie and Griffiths, Thomas L. and Gopnik, Alison},
	year = {2014},
	pages = {284--299},
	file = {Microsoft Word - kvaPreprint.docx - kvaPreprint.pdf:/home/user/Zotero/storage/45D3Z88K/kvaPreprint.pdf:application/pdf}
}

@article{lucas-child-2014,
	title = {The {Child} as {Econometrician}: {A} {Rational} {Model} of {Preference} {Understanding} in {Children}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {Child} as {Econometrician}},
	url = {http://dx.plos.org/10.1371/journal.pone.0092160},
	doi = {10.1371/journal.pone.0092160},
	language = {en},
	number = {3},
	urldate = {2016-04-25},
	journal = {PLoS ONE},
	author = {Lucas, Christopher G. and Griffiths, Thomas L. and Xu, Fei and Fawcett, Christine and Gopnik, Alison and Kushnir, Tamar and Markson, Lori and Hu, Jane},
	editor = {Daunizeau, Jean},
	month = mar,
	year = {2014},
	pages = {e92160},
	file = {pone.0092160 1..9 - preferencesPlos.pdf:/home/user/Zotero/storage/F4EGDVIK/preferencesPlos.pdf:application/pdf}
}

@article{lucas-rational-2015,
	title = {A rational model of function learning},
	volume = {22},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/article/10.3758/s13423-015-0808-5},
	doi = {10.3758/s13423-015-0808-5},
	abstract = {Theories of how people learn relationships between continuous variables have tended to focus on two possibilities: one, that people are estimating explicit functions, or two that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, which provide a probabilistic basis for similarity-based function learning, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a rational model of human function learning that combines the strengths of both approaches and accounts for a wide variety of experimental results.},
	language = {en},
	number = {5},
	urldate = {2016-04-25},
	journal = {Psychonomic Bulletin \& Review},
	author = {Lucas, Christopher G. and Griffiths, Thomas L. and Williams, Joseph J. and Kalish, Michael L.},
	month = mar,
	year = {2015},
	keywords = {Bayesian modeling, Cognitive Psychology, Function learning},
	pages = {1193--1215},
	file = {Lucas et al_2015_A rational model of function learning.pdf:/home/user/Zotero/storage/WQDCWSQP/Lucas et al_2015_A rational model of function learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/A3HEDZBI/s13423-015-0808-5.html:text/html}
}

@article{banerjee-learning-2016,
	title = {Learning {Precise} {Spike} {Train}–to–{Spike} {Train} {Transformations} in {Multilayer} {Feedforward} {Neuronal} {Networks}},
	volume = {28},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/NECO\_a\_00829},
	doi = {10.1162/NECO\_a\_00829},
	abstract = {We derive a synaptic weight update rule for learning temporally precise spike train–to–spike train transformations in multilayer feedforward networks of spiking neurons. The framework, aimed at seamlessly generalizing error backpropagation to the deterministic spiking neuron setting, is based strictly on spike timing and avoids invoking concepts pertaining to spike rates or probabilistic models of spiking. The derivation is founded on two innovations. First, an error functional is proposed that compares the spike train emitted by the output neuron of the network to the desired spike train by way of their putative impact on a virtual postsynaptic neuron. This formulation sidesteps the need for spike alignment and leads to closed-form solutions for all quantities of interest. Second, virtual assignment of weights to spikes rather than synapses enables a perturbation analysis of individual spike times and synaptic weights of the output, as well as all intermediate neurons in the network, which yields the gradients of the error functional with respect to the said entities. Learning proceeds via a gradient descent mechanism that leverages these quantities. Simulation experiments demonstrate the efficacy of the proposed learning framework. The experiments also highlight asymmetries between synapses on excitatory and inhibitory neurons.},
	number = {5},
	urldate = {2016-04-25},
	journal = {Neural Computation},
	author = {Banerjee, Arunava},
	month = mar,
	year = {2016},
	pages = {826--848},
	file = {Snapshot:/home/user/Zotero/storage/XJ2354KB/NECO_a_00829.html:text/html}
}

@article{tygert-mathematical-2016,
	title = {A {Mathematical} {Motivation} for {Complex}-{Valued} {Convolutional} {Networks}},
	volume = {28},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/NECO\_a\_00824},
	doi = {10.1162/NECO\_a\_00824},
	abstract = {A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors, followed by (2) taking the absolute value of every entry of the resulting vectors, followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as data-driven multiscale windowed power spectra, data-driven multiscale windowed absolute spectra, data-driven multiwavelet absolute values, or (in their most general configuration) data-driven nonlinear multiwavelet packets. Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (e.g., logistic or tanh) nonlinearities, or max pooling, for example, do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.},
	number = {5},
	urldate = {2016-04-25},
	journal = {Neural Computation},
	author = {Tygert, Mark and Bruna, Joan and Chintala, Soumith and LeCun, Yann and Piantino, Serkan and Szlam, Arthur},
	month = feb,
	year = {2016},
	pages = {815--825},
	file = {Snapshot:/home/user/Zotero/storage/7R6XDB9P/NECO_a_00824.html:text/html}
}

@article{noauthor-under-2016,
	title = {Under appeal},
	volume = {532},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/532147b},
	doi = {10.1038/532147b},
	number = {7598},
	urldate = {2016-04-25},
	journal = {Nature},
	month = apr,
	year = {2016},
	pages = {147--148}
}

@article{noauthor-breeding-2016,
	title = {Breeding controls},
	volume = {532},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/532147a},
	doi = {10.1038/532147a},
	number = {7598},
	urldate = {2016-04-25},
	journal = {Nature},
	month = apr,
	year = {2016},
	pages = {147--147}
}

@article{chatterjee-computation-2016,
	title = {Computation {Tree} {Logic} for {Synchronization} {Properties}},
	url = {http://arxiv.org/abs/1604.06384},
	abstract = {We present a logic that extends CTL (Computation Tree Logic) with operators that express synchronization properties. A property is synchronized in a system if it holds in all paths of a certain length. The new logic is obtained by using the same path quantifiers and temporal operators as in CTL, but allowing a different order of the quantifiers. This small syntactic variation induces a logic that can express non-regular properties for which known extensions of MSO with equality of path length are undecidable. We show that our variant of CTL is decidable and that the model-checking problem is in Delta\_3{\textasciicircum}P = P{\textasciicircum}{NP{\textasciicircum}NP}, and is DP-hard. We analogously consider quantifier exchange in extensions of CTL, and we present operators defined using basic operators of CTL* that express the occurrence of infinitely many synchronization points. We show that the model-checking problem remains in Delta\_3{\textasciicircum}P. The distinguishing power of CTL and of our new logic coincide if the Next operator is allowed in the logics, thus the classical bisimulation quotient can be used for state-space reduction before model checking.},
	urldate = {2016-04-25},
	journal = {arXiv:1604.06384 [cs]},
	author = {Chatterjee, Krishnendu and Doyen, Laurent},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06384},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NZMHQPXX/1604.html:text/html;Chatterjee_Doyen_2016_Computation Tree Logic for Synchronization Properties.pdf:/home/user/Zotero/storage/5UEHWU3F/Chatterjee_Doyen_2016_Computation Tree Logic for Synchronization Properties.pdf:application/pdf}
}

@article{dauphin-identifying-2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://arxiv.org/abs/1406.2572},
	abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
	urldate = {2016-04-22},
	journal = {arXiv:1406.2572 [cs, math, stat]},
	author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2572},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CSJA2GAM/1406.html:text/html;Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf:/home/user/Zotero/storage/RPPWUW57/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf:application/pdf}
}

@article{zhou-object-2014,
	title = {Object {Detectors} {Emerge} in {Deep} {Scene} {CNNs}},
	url = {http://arxiv.org/abs/1412.6856},
	abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
	urldate = {2016-04-22},
	journal = {arXiv:1412.6856 [cs]},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6856},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/8EKVXDU6/1412.html:text/html;Zhou et al_2014_Object Detectors Emerge in Deep Scene CNNs.pdf:/home/user/Zotero/storage/VC75UPVQ/Zhou et al_2014_Object Detectors Emerge in Deep Scene CNNs.pdf:application/pdf}
}

@article{khajetoorians-toward-2016,
	title = {Toward single-atom memory},
	volume = {352},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/352/6283/296},
	doi = {10.1126/science.aaf2481},
	abstract = {Single holmium atoms can be used as a stable magnetic memory [Also see Report by Donati et al.]
Single holmium atoms can be used as a stable magnetic memory [Also see Report by Donati et al.]},
	language = {en},
	number = {6283},
	urldate = {2016-04-22},
	journal = {Science},
	author = {Khajetoorians, Alexander Ako and Heinrich, Andreas J.},
	month = apr,
	year = {2016},
	pmid = {27081058},
	pages = {296--297},
	file = {Khajetoorians_Heinrich_2016_Toward single-atom memory.pdf:/home/user/Zotero/storage/83HV5ZZP/Khajetoorians_Heinrich_2016_Toward single-atom memory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BHREJFXQ/296.html:text/html}
}

@article{cho-fear-2016,
	title = {Fear and loathing in the hunt for gravitational waves},
	volume = {352},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/352/6283/300},
	doi = {10.1126/science.aaf5109},
	abstract = {One physicist hits the road to explore LIGO's history and the hard feelings that nearly derailed the entire project
One physicist hits the road to explore LIGO's history and the hard feelings that nearly derailed the entire project},
	language = {en},
	number = {6283},
	urldate = {2016-04-22},
	journal = {Science},
	author = {Cho, Adrian},
	month = apr,
	year = {2016},
	pages = {300--300},
	file = {Cho_2016_Fear and loathing in the hunt for gravitational waves.pdf:/home/user/Zotero/storage/QKI4P2T6/Cho_2016_Fear and loathing in the hunt for gravitational waves.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/H7FRCTP6/300.html:text/html}
}

@article{enserink-dutch-2016,
	title = {Dutch push for a quantum leap in open access},
	volume = {352},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/352/6283/279},
	doi = {10.1126/science.352.6283.279},
	abstract = {E.U. urged to free all papers by 2020.
E.U. urged to free all papers by 2020.},
	language = {en},
	number = {6283},
	urldate = {2016-04-22},
	journal = {Science},
	author = {Enserink, Martin},
	month = apr,
	year = {2016},
	pmid = {27081047},
	pages = {279--279},
	file = {Enserink_2016_Dutch push for a quantum leap in open access.pdf:/home/user/Zotero/storage/WQGXMRCF/Enserink_2016_Dutch push for a quantum leap in open access.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IH7694U2/279.html:text/html}
}

@article{piantadosi-logical-2016,
	title = {The {Logical} {Primitives} of {Thought}: {Empirical} {Foundations} for {Compositional} {Cognitive} {Models}},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	shorttitle = {The {Logical} {Primitives} of {Thought}},
	doi = {10.1037/a0039980},
	abstract = {The notion of a compositional language of thought (LOT) has been central in computational accounts of cognition from earliest attempts (Boole, 1854; Fodor, 1975) to the present day (Feldman, 2000; Penn, Holyoak, \& Povinelli, 2008; Fodor, 2008; Kemp, 2012; Goodman, Tenenbaum, \& Gerstenberg, 2015). Recent modeling work shows how statistical inferences over compositionally structured hypothesis spaces might explain learning and development across a variety of domains. However, the primitive components of such representations are typically assumed a priori by modelers and theoreticians rather than determined empirically. We show how different sets of LOT primitives, embedded in a psychologically realistic approximate Bayesian inference framework, systematically predict distinct learning curves in rule-based concept learning experiments. We use this feature of LOT models to design a set of large-scale concept learning experiments that can determine the most likely primitives for psychological concepts involving Boolean connectives and quantification. Subjects’ inferences are most consistent with a rich (nonminimal) set of Boolean operations, including first-order, but not second-order, quantification. Our results more generally show how specific LOT theories can be distinguished empirically.},
	journal = {Psychological Review},
	author = {Piantadosi, Steven T. and Tenenbaum, Joshua B. and Goodman, Noah D.},
	year = {2016},
	pages = {No Pagination Specified}
}

@article{haefner-perceptual-2016,
	title = {Perceptual {Decision}-{Making} as {Probabilistic} {Inference} by {Neural} {Sampling}},
	volume = {0},
	issn = {0896-6273},
	url = {http://www.cell.com/article/S0896627316300113/abstract},
	doi = {10.1016/j.neuron.2016.03.020},
	language = {English},
	number = {0},
	urldate = {2016-04-22},
	journal = {Neuron},
	author = {Haefner, Ralf M. and Berkes, Pietro and Fiser, József},
	month = apr,
	year = {2016},
	file = {Haefner et al_2016_Perceptual Decision-Making as Probabilistic Inference by Neural Sampling.pdf:/home/user/Zotero/storage/4M2DEXI6/Haefner et al_2016_Perceptual Decision-Making as Probabilistic Inference by Neural Sampling.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZECCUEDQ/S0896-6273(16)30011-3.html:text/html}
}

@article{ries-serial-2016,
	title = {Serial versus parallel neurobiological processes in language production: comment on {Munding}, {Dubarry}, and {Alario}, 2015},
	volume = {31},
	issn = {2327-3798},
	shorttitle = {Serial versus parallel neurobiological processes in language production},
	url = {http://dx.doi.org/10.1080/23273798.2015.1117644},
	doi = {10.1080/23273798.2015.1117644},
	number = {4},
	urldate = {2016-04-22},
	journal = {Language, Cognition and Neuroscience},
	author = {Riès, Stephanie K.},
	month = apr,
	year = {2016},
	pages = {476--479},
	file = {Riès_2016_Serial versus parallel neurobiological processes in language production.pdf:/home/user/Zotero/storage/PQVKXVZW/Riès_2016_Serial versus parallel neurobiological processes in language production.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/242MBJCD/23273798.2015.html:text/html}
}

@article{walker-empirical-2016,
	title = {Empirical and computational findings converge in support of the {Hierarchical} {State} {Feedback} {Control} theory},
	volume = {31},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1096404},
	doi = {10.1080/23273798.2015.1096404},
	number = {4},
	urldate = {2016-04-22},
	journal = {Language, Cognition and Neuroscience},
	author = {Walker, Grant M. and Hickok, Gregory},
	month = apr,
	year = {2016},
	pages = {470--470},
	file = {Snapshot:/home/user/Zotero/storage/3D4FCNPW/23273798.2015.html:text/html;Walker_Hickok_2016_Empirical and computational findings converge in support of the Hierarchical.pdf:/home/user/Zotero/storage/8UHT95FD/Walker_Hickok_2016_Empirical and computational findings converge in support of the Hierarchical.pdf:application/pdf}
}

@article{laganaro-dynamics-2016,
	title = {Dynamics of word production and processing speed},
	volume = {31},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2015.1096402},
	doi = {10.1080/23273798.2015.1096402},
	number = {4},
	urldate = {2016-04-22},
	journal = {Language, Cognition and Neuroscience},
	author = {Laganaro, Marina},
	month = apr,
	year = {2016},
	pages = {463--464},
	file = {Laganaro_2016_Dynamics of word production and processing speed.pdf:/home/user/Zotero/storage/TB9FRZK6/Laganaro_2016_Dynamics of word production and processing speed.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/R6GKU8SH/23273798.2015.html:text/html}
}

@article{kulkarni-hierarchical-2016,
	title = {Hierarchical {Deep} {Reinforcement} {Learning}: {Integrating} {Temporal} {Abstraction} and {Intrinsic} {Motivation}},
	shorttitle = {Hierarchical {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1604.06057},
	abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete MDP with stochastic transitions, and (2) the classic ATARI game `Montezuma's Revenge'.},
	urldate = {2016-04-21},
	journal = {arXiv:1604.06057 [cs, stat]},
	author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06057},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XS6QPCDB/1604.html:text/html;Kulkarni et al_2016_Hierarchical Deep Reinforcement Learning.pdf:/home/user/Zotero/storage/BQX2HD4E/Kulkarni et al_2016_Hierarchical Deep Reinforcement Learning.pdf:application/pdf}
}

@article{zeume-order-invariance-2016,
	title = {Order-{Invariance} of {Two}-{Variable} {Logic} is {Decidable}},
	url = {http://arxiv.org/abs/1604.05843},
	abstract = {It is shown that order-invariance of two-variable first-logic is decidable in the finite. This is an immediate consequence of a decision procedure obtained for the finite satisfiability problem for existential second-order logic with two first-order variables (\$\mathrm{ESO}{\textasciicircum}2\$) on structures with two linear orders and one induced successor. We also show that finite satisfiability is decidable on structures with two successors and one induced linear order. In both cases, so far only decidability for monadic \$\mathrm{ESO}{\textasciicircum}2\$ has been known. In addition, the finite satisfiability problem for \$\mathrm{ESO}{\textasciicircum}2\$ on structures with one linear order and its induced successor relation is shown to be decidable in non-deterministic exponential time.},
	urldate = {2016-04-21},
	journal = {arXiv:1604.05843 [cs]},
	author = {Zeume, Thomas and Harwath, Frederik},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.05843},
	keywords = {Computer Science - Logic in Computer Science, F.4.1},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AUVPJV8C/1604.html:text/html;Zeume_Harwath_2016_Order-Invariance of Two-Variable Logic is Decidable.pdf:/home/user/Zotero/storage/3VXCUKES/Zeume_Harwath_2016_Order-Invariance of Two-Variable Logic is Decidable.pdf:application/pdf}
}

@article{weston-dialog-based-2016,
	title = {Dialog-based {Language} {Learning}},
	url = {http://arxiv.org/abs/1604.06045},
	abstract = {A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.},
	urldate = {2016-04-21},
	journal = {arXiv:1604.06045 [cs]},
	author = {Weston, Jason},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06045},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/W7BH64SN/1604.html:text/html;Weston_2016_Dialog-based Language Learning.pdf:/home/user/Zotero/storage/B4XTBBKK/Weston_2016_Dialog-based Language Learning.pdf:application/pdf}
}

@article{janzamin-beating-2015-1,
	title = {Beating the {Perils} of {Non}-{Convexity}: {Guaranteed} {Training} of {Neural} {Networks} using {Tensor} {Methods}},
	shorttitle = {Beating the {Perils} of {Non}-{Convexity}},
	url = {http://arxiv.org/abs/1506.08473},
	abstract = {Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for learnability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we propose a computationally efficient method with guaranteed risk bounds for training neural networks with one hidden layer.},
	urldate = {2016-04-21},
	journal = {arXiv:1506.08473 [cs, stat]},
	author = {Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08473},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/SFCTMCE9/1506.html:text/html;Janzamin et al_2015_Beating the Perils of Non-Convexity.pdf:/home/user/Zotero/storage/URC5SAE3/Janzamin et al_2015_Beating the Perils of Non-Convexity.pdf:application/pdf}
}

@article{chen-learning-2014-1,
	title = {Learning {Deep} {Structured} {Models}},
	url = {http://arxiv.org/abs/1407.2538},
	abstract = {Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.},
	urldate = {2016-04-21},
	journal = {arXiv:1407.2538 [cs]},
	author = {Chen, Liang-Chieh and Schwing, Alexander G. and Yuille, Alan L. and Urtasun, Raquel},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.2538},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/FMKRIXSJ/1407.html:text/html;Chen et al_2014_Learning Deep Structured Models.pdf:/home/user/Zotero/storage/7IXWD8CI/Chen et al_2014_Learning Deep Structured Models.pdf:application/pdf}
}

@article{song-direct-2015,
	title = {Direct {Loss} {Minimization} for {Training} {Deep} {Neural} {Nets}},
	url = {http://arxiv.org/abs/1511.06411},
	abstract = {Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on specific application-specific metrics. In this paper we proposed a direct loss minimization approach to train deep neural networks, taking into account the application-specific loss functions. This can be non-trivial, when these functions are non-smooth and non-decomposable. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we propose a dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06411 [cs]},
	author = {Song, Yang and Schwing, Alexander G. and Zemel, Richard S. and Urtasun, Raquel},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06411},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Z44SDM4I/1511.html:text/html;Song et al_2015_Direct Loss Minimization for Training Deep Neural Nets.pdf:/home/user/Zotero/storage/9XAGBEZT/Song et al_2015_Direct Loss Minimization for Training Deep Neural Nets.pdf:application/pdf}
}

@article{vendrov-order-embeddings-2015-1,
	title = {Order-{Embeddings} of {Images} and {Language}},
	url = {http://arxiv.org/abs/1511.06361},
	abstract = {Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06361 [cs]},
	author = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06361},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IVXQTSIS/1511.html:text/html;Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:/home/user/Zotero/storage/4DPH326T/Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:application/pdf}
}

@article{yao-depth-gated-2015,
	title = {Depth-{Gated} {LSTM}},
	url = {http://arxiv.org/abs/1508.03790},
	abstract = {In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.},
	urldate = {2016-04-21},
	journal = {arXiv:1508.03790 [cs]},
	author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.03790},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IMJF26JM/1508.html:text/html;Yao et al_2015_Depth-Gated LSTM.pdf:/home/user/Zotero/storage/UW8284IP/Yao et al_2015_Depth-Gated LSTM.pdf:application/pdf}
}

@article{ling-finding-2015,
	title = {Finding {Function} in {Form}: {Compositional} {Character} {Models} for {Open} {Vocabulary} {Word} {Representation}},
	shorttitle = {Finding {Function} in {Form}},
	url = {http://arxiv.org/abs/1508.02096},
	abstract = {We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our "composed" word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).},
	urldate = {2016-04-21},
	journal = {arXiv:1508.02096 [cs]},
	author = {Ling, Wang and Luís, Tiago and Marujo, Luís and Astudillo, Ramón Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W. and Trancoso, Isabel},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02096},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/8WIWC6GF/1508.html:text/html;Ling et al_2015_Finding Function in Form.pdf:/home/user/Zotero/storage/PGWE6IPP/Ling et al_2015_Finding Function in Form.pdf:application/pdf}
}

@article{ballesteros-improved-2015,
	title = {Improved {Transition}-{Based} {Parsing} by {Modeling} {Characters} instead of {Words} with {LSTMs}},
	url = {http://arxiv.org/abs/1508.00657},
	abstract = {We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.},
	urldate = {2016-04-21},
	journal = {arXiv:1508.00657 [cs]},
	author = {Ballesteros, Miguel and Dyer, Chris and Smith, Noah A.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.00657},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/JHHJJCN3/1508.html:text/html;Ballesteros et al_2015_Improved Transition-Based Parsing by Modeling Characters instead of Words with.pdf:/home/user/Zotero/storage/BWQAB5V2/Ballesteros et al_2015_Improved Transition-Based Parsing by Modeling Characters instead of Words with.pdf:application/pdf}
}

@article{chen-net2net:-2015,
	title = {Net2Net: {Accelerating} {Learning} via {Knowledge} {Transfer}},
	shorttitle = {Net2Net},
	url = {http://arxiv.org/abs/1511.05641},
	abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.05641 [cs]},
	author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05641},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9UGGBCV3/1511.html:text/html;Chen et al_2015_Net2Net.pdf:/home/user/Zotero/storage/TWSV7REK/Chen et al_2015_Net2Net.pdf:application/pdf}
}

@article{li-convergent-2015,
	title = {Convergent {Learning}: {Do} different neural networks learn the same representations?},
	shorttitle = {Convergent {Learning}},
	url = {http://arxiv.org/abs/1511.07543},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.07543 [cs]},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07543},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BFF77ZRZ/1511.html:text/html;Li et al_2015_Convergent Learning.pdf:/home/user/Zotero/storage/7CDCQ89H/Li et al_2015_Convergent Learning.pdf:application/pdf}
}

@article{wieting-towards-2015-2,
	title = {Towards {Universal} {Paraphrastic} {Sentence} {Embeddings}},
	url = {http://arxiv.org/abs/1511.08198},
	abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.08198 [cs]},
	author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08198},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/59XZJVUA/1511.html:text/html;Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:/home/user/Zotero/storage/9GT2N3IG/Wieting et al_2015_Towards Universal Paraphrastic Sentence Embeddings.pdf:application/pdf}
}

@article{ji-blackout:-2015-1,
	title = {{BlackOut}: {Speeding} up {Recurrent} {Neural} {Network} {Language} {Models} {With} {Very} {Large} {Vocabularies}},
	shorttitle = {{BlackOut}},
	url = {http://arxiv.org/abs/1511.06909},
	abstract = {We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06909 [cs, stat]},
	author = {Ji, Shihao and Vishwanathan, S. V. N. and Satish, Nadathur and Anderson, Michael J. and Dubey, Pradeep},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06909},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/K9I5EHXT/1511.html:text/html;Ji et al_2015_BlackOut.pdf:/home/user/Zotero/storage/T6KJ7MM4/Ji et al_2015_BlackOut.pdf:application/pdf}
}

@article{krueger-regularizing-2015,
	title = {Regularizing {RNNs} by {Stabilizing} {Activations}},
	url = {http://arxiv.org/abs/1511.08400},
	abstract = {We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modelling and phoneme recognition, and outperforming weight noise and dropout. We set state of the art (17.5\% PER) for an RNN on the TIMIT phoneme recognition task, without using beam-search. With this penalty term, IRNN can achieve similar performance to LSTM on language modelling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.08400 [cs, stat]},
	author = {Krueger, David and Memisevic, Roland},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08400},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XBH7A9EG/1511.html:text/html;Krueger_Memisevic_2015_Regularizing RNNs by Stabilizing Activations.pdf:/home/user/Zotero/storage/WTVZAV8C/Krueger_Memisevic_2015_Regularizing RNNs by Stabilizing Activations.pdf:application/pdf}
}

@article{reed-neural-2015,
	title = {Neural {Programmer}-{Interpreters}},
	url = {http://arxiv.org/abs/1511.06279},
	abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06279 [cs]},
	author = {Reed, Scott and de Freitas, Nando},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06279},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/5PNKPPHI/1511.html:text/html;Reed_de Freitas_2015_Neural Programmer-Interpreters.pdf:/home/user/Zotero/storage/WMI3XUPQ/Reed_de Freitas_2015_Neural Programmer-Interpreters.pdf:application/pdf}
}

@article{vendrov-order-embeddings-2015-2,
	title = {Order-{Embeddings} of {Images} and {Language}},
	url = {http://arxiv.org/abs/1511.06361},
	abstract = {Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06361 [cs]},
	author = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06361},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3FGADTDV/1511.html:text/html;Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:/home/user/Zotero/storage/ES5ISTIE/Vendrov et al_2015_Order-Embeddings of Images and Language.pdf:application/pdf}
}

@article{balle-density-2015,
	title = {Density {Modeling} of {Images} using a {Generalized} {Normalization} {Transformation}},
	url = {http://arxiv.org/abs/1511.06281},
	abstract = {We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06281 [cs]},
	author = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06281},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9SNPPH95/1511.html:text/html;Ballé et al_2015_Density Modeling of Images using a Generalized Normalization Transformation.pdf:/home/user/Zotero/storage/KVHIZRAK/Ballé et al_2015_Density Modeling of Images using a Generalized Normalization Transformation.pdf:application/pdf}
}

@article{mansimov-generating-2015-1,
	title = {Generating {Images} from {Captions} with {Attention}},
	url = {http://arxiv.org/abs/1511.02793},
	abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.02793 [cs]},
	author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02793},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RXRMIQXV/1511.html:text/html;Mansimov et al_2015_Generating Images from Captions with Attention.pdf:/home/user/Zotero/storage/BT24VXAE/Mansimov et al_2015_Generating Images from Captions with Attention.pdf:application/pdf}
}

@article{lin-neural-2015,
	title = {Neural {Networks} with {Few} {Multiplications}},
	url = {http://arxiv.org/abs/1510.03009},
	abstract = {For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks.},
	urldate = {2016-04-21},
	journal = {arXiv:1510.03009 [cs]},
	author = {Lin, Zhouhan and Courbariaux, Matthieu and Memisevic, Roland and Bengio, Yoshua},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.03009},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ENGTTC8Z/1510.html:text/html;Lin et al_2015_Neural Networks with Few Multiplications.pdf:/home/user/Zotero/storage/4VDJT7MR/Lin et al_2015_Neural Networks with Few Multiplications.pdf:application/pdf}
}

@article{han-deep-2015,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2016-04-21},
	journal = {arXiv:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6VDAHDUJ/1510.html:text/html;Han et al_2015_Deep Compression.pdf:/home/user/Zotero/storage/IT45HPV4/Han et al_2015_Deep Compression.pdf:application/pdf}
}

@article{theis-note-2015,
	title = {A note on the evaluation of generative models},
	url = {http://arxiv.org/abs/1511.01844},
	abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.01844 [cs, stat]},
	author = {Theis, Lucas and Oord, Aäron van den and Bethge, Matthias},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01844},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XC6EK8BM/1511.html:text/html;Theis et al_2015_A note on the evaluation of generative models.pdf:/home/user/Zotero/storage/9XACQ277/Theis et al_2015_A note on the evaluation of generative models.pdf:application/pdf}
}

@article{tran-variational-2015,
	title = {The {Variational} {Gaussian} {Process}},
	url = {http://arxiv.org/abs/1511.06499},
	abstract = {Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.},
	urldate = {2016-04-21},
	journal = {arXiv:1511.06499 [cs, stat]},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06499},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4EQ4DNNA/1511.html:text/html;Tran et al_2015_The Variational Gaussian Process.pdf:/home/user/Zotero/storage/VTZ8C5ID/Tran et al_2015_The Variational Gaussian Process.pdf:application/pdf}
}

@misc{noauthor-theoretical-nodate,
	title = {Theoretical perspectives on eye movements in reading: {Past} controversies, current issues, and an agenda for future research - {European} {Journal} of {Cognitive} {Psychology} - {Volume} 16, {Issue} 1-2},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09541440340000295#.VxiwK5SW\_mE},
	urldate = {2016-04-21},
	file = {Theoretical perspectives on eye movements in reading\: Past controversies, current issues, and an agenda for future research - European Journal of Cognitive Psychology - Volume 16, Issue 1-2:/home/user/Zotero/storage/ER7WVFQT/09541440340000295.html:text/html}
}

@article{chow-prediction-2016,
	title = {Prediction as memory retrieval: timing and mechanisms},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Prediction as memory retrieval},
	url = {http://dx.doi.org/10.1080/23273798.2016.1160135},
	doi = {10.1080/23273798.2016.1160135},
	abstract = {In our target article [Chow, W., Smith, C., Lau, E., \& Phillips, C. (2015). A “bag-of-arguments” mechanism for initial verb predictions. Language, Cognition \& Neuroscience. Advance online publication. doi:10.1080/23273798.2015.1066832], we investigated the predictions that comprehenders initially make about an upcoming verb as they read and provided evidence that they are sensitive to the arguments’ lexical meaning but not their structural roles. Here we synthesise findings from our work with other studies that show that verb predictions are sensitive to the arguments’ roles if more time is available for prediction. We content that prediction involves computations that may require differing amounts of time. Further, we argue that prediction can be usefully framed as a memory retrieval problem, linking prediction to independently well-understood memory mechanisms in language processing. We suggest that the delayed impact of argument roles on verb predictions may reflect a mismatch between the format of linguistic cues and target event memories. We clarify points of agreement and disagreement with the commentaries, and explain why memory access mechanisms can account for the time course of prediction.},
	number = {0},
	urldate = {2016-04-21},
	journal = {Language, Cognition and Neuroscience},
	author = {Chow, Wing-Yee and Momma, Shota and Smith, Cybelle and Lau, Ellen and Phillips, Colin},
	month = apr,
	year = {2016},
	pages = {1--11},
	file = {Chow et al_2016_Prediction as memory retrieval.pdf:/home/user/Zotero/storage/JTUXH45U/Chow et al_2016_Prediction as memory retrieval.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/STVPCVTF/23273798.2016.html:text/html}
}

@article{spivey-semantics-2016,
	title = {Semantics influences speech perception: commentary on {Gow} and {Olson} (2015)},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Semantics influences speech perception},
	url = {http://dx.doi.org/10.1080/23273798.2016.1140788},
	doi = {10.1080/23273798.2016.1140788},
	number = {0},
	urldate = {2016-04-21},
	journal = {Language, Cognition and Neuroscience},
	author = {Spivey, Michael J.},
	month = apr,
	year = {2016},
	pages = {1--4},
	file = {Snapshot:/home/user/Zotero/storage/69B5RR6U/23273798.2016.html:text/html;Spivey_2016_Semantics influences speech perception.pdf:/home/user/Zotero/storage/4J7B8F2G/Spivey_2016_Semantics influences speech perception.pdf:application/pdf}
}

@article{guillon-phylogenetic-2016,
	title = {A {Phylogenetic} {Comparative} {Study} of {Bantu} {Kinship} {Terminology} {Finds} {Limited} {Support} for {Its} {Co}-{Evolution} with {Social} {Organisation}},
	volume = {11},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147920},
	doi = {10.1371/journal.pone.0147920},
	abstract = {The classification of kin into structured groups is a diverse phenomenon which is ubiquitous in human culture. For populations which are organized into large agropastoral groupings of sedentary residence but not governed within the context of a centralised state, such as our study sample of 83 historical Bantu-speaking groups of sub-Saharan Africa, cultural kinship norms guide all aspects of everyday life and social organization. Such rules operate in part through the use of differing terminological referential systems of familial organization. Although the cross-cultural study of kinship terminology was foundational in Anthropology, few modern studies have made use of statistical advances to further our sparse understanding of the structuring and diversification of terminological systems of kinship over time. In this study we use Bayesian Markov Chain Monte Carlo methods of phylogenetic comparison to investigate the evolution of Bantu kinship terminology and reconstruct the ancestral state and diversification of cousin terminology in this family of sub-Saharan ethnolinguistic groups. Using a phylogenetic tree of Bantu languages, we then test the prominent hypothesis that structured variation in systems of cousin terminology has co-evolved alongside adaptive change in patterns of descent organization, as well as rules of residence. We find limited support for this hypothesis, and argue that the shaping of systems of kinship terminology is a multifactorial process, concluding with possible avenues of future research.},
	number = {3},
	urldate = {2016-04-21},
	journal = {PLOS ONE},
	author = {Guillon, Myrtille and Mace, Ruth},
	month = mar,
	year = {2016},
	keywords = {Sociolinguistics, culture, cultural evolution, Africa, Cross-cultural studies, Languages, Phylogenetic analysis, Phylogenetics},
	pages = {e0147920},
	file = {Guillon_Mace_2016_A Phylogenetic Comparative Study of Bantu Kinship Terminology Finds Limited.pdf:/home/user/Zotero/storage/B6CREDJS/Guillon_Mace_2016_A Phylogenetic Comparative Study of Bantu Kinship Terminology Finds Limited.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3328X7CG/article.html:text/html}
}

@article{bernard-new-2016,
	title = {A {New} {Font}, {Specifically} {Designed} for {Peripheral} {Vision}, {Improves} {Peripheral} {Letter} and {Word} {Recognition}, but {Not} {Eye}-{Mediated} {Reading} {Performance}},
	volume = {11},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152506},
	doi = {10.1371/journal.pone.0152506},
	abstract = {Reading speed is dramatically reduced when readers cannot use their central vision. This is because low visual acuity and crowding negatively impact letter recognition in the periphery. In this study, we designed a new font (referred to as the Eido font) in order to reduce inter-letter similarity and consequently to increase peripheral letter recognition performance. We tested this font by running five experiments that compared the Eido font with the standard Courier font. Letter spacing and x-height were identical for the two monospaced fonts. Six normally-sighted subjects used exclusively their peripheral vision to run two aloud reading tasks (with eye movements), a letter recognition task (without eye movements), a word recognition task (without eye movements) and a lexical decision task. Results show that reading speed was not significantly different between the Eido and the Courier font when subjects had to read single sentences with a round simulated gaze-contingent central scotoma (10° diameter). In contrast, Eido significantly decreased perceptual errors in peripheral crowded letter recognition (-30\% errors on average for letters briefly presented at 6° eccentricity) and in peripheral word recognition (-32\% errors on average for words briefly presented at 6° eccentricity).},
	number = {4},
	urldate = {2016-04-21},
	journal = {PLOS ONE},
	author = {Bernard, Jean-Baptiste and Aguilar, Carlos and Castet, Eric},
	month = apr,
	year = {2016},
	keywords = {Eye Movements, vision, Eyes, Sensory perception, Luminance, Scotoma, Visual impairments, Word recognition},
	pages = {e0152506},
	file = {Bernard et al_2016_A New Font, Specifically Designed for Peripheral Vision, Improves Peripheral.pdf:/home/user/Zotero/storage/UF6J472M/Bernard et al_2016_A New Font, Specifically Designed for Peripheral Vision, Improves Peripheral.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/2FTZS5IX/article.html:text/html}
}

@article{mirkovic-does-2016,
	title = {Does {Sleep} {Improve} {Your} {Grammar}? {Preferential} {Consolidation} of {Arbitrary} {Components} of {New} {Linguistic} {Knowledge}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Does {Sleep} {Improve} {Your} {Grammar}?},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152489},
	doi = {10.1371/journal.pone.0152489},
	abstract = {We examined the role of sleep-related memory consolidation processes in learning new form-meaning mappings. Specifically, we examined a Complementary Learning Systems account, which implies that sleep-related consolidation should be more beneficial for new hippocampally dependent arbitrary mappings (e.g. new vocabulary items) relative to new systematic mappings (e.g. grammatical regularities), which can be better encoded neocortically. The hypothesis was tested using a novel language with an artificial grammatical gender system. Stem-referent mappings implemented arbitrary aspects of the new language, and determiner/suffix+natural gender mappings implemented systematic aspects (e.g.   tib    scoiff   esh    +  ballerina,   tib    mof   eem   + bride;   ked    jor   ool   + cowboy,   ked    heef   aff   + priest). Importantly, the determiner-gender and the suffix-gender mappings varied in complexity and salience, thus providing a range of opportunities to detect beneficial effects of sleep for this type of mapping. Participants were trained on the new language using a word-picture matching task, and were tested after a 2-hour delay which included sleep or wakefulness. Participants in the sleep group outperformed participants in the wake group on tests assessing memory for the arbitrary aspects of the new mappings (individual vocabulary items), whereas we saw no evidence of a sleep benefit in any of the tests assessing memory for the systematic aspects of the new mappings: Participants in both groups extracted the salient determiner-natural gender mapping, but not the more complex suffix-natural gender mapping. The data support the predictions of the complementary systems account and highlight the importance of the arbitrariness/systematicity dimension in the consolidation process for declarative memories.},
	number = {4},
	urldate = {2016-04-21},
	journal = {PLOS ONE},
	author = {Mirković, Jelena and Gaskell, M. Gareth},
	month = apr,
	year = {2016},
	keywords = {memory, language, language acquisition, Learning, Human learning, Memory consolidation, Recall (memory), Sleep},
	pages = {e0152489},
	file = {Mirković_Gaskell_2016_Does Sleep Improve Your Grammar.pdf:/home/user/Zotero/storage/2SUM5CW9/Mirković_Gaskell_2016_Does Sleep Improve Your Grammar.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3UVQ8IBZ/article.html:text/html}
}

@article{gu-continuous-2016,
	title = {Continuous {Deep} {Q}-{Learning} with {Model}-based {Acceleration}},
	url = {http://arxiv.org/abs/1603.00748},
	urldate = {2016-04-20},
	journal = {arXiv preprint arXiv:1603.00748},
	author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
	year = {2016},
	file = {1603.00748.pdf:/home/user/Zotero/storage/26D26WS7/1603.00748.pdf:application/pdf}
}

@article{neelakantan-adding-2015,
	title = {Adding {Gradient} {Noise} {Improves} {Learning} for {Very} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1511.06807},
	urldate = {2016-04-20},
	journal = {arXiv preprint arXiv:1511.06807},
	author = {Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V. and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
	year = {2015},
	file = {1511.06807.pdf:/home/user/Zotero/storage/U5PKGK5K/1511.06807.pdf:application/pdf}
}

@article{jaitly-online-2015-1,
	title = {An {Online} {Sequence}-to-{Sequence} {Model} {Using} {Partial} {Conditioning}},
	url = {http://arxiv.org/abs/1511.04868},
	urldate = {2016-04-20},
	journal = {arXiv preprint arXiv:1511.04868},
	author = {Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol and Sutskeyver, Ilya and Bengio, Samy},
	year = {2015},
	file = {1511.04868.pdf:/home/user/Zotero/storage/N3RD8Q2J/1511.04868.pdf:application/pdf}
}

@article{reichle-eye-1999,
	title = {Eye movement control in reading: accounting for initial fixation locations and refixations within the {E}-{Z} {Reader} model},
	volume = {39},
	issn = {0042-6989},
	shorttitle = {Eye movement control in reading},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698999001522},
	doi = {10.1016/S0042-6989(99)00152-2},
	abstract = {Reilly and O’Regan (1998, Vision Research, 38, 303–317) used computer simulations to evaluate how well several different word-targeting strategies could account for results which show that the distributions of fixation locations in reading are systematically related to low-level oculomotor variables, such as saccade distance and launch site [McConkie, Kerr, Reddix \&amp; Zola, (1988). Vision Research, 28, 1107–1118]. Their simulation results suggested that fixation locations are primarily determined by word length information, and that the processing of language, such as the identification of words, plays only a minimal role in deciding where to move the eyes. This claim appears to be problematic for our model of eye movement control in reading, E-Z Reader [Rayner, Reichle \&amp; Pollatsek (1998). Eye movement control in reading: an overview and model. In G. Underwood, Eye guidance in reading and scene perception (pp. 243–268). Oxford, UK: Elsevier; Reichle, Pollatsek, Fisher \&amp; Rayner (1998). Psychological Review, 105, 125–157], because it assumes that lexical access is the engine that drives the eyes forward during reading. However, we show that a newer version of E-Z Reader which still assumes that lexical access is the engine driving eye movements also predicts the locations of fixations and within-word refixations, and therefore provides a viable framework for understanding how both linguistic and oculomotor variables affect eye movements in reading.},
	number = {26},
	urldate = {2016-04-20},
	journal = {Vision Research},
	author = {Reichle, Erik D and Rayner, Keith and Pollatsek, Alexander},
	month = dec,
	year = {1999},
	keywords = {Reading, Eye movement control, E-Z Reader model, Initial fixation location, Refixation},
	pages = {4403--4411},
	file = {Reichle et al_1999_Eye movement control in reading.pdf:/home/user/Zotero/storage/98HG72FX/Reichle et al_1999_Eye movement control in reading.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/2AAJI9T4/S0042698999001522.html:text/html}
}

@article{urbat-one-2016,
	title = {One {Eilenberg} {Theorem} to {Rule} {Them} {All}},
	url = {http://arxiv.org/abs/1602.05831},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1602.05831},
	author = {Urbat, Henning and Adámek, Jiří and Chen, Liang-Ting and Milius, Stefan},
	year = {2016},
	file = {1602.05831.pdf:/home/user/Zotero/storage/F9UUM8N4/1602.05831.pdf:application/pdf}
}

@article{chen-profinite-2015-1,
	title = {Profinite {Monads}, {Profinite} {Equations} and {Reiterman}'s {Theorem}},
	url = {http://arxiv.org/abs/1511.02147},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1511.02147},
	author = {Chen, Liang-Ting and Adamek, Jiri and Milius, Stefan and Urbat, Henning},
	year = {2015},
	file = {profinitemonadsprofiniteequationsandreitermanstheorem.pdf:/home/user/Zotero/storage/M3U3IHSR/profinitemonadsprofiniteequationsandreitermanstheorem.pdf:application/pdf}
}

@article{polak-classification-2004,
	title = {A classification of rational languages by semilattice-ordered monoids},
	volume = {40},
	url = {http://www.emis.ams.org/journals/AM/04-4/am1151.pdf},
	number = {4},
	urldate = {2016-05-14},
	journal = {Arch. Math.(Brno)},
	author = {Polák, Libor and {others}},
	year = {2004},
	pages = {395--406},
	file = {am1151.pdf:/home/user/Zotero/storage/4Q7IAUEB/am1151.pdf:application/pdf}
}

@book{pin-theme-2011,
	title = {Theme and variations on the concatenation product},
	url = {http://link.springer.com/10.1007%2F978-3-642-21493-6\_3},
	urldate = {2016-05-14},
	publisher = {Springer},
	author = {Pin, Jean-Éric},
	year = {2011},
	file = {ThemeVariations.dvi - ThemeVariations.pdf:/home/user/Zotero/storage/8A6NS323/ThemeVariations.pdf:application/pdf}
}

@article{yagci-second-2015,
	title = {On the second homology of the {Schützenberger} product of monoids},
	volume = {39},
	issn = {13000098, 13036149},
	url = {http://online.journals.tubitak.gov.tr/openDoiPdf.htm?mKodu=mat-1503-79},
	doi = {10.3906/mat-1503-79},
	urldate = {2016-05-14},
	journal = {TURKISH JOURNAL OF MATHEMATICS},
	author = {YağCi, Melek and Bugay, Leyla and Ayik, Hayrullah},
	year = {2015},
	pages = {763--772},
	file = {mat-39-5-13-1503-79.pdf:/home/user/Zotero/storage/J94A9AIU/mat-39-5-13-1503-79.pdf:application/pdf}
}

@article{almeida-representation-2009,
	title = {Representation theory of finite semigroups, semigroup radicals and formal language theory},
	volume = {361},
	url = {http://www.ams.org/tran/2009-361-03/S0002-9947-08-04712-0/},
	number = {3},
	urldate = {2016-05-14},
	journal = {Transactions of the American Mathematical Society},
	author = {Almeida, Jorge and Margolis, Stuart and Steinberg, Benjamin and Volkov, Mikhail},
	year = {2009},
	pages = {1429--1461},
	file = {458_1.tif - chp%3A10.1007%2FBFb0029007.pdf:/home/user/Zotero/storage/I474TC2C/chp%3A10.1007%2FBFb0029007.pdf:application/pdf}
}

@article{pin-results-1992,
	title = {Some results on the generalized star-height problem},
	volume = {101},
	url = {http://www.sciencedirect.com/science/article/pii/089054019290063L},
	number = {2},
	urldate = {2016-05-14},
	journal = {Information and Computation},
	author = {Pin, Jean-Eric and Straubing, Howard and Thérien, Denis},
	year = {1992},
	pages = {219--250},
	file = {StarHeight.pdf:/home/user/Zotero/storage/76ETHVNT/StarHeight.pdf:application/pdf}
}

@article{oaksford-source-2016,
	title = {On the {Source} of {Human} {Irrationality}},
	volume = {20},
	issn = {1364-6613, 1879-307X},
	url = {http://www.cell.com/article/S1364661316000498/abstract},
	doi = {10.1016/j.tics.2016.03.002},
	abstract = {Reasoning and decision making are error prone. This is often attributed to a fast, phylogenetically old System 1. It is striking, however, that perceptuo-motor decision making in humans and animals is rational. These results are consistent with perceptuo-motor strategies emerging in Bayesian brain theory that also appear in human data selection. People seem to have access, although limited, to unconscious generative models that can generalise to explain other verbal reasoning results. Error does not emerge predominantly from System 1, but rather seems to emerge from the later evolved System 2 that involves working memory and language. However language also sows the seeds of error correction by moving reasoning into the social domain. This reversal of roles suggests key areas of theoretical integration and new empirical directions., System 1 is supposedly the main cause of human irrationality. However, recent work on animal decision making, human perceptuo-motor decision making, and logical intuitions shows that this phylogenetically older system is rational., Bayesian brain theory has recently proposed perceptuo-motor strategies identical to strategies proposed in Bayesian approaches to conscious verbal reasoning, suggesting that similar generative models are available at both levels., Recent approaches to conditional inference using causal Bayes nets confirm this account, which can also generalise to logical intuitions., People have only imperfect access to System 1. Errors arise from inadequate interrogation of System 1, working memory limitations, and mis-description of our records of these interrogations. However, there is evidence that such errors may be corrected by moving reasoning to the social domain facilitated by language.},
	language = {English},
	number = {5},
	urldate = {2016-05-14},
	journal = {Trends in Cognitive Sciences},
	author = {Oaksford, Mike and Hall, Simon},
	month = may,
	year = {2016},
	pages = {336--344},
	file = {Oaksford_Hall_2016_On the Source of Human Irrationality.pdf:/home/user/Zotero/storage/U6ISR4KX/Oaksford_Hall_2016_On the Source of Human Irrationality.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/INVKPW55/S1364-6613(16)00049-8.html:text/html}
}

@misc{noauthor-[1605.03705]-nodate,
	title = {[1605.03705] {Movie} {Description}},
	url = {https://arxiv.org/abs/1605.03705},
	urldate = {2016-05-13},
	file = {[1605.03705] Movie Description:/home/user/Zotero/storage/I63BNWMZ/1605.html:text/html}
}

@misc{noauthor-[1605.03956]-nodate,
	title = {[1605.03956] {On} the {Convergent} {Properties} of {Word} {Embedding} {Methods}},
	url = {https://arxiv.org/abs/1605.03956},
	urldate = {2016-05-13},
	file = {[1605.03956] On the Convergent Properties of Word Embedding Methods:/home/user/Zotero/storage/UKKX7WQ2/1605.html:text/html}
}

@misc{noauthor-[1605.03835]-nodate,
	title = {[1605.03835] {Noisy} {Parallel} {Approximate} {Decoding} for {Conditional} {Recurrent} {Language} {Model}},
	url = {https://arxiv.org/abs/1605.03835},
	urldate = {2016-05-13},
	file = {[1605.03835] Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model:/home/user/Zotero/storage/ZPWWAGZ7/1605.html:text/html}
}

@misc{noauthor-[1605.03832]-nodate,
	title = {[1605.03832] {Polyglot} {Neural} {Language} {Models}: {A} {Case} {Study} in {Cross}-{Lingual} {Phonetic} {Representation} {Learning}},
	url = {https://arxiv.org/abs/1605.03832},
	urldate = {2016-05-13},
	file = {[1605.03832] Polyglot Neural Language Models\: A Case Study in Cross-Lingual Phonetic Representation Learning:/home/user/Zotero/storage/G6SHXHWH/1605.html:text/html}
}

@article{tian-machine-2016,
	title = {Machine {Comprehension} {Based} on {Learning} to {Rank}},
	url = {http://arxiv.org/abs/1605.03284},
	abstract = {Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest. However, this dataset is too simple and too small for learning true reasoning abilities. \cite{hermann2015teaching} therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data.},
	urldate = {2016-05-12},
	journal = {arXiv:1605.03284 [cs]},
	author = {Tian, Tian and Li, Yuezhang},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03284},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6WEURMCI/1605.html:text/html;Tian_Li_2016_Machine Comprehension Based on Learning to Rank.pdf:/home/user/Zotero/storage/T7GJHIST/Tian_Li_2016_Machine Comprehension Based on Learning to Rank.pdf:application/pdf}
}

@article{mi-vocabulary-2016,
	title = {Vocabulary {Manipulation} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1605.03209},
	abstract = {In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).},
	urldate = {2016-05-12},
	journal = {arXiv:1605.03209 [cs]},
	author = {Mi, Haitao and Wang, Zhiguo and Ittycheriah, Abe},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03209},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/48AKNEU5/1605.html:text/html;Mi et al_2016_Vocabulary Manipulation for Neural Machine Translation.pdf:/home/user/Zotero/storage/W3FEUTZG/Mi et al_2016_Vocabulary Manipulation for Neural Machine Translation.pdf:application/pdf}
}

@article{straubing-generalization-1981,
	title = {A generalization of the {Schützenberger} product of finite monoids},
	volume = {13},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/0304397581900360},
	doi = {10.1016/0304-3975(81)90036-0},
	abstract = {For each n⩾1, an n-ary product ♢ on finite monoids is constructed. This product has the following property: Let Σ be a finite alphabet and Σ∗ the free monoid generated by Σ. For i = 1, …,n, let Ai be a recognizable subset of Σ∗, M(Ai) the syntactic monoid of An and M(A1⋯An) the syntactic monoid of the concatenation product A1⋯An. Then M(A1⋯An){\textless} ♢ (M(A1),…,M(An)). The case n = 2 was studied by Schützenberger. As an application of the generalized product, I prove the theorem of Brzozowski and Knast that the dot-depth hierarchy of star-free sets is infinite.},
	number = {2},
	urldate = {2016-05-12},
	journal = {Theoretical Computer Science},
	author = {Straubing, Howard},
	month = jan,
	year = {1981},
	pages = {137--150},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/VF7WXP4W/0304397581900360.html:text/html}
}

@incollection{polak-syntactic-2001,
	title = {Syntactic semiring of a language},
	url = {http://link.springer.com/chapter/10.1007/3-540-44683-4\_53},
	urldate = {2016-05-12},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 2001},
	publisher = {Springer},
	author = {Polák, Libor},
	year = {2001},
	pages = {611--620},
	file = {chp%3A10.1007%2F3-540-44683-4_53.pdf:/home/user/Zotero/storage/GFUDAPZD/chp%3A10.1007%2F3-540-44683-4_53.pdf:application/pdf}
}

@inproceedings{klima-schutzenberger-2010,
	title = {On {Schützenberger} products of semirings},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-14455-4\_26},
	urldate = {2016-05-12},
	booktitle = {Developments in {Language} {Theory}},
	publisher = {Springer},
	author = {Klíma, Ondřej and Polák, Libor},
	year = {2010},
	pages = {279--290},
	file = {Title - chp%3A10.1007%2F978-3-642-14455-4_26.pdf:/home/user/Zotero/storage/8DADZZVV/chp%3A10.1007%2F978-3-642-14455-4_26.pdf:application/pdf}
}

@misc{noauthor-[1605.03045]-nodate,
	title = {[1605.03045] {Definability} equals recognizability for graphs of bounded treewidth},
	url = {https://arxiv.org/abs/1605.03045},
	urldate = {2016-05-11},
	file = {[1605.03045] Definability equals recognizability for graphs of bounded treewidth:/home/user/Zotero/storage/JDF9TZXJ/1605.html:text/html}
}

@misc{noauthor-[1605.03148]-nodate,
	title = {[1605.03148] {A} {Coverage} {Embedding} {Model} for {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1605.03148},
	urldate = {2016-05-11},
	file = {[1605.03148] A Coverage Embedding Model for Neural Machine Translation:/home/user/Zotero/storage/UNTM8S8F/1605.html:text/html}
}

@article{sonoda-decoding-2016,
	title = {Decoding {Stacked} {Denoising} {Autoencoders}},
	url = {http://arxiv.org/abs/1605.02832},
	abstract = {Data representation in a stacked denoising autoencoder is investigated. Decoding is a simple technique for translating a stacked denoising autoencoder into a composition of denoising autoencoders in the ground space. In the infinitesimal limit, a composition of denoising autoencoders is reduced to a continuous denoising autoencoder, which is rich in analytic properties and geometric interpretation. For example, the continuous denoising autoencoder solves the backward heat equation and transports each data point so as to decrease entropy of the data distribution. Together with ridgelet analysis, an integral representation of a stacked denoising autoencoder is derived.},
	urldate = {2016-05-11},
	journal = {arXiv:1605.02832 [cs, stat]},
	author = {Sonoda, Sho and Murata, Noboru},
	month = may,
	year = {2016},
	note = {arXiv: 1605.02832},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/T3Q2UDTI/1605.html:text/html;Sonoda_Murata_2016_Decoding Stacked Denoising Autoencoders.pdf:/home/user/Zotero/storage/ZBPGR4FJ/Sonoda_Murata_2016_Decoding Stacked Denoising Autoencoders.pdf:application/pdf}
}

@article{henckell-prime-1988,
	title = {Prime decomposition theorem for arbitrary semigroups: general holonomy decomposition and synthesis theorem},
	volume = {55},
	issn = {0022-4049},
	shorttitle = {Prime decomposition theorem for arbitrary semigroups},
	url = {http://www.sciencedirect.com/science/article/pii/0022404988900436},
	doi = {10.1016/0022-4049(88)90043-6},
	abstract = {We generalize the holonomy form of the Prime Decomposition Theorem of Krohn and Rhodes for finite semigroups to arbitrary infinite semigroups. This is accomplished by embedding Š into an infinite Zeiger wreath product after applying the triple Schützenberger product which makes S finite-J-above (Rhodes' theorem).},
	number = {1},
	urldate = {2016-05-11},
	journal = {Journal of Pure and Applied Algebra},
	author = {Henckell, Karsten and Lazarus, Susan and Rhodes, John},
	month = nov,
	year = {1988},
	pages = {127--172},
	file = {PII\: 0022-4049(88)90043-6 - 1-s2.0-0022404988900436-main.pdf:/home/user/Zotero/storage/MM3XVQ28/1-s2.0-0022404988900436-main.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/BQCHCHJ7/0022404988900436.html:text/html}
}

@misc{noauthor-[1602.03483]-nodate,
	title = {[1602.03483] {Learning} {Distributed} {Representations} of {Sentences} from {Unlabelled} {Data}},
	url = {http://arxiv.org/abs/1602.03483},
	urldate = {2016-05-11},
	file = {[1602.03483] Learning Distributed Representations of Sentences from Unlabelled Data:/home/user/Zotero/storage/EBA5XQZV/1602.html:text/html}
}

@misc{noauthor-[1605.01810]-nodate,
	title = {[1605.01810] {Sch}\"utzenberger {Products} in a {Category}},
	url = {https://arxiv.org/abs/1605.01810},
	urldate = {2016-05-10},
	file = {[1605.01810] Sch\\"utzenberger Products in a Category:/home/user/Zotero/storage/PK7EMWTM/1605.html:text/html}
}

@article{rossman-average-case-2015,
	title = {An average-case depth hierarchy theorem for {Boolean} circuits},
	url = {http://arxiv.org/abs/1504.03398},
	abstract = {We prove an average-case depth hierarchy theorem for Boolean circuits over the standard basis of \$\mathsf{AND}\$, \$\mathsf{OR}\$, and \$\mathsf{NOT}\$ gates. Our hierarchy theorem says that for every \$d \geq 2\$, there is an explicit \$n\$-variable Boolean function \$f\$, computed by a linear-size depth-\$d\$ formula, which is such that any depth-\$(d-1)\$ circuit that agrees with \$f\$ on \$(1/2 + o\_n(1))\$ fraction of all inputs must have size \$\exp({n{\textasciicircum}{\Omega(1/d)}}).\$ This answers an open question posed by H{\aa}stad in his Ph.D. thesis. Our average-case depth hierarchy theorem implies that the polynomial hierarchy is infinite relative to a random oracle with probability 1, confirming a conjecture of H{\aa}stad, Cai, and Babai. We also use our result to show that there is no "approximate converse" to the results of Linial, Mansour, Nisan and Boppana on the total influence of small-depth circuits, thus answering a question posed by O'Donnell, Kalai, and Hatami. A key ingredient in our proof is a notion of \emph{random projections} which generalize random restrictions.},
	urldate = {2016-05-10},
	journal = {arXiv:1504.03398 [cs]},
	author = {Rossman, Benjamin and Servedio, Rocco A. and Tan, Li-Yang},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.03398},
	keywords = {Computer Science - Computational Complexity},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/U77PE9D4/1504.html:text/html;Rossman et al_2015_An average-case depth hierarchy theorem for Boolean circuits.pdf:/home/user/Zotero/storage/57UW6K4Z/Rossman et al_2015_An average-case depth hierarchy theorem for Boolean circuits.pdf:application/pdf}
}

@article{martens-optimizing-2015,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-factored {Approximate} {Curvature}},
	url = {http://arxiv.org/abs/1503.05671},
	abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
	urldate = {2016-05-10},
	journal = {arXiv:1503.05671 [cs, stat]},
	author = {Martens, James and Grosse, Roger},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.05671},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/I4P2EXHH/1503.html:text/html;Martens_Grosse_2015_Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf:/home/user/Zotero/storage/M4Q5IR6K/Martens_Grosse_2015_Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf:application/pdf}
}

@misc{noauthor-[1401.4025]-nodate,
	title = {[1401.4025] {Unambiguous} {Buchi} is weak},
	url = {https://arxiv.org/abs/1401.4025},
	urldate = {2016-05-10},
	file = {[1401.4025] Unambiguous Buchi is weak:/home/user/Zotero/storage/9DBN5JN4/1401.html:text/html}
}

@misc{noauthor-[1602.07572]-nodate,
	title = {[1602.07572] {Ultradense} {Word} {Embeddings} by {Orthogonal} {Transformation}},
	url = {https://arxiv.org/abs/1602.07572},
	urldate = {2016-05-10},
	file = {[1602.07572] Ultradense Word Embeddings by Orthogonal Transformation:/home/user/Zotero/storage/WNT5WUVD/1602.html:text/html}
}

@misc{noauthor-[1605.02697]-nodate,
	title = {[1605.02697] {Ask} {Your} {Neurons}: {A} {Deep} {Learning} {Approach} to {Visual} {Question} {Answering}},
	url = {https://arxiv.org/abs/1605.02697},
	urldate = {2016-05-10},
	file = {[1605.02697] Ask Your Neurons\: A Deep Learning Approach to Visual Question Answering:/home/user/Zotero/storage/FF6JZHP4/1605.html:text/html}
}

@misc{noauthor-visual-nodate,
	title = {The visual word form area {\textbar} {Brain}},
	url = {https://brain.oxfordjournals.org/content/123/2/291.full},
	urldate = {2016-05-10},
	file = {The visual word form area | Brain:/home/user/Zotero/storage/IK4XFFU6/291.html:text/html}
}

@article{hubbard-interactions-2005,
	title = {Interactions between number and space in parietal cortex},
	volume = {6},
	url = {http://www.nature.com/nrn/journal/v6/n6/abs/nrn1684.html},
	number = {6},
	urldate = {2016-05-10},
	journal = {Nature Reviews Neuroscience},
	author = {Hubbard, Edward M. and Piazza, Manuela and Pinel, Philippe and Dehaene, Stanislas},
	year = {2005},
	pages = {435--448},
	file = {[HTML] from nature.com:/home/user/Zotero/storage/VAN62CMG/nrn1684.html:text/html;Snapshot:/home/user/Zotero/storage/TTSE899P/nrn1684.html:text/html}
}

@misc{noauthor-languagespecific-nodate,
	title = {Language‐specific tuning of visual cortex? {Functional} properties of the {Visual} {Word} {Form} {Area} {\textbar} {Brain}},
	url = {http://brain.oxfordjournals.org/content/125/5/1054.full},
	urldate = {2016-05-10},
	file = {Language‐specific tuning of visual cortex? Functional properties of the Visual Word Form Area | Brain:/home/user/Zotero/storage/JMZBEC3G/1054.html:text/html}
}

@misc{noauthor-what-nodate-3,
	title = {What does it mean to predict one's own utterances? [{Commentary} on {Pickering} \&amp; {Garrod}] — {Max} {Planck} {Institute} for {Psycholinguistics}},
	url = {http://www.mpi.nl/publications/escidoc-1539528},
	urldate = {2016-05-10},
	file = {What does it mean to predict one's own utterances? [Commentary on Pickering &amp\; Garrod] — Max Planck Institute for Psycholinguistics:/home/user/Zotero/storage/HZCKIFNQ/escidoc-1539528.html:text/html}
}

@article{wen-network-based-2016-1,
	title = {A {Network}-based {End}-to-{End} {Trainable} {Task}-oriented {Dialogue} {System}},
	url = {http://arxiv.org/abs/1604.04562},
	abstract = {Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring labelled datasets and solving a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable dialogue system along with a new way of collecting task-oriented dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.},
	urldate = {2016-05-10},
	journal = {arXiv:1604.04562 [cs, stat]},
	author = {Wen, Tsung-Hsien and Gasic, Milica and Mrksic, Nikola and Rojas-Barahona, Lina M. and Su, Pei-Hao and Ultes, Stefan and Vandyke, David and Young, Steve},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.04562},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AJU9IR6U/1604.html:text/html;Wen et al_2016_A Network-based End-to-End Trainable Task-oriented Dialogue System.pdf:/home/user/Zotero/storage/RHQCIPHZ/Wen et al_2016_A Network-based End-to-End Trainable Task-oriented Dialogue System.pdf:application/pdf}
}

@article{mckenzie-hippocampal-2014,
	title = {Hippocampal {Representation} of {Related} and {Opposing} {Memories} {Develop} within {Distinct}, {Hierarchically} {Organized} {Neural} {Schemas}},
	volume = {83},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731400405X},
	doi = {10.1016/j.neuron.2014.05.019},
	abstract = {Summary
Recent evidence suggests that the hippocampus may integrate overlapping memories into relational representations, or schemas, that link indirectly related events and support flexible memory expression. Here we explored the nature of hippocampal neural population representations for multiple features of events and the locations and contexts in which they occurred. Hippocampal networks developed hierarchical organizations of associated elements of related but separately acquired memories within a context, and distinct organizations for memories where the contexts differentiated object-reward associations. These findings reveal neural mechanisms for the development and organization of relational representations.
Video Abstract},
	number = {1},
	urldate = {2016-05-09},
	journal = {Neuron},
	author = {McKenzie, Sam and Frank, Andrea J. and Kinsky, Nathaniel R. and Porter, Blake and Rivière, Pamela D. and Eichenbaum, Howard},
	month = jul,
	year = {2014},
	pages = {202--215},
	file = {McKenzie et al_2014_Hippocampal Representation of Related and Opposing Memories Develop within.pdf:/home/user/Zotero/storage/BRX9RCZD/McKenzie et al_2014_Hippocampal Representation of Related and Opposing Memories Develop within.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/GMTPFSW3/S089662731400405X.html:text/html}
}

@article{sadtler-neural-2014,
	title = {Neural constraints on learning},
	volume = {512},
	issn = {1476-4687},
	doi = {10.1038/nature13665},
	abstract = {Learning, whether motor, sensory or cognitive, requires networks of neurons to generate new activity patterns. As some behaviours are easier to learn than others, we asked if some neural activity patterns are easier to generate than others. Here we investigate whether an existing network constrains the patterns that a subset of its neurons is capable of exhibiting, and if so, what principles define this constraint. We employed a closed-loop intracortical brain-computer interface learning paradigm in which Rhesus macaques (Macaca mulatta) controlled a computer cursor by modulating neural activity patterns in the primary motor cortex. Using the brain-computer interface paradigm, we could specify and alter how neural activity mapped to cursor velocity. At the start of each session, we observed the characteristic activity patterns of the recorded neural population. The activity of a neural population can be represented in a high-dimensional space (termed the neural space), wherein each dimension corresponds to the activity of one neuron. These characteristic activity patterns comprise a low-dimensional subspace (termed the intrinsic manifold) within the neural space. The intrinsic manifold presumably reflects constraints imposed by the underlying neural circuitry. Here we show that the animals could readily learn to proficiently control the cursor using neural activity patterns that were within the intrinsic manifold. However, animals were less able to learn to proficiently control the cursor using activity patterns that were outside of the intrinsic manifold. These results suggest that the existing structure of a network can shape learning. On a timescale of hours, it seems to be difficult to learn to generate neural activity patterns that are not consistent with the existing network structure. These findings offer a network-level explanation for the observation that we are more readily able to learn new skills when they are related to the skills that we already possess.},
	language = {eng},
	number = {7515},
	journal = {Nature},
	author = {Sadtler, Patrick T. and Quick, Kristin M. and Golub, Matthew D. and Chase, Steven M. and Ryu, Stephen I. and Tyler-Kabara, Elizabeth C. and Yu, Byron M. and Batista, Aaron P.},
	month = aug,
	year = {2014},
	pmid = {25164754},
	pmcid = {PMC4393644},
	keywords = {Learning, Male, Models, Neurological, Animals, Brain-Computer Interfaces, Computers, Macaca mulatta, Motor Cortex, Motor Skills, Nerve Net, Neurons},
	pages = {423--426}
}

@article{rich-place-2014,
	title = {Place cells. {Large} environments reveal the statistical structure governing hippocampal representations},
	volume = {345},
	issn = {1095-9203},
	doi = {10.1126/science.1255635},
	abstract = {The rules governing the formation of spatial maps in the hippocampus have not been determined. We investigated the large-scale structure of place field activity by recording hippocampal neurons in rats exploring a previously unencountered 48-meter-long track. Single-cell and population activities were well described by a two-parameter stochastic model. Individual neurons had their own characteristic propensity for forming fields randomly along the track, with some cells expressing many fields and many exhibiting few or none. Because of the particular distribution of propensities across cells, the number of neurons with fields scaled logarithmically with track length over a wide, ethological range. These features constrain hippocampal memory mechanisms, may allow efficient encoding of environments and experiences of vastly different extents and durations, and could reflect general principles of population coding.},
	language = {eng},
	number = {6198},
	journal = {Science (New York, N.Y.)},
	author = {Rich, P. Dylan and Liaw, Hua-Peng and Lee, Albert K.},
	month = aug,
	year = {2014},
	pmid = {25124440},
	keywords = {memory, Male, Poisson Distribution, Animals, Action Potentials, Brain Mapping, CA1 Region, Hippocampal, Electrodes, Implanted, Exploratory Behavior, Maze Learning, Orientation, Pyramidal Cells, Rats, Rats, Long-Evans, Space Perception},
	pages = {814--817}
}

@article{schlenker-unity-2016,
	title = {The {Unity} of {Focus}: {Evidence} from {Sign} {Language} ({ASL} and {LSF})},
	volume = {47},
	issn = {0024-3892},
	shorttitle = {The {Unity} of {Focus}},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00215},
	doi = {10.1162/LING\_a\_00215},
	number = {2},
	urldate = {2016-05-09},
	journal = {Linguistic Inquiry},
	author = {Schlenker, Philippe and Aristodemo, Valentina and Ducasse, Ludovic and Lamberton, Jonathan and Santoro, Mirko},
	month = apr,
	year = {2016},
	pages = {363--381},
	file = {Snapshot:/home/user/Zotero/storage/WZ4G9R9I/LING_a_00215.html:text/html}
}

@article{rasin-evaluation-2016,
	title = {On {Evaluation} {Metrics} in {Optimality} {Theory}},
	volume = {47},
	issn = {0024-3892},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/LING\_a\_00210},
	doi = {10.1162/LING\_a\_00210},
	abstract = {We develop an evaluation metric for Optimality Theory that allows a learner to induce a lexicon and a phonological grammar from unanalyzed surface forms. We wish to model aspects of knowledge such as the English-speaking child’s knowledge that the aspiration of the first segment of khæt is predictable and the French-speaking child’s knowledge that the final l of table ‘table’ is optional and can be deleted while that of parle ‘speak’ cannot. We show that the learner we present succeeds in obtaining this kind of knowledge and is better equipped to do so than other existing learners in the literature.},
	number = {2},
	urldate = {2016-05-09},
	journal = {Linguistic Inquiry},
	author = {Rasin, Ezer and Katzir, Roni},
	month = apr,
	year = {2016},
	pages = {235--282},
	file = {Snapshot:/home/user/Zotero/storage/D5IFJKKK/LING_a_00210.html:text/html}
}

@article{ferrer-i-cancho-compression-2015,
	title = {Compression and the origins of {Zipf}'s law of abbreviation},
	url = {http://arxiv.org/abs/1504.04884},
	abstract = {Languages across the world exhibit Zipf's law of abbreviation, namely more frequent words tend to be shorter. The generalized version of the law - an inverse relationship between the frequency of a unit and its magnitude - holds also for the behaviours of other species and the genetic code. The apparent universality of this pattern in human language and its ubiquity in other domains calls for a theoretical understanding of its origins. To this end, we generalize the information theoretic concept of mean code length as a mean energetic cost function over the probability and the magnitude of the types of the repertoire. We show that the minimization of that cost function and a negative correlation between probability and the magnitude of types are intimately related.},
	urldate = {2016-05-09},
	journal = {arXiv:1504.04884 [physics]},
	author = {Ferrer-i-Cancho, R. and Bentz, C. and Seguin, C.},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.04884},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks, Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WEA2QFPP/1504.html:text/html;Ferrer-i-Cancho et al_2015_Compression and the origins of Zipf's law of abbreviation.pdf:/home/user/Zotero/storage/3AHSW7U7/Ferrer-i-Cancho et al_2015_Compression and the origins of Zipf's law of abbreviation.pdf:application/pdf}
}

@article{ferrer-i-cancho-compression-2015-1,
	title = {Compression and the origins of {Zipf}'s law of abbreviation},
	url = {http://arxiv.org/abs/1504.04884},
	abstract = {Languages across the world exhibit Zipf's law of abbreviation, namely more frequent words tend to be shorter. The generalized version of the law - an inverse relationship between the frequency of a unit and its magnitude - holds also for the behaviours of other species and the genetic code. The apparent universality of this pattern in human language and its ubiquity in other domains calls for a theoretical understanding of its origins. To this end, we generalize the information theoretic concept of mean code length as a mean energetic cost function over the probability and the magnitude of the types of the repertoire. We show that the minimization of that cost function and a negative correlation between probability and the magnitude of types are intimately related.},
	urldate = {2016-05-09},
	journal = {arXiv:1504.04884 [physics]},
	author = {Ferrer-i-Cancho, R. and Bentz, C. and Seguin, C.},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.04884},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks, Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XAEWXTVC/1504.html:text/html;Ferrer-i-Cancho et al_2015_Compression and the origins of Zipf's law of abbreviation.pdf:/home/user/Zotero/storage/5B3J2AWH/Ferrer-i-Cancho et al_2015_Compression and the origins of Zipf's law of abbreviation.pdf:application/pdf}
}

@article{ferrer-i-cancho-parallels-2016,
	title = {Parallels of human language in the behavior of bottlenose dolphins},
	url = {http://arxiv.org/abs/1605.01661},
	abstract = {A short review of similarities between dolphins and humans with the help of quantitative linguistics and information theory.},
	urldate = {2016-05-09},
	journal = {arXiv:1605.01661 [cs, q-bio]},
	author = {Ferrer-i-Cancho, R. and Lusseau, D. and McCowan, B.},
	month = may,
	year = {2016},
	note = {arXiv: 1605.01661},
	keywords = {Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BDMTFTRW/1605.html:text/html;Ferrer-i-Cancho et al_2016_Parallels of human language in the behavior of bottlenose dolphins.pdf:/home/user/Zotero/storage/NRZZAZK3/Ferrer-i-Cancho et al_2016_Parallels of human language in the behavior of bottlenose dolphins.pdf:application/pdf}
}

@article{brown-complexity-2016,
	title = {Complexity, {Action}, and {Black} {Holes}},
	volume = {93},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1512.04993},
	doi = {10.1103/PhysRevD.93.086006},
	abstract = {Our earlier paper "Complexity Equals Action" conjectured that the quantum computational complexity of a holographic state is given by the classical action of a region in the bulk (the `Wheeler-DeWitt' patch). We provide calculations for the results quoted in that paper, explain how it fits into a broader (tensor) network of ideas, and elaborate on the hypothesis that black holes are fastest computers in nature.},
	number = {8},
	urldate = {2016-05-04},
	journal = {Physical Review D},
	author = {Brown, Adam and Roberts, Daniel A. and Susskind, Leonard and Swingle, Brian and Zhao, Ying},
	month = apr,
	year = {2016},
	note = {arXiv: 1512.04993},
	keywords = {General Relativity and Quantum Cosmology, High Energy Physics - Theory, Quantum Physics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/N5HMTR7A/1512.html:text/html;Brown et al_2016_Complexity, Action, and Black Holes.pdf:/home/user/Zotero/storage/CG3WQ53X/Brown et al_2016_Complexity, Action, and Black Holes.pdf:application/pdf}
}

@article{van-dam-semantics-2016,
	title = {The {Semantics} of {Syntax}: {The} {Grounding} of {Transitive} and {Intransitive} {Constructions}},
	volume = {28},
	issn = {0898-929X},
	shorttitle = {The {Semantics} of {Syntax}},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/jocn\_a\_00926},
	doi = {10.1162/jocn\_a\_00926},
	abstract = {Embodied theories of language maintain that brain areas associated with perception and action are also involved in the processing and representation of word meaning. A number of studies have shown that sentences with action verbs elicit activation within sensory–motor brain regions, arguing that sentence-induced mental simulations provide a means for grounding their lexical-semantic meaning. Constructionist theories argue, however, that form–meaning correspondence is present not only at the lexical level but also at the level of constructions. We investigated whether sentence-induced motor resonance is present for syntactic constructions. We measured the BOLD signal while participants read sentences with (di)transitive (caused motion) or intransitive constructions that contained either action or abstract verbs. The results showed a distinct neuronal signature for caused motion and intransitive syntactic frames. Caused motion frames activated regions associated with reaching and grasping actions, including the left anterior intraparietal sulcus and the parietal reach region. Intransitive frames activated lateral temporal regions commonly associated with abstract word processing. The left pars orbitalis showed an interaction between the syntactic frame and verb class. These findings show that sensory–motor activation elicited by sentences entails both motor resonance evoked by single words as well as at the level of syntactic constructions.},
	number = {5},
	urldate = {2016-05-04},
	journal = {Journal of Cognitive Neuroscience},
	author = {van Dam, Wessel O. and Desai, Rutvik H.},
	month = jan,
	year = {2016},
	pages = {693--709},
	file = {Snapshot:/home/user/Zotero/storage/C3MASZEP/jocn_a_00926.html:text/html}
}

@article{white-investigating-2016,
	title = {Investigating practical linear temporal difference learning},
	url = {http://arxiv.org/abs/1602.08771},
	abstract = {Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use.},
	urldate = {2016-05-04},
	journal = {arXiv:1602.08771 [cs, stat]},
	author = {White, Adam and White, Martha},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.08771},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/U6N3XS6K/1602.html:text/html;White_White_2016_Investigating practical linear temporal difference learning.pdf:/home/user/Zotero/storage/73WVQ8VH/White_White_2016_Investigating practical linear temporal difference learning.pdf:application/pdf}
}

@article{louizos-structured-2016,
	title = {Structured and {Efficient} {Variational} {Deep} {Learning} with {Matrix} {Gaussian} {Posteriors}},
	url = {http://arxiv.org/abs/1603.04733},
	abstract = {We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.},
	urldate = {2016-05-04},
	journal = {arXiv:1603.04733 [cs, stat]},
	author = {Louizos, Christos and Welling, Max},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04733},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/82WR5EZV/1603.html:text/html;Louizos_Welling_2016_Structured and Efficient Variational Deep Learning with Matrix Gaussian.pdf:/home/user/Zotero/storage/DSAR57EX/Louizos_Welling_2016_Structured and Efficient Variational Deep Learning with Matrix Gaussian.pdf:application/pdf}
}

@article{yin-why-2016,
	title = {Why and {How} to {Pay} {Different} {Attention} to {Phrase} {Alignments} of {Different} {Intensities}},
	url = {http://arxiv.org/abs/1604.06896},
	urldate = {2016-05-04},
	journal = {arXiv preprint arXiv:1604.06896},
	author = {Yin, Wenpeng and Schütze, Hinrich},
	year = {2016},
	file = {1604.06896v1.pdf:/home/user/Zotero/storage/5RS89SFE/1604.06896v1.pdf:application/pdf}
}

@article{andreas-learning-2016-1,
	title = {Learning to {Compose} {Neural} {Networks} for {Question} {Answering}},
	url = {http://arxiv.org/abs/1601.01705},
	urldate = {2016-05-04},
	journal = {arXiv preprint arXiv:1601.01705},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	year = {2016},
	file = {1601.01705v1.pdf:/home/user/Zotero/storage/S7PZTMZ7/1601.01705v1.pdf:application/pdf}
}

@article{bouchard-approximate-2015,
	title = {On approximate reasoning capabilities of low-rank vector spaces},
	url = {http://sameersingh.org/files/papers/logicmf-krr15.pdf},
	urldate = {2016-05-04},
	journal = {AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches},
	author = {Bouchard, Guillaume and Singh, Sameer and Trouillon, Theo},
	year = {2015},
	file = {On Approximate Reasoning Capabilities of Low-Rank Vector Spaces - logicmf-krr15.pdf:/home/user/Zotero/storage/U38JHTDV/logicmf-krr15.pdf:application/pdf}
}

@article{gao-deep-2016,
	title = {Deep {Gate} {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1604.02910},
	urldate = {2016-05-04},
	journal = {arXiv preprint arXiv:1604.02910},
	author = {Gao, Yuan and Glowacka, Dorota},
	year = {2016},
	file = {1604.02910.pdf:/home/user/Zotero/storage/PXQ2TMEN/1604.02910.pdf:application/pdf}
}

@article{benzmuller-quantified-2013,
	title = {Quantified multimodal logics in simple type theory},
	volume = {7},
	url = {http://link.springer.com/article/10.1007/s11787-012-0052-y},
	number = {1},
	urldate = {2016-05-04},
	journal = {Logica Universalis},
	author = {Benzmüller, Christoph and Paulson, Lawrence C.},
	year = {2013},
	pages = {7--20},
	file = {J23.pdf:/home/user/Zotero/storage/VGKKS3HV/J23.pdf:application/pdf}
}

@incollection{benzmuller-higher-order-2015,
	title = {Higher-order modal logics: {Automation} and applications},
	shorttitle = {Higher-order modal logics},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-21768-0\_2},
	urldate = {2016-05-04},
	booktitle = {Reasoning {Web}. {Web} {Logic} {Rules}},
	publisher = {Springer},
	author = {Benzmüller, Christoph and Paleo, Bruno Woltzenlogel},
	year = {2015},
	pages = {32--74},
	file = {C46.pdf:/home/user/Zotero/storage/JSKFCVNZ/C46.pdf:application/pdf}
}

@article{benzmuller-combining-2011,
	title = {Combining and automating classical and non-classical logics in classical higher-order logics},
	volume = {62},
	url = {http://link.springer.com/article/10.1007/s10472-011-9249-7},
	number = {1-2},
	urldate = {2016-05-04},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Benzmüller, Christoph},
	year = {2011},
	pages = {103--128},
	file = {J25.pdf:/home/user/Zotero/storage/BXB2QGDD/J25.pdf:application/pdf}
}

@article{sutcliffe-automated-2010,
	title = {Automated reasoning in higher-order logic using the {TPTP} {THF} infrastructure},
	volume = {3},
	url = {http://jfr.unibo.it/article/download/1710/1179},
	number = {1},
	urldate = {2016-05-04},
	journal = {Journal of Formalized Reasoning},
	author = {Sutcliffe, Geoff and Benzmüller, Christoph},
	year = {2010},
	pages = {1--27},
	file = {J22.pdf:/home/user/Zotero/storage/HDRAE8XX/J22.pdf:application/pdf}
}

@misc{noauthor-[0905.2435]-nodate,
	title = {[0905.2435] {Quantified} {Multimodal} {Logics} in {Simple} {Type} {Theory}},
	url = {http://arxiv.org/abs/0905.2435},
	urldate = {2016-05-04},
	file = {[0905.2435] Quantified Multimodal Logics in Simple Type Theory:/home/user/Zotero/storage/52SDCEF2/0905.html:text/html}
}

@misc{noauthor-[0905.4369]-nodate,
	title = {[0905.4369] {Automating} {Quantified} {Multimodal} {Logics} in {Simple} {Type} {Theory} -- {A} {Case} {Study}},
	url = {http://arxiv.org/abs/0905.4369},
	urldate = {2016-05-04},
	file = {[0905.4369] Automating Quantified Multimodal Logics in Simple Type Theory -- A Case Study:/home/user/Zotero/storage/XWNU3EA2/0905.html:text/html}
}

@incollection{blanchette-encoding-2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Encoding {Monomorphic} and {Polymorphic} {Types}},
	copyright = {©2013 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-36741-0 978-3-642-36742-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-36742-7\_34},
	abstract = {Most automatic theorem provers are restricted to untyped logics, and existing translations from typed logics are bulky or unsound. Recent research proposes monotonicity as a means to remove some clutter. Here we pursue this approach systematically, analysing formally a variety of encodings that further improve on efficiency while retaining soundness and completeness. We extend the approach to rank-1 polymorphism and present alternative schemes that lighten the translation of polymorphic symbols based on the novel notion of “cover”. The new encodings are implemented, and partly proved correct, in Isabelle/HOL. Our evaluation finds them vastly superior to previous schemes.},
	language = {en},
	number = {7795},
	urldate = {2016-05-04},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blanchette, Jasmin Christian and Böhme, Sascha and Popescu, Andrei and Smallbone, Nicholas},
	editor = {Piterman, Nir and Smolka, Scott A.},
	month = mar,
	year = {2013},
	doi = {10.1007/978-3-642-36742-7\_34},
	keywords = {Algorithm Analysis and Problem Complexity, Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {493--507},
	file = {Blanchette et al_2013_Encoding Monomorphic and Polymorphic Types.pdf:/home/user/Zotero/storage/CEVFMNSQ/Blanchette et al_2013_Encoding Monomorphic and Polymorphic Types.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/DJPVVMNK/10.html:text/html}
}

@article{sutcliffe-tptp-2009,
	title = {The {TPTP} {Problem} {Library} and {Associated} {Infrastructure}},
	volume = {43},
	issn = {0168-7433, 1573-0670},
	url = {http://link.springer.com/article/10.1007/s10817-009-9143-8},
	doi = {10.1007/s10817-009-9143-8},
	abstract = {This paper describes the First-Order Form (FOF) and Clause Normal Form (CNF) parts of the TPTP problem library, and the associated infrastructure. TPTP v3.5.0 was the last release containing only FOF and CNF problems, and thus serves as the exemplar. This paper summarizes the history and development of the TPTP, describes the structure and contents of the TPTP, and gives an overview of TPTP related projects and tools.},
	language = {en},
	number = {4},
	urldate = {2016-05-04},
	journal = {Journal of Automated Reasoning},
	author = {Sutcliffe, Geoff},
	month = jul,
	year = {2009},
	keywords = {Artificial Intelligence (incl. Robotics), ATP system evaluation, Mathematical Logic and Formal Languages, Mathematical Logic and Foundations, Symbolic and Algebraic Manipulation, TPTP},
	pages = {337--362},
	file = {Snapshot:/home/user/Zotero/storage/EXQP9I3Z/s10817-009-9143-8.html:text/html;Sutcliffe_2009_The TPTP Problem Library and Associated Infrastructure.pdf:/home/user/Zotero/storage/B3DZ2CPE/Sutcliffe_2009_The TPTP Problem Library and Associated Infrastructure.pdf:application/pdf}
}

@article{georgescu-black-2016,
	title = {Black holes: {Complexity} growth},
	volume = {12},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1745-2473},
	shorttitle = {Black holes},
	url = {http://www.nature.com/nphys/journal/v12/n5/full/nphys3763.html?WT.ec\_id=NPHYS-201605&spMailingID=51289241&spUserID=MTUyNDc1MTk2MTA2S0&spJobID=920348671&spReportId=OTIwMzQ4NjcxS0},
	doi = {10.1038/nphys3763},
	language = {en},
	number = {5},
	urldate = {2016-05-04},
	journal = {Nature Physics},
	author = {Georgescu, Iulia},
	month = may,
	year = {2016},
	keywords = {Quantum information, Theoretical physics},
	pages = {376--376},
	file = {Georgescu_2016_Black holes.pdf:/home/user/Zotero/storage/AVHSN9BF/Georgescu_2016_Black holes.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/U8MHAIRN/nphys3763.html:text/html}
}

@article{tervo-toward-2016,
	series = {Neurobiology of cognitive behavior},
	title = {Toward the neural implementation of structure learning},
	volume = {37},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S095943881600026X},
	doi = {10.1016/j.conb.2016.01.014},
	abstract = {Despite significant advances in neuroscience, the neural bases of intelligence remain poorly understood. Arguably the most elusive aspect of intelligence is the ability to make robust inferences that go far beyond one's experience. Animals categorize objects, learn to vocalize and may even estimate causal relationships — all in the face of data that is often ambiguous and sparse. Such inductive leaps are thought to result from the brain's ability to infer latent structure that governs the environment. However, we know little about the neural computations that underlie this ability. Recent advances in developing computational frameworks that can support efficient structure learning and inductive inference may provide insight into the underlying component processes and help pave the path for uncovering their neural implementation.},
	urldate = {2016-05-04},
	journal = {Current Opinion in Neurobiology},
	author = {Tervo, D Gowanlock R and Tenenbaum, Joshua B and Gershman, Samuel J},
	month = apr,
	year = {2016},
	pages = {99--105},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/D3JGES54/S095943881600026X.html:text/html;Tervo et al_2016_Toward the neural implementation of structure learning.pdf:/home/user/Zotero/storage/Z537749X/Tervo et al_2016_Toward the neural implementation of structure learning.pdf:application/pdf}
}

@article{haberkern-studying-2016,
	series = {Neurobiology of cognitive behavior},
	title = {Studying small brains to understand the building blocks of cognition},
	volume = {37},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438816000088},
	doi = {10.1016/j.conb.2016.01.007},
	abstract = {Cognition encompasses a range of higher-order mental processes, such as attention, working memory, and model-based decision-making. These processes are thought to involve the dynamic interaction of multiple central brain regions. A mechanistic understanding of such computations requires not only monitoring and manipulating specific neural populations during behavior, but also knowing the connectivity of the underlying circuitry. These goals are experimentally challenging in mammals, but are feasible in numerically simpler insect brains. In Drosophila melanogaster in particular, genetic tools enable precisely targeted physiology and optogenetics in actively behaving animals. In this article we discuss how these advantages are increasingly being leveraged to study abstract neural representations and sensorimotor computations that may be relevant for cognition in both insects and mammals.},
	urldate = {2016-05-04},
	journal = {Current Opinion in Neurobiology},
	author = {Haberkern, Hannah and Jayaraman, Vivek},
	month = apr,
	year = {2016},
	pages = {59--65},
	file = {Haberkern_Jayaraman_2016_Studying small brains to understand the building blocks of cognition.pdf:/home/user/Zotero/storage/HTD8BEP4/Haberkern_Jayaraman_2016_Studying small brains to understand the building blocks of cognition.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DTFWBPN7/S0959438816000088.html:text/html}
}

@article{karpova-editorial-2016,
	series = {Neurobiology of cognitive behavior},
	title = {Editorial overview: {Neurobiology} of cognitive behavior: {Complexity} of neural computation and cognition},
	volume = {37},
	issn = {0959-4388},
	shorttitle = {Editorial overview},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438816300125},
	doi = {10.1016/j.conb.2016.03.003},
	urldate = {2016-05-04},
	journal = {Current Opinion in Neurobiology},
	author = {Karpova, Alla and Kiani, Roozbeh},
	month = apr,
	year = {2016},
	pages = {v--viii},
	file = {Karpova_Kiani_2016_Editorial overview.pdf:/home/user/Zotero/storage/N4UEDMWA/Karpova_Kiani_2016_Editorial overview.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/MWA8JCRW/S0959438816300125.html:text/html}
}

@article{yin-why-2016-1,
	title = {Why and {How} to {Pay} {Different} {Attention} to {Phrase} {Alignments} of {Different} {Intensities}},
	url = {http://arxiv.org/abs/1604.06896},
	abstract = {This work studies comparatively two typical sentence pair classification tasks: textual entailment (TE) and answer selection (AS), observing that phrase alignments of different intensities contribute differently in these tasks. We address the problems of identifying phrase alignments of flexible granularity and pooling alignments of different intensities for these tasks. Examples for flexible granularity are alignments between two single words, between a single word and a phrase and between a short phrase and a long phrase. By intensity we roughly mean the degree of match, it ranges from identity over surface-form co-occurrence, rephrasing and other semantic relatedness to unrelated words as in lots of parenthesis text. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single attention mechanism over alignment intensities without considering the characteristics of specific tasks, which limits the system's effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific focusing of phrase alignments between two sentences by attention pooling. Experimental results on TE and AS match our observation and are state-of-the-art.},
	urldate = {2016-05-02},
	journal = {arXiv:1604.06896 [cs]},
	author = {Yin, Wenpeng and Schütze, Hinrich},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06896},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/P9ZJVXWR/1604.html:text/html;Yin_Schütze_2016_Why and How to Pay Different Attention to Phrase Alignments of Different.pdf:/home/user/Zotero/storage/IVWCP86I/Yin_Schütze_2016_Why and How to Pay Different Attention to Phrase Alignments of Different.pdf:application/pdf}
}

@article{gregor-towards-2016,
	title = {Towards {Conceptual} {Compression}},
	url = {http://arxiv.org/abs/1604.08772},
	abstract = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
	urldate = {2016-05-02},
	journal = {arXiv:1604.08772 [cs, stat]},
	author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08772},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3A9PZQN4/1604.html:text/html;Gregor et al_2016_Towards Conceptual Compression.pdf:/home/user/Zotero/storage/VA5GKW8C/Gregor et al_2016_Towards Conceptual Compression.pdf:application/pdf}
}

@article{baroni-frege-2014,
	title = {Frege in {Space}: {A} {Program} of {Compositional} {Distributional} {Semantics}},
	volume = {9},
	copyright = {Copyright (c)},
	issn = {1945-3604},
	shorttitle = {Frege in {Space}},
	url = {http://csli-lilt.stanford.edu/ojs/index.php/LiLT/article/view/6},
	abstract = {The lexicon of any natural language encodes a huge number of distinct word meanings. Just to understand this article, you will need to know what thousands of words mean. The space of possible sentential meanings is infinite: In this article alone, you will encounter many sentences that express ideas you have never heard before, we hope. Statistical semantics has addressed the issue of the vastness of word meaning by proposing methods to harvest meaning automatically from large collections of text (corpora). Formal semantics in the Fregean tradition has developed methods to account for the infinity of sentential meaning based on the crucial insight of compositionality, the idea that meaning of sentences is built incrementally by combining the meanings of their constituents. This article sketches a new approach to semantics that brings together ideas from statistical and formal semantics to account, in parallel, for the richness of lexical meaning and the combinatorial power of sentential semantics. We adopt, in particular, the idea that word meaning can be approximated by the patterns of co-occurrence of words in corpora from statistical semantics, and the idea that compositionality can be captured in terms of a syntax-driven calculus of function application from formal semantics.},
	language = {en},
	number = {0},
	urldate = {2016-05-02},
	journal = {LiLT (Linguistic Issues in Language Technology)},
	author = {Baroni, Marco and Bernardi, Raffaela and Zamparelli, Roberto},
	year = {2014},
	keywords = {compositional semantics, distributional semantics},
	file = {Baroni et al_2014_Frege in Space.pdf:/home/user/Zotero/storage/AKQQCRE9/Baroni et al_2014_Frege in Space.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7KSZDS6G/6.html:text/html}
}

@article{ion-androutsopoulos-survey-2010,
	title = {A {Survey} of {Paraphrasing} and {Textual} {Entailment} {Methods}},
	volume = {38},
	url = {http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3804.pdf},
	urldate = {2016-05-01},
	journal = {Journal of Artificial Intelligence Research},
	author = {{Ion Androutsopoulos} and {Prodromos Malakasiotis}},
	year = {2010},
	pages = {135--187},
	file = {Using Local Alignments for Relation Recognition - JAIR-3804.pdf:/home/user/Zotero/storage/6QB77WC4/JAIR-3804.pdf:application/pdf}
}

@article{schmaltz-word-2016,
	title = {Word {Ordering} {Without} {Syntax}},
	url = {http://arxiv.org/abs/1604.08633},
	abstract = {Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.},
	urldate = {2016-05-02},
	journal = {arXiv:1604.08633 [cs]},
	author = {Schmaltz, Allen and Rush, Alexander M. and Shieber, Stuart M.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08633},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZF8KIU77/1604.html:text/html;Schmaltz et al_2016_Word Ordering Without Syntax.pdf:/home/user/Zotero/storage/FMA2JP45/Schmaltz et al_2016_Word Ordering Without Syntax.pdf:application/pdf}
}

@article{srinivas-taxonomy-2016,
	title = {A {Taxonomy} of {Deep} {Convolutional} {Neural} {Nets} for {Computer} {Vision}},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2015.00036/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Robotics\_and\_AI-w18-2016},
	doi = {10.3389/frobt.2015.00036},
	abstract = {Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative – that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e., deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision – convolutional neural networks (CNNs). We start with “AlexNet” as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision.},
	urldate = {2016-05-02},
	journal = {Vision Systems Theory, Tools and Applications},
	author = {Srinivas, Suraj and Sarvadevabhatla, Ravi Kiran and Mopuri, Konda Reddy and Prabhu, Nikita and Kruthiventi, Srinivas S. S. and Babu, R. Venkatesh},
	year = {2016},
	keywords = {deep learning, supervised learning, convolutional neural networks, object classification, recurrent neural networks},
	pages = {36}
}

@article{der-search-2016,
	title = {In {Search} for the {Neural} {Mechanisms} of {Individual} {Development}: {Behavior}-{Driven} {Differential} {Hebbian} {Learning}},
	shorttitle = {In {Search} for the {Neural} {Mechanisms} of {Individual} {Development}},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2015.00037/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Robotics\_and\_AI-w18-2016},
	doi = {10.3389/frobt.2015.00037},
	abstract = {When Donald Hebb published his 1949 book “The Organization of Behavior” he opened a new way of thinking in theoretical neuroscience that, in retrospective, is very close to contemporary ideas in self-organization. His metaphor of “wiring” together what “fires together” matches very closely the common paradigm that global organization can derive from simple local rules. While ingenious at his time and inspiring the research over decades, the results still fall short of the expectations. For instance, unsupervised as they are, such neural mechanisms should be able to explain and realize the self-organized acquisition of sensorimotor competencies. This paper proposes a new synaptic law that replaces Hebb’s original metaphor by that of “chaining together” what “changes together.” Starting from differential Hebbian learning, the new rule grounds the behavior of the agent directly in the internal synaptic dynamics. Therefore, one may call this a behavior-driven synaptic plasticity. Neurorobotics is an ideal testing ground for this new, unsupervised learning rule. This paper focuses on the close coupling between body, control, and environment in challenging physical settings. The examples demonstrate how the new synaptic mechanism induces a self-determined “search and converge” strategy in behavior space, generating spontaneously a variety of sensorimotor competencies. The emerging behavior patterns are qualified by involving body and environment in an irreducible conjunction with the internal mechanism. The results may not only be of immediate interest for the further development of embodied intelligence. They also offer a new view on the role of self-learning processes in natural evolution and in the brain. Videos and further details may be found under http://robot.informatik.uni-leipzig.de/research/supplementary/NeuroAutonomy/.},
	urldate = {2016-05-02},
	journal = {Computational Intelligence},
	author = {Der, Ralf},
	year = {2016},
	keywords = {Self-organization, neural networks, Learning, dynamical systems theory, robotics},
	pages = {37}
}

@article{dubois-building-2016,
	title = {Building a {Science} of {Individual} {Differences} from {fMRI}},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {http://www.cell.com/article/S1364661316300079/abstract},
	doi = {10.1016/j.tics.2016.03.014},
	language = {English},
	number = {0},
	urldate = {2016-05-02},
	journal = {Trends in Cognitive Sciences},
	author = {Dubois, Julien and Adolphs, Ralph},
	month = apr,
	year = {2016},
	keywords = {individual differences, prediction, BOLD fMRI, neurometrics, reliability, validity},
	file = {Dubois_Adolphs_2016_Building a Science of Individual Differences from fMRI.pdf:/home/user/Zotero/storage/PEBXES74/Dubois_Adolphs_2016_Building a Science of Individual Differences from fMRI.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/4ETG9FPG/S1364-6613(16)30007-9.html:text/html}
}

@article{burrows-eras-2014,
	title = {The {Eras} and {Trends} of {Automatic} {Short} {Answer} {Grading}},
	volume = {25},
	issn = {1560-4292, 1560-4306},
	url = {http://link.springer.com/article/10.1007/s40593-014-0026-8},
	doi = {10.1007/s40593-014-0026-8},
	abstract = {Automatic short answer grading (ASAG) is the task of assessing short natural language responses to objective questions using computational methods. The active research in this field has increased enormously of late with over 80 papers fitting a definition of ASAG. However, the past efforts have generally been ad-hoc and non-comparable until recently, hence the need for a unified view of the whole field. The goal of this paper is to address this aim with a comprehensive review of ASAG research and systems according to history and components. Our historical analysis identifies 35 ASAG systems within 5 temporal themes that mark advancement in methodology or evaluation. In contrast, our component analysis reviews 6 common dimensions from preprocessing to effectiveness. A key conclusion is that an era of evaluation is the newest trend in ASAG research, which is paving the way for the consolidation of the field.},
	language = {en},
	number = {1},
	urldate = {2016-05-01},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Burrows, Steven and Gurevych, Iryna and Stein, Benno},
	month = oct,
	year = {2014},
	keywords = {Artificial Intelligence (incl. Robotics), Natural language processing, Automatic grading, Computers and Education, Educational Technology, Short answer, User Interfaces and Human Computer Interaction},
	pages = {60--117},
	file = {Burrows et al_2014_The Eras and Trends of Automatic Short Answer Grading.pdf:/home/user/Zotero/storage/IKFTFTS9/Burrows et al_2014_The Eras and Trends of Automatic Short Answer Grading.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RBCSRPI5/s40593-014-0026-8.html:text/html}
}

@article{haley-mathematical-2016,
	title = {Mathematical physics: {Glitches} in time},
	volume = {532},
	copyright = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	shorttitle = {Mathematical physics},
	url = {http://www.nature.com/nature/journal/v532/n7600/full/532450a.html?WT.ec\_id=NATURE-20160428&spMailingID=51249830&spUserID=MTUyNDc1MTk2MTAzS0&spJobID=903461217&spReportId=OTAzNDYxMjE3S0},
	doi = {10.1038/532450a},
	abstract = {A mathematical technique has now been developed that reveals the underlying dynamics of time-dependent data collected with extreme temporal uncertainty, without using additional, costly instrumentation. See Letter p.471},
	language = {en},
	number = {7600},
	urldate = {2016-05-01},
	journal = {Nature},
	author = {Haley, Charlotte A. L.},
	month = apr,
	year = {2016},
	keywords = {Physics, Atomic and molecular physics, Mathematics and computing},
	pages = {450--451},
	file = {Haley_2016_Mathematical physics.pdf:/home/user/Zotero/storage/FHKPXRX6/Haley_2016_Mathematical physics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/S3727WPD/532450a.html:text/html}
}

@article{spychalska-investigating-2016,
	title = {Investigating scalar implicatures in a truth-value judgement task: evidence from event-related brain potentials},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Investigating scalar implicatures in a truth-value judgement task},
	url = {http://dx.doi.org/10.1080/23273798.2016.1161806},
	doi = {10.1080/23273798.2016.1161806},
	abstract = {It is considered underinformative to saySome As are B when it is known that allAs areB. Such underinformative sentences receive divergent truth-value judgements: whereas so-calledlogical responders evaluate them to be true,pragmatic responders reject them as false. In a sentence-picture verification experiment, we found that the split in the behavioural responses correlates with the difference in the event-related potentials (ERP) signal (N400 and P600) recorded for underinformative and for unambiguously true or false sentences withsome. However, the ERP patterns for sentences withall are similar for both groups. In contrast to previous findings, the effect is independent of the subjects' autistic spectrum quotients. Assuming that the N400 amplitude is inversely correlated with the expected probability of the critical word we argue that the observed between-group difference in the ERP pattern can be explained by the hypothesis that ‘logicians’ and ‘pragmatists’ use distinct verification strategies in evaluating sentences with the quantifiersome.},
	number = {0},
	urldate = {2016-05-01},
	journal = {Language, Cognition and Neuroscience},
	author = {Spychalska, Maria and Kontinen, Jarmo and Werning, Markus},
	month = apr,
	year = {2016},
	pages = {1--24},
	file = {Snapshot:/home/user/Zotero/storage/QV3GUSR5/23273798.2016.html:text/html;Spychalska et al_2016_Investigating scalar implicatures in a truth-value judgement task.pdf:/home/user/Zotero/storage/BZ7P3IKZ/Spychalska et al_2016_Investigating scalar implicatures in a truth-value judgement task.pdf:application/pdf}
}

@article{mccarthy-word-2016-1,
	title = {Word {Sense} {Clustering} and {Clusterability}},
	issn = {0891-2017},
	url = {http://dx.doi.org/10.1162/COLI\_a\_00247},
	doi = {10.1162/COLI\_a\_00247},
	abstract = {Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.},
	urldate = {2016-05-01},
	journal = {Computational Linguistics},
	author = {McCarthy, Diana and Apidianaki, Marianna and Erk, Katrin},
	month = mar,
	year = {2016},
	pages = {1--31},
	file = {Computational Linguistics Snapshot:/home/user/Zotero/storage/29JF92TP/COLI_a_00247.html:text/html;McCarthy et al_2016_Word Sense Clustering and Clusterability.pdf:/home/user/Zotero/storage/BQHD34S3/McCarthy et al_2016_Word Sense Clustering and Clusterability.pdf:application/pdf}
}

@article{arora-contrastive-2016,
	title = {Contrastive {Entropy}: {A} new evaluation metric for unnormalized language models},
	shorttitle = {Contrastive {Entropy}},
	url = {http://arxiv.org/abs/1601.00248},
	abstract = {Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric.},
	urldate = {2016-04-29},
	journal = {arXiv:1601.00248 [cs]},
	author = {Arora, Kushal and Rangarajan, Anand},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00248},
	keywords = {Computer Science - Computation and Language},
	file = {Arora_Rangarajan_2016_Contrastive Entropy.pdf:/home/user/Zotero/storage/I9899T23/Arora_Rangarajan_2016_Contrastive Entropy.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/W5GQJMRC/1601.html:text/html}
}

@article{sennrich-improving-2015-2,
	title = {Improving {Neural} {Machine} {Translation} {Models} with {Monolingual} {Data}},
	url = {http://arxiv.org/abs/1511.06709},
	abstract = {Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English{\textless}-{\textgreater}German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish-{\textgreater}English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English-{\textgreater}German.},
	urldate = {2016-04-29},
	journal = {arXiv:1511.06709 [cs]},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06709},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KIPKW49Q/1511.html:text/html;Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:/home/user/Zotero/storage/5VHB6CMT/Sennrich et al_2015_Improving Neural Machine Translation Models with Monolingual Data.pdf:application/pdf}
}

@article{xie-neural-2016,
	title = {Neural {Language} {Correction} with {Character}-{Based} {Attention}},
	url = {http://arxiv.org/abs/1603.09727},
	abstract = {Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art \$F\_{0.5}\$-score on the CoNLL 2014 Shared Task. We further demonstrate that training the network on additional data with synthesized errors can improve performance.},
	urldate = {2016-04-29},
	journal = {arXiv:1603.09727 [cs]},
	author = {Xie, Ziang and Avati, Anand and Arivazhagan, Naveen and Jurafsky, Dan and Ng, Andrew Y.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.09727},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3MM7XHF2/1603.html:text/html;Xie et al_2016_Neural Language Correction with Character-Based Attention.pdf:/home/user/Zotero/storage/SRT62UNF/Xie et al_2016_Neural Language Correction with Character-Based Attention.pdf:application/pdf}
}

@article{luan-lstm-2016,
	title = {{LSTM} based {Conversation} {Models}},
	url = {http://arxiv.org/abs/1603.09457},
	abstract = {In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.},
	urldate = {2016-04-29},
	journal = {arXiv:1603.09457 [cs]},
	author = {Luan, Yi and Ji, Yangfeng and Ostendorf, Mari},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.09457},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/JK8ABK3X/1603.html:text/html;Luan et al_2016_LSTM based Conversation Models.pdf:/home/user/Zotero/storage/EI7UU8QE/Luan et al_2016_LSTM based Conversation Models.pdf:application/pdf}
}

@article{hyona-background-2016,
	title = {Background {Speech} {Effects} on {Sentence} {Processing} during {Reading}: {An} {Eye} {Movement} {Study}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Background {Speech} {Effects} on {Sentence} {Processing} during {Reading}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152133},
	doi = {10.1371/journal.pone.0152133},
	abstract = {Effects of background speech on reading were examined by playing aloud different types of background speech, while participants read long, syntactically complex and less complex sentences embedded in text. Readers’ eye movement patterns were used to study online sentence comprehension. Effects of background speech were primarily seen in rereading time. In Experiment 1, foreign-language background speech did not disrupt sentence processing. Experiment 2 demonstrated robust disruption in reading as a result of semantically and syntactically anomalous scrambled background speech preserving normal sentence-like intonation. Scrambled speech that was constructed from the text to-be read did not disrupt reading more than scrambled speech constructed from a different, semantically unrelated text. Experiment 3 showed that scrambled speech exacerbated the syntactic complexity effect more than coherent background speech, which also interfered with reading. Experiment 4 demonstrated that both semantically and syntactically anomalous speech produced no more disruption in reading than semantically anomalous but syntactically correct background speech. The pattern of results is best explained by a semantic account that stresses the importance of similarity in semantic processing, but not similarity in semantic content, between the reading task and background speech.},
	number = {3},
	urldate = {2016-04-29},
	journal = {PLOS ONE},
	author = {Hyönä, Jukka and Ekholm, Miia},
	month = mar,
	year = {2016},
	keywords = {language, Syntax, Sentence processing, Semantics, Phonology, speech, Computer security, Conceptual semantics},
	pages = {e0152133},
	file = {Hyönä_Ekholm_2016_Background Speech Effects on Sentence Processing during Reading.pdf:/home/user/Zotero/storage/DV4S9BUC/Hyönä_Ekholm_2016_Background Speech Effects on Sentence Processing during Reading.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/I877ATGX/article.html:text/html}
}

@article{protzko-believing-2016,
	title = {Believing there is no free will corrupts intuitive cooperation},
	volume = {151},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716300427},
	doi = {10.1016/j.cognition.2016.02.014},
	abstract = {Regardless of whether free will exists, believing that it does affects one’s behavior. When an individual’s belief in free will is challenged, one can become more likely to act in an uncooperative manner. The mechanism behind the relationship between one’s belief in free will and behavior is still debated. The current study uses an economic contribution game under varying time constraints to elucidate whether reducing belief in free will allows one to justify negative behavior or if the effects occur at a more intuitive level of processing. Here we show that although people are intuitively cooperative, challenging their belief in free will corrupts this behavior, leading to impulsive selfishness. If given time to think, however, people are able to override the initial inclination toward self-interest induced by discouraging a belief in free will.},
	urldate = {2016-04-29},
	journal = {Cognition},
	author = {Protzko, John and Ouimette, Brett and Schooler, Jonathan},
	month = jun,
	year = {2016},
	keywords = {cooperation, Dual process, Free will, Morality},
	pages = {6--9},
	file = {Protzko et al_2016_Believing there is no free will corrupts intuitive cooperation.pdf:/home/user/Zotero/storage/PFZDIRRI/Protzko et al_2016_Believing there is no free will corrupts intuitive cooperation.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/AWPXSRKA/S0010027716300427.html:text/html}
}

@article{ishida-people-2016,
	title = {Some people are “{More} {Lexical}” than others},
	volume = {151},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716300634},
	doi = {10.1016/j.cognition.2016.03.008},
	abstract = {People can understand speech under poor conditions, even when successive pieces of the waveform are flipped in time. Using a new method to measure perception of such stimuli, we show that words with sounds based on rapid spectral changes (stop consonants) are much more impaired by reversing speech segments than words with fewer such sounds, and that words are much more resistant to disruption than pseudowords. We then demonstrate that this lexical advantage is more characteristic of some people than others. Participants listened to speech that was degraded in two very different ways, and we measured each person’s reliance on lexical support for each task. Listeners who relied on the lexicon for help in perceiving one kind of degraded speech also relied on the lexicon when dealing with a quite different kind of degraded speech. Thus, people differ in their relative reliance on the speech signal versus their pre-existing knowledge.},
	urldate = {2016-04-29},
	journal = {Cognition},
	author = {Ishida, Mako and Samuel, Arthur G. and Arai, Takayuki},
	month = jun,
	year = {2016},
	keywords = {Degraded speech, Lexical support for speech perception, Perceptual units},
	pages = {68--75},
	file = {Ishida et al_2016_Some people are “More Lexical” than others.pdf:/home/user/Zotero/storage/G553ZPDC/Ishida et al_2016_Some people are “More Lexical” than others.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/NDZN5ND7/S0010027716300634.html:text/html}
}

@inproceedings{nilsson-towards-2010,
	title = {Towards a data-driven model of eye movement control in reading},
	url = {http://dl.acm.org/citation.cfm?id=1870073},
	urldate = {2016-04-20},
	booktitle = {Proceedings of the 2010 workshop on cognitive modeling and computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Nilsson, Mattias and Nivre, Joakim},
	year = {2010},
	pages = {63--71},
	file = {Towards a Data-Driven Model of Eye Movement Control in Reading - W10-2008:/home/user/Zotero/storage/EA556JJ5/W10-2008.pdf:application/pdf}
}

@article{reichle-using-2009,
	title = {Using {E}-{Z} {Reader} to model the effects of higher level language processing on eye movements during reading},
	volume = {16},
	issn = {1069-9384, 1531-5320},
	url = {http://www.springerlink.com/index/10.3758/PBR.16.1.1},
	doi = {10.3758/PBR.16.1.1},
	language = {en},
	number = {1},
	urldate = {2016-04-20},
	journal = {Psychonomic Bulletin \& Review},
	author = {Reichle, E. D. and Warren, T. and McConnell, K.},
	month = feb,
	year = {2009},
	pages = {1--21},
	file = {Using E-Z reader to model the effects of higher level language processing on eye movements during reading - art%3A10.3758%2FPBR.16.1.1.pdf:/home/user/Zotero/storage/HKFW4V65/art%3A10.3758%2FPBR.16.1.1.pdf:application/pdf}
}

@article{engbert-swift:-2005-1,
	title = {{SWIFT}: {A} {Dynamical} {Model} of {Saccade} {Generation} {During} {Reading}.},
	volume = {112},
	issn = {1939-1471, 0033-295X},
	shorttitle = {{SWIFT}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.112.4.777},
	doi = {10.1037/0033-295X.112.4.777},
	language = {en},
	number = {4},
	urldate = {2016-04-20},
	journal = {Psychological Review},
	author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
	year = {2005},
	pages = {777--813},
	file = {engbert_etal_05.pdf:/home/user/Zotero/storage/CXP84532/engbert_etal_05.pdf:application/pdf}
}

@article{engbert-dynamical-2002-1,
	title = {A dynamical model of saccade generation in reading based on spatially distributed lexical processing},
	volume = {42},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698901003017},
	number = {5},
	urldate = {2016-04-20},
	journal = {Vision research},
	author = {Engbert, Ralf and Longtin, André and Kliegl, Reinhold},
	year = {2002},
	pages = {621--636},
	file = {PII\: S0042-6989(01)00301-7 - 1-s2.0-S0042698901003017-main.pdf:/home/user/Zotero/storage/RBFUWMAU/1-s2.0-S0042698901003017-main.pdf:application/pdf}
}

@article{reichle-toward-1998-1,
	title = {Toward a model of eye movement control in reading.},
	volume = {105},
	issn = {0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.105.1.125},
	doi = {10.1037/0033-295X.105.1.125},
	language = {en},
	number = {1},
	urldate = {2016-04-20},
	journal = {Psychological Review},
	author = {Reichle, Erik D. and Pollatsek, Alexander and Fisher, Donald L. and Rayner, Keith},
	year = {1998},
	pages = {125--157},
	file = {004635162e6ff8c240000000.pdf:/home/user/Zotero/storage/RDAK6J2R/004635162e6ff8c240000000.pdf:application/pdf}
}

@article{reichle-e-z-2003-2,
	title = {The {E}-{Z} reader model of eye-movement control in reading: comparisons to other models},
	volume = {26},
	issn = {0140-525X},
	shorttitle = {The {E}-{Z} reader model of eye-movement control in reading},
	abstract = {The E-Z Reader model (Reichle et al. 1998; 1999) provides a theoretical framework for understanding how word identification, visual processing, attention, and oculomotor control jointly determine when and where the eyes move during reading. In this article, we first review what is known about eye movements during reading. Then we provide an updated version of the model (E-Z Reader 7) and describe how it accounts for basic findings about eye movement control in reading. We then review several alternative models of eye movement control in reading, discussing both their core assumptions and their theoretical scope. On the basis of this discussion, we conclude that E-Z Reader provides the most comprehensive account of eye movement control during reading. Finally, we provide a brief overview of what is known about the neural systems that support the various components of reading, and suggest how the cognitive constructs of our model might map onto this neural architecture.},
	language = {eng},
	number = {4},
	journal = {The Behavioral and Brain Sciences},
	author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
	month = aug,
	year = {2003},
	pmid = {15067951},
	keywords = {Humans, Eye Movements, Reading, Models, Psychological, Fixation, Ocular, Models, Neurological, Oculomotor Muscles, Verbal Behavior, Vision, Binocular},
	pages = {445--476; discussion 477--526}
}

@article{hachaichi-logic-2016,
	title = {Logic for {Unambiguous} {Context}-{Free} {Languages}},
	url = {http://arxiv.org/abs/1604.04139},
	abstract = {We give in this paper a logical characterization for unambiguous Context Free Languages, in the vein of descriptive complexity. A fragment of the logic characterizing context free languages given by Lautemann, Schwentick and Th\'erien [18] based on implicit definability is used for this aim. We obtain a new connection between two undecidable problems, a logical one and a language theoretical one.},
	urldate = {2016-04-20},
	journal = {arXiv:1604.04139 [cs]},
	author = {Hachaïchi, Yassine},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.04139},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TQMXQ7MT/1604.html:text/html;Hachaïchi_2016_Logic for Unambiguous Context-Free Languages.pdf:/home/user/Zotero/storage/7K9ND5UW/Hachaïchi_2016_Logic for Unambiguous Context-Free Languages.pdf:application/pdf}
}

@article{hachaichi-logic-2016-1,
	title = {Logic for {Unambiguous} {Context}-{Free} {Languages}},
	url = {http://arxiv.org/abs/1604.04139},
	abstract = {We give in this paper a logical characterization for unambiguous Context Free Languages, in the vein of descriptive complexity. A fragment of the logic characterizing context free languages given by Lautemann, Schwentick and Th\'erien [18] based on implicit definability is used for this aim. We obtain a new connection between two undecidable problems, a logical one and a language theoretical one.},
	urldate = {2016-04-20},
	journal = {arXiv:1604.04139 [cs]},
	author = {Hachaïchi, Yassine},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.04139},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Q2EMJSWR/1604.html:text/html;Hachaïchi_2016_Logic for Unambiguous Context-Free Languages.pdf:/home/user/Zotero/storage/WTA94NDS/Hachaïchi_2016_Logic for Unambiguous Context-Free Languages.pdf:application/pdf}
}

@article{hu-harnessing-2016,
	title = {Harnessing {Deep} {Neural} {Networks} with {Logic} {Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	urldate = {2016-04-20},
	journal = {arXiv:1603.06318 [cs, stat]},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.06318},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AU3URAR8/1603.html:text/html;Hu et al_2016_Harnessing Deep Neural Networks with Logic Rules.pdf:/home/user/Zotero/storage/KHNNQZDX/Hu et al_2016_Harnessing Deep Neural Networks with Logic Rules.pdf:application/pdf}
}

@article{ravanbakhsh-stochastic-2015,
	title = {Stochastic {Neural} {Networks} with {Monotonic} {Activation} {Functions}},
	url = {http://arxiv.org/abs/1601.00034},
	abstract = {We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units.},
	urldate = {2016-04-20},
	journal = {arXiv:1601.00034 [cs, stat]},
	author = {Ravanbakhsh, Siamak and Poczos, Barnabas and Schneider, Jeff and Schuurmans, Dale and Greiner, Russell},
	month = dec,
	year = {2015},
	note = {arXiv: 1601.00034},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/JWXUE5T6/1601.html:text/html;Ravanbakhsh et al_2015_Stochastic Neural Networks with Monotonic Activation Functions.pdf:/home/user/Zotero/storage/2VMG7J5Q/Ravanbakhsh et al_2015_Stochastic Neural Networks with Monotonic Activation Functions.pdf:application/pdf}
}

@article{scherbakov-incremental-2016,
	title = {From {Incremental} {Meaning} to {Semantic} {Unit} (phrase by phrase)},
	url = {http://arxiv.org/abs/1604.04873},
	abstract = {This paper describes an experimental approach to Detection of Minimal Semantic Units and their Meaning (DiMSUM), explored within the framework of SemEval 2016 Task 10. The approach is primarily based on a combination of word embeddings and parserbased features, and employs unidirectional incremental computation of compositional embeddings for multiword expressions.},
	urldate = {2016-04-20},
	journal = {arXiv:1604.04873 [cs]},
	author = {Scherbakov, Andreas and Vylomova, Ekaterina and Liu, Fei and Baldwin, Timothy},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.04873},
	keywords = {Computer Science - Computation and Language, I.2.7, 68T50},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XFRKFGX4/1604.html:text/html;Scherbakov et al_2016_From Incremental Meaning to Semantic Unit (phrase by phrase).pdf:/home/user/Zotero/storage/TCAQGSXJ/Scherbakov et al_2016_From Incremental Meaning to Semantic Unit (phrase by phrase).pdf:application/pdf}
}

@article{wan-match-srnn:-2016,
	title = {Match-{SRNN}: {Modeling} the {Recursive} {Matching} {Structure} with {Spatial} {RNN}},
	shorttitle = {Match-{SRNN}},
	url = {http://arxiv.org/abs/1604.04378},
	abstract = {Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.},
	urldate = {2016-04-20},
	journal = {arXiv:1604.04378 [cs]},
	author = {Wan, Shengxian and Lan, Yanyan and Xu, Jun and Guo, Jiafeng and Pang, Liang and Cheng, Xueqi},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.04378},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C84IM9TF/1604.html:text/html;Wan et al_2016_Match-SRNN.pdf:/home/user/Zotero/storage/5A2BGFIV/Wan et al_2016_Match-SRNN.pdf:application/pdf}
}

@article{baronchelli-sharp-2006,
	title = {Sharp transition towards shared vocabularies in multi-agent systems},
	volume = {2006},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/physics/0509075},
	doi = {10.1088/1742-5468/2006/06/P06014},
	abstract = {What processes can explain how very large populations are able to converge on the use of a particular word or grammatical construction without global coordination? Answering this question helps to understand why new language constructs usually propagate along an S-shaped curve with a rather sudden transition towards global agreement. It also helps to analyze and design new technologies that support or orchestrate self-organizing communication systems, such as recent social tagging systems for the web. The article introduces and studies a microscopic model of communicating autonomous agents performing language games without any central control. We show that the system undergoes a disorder/order transition, going trough a sharp symmetry breaking process to reach a shared set of conventions. Before the transition, the system builds up non-trivial scale-invariant correlations, for instance in the distribution of competing synonyms, which display a Zipf-like law. These correlations make the system ready for the transition towards shared conventions, which, observed on the time-scale of collective behaviors, becomes sharper and sharper with system size. This surprising result not only explains why human language can scale up to very large populations but also suggests ways to optimize artificial semiotic dynamics.},
	number = {06},
	urldate = {2016-04-17},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Baronchelli, A. and Felici, M. and Caglioti, E. and Loreto, V. and Steels, L.},
	month = jun,
	year = {2006},
	note = {arXiv: physics/0509075},
	keywords = {Computer Science - Computer Science and Game Theory, Physics - Physics and Society, Computer Science - Multiagent Systems, Condensed Matter - Statistical Mechanics},
	pages = {P06014--P06014},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Z9IPD8BK/0509075.html:text/html;Baronchelli et al_2006_Sharp transition towards shared vocabularies in multi-agent systems.pdf:/home/user/Zotero/storage/5XTK8CKK/Baronchelli et al_2006_Sharp transition towards shared vocabularies in multi-agent systems.pdf:application/pdf}
}

@article{de-vylder-how-2006,
	title = {How to reach linguistic consensus: {A} proof of convergence for the naming game},
	volume = {242},
	issn = {0022-5193},
	shorttitle = {How to reach linguistic consensus},
	url = {http://www.sciencedirect.com/science/article/pii/S0022519306002189},
	doi = {10.1016/j.jtbi.2006.05.024},
	abstract = {In this paper we introduce a mathematical model of naming games. Naming games have been widely used within research on the origins and evolution of language. Despite the many interesting empirical results these studies have produced, most of this research lacks a formal elucidating theory. In this paper we show how a population of agents can reach linguistic consensus, i.e. learn to use one common language to communicate with one another. Our approach differs from existing formal work in two important ways: one, we relax the too strong assumption that an agent samples infinitely often during each time interval. This assumption is usually made to guarantee convergence of an empirical learning process to a deterministic dynamical system. Two, we provide a proof that under these new realistic conditions, our model converges to a common language for the entire population of agents. Finally the model is experimentally validated.},
	number = {4},
	urldate = {2016-04-17},
	journal = {Journal of Theoretical Biology},
	author = {De Vylder, Bart and Tuyls, Karl},
	month = oct,
	year = {2006},
	keywords = {language acquisition, cultural evolution, Amplification, Horizontal transmission, Naming games, Sampling-response model},
	pages = {818--831},
	file = {De Vylder_Tuyls_2006_How to reach linguistic consensus.pdf:/home/user/Zotero/storage/9RUWSRE8/De Vylder_Tuyls_2006_How to reach linguistic consensus.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/5GB9U8DP/S0022519306002189.html:text/html}
}

@article{gao-naming-2014,
	title = {Naming {Game} on {Networks}: {Let} {Everyone} be {Both} {Speaker} and {Hearer}},
	volume = {4},
	issn = {2045-2322},
	shorttitle = {Naming {Game} on {Networks}},
	url = {http://www.nature.com/articles/srep06149},
	doi = {10.1038/srep06149},
	urldate = {2016-04-17},
	journal = {Scientific Reports},
	author = {Gao, Yuan and Chen, Guanrong and Chan, Rosa H. M.},
	month = aug,
	year = {2014},
	pages = {6149}
}

@article{bornschein-reweighted-2014,
	title = {Reweighted {Wake}-{Sleep}},
	url = {http://arxiv.org/abs/1406.2751},
	abstract = {Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufficiently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.},
	urldate = {2016-04-16},
	journal = {arXiv:1406.2751 [cs]},
	author = {Bornschein, Jörg and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2751},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6P3HF24V/1406.html:text/html;Bornschein_Bengio_2014_Reweighted Wake-Sleep.pdf:/home/user/Zotero/storage/ZPK478FV/Bornschein_Bengio_2014_Reweighted Wake-Sleep.pdf:application/pdf}
}

@inproceedings{agirre-study-2009,
	title = {A study on similarity and relatedness using distributional and wordnet-based approaches},
	url = {http://dl.acm.org/citation.cfm?id=1620758},
	urldate = {2016-04-13},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Agirre, Eneko and Alfonseca, Enrique and Hall, Keith and Kravalova, Jana and Paşca, Marius and Soroa, Aitor},
	year = {2009},
	pages = {19--27},
	file = {A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches - N09-1003.pdf:/home/user/Zotero/storage/4WBHDS4X/N09-1003.pdf:application/pdf}
}

@article{rohrbach-attributes-2016,
	title = {Attributes as {Semantic} {Units} between {Natural} {Language} and {Visual} {Recognition}},
	url = {http://arxiv.org/abs/1604.03249},
	abstract = {Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.},
	urldate = {2016-04-13},
	journal = {arXiv:1604.03249 [cs]},
	author = {Rohrbach, Marcus},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03249},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Z97TPJNF/1604.html:text/html;Rohrbach_2016_Attributes as Semantic Units between Natural Language and Visual Recognition.pdf:/home/user/Zotero/storage/4MTDEEQE/Rohrbach_2016_Attributes as Semantic Units between Natural Language and Visual Recognition.pdf:application/pdf}
}

@article{klerke-improving-2016,
	title = {Improving sentence compression by learning to predict gaze},
	url = {http://arxiv.org/abs/1604.03357},
	abstract = {We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.},
	urldate = {2016-04-13},
	journal = {arXiv:1604.03357 [cs]},
	author = {Klerke, Sigrid and Goldberg, Yoav and Søgaard, Anders},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03357},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/S5APENP5/1604.html:text/html;Klerke et al_2016_Improving sentence compression by learning to predict gaze.pdf:/home/user/Zotero/storage/4EJVZN3E/Klerke et al_2016_Improving sentence compression by learning to predict gaze.pdf:application/pdf}
}

@article{drieghe-how-2007,
	title = {How important are linguistic factors in word skipping during reading?},
	volume = {98},
	url = {http://onlinelibrary.wiley.com/doi/10.1348/000712606X111258/full},
	number = {1},
	urldate = {2016-04-12},
	journal = {British Journal of Psychology},
	author = {Drieghe, Denis and Desmet, Timothy and Brysbaert, Marc},
	year = {2007},
	pages = {157--171},
	file = {0BzgPG-UCd7DiM2lMamR4TXdRUkk.pdf:/home/user/Zotero/storage/EPFGPHZI/0BzgPG-UCd7DiM2lMamR4TXdRUkk.pdf:application/pdf}
}

@inproceedings{schein-bayesian-2015,
	title = {Bayesian poisson tensor factorization for inferring multilateral relations from sparse dyadic event counts},
	url = {http://dl.acm.org/citation.cfm?id=2783414},
	urldate = {2016-04-12},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Schein, Aaron and Paisley, John and Blei, David M. and Wallach, Hanna},
	year = {2015},
	pages = {1045--1054},
	file = {0BzgPG-UCd7DiZTZsTERHMG81OWs.pdf:/home/user/Zotero/storage/TCJ6EE5T/0BzgPG-UCd7DiZTZsTERHMG81OWs.pdf:application/pdf}
}

@article{richardson-what-nodate,
	title = {What defines a category? {Evidence} that listeners’ perception is governed by generalizations},
	shorttitle = {What defines a category?},
	url = {http://ling.umd.edu/~nhf/papers/SpeakingRate.pdf},
	urldate = {2016-04-12},
	author = {Richardson, Rachael R. and Feldman, Naomi H. and Idsardi, William},
	file = {0BzgPG-UCd7DiWk5VMUFKeDRweGc.pdf:/home/user/Zotero/storage/PH62MM3Z/0BzgPG-UCd7DiWk5VMUFKeDRweGc.pdf:application/pdf}
}

@inproceedings{voigt-speaker-2014,
	title = {Speaker movement correlates with prosodic indicators of engagement},
	volume = {7},
	url = {http://web.stanford.edu/~jurafsky/pubs/speechprosody\_voigt.pdf},
	urldate = {2016-04-12},
	booktitle = {Speech {Prosody}},
	author = {Voigt, Rob and Podesva, Robert J. and Jurafsky, Dan},
	year = {2014},
	file = {0BzgPG-UCd7DiT1pnajk3NEZIN0E.pdf:/home/user/Zotero/storage/5XXF7BNK/0BzgPG-UCd7DiT1pnajk3NEZIN0E.pdf:application/pdf}
}

@inproceedings{gagliardi-when-2012,
	title = {When suboptimal behavior is optimal and why: {Modeling} the acquisition of noun classes in {Tsez}},
	shorttitle = {When suboptimal behavior is optimal and why},
	url = {https://mindmodeling.org/cogsci2012/papers/0074/paper0074.pdf},
	urldate = {2016-04-12},
	booktitle = {Proceedings of the 34th annual conference of the {Cognitive} {Science} {Society}},
	author = {Gagliardi, Annie and Feldman, Naomi H. and Lidz, Jeffrey},
	year = {2012},
	pages = {360--365},
	file = {0BzgPG-UCd7DiS0RQeHZHTEhfUzQ.pdf:/home/user/Zotero/storage/5M8G7T9D/0BzgPG-UCd7DiS0RQeHZHTEhfUzQ.pdf:application/pdf}
}

@article{baudis-sentence-2016,
	title = {Sentence {Pair} {Scoring}: {Towards} {Unified} {Framework} for {Text} {Comprehension}},
	shorttitle = {Sentence {Pair} {Scoring}},
	url = {http://arxiv.org/abs/1603.06127},
	urldate = {2016-04-12},
	journal = {arXiv preprint arXiv:1603.06127},
	author = {Baudiš, Petr and Šedivỳ, Jan},
	year = {2016},
	file = {0BzgPG-UCd7DicVJ5a3JoU0hRYWM.pdf:/home/user/Zotero/storage/7BQ28JME/0BzgPG-UCd7DicVJ5a3JoU0hRYWM.pdf:application/pdf}
}

@article{rezende-stochastic-2014,
	title = {Stochastic backpropagation and approximate inference in deep generative models},
	url = {http://arxiv.org/abs/1401.4082},
	urldate = {2016-04-12},
	journal = {arXiv preprint arXiv:1401.4082},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	year = {2014},
	file = {0BzgPG-UCd7DiLU9VVDFlRU9TeEE.pdf:/home/user/Zotero/storage/24A3NG8F/0BzgPG-UCd7DiLU9VVDFlRU9TeEE.pdf:application/pdf}
}

@article{kingma-auto-encoding-2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2016-04-12},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BD7SNNI4/1312.html:text/html;Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:/home/user/Zotero/storage/48XHRXCA/Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:application/pdf}
}

@article{fabius-variational-2014-1,
	title = {Variational {Recurrent} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1412.6581},
	abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
	urldate = {2016-04-12},
	journal = {arXiv:1412.6581 [cs, stat]},
	author = {Fabius, Otto and van Amersfoort, Joost R.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6581},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GVFHE7GG/1412.html:text/html;Fabius_van Amersfoort_2014_Variational Recurrent Auto-Encoders.pdf:/home/user/Zotero/storage/C45C2A7A/Fabius_van Amersfoort_2014_Variational Recurrent Auto-Encoders.pdf:application/pdf}
}

@article{rayner-eye-2009-5,
	title = {Eye movements and attention in reading, scene perception, and visual search},
	volume = {62},
	issn = {1747-0218, 1747-0226},
	url = {http://www.tandfonline.com/doi/abs/10.1080/17470210902816461},
	doi = {10.1080/17470210902816461},
	language = {en},
	number = {8},
	urldate = {2016-04-11},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Rayner, Keith},
	month = aug,
	year = {2009},
	pages = {1457--1506},
	file = {17470210902816461:/home/user/Zotero/storage/NVKP8EU2/17470210902816461.pdf:application/pdf}
}

@article{rayner-eye-1998,
	title = {Eye movements in reading and information processing: 20 years of research},
	volume = {124},
	issn = {0033-2909},
	shorttitle = {Eye movements in reading and information processing},
	abstract = {Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed.},
	language = {eng},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rayner, K.},
	month = nov,
	year = {1998},
	pmid = {9849112},
	keywords = {Humans, Mental Processes, Reading, Cognition, Saccades},
	pages = {372--422},
	file = {Rayner (1998) Eye movements in reading and information processing. 20 years of research - rayner (1998) eye movements in reading and information processing. 20 years of research.pdf:/home/user/Zotero/storage/8RQVSG2U/rayner (1998) eye movements in reading and information processing. 20 years of research.pdf:application/pdf}
}

@article{lake-building-2016,
	title = {Building {Machines} {That} {Learn} and {Think} {Like} {People}},
	url = {http://arxiv.org/abs/1604.00289},
	abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	urldate = {2016-04-10},
	journal = {arXiv:1604.00289 [cs, stat]},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00289},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GPAJW94C/1604.html:text/html;Lake et al_2016_Building Machines That Learn and Think Like People.pdf:/home/user/Zotero/storage/WGBNCVQD/Lake et al_2016_Building Machines That Learn and Think Like People.pdf:application/pdf}
}

@article{nathans-how-2016,
	title = {How scientists can reduce their carbon footprint},
	volume = {5},
	copyright = {© 2016, Nathans et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/5/e15928v1},
	doi = {10.7554/eLife.15928},
	abstract = {{\textless}meta name="DC.Rights" content="© 2016, Nathans et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited."/{\textgreater}
{\textless}meta name="DC.Contributor" content="Jeremy Nathans"/{\textgreater}
{\textless}meta name="DC.Contributor" content="Peter Sterling"/{\textgreater}
{\textless}meta name="citation\_author" content="Jeremy Nathans"/{\textgreater}
{\textless}meta name="citation\_author" content="Peter Sterling"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_author=Climate Law Institute;citation\_title=Up in the air: How airplane carbon pollution jeopardizes global climate goals;citation\_year=2015"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_author=Index Mundi;citation\_title=World jet fuel consumption by year;citation\_year=2016"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_author=Union of Concerned Scientists.;citation\_title=Cooler Smarter: Practical Steps for Low Carbon Living;citation\_year=2012"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_author=World Bank;citation\_title=CO2 emissions (metric tons per capita);citation\_year=2016"/{\textgreater}},
	language = {en},
	urldate = {2016-04-10},
	journal = {eLife},
	author = {Nathans, Jeremy and Sterling, Peter},
	month = mar,
	year = {2016},
	keywords = {climate change, point of view, scientific meetings},
	pages = {e15928},
	file = {Nathans_Sterling_2016_How scientists can reduce their carbon footprint.pdf:/home/user/Zotero/storage/HK2EXF2X/Nathans_Sterling_2016_How scientists can reduce their carbon footprint.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HNBQK2CK/e15928v1.html:text/html}
}

@article{masullo-crawling-2016,
	title = {Crawling towards a map of the brain},
	volume = {5},
	copyright = {© 2016, Masullo et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {http://elifesciences.org/content/5/e15438v1},
	doi = {10.7554/eLife.15438},
	abstract = {{\textless}meta name="DC.Rights" content="© 2016, Masullo et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited."/{\textgreater}
{\textless}meta name="DC.Contributor" content="Laura Masullo"/{\textgreater}
{\textless}meta name="DC.Contributor" content="Marco Tripodi"/{\textgreater}
{\textless}meta name="citation\_author" content="Laura Masullo"/{\textgreater}
{\textless}meta name="citation\_author\_institution" content="MRC Laboratory of Molecular Biology"/{\textgreater}
{\textless}meta name="citation\_author\_orcid" content="http://orcid.org/0000-0001-8267-3708"/{\textgreater}
{\textless}meta name="citation\_author" content="Marco Tripodi"/{\textgreater}
{\textless}meta name="citation\_author\_institution" content="MRC Laboratory of Molecular Biology"/{\textgreater}
{\textless}meta name="citation\_author\_email" content="mtripodi@mrc-lmb.cam.ac.uk"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Journal of Neurophysiology;citation\_author=J. Cang;citation\_author=WO. Friesen;citation\_title=Model for intersegmental coordination of leech swimming: central and sensory mechanisms;citation\_pages=2760-2769;citation\_volume=87;citation\_year=2002"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Experimental Brain Research;citation\_author=TG. Deliagina;citation\_author=GN. Orlovsky;citation\_author=GA. Pavlova;citation\_title=The capacity for generation of rhythmic oscillations is distributed in the lumbosacral spinal cord of the cat;citation\_pages=81-90;citation\_volume=53;citation\_year=1983;citation\_doi=10.1007/BF00239400"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=eLife;citation\_author=A. Fushiki;citation\_author=MF. Zwart;citation\_author=H. Kohsaka;citation\_author=RD. Fetter;citation\_author=A. Cardona;citation\_author=A. Nose;citation\_title=A circuit mechanism for the propagation of waves of muscle contraction in Drosophila;citation\_volume=5;citation\_year=2016;citation\_doi=10.7554/eLife.13253"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Frontiers in Computational Neuroscience;citation\_author=J. Gjorgjieva;citation\_author=J. Berni;citation\_author=JF. Evers;citation\_author=SJ. Eglen;citation\_title=Neural circuits for peristaltic wave propagation in crawling Drosophila larvae: analysis and modeling;citation\_pages=24;citation\_volume=7;citation\_year=2013;citation\_doi=10.3389/fncom.2013.00024"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Neuron;citation\_author=S. Grillner;citation\_title=Biological pattern generation: the cellular and computational logic of networks in motion;citation\_pages=751-766;citation\_volume=52;citation\_year=2006;citation\_doi=10.1016/j.neuron.2006.11.008"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Current Biology;citation\_author=E. Marder;citation\_author=D. Bucher;citation\_author=DJ. Schulz;citation\_author=AL. Taylor;citation\_title=Invertebrate central pattern generation moves along;citation\_pages=R685-699;citation\_volume=15;citation\_year=2005;citation\_doi=10.1016/j.cub.2005.08.022"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Journal of Neurophysiology;citation\_author=T. Matsushima;citation\_author=S. Grillner;citation\_title=Neural mechanisms of intersegmental coordination in lamprey: local excitability changes modify the phase coupling along the spinal cord;citation\_pages=373-388;citation\_volume=67;citation\_year=1992"/{\textgreater}
{\textless}meta name="citation\_reference" content="citation\_journal\_title=Nature;citation\_author=ML. Suster;citation\_author=M. Bate;citation\_title=Embryonic assembly of a central pattern generator without sensory input;citation\_pages=174-178;citation\_volume=416;citation\_year=2002;citation\_doi=10.1038/416174a"/{\textgreater}},
	language = {en},
	urldate = {2016-04-10},
	journal = {eLife},
	author = {Masullo, Laura and Tripodi, Marco},
	month = mar,
	year = {2016},
	keywords = {\textit{D. melanogaster}, central pattern generator, motor circuits, wiring diagram},
	pages = {e15438},
	file = {Masullo_Tripodi_2016_Crawling towards a map of the brain.pdf:/home/user/Zotero/storage/A4P8RJIJ/Masullo_Tripodi_2016_Crawling towards a map of the brain.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9GSK8S6S/e15438v1.html:text/html}
}

@misc{noauthor-redundancy-nodate,
	title = {Redundancy and reduction: speakers manage syntactic information density. - {PubMed} - {NCBI}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20434141},
	urldate = {2016-04-10},
	file = {Redundancy and reduction\: speakers manage syntactic information density. - PubMed - NCBI:/home/user/Zotero/storage/DGBFTDQ9/20434141.html:text/html}
}

@book{piketty-capital-2014,
	address = {Cambridge Massachusetts},
	title = {Capital in the twenty-first century},
	isbn = {978-0-674-43000-6},
	language = {eng},
	publisher = {The Belknap Press of Harvard University Press},
	author = {Piketty, Thomas and Goldhammer, Arthur},
	year = {2014},
	keywords = {Capital, Income distribution, Labor economics, Wealth},
	file = {[Thomas_Piketty,_Arthur_Goldhammer]_Capital_in_the(BookZZ.org).pdf:/home/user/Zotero/storage/VGKZIDHT/[Thomas_Piketty,_Arthur_Goldhammer]_Capital_in_the(BookZZ.org).pdf:application/pdf}
}

@article{bennett-notes-2003-1,
	title = {Notes on {Landauer}'s principle, reversible computation, and {Maxwell}'s {Demon}},
	volume = {34},
	issn = {13552198},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S135521980300039X},
	doi = {10.1016/S1355-2198(03)00039-X},
	language = {en},
	number = {3},
	urldate = {2016-04-10},
	journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
	author = {Bennett, Charles H.},
	month = sep,
	year = {2003},
	pages = {501--510},
	file = {bennett03.pdf:/home/user/Zotero/storage/Q4CSJ77D/bennett03.pdf:application/pdf}
}

@book{aoun-syntax-2009,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A056.pdf:/home/user/Zotero/storage/J5RA3W66/CBO9780511691775A056.pdf:application/pdf}
}

@book{aoun-syntax-2009-1,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A062.pdf:/home/user/Zotero/storage/WUPBIF53/CBO9780511691775A062.pdf:application/pdf}
}

@book{aoun-syntax-2009-2,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A026.pdf:/home/user/Zotero/storage/VQCCSXIM/CBO9780511691775A026.pdf:application/pdf}
}

@book{aoun-syntax-2009-3,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A026 (1).pdf:/home/user/Zotero/storage/498SEVI6/CBO9780511691775A026 (1).pdf:application/pdf}
}

@book{aoun-syntax-2009-4,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A019.pdf:/home/user/Zotero/storage/IMTGN2F7/CBO9780511691775A019.pdf:application/pdf}
}

@article{johnson-evidence-2013-1,
	title = {Evidence for automatic accessing of constructional meaning: {Jabberwocky} sentences prime associated verbs},
	volume = {28},
	issn = {0169-0965, 1464-0732},
	shorttitle = {Evidence for automatic accessing of constructional meaning},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2012.717632},
	doi = {10.1080/01690965.2012.717632},
	language = {en},
	number = {10},
	urldate = {2016-04-10},
	journal = {Language and Cognitive Processes},
	author = {Johnson, Matt A. and Goldberg, Adele E.},
	month = dec,
	year = {2013},
	pages = {1439--1452},
	file = {13Jabberwocky-published.pdf:/home/user/Zotero/storage/3PBHVH7F/13Jabberwocky-published.pdf:application/pdf}
}

@book{aoun-syntax-2009-5,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A011.pdf:/home/user/Zotero/storage/JHZF9D7X/CBO9780511691775A011.pdf:application/pdf}
}

@book{aoun-syntax-2009-6,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-04-10},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A033.pdf:/home/user/Zotero/storage/62VH2NQZ/CBO9780511691775A033.pdf:application/pdf}
}

@article{zeschel-introduction-2008-1,
	title = {Introduction},
	volume = {19},
	issn = {0936-5907, 1613-3641},
	url = {http://www.degruyter.com/view/j/cogl.2008.19.issue-3/cogl.2008.013/cogl.2008.013.xml},
	doi = {10.1515/COGL.2008.013},
	number = {3},
	urldate = {2016-04-10},
	journal = {Cognitive Linguistics},
	author = {Zeschel, Arne},
	month = jan,
	year = {2008},
	file = {08AmbridgeGoldberg-islands.pdf:/home/user/Zotero/storage/JBWIEPIR/08AmbridgeGoldberg-islands.pdf:application/pdf}
}

@article{andreas-accuracy-2015,
	title = {On the accuracy of self-normalized log-linear models},
	url = {http://arxiv.org/abs/1506.04147},
	abstract = {Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as "self-normalization", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.},
	urldate = {2016-04-10},
	journal = {arXiv:1506.04147 [cs, stat]},
	author = {Andreas, Jacob and Rabinovich, Maxim and Klein, Dan and Jordan, Michael I.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04147},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Methodology, Statistics - Machine Learning},
	file = {Andreas et al_2015_On the accuracy of self-normalized log-linear models.pdf:/home/user/Zotero/storage/P3EW8CN2/Andreas et al_2015_On the accuracy of self-normalized log-linear models.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/AT4KWEKB/1506.html:text/html}
}

@article{andreas-deep-2015-2,
	title = {Deep {Compositional} {Question} {Answering} with {Neural} {Module} {Networks}},
	url = {http://arxiv.org/abs/1511.02799},
	abstract = {Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
	urldate = {2016-04-10},
	journal = {arXiv:1511.02799 [cs]},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02799},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Andreas et al_2015_Deep Compositional Question Answering with Neural Module Networks.pdf:/home/user/Zotero/storage/627HW7BU/Andreas et al_2015_Deep Compositional Question Answering with Neural Module Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/UBGMF5M8/1511.html:text/html}
}

@article{andreas-learning-2016-2,
	title = {Learning to {Compose} {Neural} {Networks} for {Question} {Answering}},
	url = {http://arxiv.org/abs/1601.01705},
	abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
	urldate = {2016-04-10},
	journal = {arXiv:1601.01705 [cs]},
	author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01705},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Andreas et al_2016_Learning to Compose Neural Networks for Question Answering.pdf:/home/user/Zotero/storage/49PAAP34/Andreas et al_2016_Learning to Compose Neural Networks for Question Answering.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9ZC7WAJ4/1601.html:text/html}
}

@article{andreas-reasoning-2016,
	title = {Reasoning {About} {Pragmatics} with {Neural} {Listeners} and {Speakers}},
	url = {http://arxiv.org/abs/1604.00562},
	abstract = {We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural "listener" and "speaker" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated \_without\_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81\% of the time, compared to a 64\% success rate using existing techniques.},
	urldate = {2016-04-10},
	journal = {arXiv:1604.00562 [cs]},
	author = {Andreas, Jacob and Klein, Dan},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00562},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {Andreas_Klein_2016_Reasoning About Pragmatics with Neural Listeners and Speakers.pdf:/home/user/Zotero/storage/GZ3K3SEM/Andreas_Klein_2016_Reasoning About Pragmatics with Neural Listeners and Speakers.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9XPEHXCC/1604.html:text/html}
}

@article{dulac-arnold-deep-2015,
	title = {Deep {Reinforcement} {Learning} in {Large} {Discrete} {Action} {Spaces}},
	url = {http://arxiv.org/abs/1512.07679},
	abstract = {Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.},
	urldate = {2016-04-10},
	journal = {arXiv:1512.07679 [cs, stat]},
	author = {Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.07679},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AVZDP7VQ/1512.html:text/html;Dulac-Arnold et al_2015_Deep Reinforcement Learning in Large Discrete Action Spaces.pdf:/home/user/Zotero/storage/IAAVA5MI/Dulac-Arnold et al_2015_Deep Reinforcement Learning in Large Discrete Action Spaces.pdf:application/pdf}
}

@article{dyer-recurrent-2016,
	title = {Recurrent {Neural} {Network} {Grammars}},
	url = {http://arxiv.org/abs/1602.07776},
	abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.},
	urldate = {2016-04-10},
	journal = {arXiv:1602.07776 [cs]},
	author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.07776},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GFWMV38E/1602.html:text/html;Dyer et al_2016_Recurrent Neural Network Grammars.pdf:/home/user/Zotero/storage/NDR8PQ6W/Dyer et al_2016_Recurrent Neural Network Grammars.pdf:application/pdf}
}

@book{cohen-proceedings-2006-1,
	address = {New York, NY},
	title = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}: [{June} 25 - 29, 2006, {Pittsburgh}, {PA}, {USA}]},
	isbn = {978-1-59593-383-6},
	shorttitle = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}},
	language = {eng},
	publisher = {ACM},
	editor = {Cohen, William W. and Moore, Andrew and {International Conference on Machine Learning}},
	year = {2006},
	keywords = {Kongress, Maschinelles Lernen},
	file = {1511.08400v3.pdf:/home/user/Zotero/storage/7PHCGSDQ/1511.08400v3.pdf:application/pdf}
}

@article{villumsen-simulations-1982,
	title = {Simulations of galaxy mergers},
	volume = {199},
	url = {http://mnras.oxfordjournals.org/content/199/3/493.short},
	number = {3},
	urldate = {2016-04-10},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Villumsen, Jens Verner},
	year = {1982},
	pages = {493--516},
	file = {NKNJTB01.pdf:/home/user/Zotero/storage/IB3GUCQW/NKNJTB01.pdf:application/pdf}
}

@article{parr-proof-nodate-1,
	title = {Proof {Delivery} {Form} {Psychological} {Medicine}},
	author = {Parr, Mr SJ},
	file = {11JCLBoydGoldbergConservative-1.pdf:/home/user/Zotero/storage/IAZI4KS7/11JCLBoydGoldbergConservative-1.pdf:application/pdf}
}

@article{kulpmann-argument-2015-1,
	title = {Argument {Omission} between {Valency} and {Construction}},
	url = {https://hsbiblio.uni-tuebingen.de/xmlui/handle/10900/67218},
	urldate = {2016-04-10},
	author = {Külpmann, Robert and Symanczyk Joppe, Vilma},
	year = {2015},
	file = {Külpmann_SymanczykJoppe.pdf:/home/user/Zotero/storage/ACPPGNMS/Külpmann_SymanczykJoppe.pdf:application/pdf}
}

@article{allen-distinguishing-2012-2,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X12001599},
	number = {3},
	urldate = {2016-04-10},
	journal = {Brain and language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	year = {2012},
	pages = {174--182},
	file = {12MVPA-final.pdf:/home/user/Zotero/storage/RQHCZJ8C/12MVPA-final.pdf:application/pdf}
}

@article{pasupat-compositional-2015-1,
	title = {Compositional semantic parsing on semi-structured tables},
	url = {http://arxiv.org/abs/1508.00305},
	urldate = {2016-04-10},
	journal = {arXiv preprint arXiv:1508.00305},
	author = {Pasupat, Panupong and Liang, Percy},
	year = {2015},
	file = {pasupat-liang-acl2015.pdf:/home/user/Zotero/storage/D4CEV9H4/pasupat-liang-acl2015.pdf:application/pdf}
}

@inproceedings{chaganty-estimating-2014-1,
	title = {Estimating latent-variable graphical models using moments and likelihoods},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2\_chaganty14},
	urldate = {2016-04-10},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Chaganty, Arun T. and Liang, Percy},
	year = {2014},
	pages = {1872--1880},
	file = {graphical-icml2014.pdf:/home/user/Zotero/storage/9EPWKZI9/graphical-icml2014.pdf:application/pdf}
}

@techreport{goodman-concepts-2014-1,
	title = {Concepts in a probabilistic language of thought},
	url = {http://dspace.mit.edu/handle/1721.1/100174},
	urldate = {2016-04-10},
	institution = {Center for Brains, Minds and Machines (CBMM)},
	author = {Goodman, Noah D. and Tenenbaum, Joshua B. and Gerstenberg, Tobias},
	year = {2014},
	file = {GoodmanEtAl2015-Chapter.pdf:/home/user/Zotero/storage/Z25ICUPI/GoodmanEtAl2015-Chapter.pdf:application/pdf}
}

@article{allen-distinguishing-2012-3,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	issn = {0093934X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0093934X12001599},
	doi = {10.1016/j.bandl.2012.08.005},
	language = {en},
	number = {3},
	urldate = {2016-04-10},
	journal = {Brain and Language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	month = dec,
	year = {2012},
	pages = {174--182},
	file = {BrainLanguagepaper.pdf:/home/user/Zotero/storage/U2BQTHMH/BrainLanguagepaper.pdf:application/pdf}
}

@article{robenalt-judgment-2015-1,
	title = {Judgment evidence for statistical preemption: {It} is relatively better to vanish than to disappear a rabbit, but a lifeguard can equally well backstroke or swim children to shore},
	volume = {26},
	shorttitle = {Judgment evidence for statistical preemption},
	url = {http://www.degruyter.com/view/j/cogl.2015.26.issue-3/cog-2015-0004/cog-2015-0004.xml},
	number = {3},
	urldate = {2016-04-10},
	journal = {Cognitive Linguistics},
	author = {Robenalt, Clarice and Goldberg, Adele E.},
	year = {2015},
	pages = {467--503},
	file = {CogLing15disappear-vanish-final-wappendix1.pdf:/home/user/Zotero/storage/U7S4TWHC/CogLing15disappear-vanish-final-wappendix1.pdf:application/pdf}
}

@article{anandkumar-tensor-2014,
	title = {Tensor decompositions for learning latent variable models},
	volume = {15},
	url = {http://dl.acm.org/citation.cfm?id=2697055},
	number = {1},
	urldate = {2016-04-10},
	journal = {The Journal of Machine Learning Research},
	author = {Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	year = {2014},
	pages = {2773--2832},
	file = {1210.7559v4.pdf:/home/user/Zotero/storage/3Q3IPQC9/1210.7559v4.pdf:application/pdf}
}

@article{wilson-deep-2015-1,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	urldate = {2016-04-10},
	journal = {arXiv preprint arXiv:1511.02222},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2015},
	file = {1511.02222v1.pdf:/home/user/Zotero/storage/JIWQW3TK/1511.02222v1.pdf:application/pdf}
}

@article{goldberg-argument-2013-1,
	title = {Argument structure constructions versus lexical rules or derivational verb templates},
	volume = {28},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/mila.12026/full},
	number = {4},
	urldate = {2016-04-10},
	journal = {Mind \& Language},
	author = {Goldberg, Adele E.},
	year = {2013},
	pages = {435--465},
	file = {13Mind-Language-asc-not-rules.pdf:/home/user/Zotero/storage/WC53TU9B/13Mind-Language-asc-not-rules.pdf:application/pdf}
}

@article{robenalt-nonnative-2016-1,
	title = {Nonnative {Speakers} {Do} {Not} {Take} {Competing} {Alternative} {Expressions} {Into} {Account} the {Way} {Native} {Speakers} {Do}},
	volume = {66},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/lang.12149/full},
	number = {1},
	urldate = {2016-04-10},
	journal = {Language Learning},
	author = {Robenalt, Clarice and Goldberg, Adele E.},
	year = {2016},
	pages = {60--93},
	file = {Robenalt_GoldbergLLPreemptionL2-.pdf:/home/user/Zotero/storage/EI2IWZZF/Robenalt_GoldbergLLPreemptionL2-.pdf:application/pdf}
}

@article{berant-imitation-2015-2,
	title = {Imitation {Learning} of {Agenda}-based {Semantic} {Parsers}},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/646},
	urldate = {2016-04-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2015},
	pages = {545--558},
	file = {agenda-tacl2015.pdf:/home/user/Zotero/storage/UE4Z4DKQ/agenda-tacl2015.pdf:application/pdf}
}

@article{mcfarland-making-2013-1,
	title = {Making the {Connection}: {Social} {Bonding} in {Courtship} {Situations} $^{\textrm{1}}$},
	volume = {118},
	issn = {0002-9602, 1537-5390},
	shorttitle = {Making the {Connection}},
	url = {http://www.journals.uchicago.edu/doi/10.1086/670240},
	doi = {10.1086/670240},
	language = {en},
	number = {6},
	urldate = {2016-04-10},
	journal = {American Journal of Sociology},
	author = {McFarland, Daniel A. and Jurafsky, Dan and Rawlings, Craig},
	month = may,
	year = {2013},
	pages = {1596--1649},
	file = {mcfarlandjurafskyrawlings.pdf:/home/user/Zotero/storage/TN6U5XUP/mcfarlandjurafskyrawlings.pdf:application/pdf}
}

@article{johnson-neural-2016-1,
	title = {Neural systems involved in processing novel linguistic constructions and their visual referents},
	volume = {31},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1055280},
	number = {1},
	urldate = {2016-04-10},
	journal = {Language, Cognition and Neuroscience},
	author = {Johnson, Matthew A. and Turk-Browne, Nicholas B. and Goldberg, Adele E.},
	year = {2016},
	pages = {129--144},
	file = {15LCN-Prediction-Cx-Neuro.pdf:/home/user/Zotero/storage/6M8R59S6/15LCN-Prediction-Cx-Neuro.pdf:application/pdf}
}

@inproceedings{gens-discriminative-2012-1,
	title = {Discriminative learning of sum-product networks},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2012\_1484.pdf},
	urldate = {2016-04-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gens, Robert and Domingos, Pedro},
	year = {2012},
	pages = {3248--3256},
	file = {dspn.pdf:/home/user/Zotero/storage/AG7D8PG9/dspn.pdf:application/pdf}
}

@article{jager-factoring-2015-1,
	title = {Factoring lexical and phonetic phylogenetic characters from word lists},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67205},
	urldate = {2016-04-10},
	author = {Jäger, Gerhard and List, Johann-Mattis},
	year = {2015},
	file = {Jaeger_List.pdf:/home/user/Zotero/storage/C65GWHM8/Jaeger_List.pdf:application/pdf}
}

@book{klein-learn-2013,
	address = {United States},
	edition = {3. edition},
	title = {Learn {German} with stories: {Café} in {Berlin}: 10 short stories for beginners},
	isbn = {978-1-4923-9949-0},
	shorttitle = {Learn {German} with stories},
	abstract = {DIe Wohngemeinschaft -- Multikulti -- Ingrid -- DIe Waschmaschine -- Masken -- Im Prinzenbad -- Ohne Moos nix los -- Kohle, Ratten und Gespenster -- Die Dänische Dogge -- Auf wiedersehen, Berlin!. - In these ten short stories, "[e]xperience daily life in the German capital through the eyes of a newcomer, learn about the country and its people, and improve your German effortlessly along the way!"},
	publisher = {LearnOutLive.com},
	author = {Klein, André},
	year = {2013},
	keywords = {German language, German language Readers, German language Study and teaching Foreign speakers, German language Study and teaching Foreign speakers Readers},
	file = {Learn German with Stories_ Cafe in Berlin.pdf:/home/user/Zotero/storage/U84FXA8T/Learn German with Stories_ Cafe in Berlin.pdf:application/pdf}
}

@article{wonnacott-input-2012-1,
	title = {Input effects on the acquisition of a novel phrasal construction in 5year olds},
	volume = {66},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X11001148},
	doi = {10.1016/j.jml.2011.11.004},
	language = {en},
	number = {3},
	urldate = {2016-04-10},
	journal = {Journal of Memory and Language},
	author = {Wonnacott, Elizabeth and Boyd, Jeremy K. and Thomson, Jennifer and Goldberg, Adele E.},
	month = apr,
	year = {2012},
	pages = {458--478},
	file = {12WonnacottBoydGoldbergJML.pdf:/home/user/Zotero/storage/BV3DWAA5/12WonnacottBoydGoldbergJML.pdf:application/pdf}
}

@article{perek-generalizing-2015-1,
	title = {Generalizing beyond the input: {The} functions of the constructions matter},
	volume = {84},
	issn = {0749596X},
	shorttitle = {Generalizing beyond the input},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000601},
	doi = {10.1016/j.jml.2015.04.006},
	language = {en},
	urldate = {2016-04-10},
	journal = {Journal of Memory and Language},
	author = {Perek, Florent and Goldberg, Adele E.},
	month = oct,
	year = {2015},
	pages = {108--127},
	file = {15JMLPerek-Goldberg.pdf:/home/user/Zotero/storage/DI5GH8BP/15JMLPerek-Goldberg.pdf:application/pdf}
}

@article{goodman-probabilistic-2014-2,
	title = {Probabilistic semantics and pragmatics: {Uncertainty} in language and thought},
	shorttitle = {Probabilistic semantics and pragmatics},
	url = {http://cocolab.stanford.edu/papers/GoodmanLassiter2015-Chapter.pdf},
	urldate = {2016-04-10},
	journal = {Handbook of Contemporary Semantic Theory. Wiley-Blackwell},
	author = {Goodman, Noah D. and Lassiter, Daniel},
	year = {2014},
	file = {GoodmanLassiter2015-Chapter.pdf:/home/user/Zotero/storage/FCDMGK2N/GoodmanLassiter2015-Chapter.pdf:application/pdf}
}

@inproceedings{shi-learning-2015-2,
	title = {Learning {Where} to {Sample} in {Structured} {Prediction}.},
	url = {http://www-cs.stanford.edu/~jsteinhardt/publications/adainfer/paper.pdf},
	urldate = {2016-04-10},
	booktitle = {{AISTATS}},
	author = {Shi, Tianlin and Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {sample-aistats2015.pdf:/home/user/Zotero/storage/U63FUWCH/sample-aistats2015.pdf:application/pdf}
}

@article{fengler-brain-2015-2,
	title = {Brain structural correlates of complex sentence comprehension in children},
	volume = {15},
	issn = {18789293},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1878929315000900},
	doi = {10.1016/j.dcn.2015.09.004},
	language = {en},
	urldate = {2016-04-10},
	journal = {Developmental Cognitive Neuroscience},
	author = {Fengler, Anja and Meyer, Lars and Friederici, Angela D.},
	month = oct,
	year = {2015},
	pages = {48--57},
	file = {1-s2.0-S1878929315000900-main.pdf:/home/user/Zotero/storage/ZXXBCEXZ/1-s2.0-S1878929315000900-main.pdf:application/pdf}
}

@inproceedings{gens-deep-2014-1,
	title = {Deep symmetry networks},
	url = {http://papers.nips.cc/paper/5424-deep-symmetry-networks},
	urldate = {2016-04-10},
	booktitle = {Advances in neural information processing systems},
	author = {Gens, Robert and Domingos, Pedro M.},
	year = {2014},
	pages = {2537--2545},
	file = {dsn.pdf:/home/user/Zotero/storage/PMQ4XGTB/dsn.pdf:application/pdf}
}

@article{mansimov-generating-2015-2,
	title = {Generating {Images} from {Captions} with {Attention}},
	url = {http://arxiv.org/abs/1511.02793},
	urldate = {2016-04-10},
	journal = {arXiv preprint arXiv:1511.02793},
	author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.02793v1.pdf:/home/user/Zotero/storage/JIQW37QC/1511.02793v1.pdf:application/pdf}
}

@article{vandevoorde-distributional-2015-1,
	title = {Distributional and translational solutions to the visualization of semantic differences between translated and non-translated {Dutch}},
	url = {https://bibliographie.uni-tuebingen.de/xmlui/handle/10900/67190},
	urldate = {2016-04-10},
	author = {Vandevoorde, Lore and De Baets, Pauline and Lefever, Els and Plevoets, Koen and De Sutter, Gert},
	year = {2015},
	file = {Vandevoorde_DeBaets_Lefever_Plevoets_DeSutter.pdf:/home/user/Zotero/storage/CSXSKHAC/Vandevoorde_DeBaets_Lefever_Plevoets_DeSutter.pdf:application/pdf}
}

@article{kober-learning-2014-1,
	title = {Learning {Motor} {Skills}},
	volume = {97},
	url = {http://link.springer.com/content/pdf/10.1007/978-3-319-03194-1.pdf},
	urldate = {2016-04-10},
	journal = {Springer Tracts in Advanced Robotics},
	author = {Kober, Jens and Peters, Jan},
	year = {2014},
	file = {Kober_IJRR_2013.pdf:/home/user/Zotero/storage/VC4GVFZ5/Kober_IJRR_2013.pdf:application/pdf}
}

@article{griffiths-phoneme-2015-1,
	title = {From {Phoneme} to {Morpheme}: {A} {Computational} {Model}},
	shorttitle = {From {Phoneme} to {Morpheme}},
	url = {https://hsbiblio.uni-tuebingen.de/xmlui/handle/10900/67219},
	urldate = {2016-04-10},
	author = {Griffiths, Sascha and Purver, Matthew and Wiggins, Geraint},
	year = {2015},
	file = {Griffiths.pdf:/home/user/Zotero/storage/KBXIBW2C/Griffiths.pdf:application/pdf}
}

@article{cohen-products-2011-1,
	title = {Products of weighted logic programs},
	volume = {11},
	issn = {1471-0684, 1475-3081},
	url = {http://www.journals.cambridge.org/abstract\_S1471068410000529},
	doi = {10.1017/S1471068410000529},
	language = {en},
	number = {2-3},
	urldate = {2016-04-10},
	journal = {Theory and Practice of Logic Programming},
	author = {Cohen, Shay B. and Simmons, Robert J. and Smith, Noah A.},
	month = mar,
	year = {2011},
	pages = {263--296},
	file = {tplp11product.pdf:/home/user/Zotero/storage/VB2TSEPK/tplp11product.pdf:application/pdf}
}

@book{sag-syntactic-1999,
	title = {Syntactic theory: {A} formal introduction},
	volume = {92},
	shorttitle = {Syntactic theory},
	url = {https://wiki.eecs.yorku.ca/course\_archive/2014-15/W/6339/\_media/sag\_i.a.\_wasow\_t.\_bender\_e.m.\_syntax..\_a\_formal\_introduction\_book\_draft\_2ed.\_csli\_2003\_472s\_lf\_.pdf},
	urldate = {2016-04-10},
	publisher = {Center for the Study of Language and Information Stanford, CA},
	author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M. and Sag, Ivan A.},
	year = {1999},
	file = {[Sag_I.A.,_Wasow_T.,_Bender_E.M.]_Syntax_a_formal(BookZZ.org).pdf:/home/user/Zotero/storage/4NRUQSF8/[Sag_I.A.,_Wasow_T.,_Bender_E.M.]_Syntax_a_formal(BookZZ.org).pdf:application/pdf}
}

@book{sag-syntactic-1999-1,
	title = {Syntactic theory: {A} formal introduction},
	volume = {92},
	shorttitle = {Syntactic theory},
	url = {https://wiki.eecs.yorku.ca/course\_archive/2014-15/W/6339/\_media/sag\_i.a.\_wasow\_t.\_bender\_e.m.\_syntax..\_a\_formal\_introduction\_book\_draft\_2ed.\_csli\_2003\_472s\_lf\_.pdf},
	urldate = {2016-04-10},
	publisher = {Center for the Study of Language and Information Stanford, CA},
	author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M. and Sag, Ivan A.},
	year = {1999},
	file = {[Ivan_A._Sag,_Thomas_Wasow,_Emily_M._Bender]_Synta(BookZZ.org).pdf:/home/user/Zotero/storage/D52IRXQZ/[Ivan_A._Sag,_Thomas_Wasow,_Emily_M._Bender]_Synta(BookZZ.org).pdf:application/pdf}
}

@article{srivastava-dropout:-2014,
	title = {Dropout: {A} simple way to prevent neural networks from overfitting},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://dl.acm.org/citation.cfm?id=2670313},
	number = {1},
	urldate = {2016-04-10},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {srivastava14a.pdf:/home/user/Zotero/storage/D99WMM6D/srivastava14a.pdf:application/pdf}
}

@article{burda-importance-2015-1,
	title = {Importance weighted autoencoders},
	url = {http://arxiv.org/abs/1509.00519},
	urldate = {2016-04-10},
	journal = {arXiv preprint arXiv:1509.00519},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1509.00519v2.pdf:/home/user/Zotero/storage/FHGBC559/1509.00519v2.pdf:application/pdf}
}

@article{czarnetzki-using-2015,
	title = {Using {Duality} in {Circuit} {Complexity}},
	url = {http://arxiv.org/abs/1510.04849},
	abstract = {We investigate in a method for proving separation results for abstract classes of languages. A well established method to characterize varieties of regular languages are identities. We use a recently established generalization of these identities to non-regular languages by Gehrke, Grigorieff, and Pin: so called equations, which are capable of describing arbitrary Boolean algebras of languages. While the main concern of their result is the existence of these equations, we investigate in a general method that could allow to find equations for language classes in an inductive manner. Thereto we extend an important tool -- the block product or substitution principle -- known from logic and algebra, to non-regular language classes. Furthermore, we abstract this concept by defining it directly as an operation on (non-regular) language classes. We show that this principle can be used to obtain equations for certain circuit classes, given equations for the gate types. Concretely, we demonstrate the applicability of this method by obtaining a description via equations for all languages recognized by circuit families that contain a constant number of (inner) gates, given a description of the gate types via equations.},
	urldate = {2016-07-16},
	journal = {arXiv:1510.04849 [cs]},
	author = {Czarnetzki, Silke and Krebs, Andreas},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.04849},
	keywords = {Computer Science - Computational Complexity, F.1.3},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/PUNDSWJK/1510.html:text/html;Czarnetzki_Krebs_2015_Using Duality in Circuit Complexity.pdf:/home/user/Zotero/storage/7BQCB8U9/Czarnetzki_Krebs_2015_Using Duality in Circuit Complexity.pdf:application/pdf}
}

@misc{noauthor-[1510.03387]-nodate,
	title = {[1510.03387] {Topological} lower bounds for arithmetic networks},
	url = {https://arxiv.org/abs/1510.03387},
	urldate = {2016-07-15},
	file = {[1510.03387] Topological lower bounds for arithmetic networks:/home/user/Zotero/storage/7I4EP64V/1510.html:text/html}
}

@misc{noauthor-[1607.03542]-nodate,
	title = {[1607.03542] {Open}-{Vocabulary} {Semantic} {Parsing} with both {Distributional} {Statistics} and {Formal} {Knowledge}},
	url = {https://arxiv.org/abs/1607.03542},
	urldate = {2016-07-15},
	file = {[1607.03542] Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge:/home/user/Zotero/storage/GSSJP468/1607.html:text/html}
}

@misc{noauthor-[1607.03474]-nodate,
	title = {[1607.03474] {Recurrent} {Highway} {Networks}},
	url = {https://arxiv.org/abs/1607.03474},
	urldate = {2016-07-13},
	file = {[1607.03474] Recurrent Highway Networks:/home/user/Zotero/storage/7HS4JI5Q/1607.html:text/html}
}

@misc{noauthor-[1607.01432]-nodate,
	title = {[1607.01432] {Global} {Neural} {CCG} {Parsing} with {Optimality} {Guarantees}},
	url = {https://arxiv.org/abs/1607.01432},
	urldate = {2016-07-12},
	file = {[1607.01432] Global Neural CCG Parsing with Optimality Guarantees:/home/user/Zotero/storage/NUJX7MQQ/1607.html:text/html}
}

@misc{noauthor-[1607.01426]-nodate,
	title = {[1607.01426] {Chains} of {Reasoning} over {Entities}, {Relations}, and {Text} using {Recurrent} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1607.01426},
	urldate = {2016-07-12},
	file = {[1607.01426] Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks:/home/user/Zotero/storage/79AKIVE9/1607.html:text/html}
}

@misc{noauthor-[1607.02109]-nodate,
	title = {[1607.02109] {Predicting} and {Understanding} {Law}-{Making} with {Machine} {Learning}},
	url = {https://arxiv.org/abs/1607.02109},
	urldate = {2016-07-12},
	file = {[1607.02109] Predicting and Understanding Law-Making with Machine Learning:/home/user/Zotero/storage/X9RT2QCN/1607.html:text/html}
}

@misc{noauthor-[1607.01963]-nodate,
	title = {[1607.01963] {Sequence} {Training} and {Adaptation} of {Highway} {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1607.01963},
	urldate = {2016-07-12},
	file = {[1607.01963] Sequence Training and Adaptation of Highway Deep Neural Networks:/home/user/Zotero/storage/NW42EMX4/1607.html:text/html}
}

@misc{noauthor-[1607.02310]-nodate,
	title = {[1607.02310] {Collaborative} {Training} of {Tensors} for {Compositional} {Distributional} {Semantics}},
	url = {https://arxiv.org/abs/1607.02310},
	urldate = {2016-07-11},
	file = {[1607.02310] Collaborative Training of Tensors for Compositional Distributional Semantics:/home/user/Zotero/storage/QKUVFX37/1607.html:text/html}
}

@misc{noauthor-[1607.02250]-nodate,
	title = {[1607.02250] {Consensus} {Attention}-based {Neural} {Networks} for {Chinese} {Reading} {Comprehension}},
	url = {https://arxiv.org/abs/1607.02250},
	urldate = {2016-07-11},
	file = {[1607.02250] Consensus Attention-based Neural Networks for Chinese Reading Comprehension:/home/user/Zotero/storage/N9SG7GGD/1607.html:text/html}
}

@misc{noauthor-[1603.07252]-nodate,
	title = {[1603.07252] {Neural} {Summarization} by {Extracting} {Sentences} and {Words}},
	url = {https://arxiv.org/abs/1603.07252},
	urldate = {2016-07-05},
	file = {[1603.07252] Neural Summarization by Extracting Sentences and Words:/home/user/Zotero/storage/DMNITPDD/1603.html:text/html}
}

@misc{noauthor-[1606.09058]-nodate,
	title = {[1606.09058] {A} {Distributional} {Semantics} {Approach} to {Implicit} {Language} {Learning}},
	url = {https://arxiv.org/abs/1606.09058},
	urldate = {2016-07-04},
	file = {[1606.09058] A Distributional Semantics Approach to Implicit Language Learning:/home/user/Zotero/storage/8WU6GB8X/1606.html:text/html}
}

@misc{noauthor-[1606.09274]-nodate,
	title = {[1606.09274] {Compression} of {Neural} {Machine} {Translation} {Models} via {Pruning}},
	url = {https://arxiv.org/abs/1606.09274},
	urldate = {2016-07-04},
	file = {[1606.09274] Compression of Neural Machine Translation Models via Pruning:/home/user/Zotero/storage/3AQ7CXE5/1606.html:text/html}
}

@inproceedings{pal-hybrid-2013,
	address = {Sofia, Bulgaria},
	title = {A {Hybrid} {Word} {Alignment} {Model} for {Phrase}-{Based} {Statistical} {Machine} {Translation}},
	url = {http://www.aclweb.org/anthology/W13-2814},
	booktitle = {Proceedings of the {Second} {Workshop} on {Hybrid} {Approaches} to {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Pal, Santanu and Naskar, Sudip and Bandyopadhyay, Sivaji},
	month = aug,
	year = {2013},
	pages = {94--101}
}

@inproceedings{rapp-extracting-2014,
	address = {Gothenburg, Sweden},
	title = {Extracting {Multiword} {Translations} from {Aligned} {Comparable} {Documents}},
	url = {http://www.aclweb.org/anthology/W14-1016},
	booktitle = {Proceedings of the 3rd {Workshop} on {Hybrid} {Approaches} to {Machine} {Translation} ({HyTra})},
	publisher = {Association for Computational Linguistics},
	author = {Rapp, Reinhard and Sharoff, Serge},
	month = apr,
	year = {2014},
	pages = {87--95}
}

@inproceedings{de-marneffe-multi-word-2009,
	address = {Suntec, Singapore},
	title = {Multi-word expressions in textual inference: {Much} ado about nothing?},
	url = {http://www.aclweb.org/anthology/W09-2501},
	booktitle = {Proceedings of the 2009 {Workshop} on {Applied} {Textual} {Inference}},
	publisher = {Association for Computational Linguistics},
	author = {de Marneffe, Marie-Catherine and Padó, Sebastian and Manning, Christopher D.},
	month = aug,
	year = {2009},
	pages = {1--9}
}

@book{smyth-thai:-2002,
	address = {London; New York},
	title = {Thai: an essential grammar},
	isbn = {978-0-415-22613-4 978-0-415-22614-1 978-0-203-99504-4},
	shorttitle = {Thai},
	language = {English},
	publisher = {Routledge},
	author = {Smyth, David},
	year = {2002},
	note = {OCLC: 49785813},
	file = {[David_Smyth]_Thai_An_Essential_Grammar(BookZZ.org).pdf:/home/user/Zotero/storage/U738Z3R2/[David_Smyth]_Thai_An_Essential_Grammar(BookZZ.org).pdf:application/pdf}
}

@article{evans-diversity-2009,
	title = {With diversity in mind: {Freeing} the language sciences from {Universal} {Grammar}},
	volume = {32},
	shorttitle = {With diversity in mind},
	url = {http://journals.cambridge.org/abstract\_S0140525X09990525},
	number = {05},
	urldate = {2016-06-27},
	journal = {Behavioral and Brain Sciences},
	author = {Evans, Nicholas and Levinson, Stephen C.},
	year = {2009},
	pages = {472--492},
	file = {Evans-Levinson09_preprint.pdf:/home/user/Zotero/storage/APUGJS78/Evans-Levinson09_preprint.pdf:application/pdf}
}

@misc{noauthor-[1606.06737]-nodate,
	title = {[1606.06737] {Critical} {Behavior} from {Deep} {Dynamics}: {A} {Hidden} {Dimension} in {Natural} {Language}},
	url = {https://arxiv.org/abs/1606.06737},
	urldate = {2016-06-23},
	file = {[1606.06737] Critical Behavior from Deep Dynamics\: A Hidden Dimension in Natural Language:/home/user/Zotero/storage/RHP5UT7W/1606.html:text/html}
}

@misc{noauthor-[1606.06996]-nodate,
	title = {[1606.06996] {The} word entropy of natural languages},
	url = {https://arxiv.org/abs/1606.06996},
	urldate = {2016-06-23},
	file = {[1606.06996] The word entropy of natural languages:/home/user/Zotero/storage/9JSKQIIX/1606.html:text/html}
}

@misc{noauthor-[1606.06950]-nodate,
	title = {[1606.06950] {A} segmental framework for fully-unsupervised large-vocabulary speech recognition},
	url = {https://arxiv.org/abs/1606.06950},
	urldate = {2016-06-23},
	file = {[1606.06950] A segmental framework for fully-unsupervised large-vocabulary speech recognition:/home/user/Zotero/storage/8G9JB9TQ/1606.html:text/html}
}

@misc{noauthor-[1606.06900]-nodate,
	title = {[1606.06900] {Inferring} {Logical} {Forms} {From} {Denotations}},
	url = {https://arxiv.org/abs/1606.06900},
	urldate = {2016-06-23},
	file = {[1606.06900] Inferring Logical Forms From Denotations:/home/user/Zotero/storage/WNNK7CUF/1606.html:text/html}
}

@misc{noauthor-[1511.08999]-nodate,
	title = {[1511.08999] {Deciding} {First}-{Order} {Satisfiability} when {Universal} and {Existential} {Variables} are {Separated}},
	url = {https://arxiv.org/abs/1511.08999},
	urldate = {2016-06-22},
	file = {[1511.08999] Deciding First-Order Satisfiability when Universal and Existential Variables are Separated:/home/user/Zotero/storage/6F44424A/1511.html:text/html}
}

@misc{noauthor-[1606.06086]-nodate,
	title = {[1606.06086] {Uncertainty} in {Neural} {Network} {Word} {Embedding}: {Exploration} of {Threshold} for {Similarity}},
	url = {https://arxiv.org/abs/1606.06086},
	urldate = {2016-06-22},
	file = {[1606.06086] Uncertainty in Neural Network Word Embedding\: Exploration of Threshold for Similarity:/home/user/Zotero/storage/Q6PP7ANC/1606.html:text/html}
}

@misc{noauthor-[1606.05679]-nodate,
	title = {[1606.05679] {Two} {Discourse} {Driven} {Language} {Models} for {Semantics}},
	url = {https://arxiv.org/abs/1606.05679},
	urldate = {2016-06-22},
	file = {[1606.05679] Two Discourse Driven Language Models for Semantics:/home/user/Zotero/storage/XZCB7VCI/1606.html:text/html}
}

@article{sennrich-controlling-nodate,
	title = {Controlling {Politeness} in {Neural} {Machine} {Translation} via {Side} {Constraints}},
	url = {http://homepages.inf.ed.ac.uk/abmayne/publications/sennrich2016NAACL.pdf},
	urldate = {2016-06-22},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	file = {N16-1005.pdf:/home/user/Zotero/storage/6ESHAKXU/N16-1005.pdf:application/pdf}
}

@inproceedings{cherry-empirical-2016,
	title = {An {Empirical} {Evaluation} of {Noise} {Contrastive} {Estimation} for the {Neural} {Network} {Joint} {Model} of {Translation}},
	url = {http://www.aclweb.org/anthology/N/N16/N16-1006.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Cherry, Colin},
	year = {2016},
	pages = {41--46},
	file = {N16-1006.pdf:/home/user/Zotero/storage/9MSBGZU6/N16-1006.pdf:application/pdf}
}

@article{chopra-abstractive-nodate,
	title = {Abstractive {Sentence} {Summarization} with {Attentive} {Recurrent} {Neural} {Networks}},
	url = {http://nlp.seas.harvard.edu/papers/naacl16\_summary.pdf},
	urldate = {2016-06-22},
	author = {Chopra, Sumit and Auli, Michael and Rush, Alexander M. and Harvard, SEAS},
	file = {N16-1012.pdf:/home/user/Zotero/storage/SSAXRU4P/N16-1012.pdf:application/pdf}
}

@inproceedings{asher-integer-2016,
	title = {Integer {Linear} {Programming} for {Discourse} {Parsing}},
	url = {http://anthology.aclweb.org/N/N16/N16-1013.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Asher, Jérémy Perret Stergos Afantenos Nicholas and Morey, Mathieu},
	year = {2016},
	pages = {99--109},
	file = {N16-1013.pdf:/home/user/Zotero/storage/KEFXVV2E/N16-1013.pdf:application/pdf}
}

@article{mrksic-counter-fitting-2016,
	title = {Counter-fitting {Word} {Vectors} to {Linguistic} {Constraints}},
	url = {http://arxiv.org/abs/1603.00892},
	urldate = {2016-06-22},
	journal = {arXiv preprint arXiv:1603.00892},
	author = {Mrkšić, Nikola and Séaghdha, Diarmuid Ó and Thomson, Blaise and Gašić, Milica and Rojas-Barahona, Lina and Su, Pei-Hao and Vandyke, David and Wen, Tsung-Hsien and Young, Steve},
	year = {2016},
	file = {N16-1018.pdf:/home/user/Zotero/storage/2FJ9JU4K/N16-1018.pdf:application/pdf}
}

@article{yang-grounded-nodate,
	title = {Grounded {Semantic} {Role} {Labeling}},
	url = {https://pdfs.semanticscholar.org/3f80/05e94e39836bb3667bbe0ff6a24f21fcf3ca.pdf},
	urldate = {2016-06-22},
	author = {Yang, Shaohua and Gao, Qiaozi and Liu, Changsong and Xiong, Caiming and Zhu, Song-Chun and Chai, Joyce Y.},
	file = {N16-1019.pdf:/home/user/Zotero/storage/5684452P/N16-1019.pdf:application/pdf}
}

@article{rajendran-bridge-2015,
	title = {Bridge {Correlational} {Neural} {Networks} for {Multilingual} {Multimodal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1510.03519},
	urldate = {2016-06-22},
	journal = {arXiv preprint arXiv:1510.03519},
	author = {Rajendran, Janarthanan and Khapra, Mitesh M. and Chandar, Sarath and Ravindran, Balaraman},
	year = {2015},
	file = {N16-1021.pdf:/home/user/Zotero/storage/VKG6MHHS/N16-1021.pdf:application/pdf}
}

@article{gella-unsupervised-2016,
	title = {Unsupervised {Visual} {Sense} {Disambiguation} for {Verbs} using {Multimodal} {Embeddings}},
	url = {http://arxiv.org/abs/1603.09188},
	urldate = {2016-06-22},
	journal = {arXiv preprint arXiv:1603.09188},
	author = {Gella, Spandana and Lapata, Mirella and Keller, Frank},
	year = {2016},
	file = {N16-1022.pdf:/home/user/Zotero/storage/QV9652FT/N16-1022.pdf:application/pdf}
}

@article{yatskar-stating-nodate,
	title = {Stating the {Obvious}: {Extracting} {Visual} {Common} {Sense} {Knowledge}},
	shorttitle = {Stating the {Obvious}},
	url = {http://homes.cs.washington.edu/~my89/publications/commonsense.pdf},
	urldate = {2016-06-22},
	author = {Yatskar, Mark and Ordonez, Vicente and Farhadi, Ali},
	file = {N16-1023.pdf:/home/user/Zotero/storage/AD9ZKA8Q/N16-1023.pdf:application/pdf}
}

@inproceedings{xu-expected-2016,
	title = {Expected {F}-{Measure} {Training} for {Shift}-{Reduce} {Parsing} with {Recurrent} {Neural} {Networks}},
	url = {http://www.anthology.aclweb.org/N/N16/N16-1025.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Xu, Wenduan and Auli, Michael and Clark, Stephen},
	year = {2016},
	pages = {210--220},
	file = {N16-1025.pdf:/home/user/Zotero/storage/Z6CE92HD/N16-1025.pdf:application/pdf}
}

@inproceedings{lewis-lstm-2016,
	title = {{LSTM} {CCG} {Parsing}},
	url = {http://homes.cs.washington.edu/~lsz/papers/llz-naacl16.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of the 15th {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Lewis, Mike and Lee, Kenton and Zettlemoyer, Luke},
	year = {2016},
	file = {N16-1026.pdf:/home/user/Zotero/storage/4H42H6EC/N16-1026.pdf:application/pdf}
}

@inproceedings{zhang-name-2016,
	title = {Name {Tagging} for {Low}-resource {Incident} {Languages} based on {Expectation}-driven {Learning}},
	url = {http://www.anthology.aclweb.org/N/N16/N16-1029.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Zhang, Boliang and Pan, Xiaoman and Wang, Tianlu and Vaswani, Ashish and Ji, Heng and Knight, Kevin and Marcu, Daniel},
	year = {2016},
	pages = {249--259},
	file = {N16-1029.pdf:/home/user/Zotero/storage/5X8VPIVA/N16-1029.pdf:application/pdf}
}

@article{lample-neural-2016,
	title = {Neural architectures for named entity recognition},
	url = {http://arxiv.org/abs/1603.01360},
	urldate = {2016-06-22},
	journal = {arXiv preprint arXiv:1603.01360},
	author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
	year = {2016},
	file = {N16-1030.pdf:/home/user/Zotero/storage/EXMQX8RQ/N16-1030.pdf:application/pdf}
}

@inproceedings{choi-dynamic-2016,
	title = {Dynamic {Feature} {Induction}: {The} {Last} {Gist} to the {State}-of-the-{Art}},
	shorttitle = {Dynamic {Feature} {Induction}},
	url = {http://anthology.aclweb.org/N/N16/N16-1031.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Choi, Jinho D.},
	year = {2016},
	pages = {271--281},
	file = {N16-1031.pdf:/home/user/Zotero/storage/EBBW7XNV/N16-1031.pdf:application/pdf}
}

@article{zhang-top-down-2016,
	title = {Top-down tree long short-term memory networks},
	url = {https://pdfs.semanticscholar.org/c9c8/1b4bd4bca4db9ecc6955c38ee80dd02afc19.pdf},
	urldate = {2016-06-22},
	journal = {arXiv preprint arXiv:1511.00060},
	author = {Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
	year = {2016},
	pages = {0--5},
	file = {N16-1035.pdf:/home/user/Zotero/storage/5Z5JG9CE/N16-1035.pdf:application/pdf}
}

@inproceedings{monz-recurrent-2016,
	title = {Recurrent {Memory} {Networks} for {Language} {Modeling}},
	url = {http://anthology.aclweb.org/N/N16/N16-1036.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Monz, Ke Tran Arianna Bisazza Christof},
	year = {2016},
	pages = {321--331},
	file = {N16-1036.pdf:/home/user/Zotero/storage/WQG373PQ/N16-1036.pdf:application/pdf}
}

@inproceedings{koper-distinguishing-2016,
	title = {Distinguishing {Literal} and {Non}-{Literal} {Usage} of {German} {Particle} {Verbs}},
	url = {http://anthology.aclweb.org/N/N16/N16-1039.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Köper, Maximilian and im Walde, Sabine Schulte},
	year = {2016},
	pages = {353--362},
	file = {N16-1039.pdf:/home/user/Zotero/storage/E43FNV6E/N16-1039.pdf:application/pdf}
}

@inproceedings{liu-phrasal-2016,
	title = {Phrasal {Substitution} of {Idiomatic} {Expressions}},
	url = {http://www.aclweb.org/anthology/N/N16/N16-1040.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Liu, Changsheng and Hwa, Rebecca},
	year = {2016},
	pages = {363--373},
	file = {N16-1040.pdf:/home/user/Zotero/storage/QRCP9UUT/N16-1040.pdf:application/pdf}
}

@inproceedings{liu-agreement-2016,
	title = {Agreement on {Target}-bidirectional {Neural} {Machine} {Translation}},
	url = {http://anthology.aclweb.org/N/N16/N16-1046.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Liu, Lemao and Utiyama, Masao and Finch, Andrew and Sumita, Eiichiro},
	year = {2016},
	pages = {411--416},
	file = {N16-1046.pdf:/home/user/Zotero/storage/JBWQFPXP/N16-1046.pdf:application/pdf}
}

@inproceedings{girlea-psycholinguistic-2016,
	title = {Psycholinguistic {Features} for {Deceptive} {Role} {Detection} in {Werewolf}},
	url = {http://www.aclweb.org/anthology/N/N16/N16-1047.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Girlea, Codruta and Girju, Roxana and Amir, Eyal},
	year = {2016},
	pages = {417--422},
	file = {N16-1047.pdf:/home/user/Zotero/storage/XVVGK4CF/N16-1047.pdf:application/pdf}
}

@inproceedings{ferreira-individual-2016,
	title = {Individual {Variation} in the {Choice} of {Referential} {Form}},
	url = {http://anthology.aclweb.org/N/N16/N16-1048.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Ferreira, Thiago Castro and Krahmer, Emiel and Wubben, Sander},
	year = {2016},
	pages = {423--427},
	file = {N16-1048.pdf:/home/user/Zotero/storage/57K5ZGSR/N16-1048.pdf:application/pdf}
}

@inproceedings{paetzold-inferring-2016,
	title = {Inferring {Psycholinguistic} {Properties} of {Words}},
	url = {http://anthology.aclweb.org/N/N16/N16-1050.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Paetzold, Gustavo Henrique and Specia, Lucia},
	year = {2016},
	pages = {435--440},
	file = {N16-1050.pdf:/home/user/Zotero/storage/X5A4UV4N/N16-1050.pdf:application/pdf}
}

@inproceedings{ambati-shift-reduce-2016,
	title = {Shift-{Reduce} {CCG} {Parsing} using {Neural} {Network} {Models}},
	url = {http://anthology.aclweb.org/N/N16/N16-1052.pdf},
	urldate = {2016-06-22},
	booktitle = {Proceedings of {NAACL}-{HLT}},
	author = {Ambati, Bharat Ram and Deoskar, Tejaswini and Steedman, Mark},
	year = {2016},
	pages = {447--453},
	file = {N16-1052.pdf:/home/user/Zotero/storage/MT2SEAWZ/N16-1052.pdf:application/pdf}
}

@article{gehrke-stone-2016,
	title = {{STONE} {DUALITY}, {TOPOLOGICAL} {ALGEBRA}, {AND} {RECOGNITION}},
	url = {https://hal.archives-ouvertes.fr/hal-00859717},
	abstract = {Our main result is that any topological algebra based on a Boolean space is the extended Stone dual space of a certain associated Boolean algebra with additional operations. A particular case of this result is that the profinite completion of any abstract algebra is the extended Stone dual space of the Boolean algebra of recognizable subsets of the abstract algebra endowed with certain residuation operations. These results identify a connection between topological algebra as applied in algebra and Stone duality as applied in logic, and show that the notion of recognition originating in computer science is intrinsic to profinite completion in mathematics in general. This connection underlies a number of recent results in automata theory including a generalization of Eilenberg-Reiterman theory for regular languages and a new notion of compact recognition applying beyond the setting of regular languages. The purpose of this paper is to give the underlying duality theoretic result in its general form. Further we illustrate the power of the result by providing a few applications in topological algebra and language theory. In particular, we give a simple proof of the fact that any topological algebra quotient of a profinite algebra which is again based on a Boolean space is in fact a profinite algebra and we derive the conditions dual to the ones of the original Eilenberg theorem in a fully modular manner. We cast our results in the setting of extended Priestley duality for distributive lattices with additional operations as some classes of languages of interest in automata fail to be closed under complementation.},
	urldate = {2016-06-22},
	journal = {Journal of Pure and Applied Algebra},
	author = {Gehrke, Mai},
	year = {2016},
	keywords = {automata and recognition, profinite completion, Stone/Priestley duality for lattices with additional operations, topological algebra},
	file = {HAL Snapshot:/home/user/Zotero/storage/SWI5NKM2/hal-00859717.html:text/html}
}

@article{therien-temporal-2001,
	title = {Temporal {Logic} and {Semidirect} {Products}: {An} {Effective} {Characterization} of the {Until} {Hierarchy}},
	volume = {31},
	issn = {0097-5397},
	shorttitle = {Temporal {Logic} and {Semidirect} {Products}},
	url = {http://epubs.siam.org/doi/abs/10.1137/S0097539797322772},
	doi = {10.1137/S0097539797322772},
	abstract = {We reveal an intimate connection between semidirect products of finite semigroups and substitution of formulas in linear temporal logic. We use this connection to obtain an algebraic characterization of the until hierarchy of linear temporal logic. (The kth level of that hierarchy is comprised of all temporal properties that are expressible by a formula of nesting depth k in the until operator.) Applying deep results from finite semigroup theory we are able to prove that each level of the until hierarchy is decidable. By means of Ehrenfeucht--Fraissé games, we extend the results from linear temporal logic over finite sequences to linear temporal logic over infinite sequences.},
	number = {3},
	urldate = {2016-06-21},
	journal = {SIAM Journal on Computing},
	author = {Thérien, D. and Wilke, T.},
	month = jan,
	year = {2001},
	pages = {777--798},
	file = {Snapshot:/home/user/Zotero/storage/WIUW3CHH/S0097539797322772.html:text/html}
}

@article{wilke-twentieth-1996,
	title = {Twentieth {International} {Colloquium} on {Automata}, {Languages} and {ProgrammingAn} algebraic characterization of frontier testable tree languages},
	volume = {154},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/030439759500131X},
	doi = {10.1016/0304-3975(95)00131-X},
	abstract = {The class of frontier testable (i.e., reverse definite) tree languages is characterized by a finite set of pseudoidentities for tree algebras, which are introduced here for this characterization. An efficient algorithm is presented that decides whether a given tree automaton recognizes a frontier testable tree language. The algorithm runs in time O(mn3 + m2n2), where m is the cardinality of the alphabet and n is the number of states of the automaton.},
	number = {1},
	urldate = {2016-06-21},
	journal = {Theoretical Computer Science},
	author = {Wilke, Thomas},
	month = jan,
	year = {1996},
	pages = {85--106},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/S33JNK4N/030439759500131X.html:text/html}
}

@misc{noauthor-[1606.01541]-nodate,
	title = {[1606.01541] {Deep} {Reinforcement} {Learning} for {Dialogue} {Generation}},
	url = {https://arxiv.org/abs/1606.01541},
	urldate = {2016-06-21},
	file = {[1606.01541] Deep Reinforcement Learning for Dialogue Generation:/home/user/Zotero/storage/SVNIJHAG/1606.html:text/html}
}

@misc{noauthor-[1606.03217]-nodate,
	title = {[1606.03217] {Decidable} {Characterization} of {FO}2(\&lt;,+1) and locality of {DA}},
	url = {https://arxiv.org/abs/1606.03217},
	urldate = {2016-06-21},
	file = {[1606.03217] Decidable Characterization of FO2(&lt\;,+1) and locality of DA:/home/user/Zotero/storage/22SWU3HE/1606.html:text/html}
}

@misc{noauthor-[1606.03153]-nodate,
	title = {[1606.03153] {Unsupervised} {Learning} of {Word}-{Sequence} {Representations} from {Scratch} via {Convolutional} {Tensor} {Decomposition}},
	url = {https://arxiv.org/abs/1606.03153},
	urldate = {2016-06-21},
	file = {[1606.03153] Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition:/home/user/Zotero/storage/8E9MTVCC/1606.html:text/html}
}

@misc{noauthor-[1606.05378]-nodate,
	title = {[1606.05378] {Simpler} {Context}-{Dependent} {Logical} {Forms} via {Model} {Projections}},
	url = {https://arxiv.org/abs/1606.05378},
	urldate = {2016-06-20},
	file = {[1606.05378] Simpler Context-Dependent Logical Forms via Model Projections:/home/user/Zotero/storage/PQ249HBX/1606.html:text/html}
}

@misc{noauthor-[1511.05666]-nodate,
	title = {[1511.05666] {Super}-{Resolution} with {Deep} {Convolutional} {Sufficient} {Statistics}},
	url = {http://arxiv.org/abs/1511.05666},
	urldate = {2016-06-20},
	file = {[1511.05666] Super-Resolution with Deep Convolutional Sufficient Statistics:/home/user/Zotero/storage/73693BC7/1511.html:text/html}
}

@misc{noauthor-[1511.06732]-nodate,
	title = {[1511.06732] {Sequence} {Level} {Training} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06732},
	urldate = {2016-06-20},
	file = {[1511.06732] Sequence Level Training with Recurrent Neural Networks:/home/user/Zotero/storage/UNRUUQEV/1511.html:text/html}
}

@misc{noauthor-[1511.05122]-nodate,
	title = {[1511.05122] {Adversarial} {Manipulation} of {Deep} {Representations}},
	url = {http://arxiv.org/abs/1511.05122},
	urldate = {2016-06-20},
	file = {[1511.05122] Adversarial Manipulation of Deep Representations:/home/user/Zotero/storage/WKAM3AIM/1511.html:text/html}
}

@misc{noauthor-[1511.06394]-nodate-1,
	title = {[1511.06394] {Geodesics} of learned representations},
	url = {http://arxiv.org/abs/1511.06394},
	urldate = {2016-06-20},
	file = {[1511.06394] Geodesics of learned representations:/home/user/Zotero/storage/7DQIGR9N/1511.html:text/html}
}

@misc{noauthor-[1511.08228]-nodate-1,
	title = {[1511.08228] {Neural} {GPUs} {Learn} {Algorithms}},
	url = {http://arxiv.org/abs/1511.08228},
	urldate = {2016-06-20},
	file = {[1511.08228] Neural GPUs Learn Algorithms:/home/user/Zotero/storage/P3VKRB73/1511.html:text/html}
}

@misc{noauthor-[1511.06342]-nodate,
	title = {[1511.06342] {Actor}-{Mimic}: {Deep} {Multitask} and {Transfer} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06342},
	urldate = {2016-06-20},
	file = {[1511.06342] Actor-Mimic\: Deep Multitask and Transfer Reinforcement Learning:/home/user/Zotero/storage/TPMMIRAT/1511.html:text/html}
}

@misc{noauthor-[1511.04561]-nodate-1,
	title = {[1511.04561] 8-{Bit} {Approximations} for {Parallelism} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1511.04561},
	urldate = {2016-06-20},
	file = {[1511.04561] 8-Bit Approximations for Parallelism in Deep Learning:/home/user/Zotero/storage/4S9NZHQ3/1511.html:text/html}
}

@misc{noauthor-[1511.05939]-nodate,
	title = {[1511.05939] {Metric} {Learning} with {Adaptive} {Density} {Discrimination}},
	url = {http://arxiv.org/abs/1511.05939},
	urldate = {2016-06-20},
	file = {[1511.05939] Metric Learning with Adaptive Density Discrimination:/home/user/Zotero/storage/6UHX6WG4/1511.html:text/html}
}

@misc{noauthor-[1511.05493]-nodate-1,
	title = {[1511.05493] {Gated} {Graph} {Sequence} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	urldate = {2016-06-20},
	file = {[1511.05493] Gated Graph Sequence Neural Networks:/home/user/Zotero/storage/422Q4QNH/1511.html:text/html}
}

@misc{noauthor-[1511.06410]-nodate-1,
	title = {[1511.06410] {Better} {Computer} {Go} {Player} with {Neural} {Network} and {Long}-term {Prediction}},
	url = {http://arxiv.org/abs/1511.06410},
	urldate = {2016-06-20},
	file = {[1511.06410] Better Computer Go Player with Neural Network and Long-term Prediction:/home/user/Zotero/storage/TJBMKWZT/1511.html:text/html}
}

@misc{noauthor-[1511.04143]-nodate-1,
	title = {[1511.04143] {Deep} {Reinforcement} {Learning} in {Parameterized} {Action} {Space}},
	url = {http://arxiv.org/abs/1511.04143},
	urldate = {2016-06-20},
	file = {[1511.04143] Deep Reinforcement Learning in Parameterized Action Space:/home/user/Zotero/storage/5CJBXJGC/1511.html:text/html}
}

@misc{noauthor-[1511.05176]-nodate-1,
	title = {[1511.05176] {MuProp}: {Unbiased} {Backpropagation} for {Stochastic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05176},
	urldate = {2016-06-20},
	file = {[1511.05176] MuProp\: Unbiased Backpropagation for Stochastic Neural Networks:/home/user/Zotero/storage/FPFB4P7S/1511.html:text/html}
}

@misc{noauthor-[1511.04834]-nodate-1,
	title = {[1511.04834] {Neural} {Programmer}: {Inducing} {Latent} {Programs} with {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1511.04834},
	urldate = {2016-06-20},
	file = {[1511.04834] Neural Programmer\: Inducing Latent Programs with Gradient Descent:/home/user/Zotero/storage/NJC8KMXD/1511.html:text/html}
}

@misc{noauthor-[1509.00519]-nodate-1,
	title = {[1509.00519] {Importance} {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	urldate = {2016-06-20},
	file = {[1509.00519] Importance Weighted Autoencoders:/home/user/Zotero/storage/NIPDFTFK/1509.html:text/html}
}

@misc{noauthor-[1511.05952]-nodate,
	title = {[1511.05952] {Prioritized} {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	urldate = {2016-06-20},
	file = {[1511.05952] Prioritized Experience Replay:/home/user/Zotero/storage/CFWF6555/1511.html:text/html}
}

@misc{noauthor-[1511.03677]-nodate-1,
	title = {[1511.03677] {Learning} to {Diagnose} with {LSTM} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.03677},
	urldate = {2016-06-20},
	file = {[1511.03677] Learning to Diagnose with LSTM Recurrent Neural Networks:/home/user/Zotero/storage/Z5JW2FCQ/1511.html:text/html}
}

@misc{noauthor-bcbl-nodate,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2428/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/TQWGT3UE/2428.html:text/html}
}

@misc{noauthor-bcbl-nodate-1,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2349/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/QAMDU5KK/2349.html:text/html}
}

@misc{noauthor-bcbl-nodate-2,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2120/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/HA9P9SIP/2120.html:text/html}
}

@misc{noauthor-bcbl-nodate-3,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2201/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/MQ2N2K3M/2201.html:text/html}
}

@misc{noauthor-bcbl-nodate-4,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2307/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/4SHFRCEV/2307.html:text/html}
}

@misc{noauthor-bcbl-nodate-5,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2419/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/C7KCFJBB/2419.html:text/html}
}

@misc{noauthor-bcbl-nodate-6,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2072/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/QPJ2XT2V/2072.html:text/html}
}

@misc{noauthor-bcbl-nodate-7,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2327/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/RHDWPCAC/2327.html:text/html}
}

@misc{noauthor-bcbl-nodate-8,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2080/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/JU5VFAWJ/2080.html:text/html}
}

@misc{noauthor-bcbl-nodate-9,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2150/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/36KD6XZM/2150.html:text/html}
}

@misc{noauthor-bcbl-nodate-10,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2070/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/E699JH68/2070.html:text/html}
}

@misc{noauthor-bcbl-nodate-11,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2389/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/KR254CZC/2389.html:text/html}
}

@misc{noauthor-bcbl-nodate-12,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2283/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/HH6BN4QJ/2283.html:text/html}
}

@misc{noauthor-bcbl-nodate-13,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2095/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/QGC789AU/2095.html:text/html}
}

@misc{noauthor-plos-nodate-2,
	title = {{PLOS} {ONE}: {The} {Formation} of {Social} {Conventions} in {Real}-{Time} {Environments}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0151670},
	urldate = {2016-08-13},
	file = {PLOS ONE\: The Formation of Social Conventions in Real-Time Environments:/home/user/Zotero/storage/4HISQVES/article.html:text/html}
}

@article{futrell-response-2015,
	title = {Response to {Liu}, {Xu}, and {Liang} (2015) and {Ferrer}-i-{Cancho} and {G}\'omez-{Rodr}\'iguez (2015) on {Dependency} {Length} {Minimization}},
	url = {http://arxiv.org/abs/1510.00436},
	abstract = {We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\'omez-Rodr\'iguez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)'s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories.},
	urldate = {2016-08-13},
	journal = {arXiv:1510.00436 [cs]},
	author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.00436},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/WBNWWHAV/1510.html:text/html;Futrell et al_2015_Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and.pdf:/home/user/Zotero/storage/K7UKA7PG/Futrell et al_2015_Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and.pdf:application/pdf}
}

@article{liu-dependency-2015,
	title = {Dependency length minimization: {Puzzles} and {Promises}},
	shorttitle = {Dependency length minimization},
	url = {http://arxiv.org/abs/1509.04393},
	abstract = {In the recent issue of PNAS, Futrell et al. claims that their study of 37 languages gives the first large scale cross-language evidence for Dependency Length Minimization, which is an overstatement that ignores similar previous researches. In addition,this study seems to pay no attention to factors like the uniformity of genres,which weakens the validity of the argument that DLM is universal. Another problem is that this study sets the baseline random language as projective, which fails to truly uncover the difference between natural language and random language, since projectivity is an important feature of many natural languages. Finally, the paper contends an "apparent relationship between head finality and dependency length" despite the lack of an explicit statistical comparison, which renders this conclusion rather hasty and improper.},
	urldate = {2016-08-13},
	journal = {arXiv:1509.04393 [cs]},
	author = {Liu, Haitao and Xu, Chunshan and Liang, Junying},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.04393},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9C8FZ4KA/1509.html:text/html;Liu et al_2015_Dependency length minimization.pdf:/home/user/Zotero/storage/VJMKWDPX/Liu et al_2015_Dependency length minimization.pdf:application/pdf}
}

@misc{noauthor-subsegmental-nodate,
	title = {Subsegmental structure facilitates learning of phonotactic distributions {\textbar} {Linguistic} {Society} of {America}},
	url = {http://www.linguisticsociety.org/abstract/subsegmental-structure-facilitates-learning-phonotactic-distributions},
	urldate = {2016-08-13},
	file = {Subsegmental structure facilitates learning of phonotactic distributions | Linguistic Society of America:/home/user/Zotero/storage/J2BJ66CE/subsegmental-structure-facilitates-learning-phonotactic-distributions.html:text/html}
}

@misc{sebastian-schoener-masterarbeit-nodate,
	title = {masterarbeit},
	author = {{sebastian schoener}},
	file = {paper.pdf:/home/user/Zotero/storage/BASPN5GH/paper.pdf:application/pdf}
}

@article{mnih-neural-2014,
	title = {Neural {Variational} {Inference} and {Learning} in {Belief} {Networks}},
	url = {http://arxiv.org/abs/1402.0030},
	abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
	urldate = {2016-08-04},
	journal = {arXiv:1402.0030 [cs, stat]},
	author = {Mnih, Andriy and Gregor, Karol},
	month = jan,
	year = {2014},
	note = {arXiv: 1402.0030},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3KEMR634/1402.html:text/html;Mnih_Gregor_2014_Neural Variational Inference and Learning in Belief Networks.pdf:/home/user/Zotero/storage/NWRMBSZM/Mnih_Gregor_2014_Neural Variational Inference and Learning in Belief Networks.pdf:application/pdf}
}

@book{goldberg-learning-1989,
	address = {Reading, Mass},
	title = {learning simple algorithms from examples},
	isbn = {978-0-201-15767-3},
	publisher = {Addison-Wesley Pub. Co},
	author = {Goldberg, David E.},
	year = {1989},
	keywords = {Machine learning, Genetic algorithms},
	file = {zaremba16.pdf:/home/user/Zotero/storage/WMFIUG4G/zaremba16.pdf:application/pdf}
}

@article{barak-modeling-nodate,
	title = {Modeling the {Emergence} of an {Exemplar} {Verb} in {Construction} {Learning}},
	url = {http://ftp.cs.toronto.edu/dist/libbyb/publications/BarakEtAl\_cogsci13.pdf},
	urldate = {2016-08-04},
	author = {Barak, Libby and Fazly, Afsaneh and Stevenson, Suzanne},
	file = {BarakEtAl_cogsci13.pdf:/home/user/Zotero/storage/SQ9P6TZ7/BarakEtAl_cogsci13.pdf:application/pdf}
}

@article{barak-acquisition-2013,
	title = {Acquisition of desires before beliefs: {A} computational investigation},
	shorttitle = {Acquisition of desires before beliefs},
	url = {http://www.aclweb.org/anthology/W13-35#page=243},
	urldate = {2016-08-04},
	journal = {Proceedings of CoNLL-2013},
	author = {Barak, Libby and Fazly, Afsaneh and Stevenson, Suzanne},
	year = {2013},
	file = {BarakEtAl_CoNLL13.pdf:/home/user/Zotero/storage/CDSZZURN/BarakEtAl_CoNLL13.pdf:application/pdf}
}

@inproceedings{nematzadeh-computational-2012,
	title = {A computational model of memory, attention, and word learning},
	url = {http://dl.acm.org/citation.cfm?id=2390316},
	urldate = {2016-08-04},
	booktitle = {Proceedings of the 3rd {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nematzadeh, Aida and Fazly, Afsaneh and Stevenson, Suzanne},
	year = {2012},
	pages = {80--89},
	file = {nematzadeh_etal_12_cmcl.pdf:/home/user/Zotero/storage/4MTZUPVZ/nematzadeh_etal_12_cmcl.pdf:application/pdf}
}

@inproceedings{nematzadeh-interaction-2012,
	title = {Interaction of word learning and semantic category formation in late talking},
	volume = {12},
	url = {http://mindmodeling.org/cogsci2012/papers/0364/paper0364.pdf},
	urldate = {2016-08-04},
	booktitle = {Proc. of {CogSci}},
	author = {Nematzadeh, Aida and Fazly, Afsaneh and Stevenson, Suzanne},
	year = {2012},
	file = {nematzadeh_etal_12_cogsci.pdf:/home/user/Zotero/storage/DBVI7W2W/nematzadeh_etal_12_cogsci.pdf:application/pdf}
}

@inproceedings{barak-modeling-2012,
	title = {Modeling the acquisition of mental state verbs},
	url = {http://dl.acm.org/citation.cfm?id=2390306},
	urldate = {2016-08-04},
	booktitle = {Proceedings of the 3rd {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Barak, Libby and Fazly, Afsaneh and Stevenson, Suzanne},
	year = {2012},
	pages = {1--10},
	file = {BarakEtAl-CMCL2012.pdf:/home/user/Zotero/storage/T89H7BXD/BarakEtAl-CMCL2012.pdf:application/pdf}
}

@article{fazly-probabilistic-2010,
	title = {A {Probabilistic} {Computational} {Model} of {Cross}-{Situational} {Word} {Learning}},
	volume = {34},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2010.01104.x},
	doi = {10.1111/j.1551-6709.2010.01104.x},
	language = {en},
	number = {6},
	urldate = {2016-08-04},
	journal = {Cognitive Science},
	author = {Fazly, Afsaneh and Alishahi, Afra and Stevenson, Suzanne},
	month = may,
	year = {2010},
	pages = {1017--1063},
	file = {FazlyEtAl-CSJ10.pdf:/home/user/Zotero/storage/2TFRW75U/FazlyEtAl-CSJ10.pdf:application/pdf}
}

@inproceedings{parisien-learning-2010,
	title = {Learning verb alternations in a usage-based {Bayesian} model},
	url = {http://mindmodeling.org/cogsci2010/papers/0641/paper0641.pdf},
	urldate = {2016-08-04},
	booktitle = {Proceedings of the 32nd annual meeting of the {Cognitive} {Science} {Society}},
	author = {Parisien, Christopher and Stevenson, Suzanne},
	year = {2010},
	file = {ParisienStevenson-Cogsci2010.pdf:/home/user/Zotero/storage/JKNVWFRJ/ParisienStevenson-Cogsci2010.pdf:application/pdf}
}

@article{deri-grapheme--phoneme-nodate,
	title = {Grapheme-to-{Phoneme} {Models} for ({Almost}) {Any} {Language}},
	url = {http://isi.edu/~aderi/papers/g2p.pdf},
	urldate = {2016-08-04},
	author = {Deri, Aliya and Knight, Kevin},
	file = {P16-1038.pdf:/home/user/Zotero/storage/HG4UQHZG/P16-1038.pdf:application/pdf}
}

@article{angeli-combining-nodate,
	title = {Combining {Natural} {Logic} and {Shallow} {Reasoning} for {Question} {Answering}},
	url = {http://nlp.stanford.edu/pubs/angeli2016naturalli.pdf},
	urldate = {2016-08-04},
	author = {Angeli, Gabor and Nayak, Neha and Manning, Christopher D.},
	file = {P16-1042.pdf:/home/user/Zotero/storage/6RNGUDRM/P16-1042.pdf:application/pdf}
}

@article{xu-entropy-nodate,
	title = {Entropy {Converges} {Between} {Dialogue} {Participants}: {Explanations} from an {Information}-{Theoretic} {Perspective}},
	shorttitle = {Entropy {Converges} {Between} {Dialogue} {Participants}},
	url = {http://www.david-reitter.com/pub/xu\_reitter\_entropyconvergence\_ACL2016.pdf},
	urldate = {2016-08-04},
	author = {Xu, Yang and Reitter, David},
	file = {P16-1051.pdf:/home/user/Zotero/storage/8JR63S68/P16-1051.pdf:application/pdf}
}

@article{khani-unanimous-2016,
	title = {Unanimous {Prediction} for 100\% {Precision} with {Application} to {Learning} {Semantic} {Mappings}},
	url = {http://arxiv.org/abs/1606.06368},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1606.06368},
	author = {Khani, Fereshte and Rinard, Martin},
	year = {2016},
	file = {P16-1090.pdf:/home/user/Zotero/storage/RIZQTSV7/P16-1090.pdf:application/pdf}
}

@article{luong-achieving-2016,
	title = {Achieving open vocabulary neural machine translation with hybrid word-character models},
	url = {http://arxiv.org/abs/1604.00788},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1604.00788},
	author = {Luong, Minh-Thang and Manning, Christopher D.},
	year = {2016},
	file = {P16-1100.pdf:/home/user/Zotero/storage/K9MKDIJN/P16-1100.pdf:application/pdf}
}

@article{mishra-harnessing-nodate,
	title = {Harnessing {Cognitive} {Features} for {Sarcasm} {Detection}},
	url = {http://www.cfilt.iitb.ac.in/cognitive-nlp/papers/acl16-sarcasm-detection.pdf},
	urldate = {2016-08-04},
	author = {Mishra, Abhijit and Kanojia, Diptesh and Nagar, Seema and Dey, Kuntal and Bhattacharyya, Pushpak},
	file = {P16-1104.pdf:/home/user/Zotero/storage/3VB57I3Z/P16-1104.pdf:application/pdf}
}

@article{shaham-deep-2016,
	title = {A {Deep} {Learning} {Approach} to {Unsupervised} {Ensemble} {Learning}},
	url = {http://arxiv.org/abs/1602.02285},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1602.02285},
	author = {Shaham, Uri and Cheng, Xiuyuan and Dror, Omer and Jaffe, Ariel and Nadler, Boaz and Chang, Joseph and Kluger, Yuval},
	year = {2016},
	file = {shaham16.pdf:/home/user/Zotero/storage/H6UISTUJ/shaham16.pdf:application/pdf}
}

@inproceedings{xie-diversity-promoting-2016,
	title = {Diversity-{Promoting} {Bayesian} {Learning} of {Latent} {Variable} {Models}},
	url = {http://www.cs.cmu.edu/~pengtaox/papers/icml16\_mabn.pdf},
	urldate = {2016-08-04},
	booktitle = {Proceedings of the 33st {International} {Conference} on {Machine} {Learning} ({ICML}-16)},
	author = {Xie, Pengtao and Zhu, Jun and Xing, Eric},
	year = {2016},
	file = {xiea16.pdf:/home/user/Zotero/storage/PBWX7S4Z/xiea16.pdf:application/pdf}
}

@article{amodei-deep-2015,
	title = {Deep speech 2: {End}-to-end speech recognition in english and mandarin},
	shorttitle = {Deep speech 2},
	url = {http://arxiv.org/abs/1512.02595},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1512.02595},
	author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and {others}},
	year = {2015},
	file = {amodei16.pdf:/home/user/Zotero/storage/48HHKUSX/amodei16.pdf:application/pdf}
}

@book{clote-feasible-1995,
	title = {Feasible mathematics {II}},
	isbn = {978-1-4612-2566-9},
	url = {http://dx.doi.org/10.1007/978-1-4612-2566-9},
	language = {English},
	urldate = {2016-08-04},
	author = {Clote, Peter and Remmel, Jeffrey B},
	year = {1995},
	note = {OCLC: 853260861},
	file = {xue16.pdf:/home/user/Zotero/storage/5CGKU6VI/xue16.pdf:application/pdf}
}

@inproceedings{ranganath-hierarchical-2016,
	title = {Hierarchical variational models},
	url = {http://www.jmlr.org/proceedings/papers/v48/ranganath16.pdf},
	urldate = {2016-08-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M.},
	year = {2016},
	file = {ranganath16.pdf:/home/user/Zotero/storage/G5JRWJV2/ranganath16.pdf:application/pdf}
}

@article{mandt-variational-2016,
	title = {A {Variational} {Analysis} of {Stochastic} {Gradient} {Algorithms}},
	url = {http://arxiv.org/abs/1602.02666},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1602.02666},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	year = {2016},
	file = {mandt16.pdf:/home/user/Zotero/storage/SGQN2X5F/mandt16.pdf:application/pdf}
}

@article{lerer-learning-2016,
	title = {Learning {Physical} {Intuition} of {Block} {Towers} by {Example}},
	url = {http://arxiv.org/abs/1603.01312},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1603.01312},
	author = {Lerer, Adam and Gross, Sam and Fergus, Rob},
	year = {2016},
	file = {lerer16.pdf:/home/user/Zotero/storage/UFRDQ5RX/lerer16.pdf:application/pdf}
}

@inproceedings{couillet-random-2016,
	title = {A {Random} {Matrix} {Approach} to {Echo}-{State} {Neural} {Networks}},
	url = {http://www.jmlr.org/proceedings/papers/v48/couillet16.pdf},
	urldate = {2016-08-04},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	author = {Couillet, Romain and FR, CENTRALESUPELEC and Wainrib, Gilles and FR, ENS and Ali, Hafiz Tiomoko},
	year = {2016},
	pages = {517--525},
	file = {couillet16.pdf:/home/user/Zotero/storage/FCGJTFFE/couillet16.pdf:application/pdf}
}

@book{sutton-reinforcement-1998,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	keywords = {reinforcement learning},
	file = {jiang16.pdf:/home/user/Zotero/storage/XMZ77AU8/jiang16.pdf:application/pdf}
}

@inproceedings{wang-analysis-2016,
	title = {Analysis of {Deep} {Neural} {Networks} with the {Extended} {Data} {Jacobian} {Matrix}},
	url = {http://www.jmlr.org/proceedings/papers/v48/wanga16.pdf},
	urldate = {2016-08-04},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	author = {Wang, Shengjie and Mohamed, Abdel-rahman and Caruana, Rich and Bilmes, Jeff and Plilipose, Matthai and Richardson, Matthew and Geras, Krzysztof and Urban, Gregor and Aslan, Ozlem},
	year = {2016},
	pages = {718--726},
	file = {wanga16.pdf:/home/user/Zotero/storage/RKU475G6/wanga16.pdf:application/pdf}
}

@article{shalev-shwartz-minimizing-2016,
	title = {Minimizing the {Maximal} {Loss}: {How} and {Why}?},
	shorttitle = {Minimizing the {Maximal} {Loss}},
	url = {http://arxiv.org/abs/1602.01690},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1602.01690},
	author = {Shalev-Shwartz, Shai and Wexler, Yonatan},
	year = {2016},
	file = {shalev-shwartzb16.pdf:/home/user/Zotero/storage/J8UHZ245/shalev-shwartzb16.pdf:application/pdf}
}

@article{zhang-$$backslash$-2015,
	title = {\$\$\backslash\$ ell\_1 \$-regularized {Neural} {Networks} are {Improperly} {Learnable} in {Polynomial} {Time}},
	url = {http://arxiv.org/abs/1510.03528},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1510.03528},
	author = {Zhang, Yuchen and Lee, Jason D. and Jordan, Michael I.},
	year = {2015},
	file = {zhangd16.pdf:/home/user/Zotero/storage/5JMRHXR6/zhangd16.pdf:application/pdf}
}

@article{gal-dropout-2015-2,
	title = {Dropout as a {Bayesian} approximation: {Representing} model uncertainty in deep learning},
	shorttitle = {Dropout as a {Bayesian} approximation},
	url = {http://arxiv.org/abs/1506.02142},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1506.02142},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = {2015},
	file = {gal16.pdf:/home/user/Zotero/storage/79NRIJUT/gal16.pdf:application/pdf}
}

@article{li-learning-2016,
	title = {Learning to {Generate} with {Memory}},
	url = {http://arxiv.org/abs/1602.07416},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1602.07416},
	author = {Li, Chongxuan and Zhu, Jun and Zhang, Bo},
	year = {2016},
	file = {lie16.pdf:/home/user/Zotero/storage/AS4S5I28/lie16.pdf:application/pdf}
}

@article{balduzzi-strongly-typed-2016,
	title = {Strongly-{Typed} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.02218},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1602.02218},
	author = {Balduzzi, David and Ghifary, Muhammad},
	year = {2016},
	file = {balduzzi16.pdf:/home/user/Zotero/storage/HS3V2MQS/balduzzi16.pdf:application/pdf}
}

@article{zhao-collapsed-nodate,
	title = {Collapsed {Variational} {Inference} for {Sum}-{Product} {Networks}},
	url = {http://www.cs.cmu.edu/~hzhao1/papers/ICML2016/BL-SPN-main.pdf},
	urldate = {2016-08-04},
	author = {Zhao, Han and Adel, Tameem and Gordon, Geoff and Amos, Brandon},
	file = {zhaoa16.pdf:/home/user/Zotero/storage/NEK95PRA/zhaoa16.pdf:application/pdf}
}

@book{eichenbaum-conditioning-2004,
	address = {Oxford; New York},
	title = {From conditioning to conscious recollection: memory systems of the brain},
	isbn = {978-0-19-517804-3},
	shorttitle = {From conditioning to conscious recollection},
	language = {English},
	publisher = {Oxford University Press},
	author = {Eichenbaum, Howard and Cohen, Neal J},
	year = {2004},
	note = {OCLC: 56695641},
	file = {kumar16.pdf:/home/user/Zotero/storage/334HWRSG/kumar16.pdf:application/pdf}
}

@article{algorta-why-nodate,
	title = {Why {Most} {Decisions} {Are} {Easy} in {Tetris}—{And} {Perhaps} in {Other} {Sequential} {Decision} {Problems}, {As} {Well}},
	url = {http://www.jmlr.org/proceedings/papers/v48/simsek16.pdf},
	urldate = {2016-08-04},
	author = {Algorta, Simón and MPG, BERLIN and Kothiyal, Amit},
	file = {simsek16.pdf:/home/user/Zotero/storage/F4U37MRC/simsek16.pdf:application/pdf}
}

@inproceedings{he-opponent-2016,
	title = {Opponent {Modeling} in {Deep} {Reinforcement} {Learning}},
	url = {http://www.jmlr.org/proceedings/papers/v48/he16.pdf},
	urldate = {2016-08-04},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	author = {He, He and EDU, UMD and Boyd-Graber, Jordan and Daumé III, Hal},
	year = {2016},
	pages = {1804--1813},
	file = {he16.pdf:/home/user/Zotero/storage/IU2E7AST/he16.pdf:application/pdf}
}

@article{bai-class-2015,
	title = {Class {Probability} {Estimation} via {Differential} {Geometric} {Regularization}},
	url = {http://arxiv.org/abs/1503.01436},
	urldate = {2016-08-04},
	journal = {arXiv preprint arXiv:1503.01436},
	author = {Bai, Qinxun and Rosenberg, Steven and Wu, Zheng and Sclaroff, Stan},
	year = {2015},
	file = {baia16.pdf:/home/user/Zotero/storage/6E6QPA43/baia16.pdf:application/pdf}
}

@article{friesen-sum-product-nodate,
	title = {The {Sum}-{Product} {Theorem}: {A} {Foundation} for {Learning} {Tractable} {Models}},
	shorttitle = {The {Sum}-{Product} {Theorem}},
	url = {http://homes.cs.washington.edu/~pedrod/papers/mlc16.pdf},
	urldate = {2016-08-04},
	author = {Friesen, Abram L. and EDU, WASHINGTON and Domingos, Pedro},
	file = {friesen16.pdf:/home/user/Zotero/storage/A9WXDFG4/friesen16.pdf:application/pdf}
}

@article{song-training-nodate,
	title = {Training {Deep} {Neural} {Networks} via {Direct} {Loss} {Minimization}},
	url = {http://www.alexander-schwing.de/papers/SongEtAl\_ICML2016.pdf},
	urldate = {2016-08-04},
	author = {Song, Yang and EDU, TSINGHUA and Schwing, Alexander G. and EDU, TORONTO and Zemel, Richard S. and Urtasun, Raquel},
	file = {songb16.pdf:/home/user/Zotero/storage/IICHWSDP/songb16.pdf:application/pdf}
}

@article{wang-isotonic-nodate,
	title = {Isotonic {Hawkes} {Processes}},
	url = {http://www.cc.gatech.edu/~ywang/papers/WanBoDuSon16.pdf},
	urldate = {2016-08-04},
	author = {Wang, Yichen and Xie, Bo and Du, Nan and Le Song, E. D. U. and EDU, GATECH},
	file = {wangg16.pdf:/home/user/Zotero/storage/KGFUKG6M/wangg16.pdf:application/pdf}
}

@book{jacobson-differential-1970,
	address = {New York},
	series = {Modern analytic and computational methods in science and mathematics},
	title = {Differential dynamic programming},
	isbn = {978-0-444-00070-5},
	number = {no. 24},
	publisher = {American Elsevier Pub. Co},
	author = {Jacobson, David H. and Mayne, David Q.},
	year = {1970},
	keywords = {Control theory, Dynamic programming, Mathematical optimization},
	file = {akrour16.pdf:/home/user/Zotero/storage/56STQ5UQ/akrour16.pdf:application/pdf}
}

@misc{noauthor-[1608.01281]-nodate,
	title = {[1608.01281] {Learning} {Online} {Alignments} with {Continuous} {Rewards} {Policy} {Gradient}},
	url = {https://arxiv.org/abs/1608.01281},
	urldate = {2016-08-04},
	file = {[1608.01281] Learning Online Alignments with Continuous Rewards Policy Gradient:/home/user/Zotero/storage/C6UH6686/1608.html:text/html}
}

@misc{noauthor-[1608.01056]-nodate,
	title = {[1608.01056] {Morphological} {Priors} for {Probabilistic} {Neural} {Word} {Embeddings}},
	url = {https://arxiv.org/abs/1608.01056},
	urldate = {2016-08-04},
	file = {[1608.01056] Morphological Priors for Probabilistic Neural Word Embeddings:/home/user/Zotero/storage/TNMIIHMU/1608.html:text/html}
}

@misc{noauthor-[1602.01595]-nodate,
	title = {[1602.01595] {Many} {Languages}, {One} {Parser}},
	url = {https://arxiv.org/abs/1602.01595},
	urldate = {2016-07-27},
	file = {[1602.01595] Many Languages, One Parser:/home/user/Zotero/storage/3667W6B2/1602.html:text/html}
}

@misc{noauthor-[1607.06556]-nodate,
	title = {[1607.06556] {Syntax}-based {Attention} {Model} for {Natural} {Language} {Inference}},
	url = {https://arxiv.org/abs/1607.06556},
	urldate = {2016-07-26},
	file = {[1607.06556] Syntax-based Attention Model for Natural Language Inference:/home/user/Zotero/storage/8AJ9EF69/1607.html:text/html}
}

@misc{noauthor-[1607.07057]-nodate,
	title = {[1607.07057] {Latent} {Tree} {Language} {Model}},
	url = {https://arxiv.org/abs/1607.07057},
	urldate = {2016-07-26},
	file = {[1607.07057] Latent Tree Language Model:/home/user/Zotero/storage/8XB5MV7C/1607.html:text/html}
}

@misc{noauthor-[1607.06852]-nodate,
	title = {[1607.06852] {CFGs}-2-{NLU}: {Sequence}-to-{Sequence} {Learning} for {Mapping} {Utterances} to {Semantics} and {Pragmatics}},
	url = {https://arxiv.org/abs/1607.06852},
	urldate = {2016-07-26},
	file = {[1607.06852] CFGs-2-NLU\: Sequence-to-Sequence Learning for Mapping Utterances to Semantics and Pragmatics:/home/user/Zotero/storage/U4PKG25T/1607.html:text/html}
}

@misc{noauthor-[1605.01326]-nodate,
	title = {[1605.01326] {Compression} and the origins of {Zipf}'s law for word frequencies},
	url = {https://arxiv.org/abs/1605.01326},
	urldate = {2016-07-21},
	file = {[1605.01326] Compression and the origins of Zipf's law for word frequencies:/home/user/Zotero/storage/DDUM4PT9/1605.html:text/html}
}

@misc{noauthor-[1603.04351]-nodate-1,
	title = {[1603.04351] {Simple} and {Accurate} {Dependency} {Parsing} {Using} {Bidirectional} {LSTM} {Feature} {Representations}},
	url = {https://arxiv.org/abs/1603.04351},
	urldate = {2016-07-21},
	file = {[1603.04351] Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations:/home/user/Zotero/storage/T2V5V6GC/1603.html:text/html}
}

@misc{noauthor-[1605.07515]-nodate,
	title = {[1605.07515] {Neural} {Semantic} {Role} {Labeling} with {Dependency} {Path} {Embeddings}},
	url = {https://arxiv.org/abs/1605.07515},
	urldate = {2016-07-19},
	file = {[1605.07515] Neural Semantic Role Labeling with Dependency Path Embeddings:/home/user/Zotero/storage/WHA2PJ5H/1605.html:text/html}
}

@misc{noauthor-[1607.04728]-nodate,
	title = {[1607.04728] {Cost} and dimension of words of zero topological entropy},
	url = {https://arxiv.org/abs/1607.04728},
	urldate = {2016-07-19},
	file = {[1607.04728] Cost and dimension of words of zero topological entropy:/home/user/Zotero/storage/IUA9CRQV/1607.html:text/html}
}

@misc{noauthor-[1607.05108]-nodate,
	title = {[1607.05108] {Neural} {Machine} {Translation} with {Recurrent} {Attention} {Modeling}},
	url = {https://arxiv.org/abs/1607.05108},
	urldate = {2016-07-19},
	file = {[1607.05108] Neural Machine Translation with Recurrent Attention Modeling:/home/user/Zotero/storage/C5XV2B3J/1607.html:text/html}
}

@misc{noauthor-[1607.04853]-nodate,
	title = {[1607.04853] {An} {Empirical} {Evaluation} of various {Deep} {Learning} {Architectures} for {Bi}-{Sequence} {Classification} {Tasks}},
	url = {https://arxiv.org/abs/1607.04853},
	urldate = {2016-07-19},
	file = {[1607.04853] An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks:/home/user/Zotero/storage/UU62BRDP/1607.html:text/html}
}

@article{mcqueen-eight-2007,
	title = {Eight questions about spoken-word recognition},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA37&dq=%22are+made+from+a+limited+set+of+words+that,+for+fluent+speakers+of+a%22+%22what+word+forms+the+speaker+intended,+in+a+plausible+order).+My+assumption+will%22+&ots=aURCaPKNvk&sig=hKcb3mI\_lKXSF-l2SfubN3S\_quU},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {McQueen, James M.},
	year = {2007},
	pages = {37--53},
	file = {oxfordhb-9780198568971-e-003.pdf:/home/user/Zotero/storage/GRRMAF5W/oxfordhb-9780198568971-e-003.pdf:application/pdf}
}

@article{gaskell-statistical-2007,
	title = {Statistical and connectionist models of speech perception and word recognition},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA55&dq=%22develop+and+adapt+to+fit+the+requirements+of+the+perceptual+system.+Section+4.4+looks+at%22+%22would+be+difficult,+and+rather+bizarre,+to+discuss+models+of+human+function+without+any+mention%22+&ots=aURCaPKNvf&sig=7mLfQ2xvmimbi4sLxj2SjvybTz8},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Gaskell, M. Gareth},
	year = {2007},
	pages = {55--69},
	file = {oxfordhb-9780198568971-e-004.pdf:/home/user/Zotero/storage/7JII7422/oxfordhb-9780198568971-e-004.pdf:application/pdf}
}

@article{pulvermuller-word-2007,
	title = {Word processing in the brain as revealed by neurophysiological imaging},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA119&dq=%22processing+in+the+brain+as+revealed+by+neurophysiological%22+%22level+of+theory%22+%22importance+of+the+%E2%80%9Cwhen%3F%E2%80%9D+question+in+psycholinguistics,+the+question+of+where+in%22+%22although+at+different+levels+of+the+causal+chain+(Henson,+2005%3B+Rugg%22+&ots=aURCaPKNul&sig=30VpK6YTXhDYHMLPTRQ1dW8v28A},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Pulvermüller, Friedemann},
	year = {2007},
	pages = {119},
	file = {oxfordhb-9780198568971-e-008.pdf:/home/user/Zotero/storage/4FGRVTZJ/oxfordhb-9780198568971-e-008.pdf:application/pdf}
}

@article{blumstein-word-2007,
	title = {Word recognition in aphasia},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA141&dq=%22recognition+in%22+%22how+and+in+what+ways+the+word+recognition+system+may+fragment.+It+also%22+%22and+often+agrammatic,+and+their+production+of+the+sound+structure+of+language%22+&ots=aURCaPKNui&sig=f-S-6QNaFyDKaGOeHGOfk6Q8WsM},
	urldate = {2016-05-28},
	journal = {The Oxford handbook of psycholinguistics},
	author = {Blumstein, S.},
	year = {2007},
	pages = {141--155},
	file = {oxfordhb-9780198568971-e-009.pdf:/home/user/Zotero/storage/3V9NIFMH/oxfordhb-9780198568971-e-009.pdf:application/pdf}
}

@article{kutas-processing-2007-1,
	title = {{PROCESSING} language is one ofthe major integrativeacts excels, asit},
	url = {https://books.google.com/books?hl=en&lr=&id=OZfRtwBu250C&oi=fnd&pg=PA385&dq=%22and+without+much+conscious+reflection+on+the+computations+and%22+%22potentials%3B+IPSPs).+The+scalp-recorded+activity+is+the+sum+of+the+EPSPs+and%22+%22different+information+types+to+create+new+structures%E2%80%94many+processes%22+&ots=aURCaPKNug&sig=Q6xZ-RF6\_XbFT4GAFJgA03rGFpk},
	urldate = {2016-05-28},
	journal = {The Oxford Handbook of Psycholinguistics},
	author = {Kutas, Marta and Federmeier, Kara D.},
	year = {2007},
	pages = {385},
	file = {oxfordhb-9780198568971-e-023.pdf:/home/user/Zotero/storage/HWRZM9B2/oxfordhb-9780198568971-e-023.pdf:application/pdf}
}

@misc{noauthor-[1605.07912]-nodate,
	title = {[1605.07912] {Encode}, {Review}, and {Decode}: {Reviewer} {Module} for {Caption} {Generation}},
	url = {https://arxiv.org/abs/1605.07912},
	urldate = {2016-05-28},
	file = {[1605.07912] Encode, Review, and Decode\: Reviewer Module for Caption Generation:/home/user/Zotero/storage/QT4JIHZQ/1605.html:text/html}
}

@misc{noauthor-[1605.07895]-nodate,
	title = {[1605.07895] {Automatic} {Extraction} of {Causal} {Relations} from {Natural} {Language} {Texts}: {A} {Comprehensive} {Survey}},
	url = {https://arxiv.org/abs/1605.07895},
	urldate = {2016-05-28},
	file = {[1605.07895] Automatic Extraction of Causal Relations from Natural Language Texts\: A Comprehensive Survey:/home/user/Zotero/storage/H3UKVQR9/1605.html:text/html}
}

@misc{noauthor-[1605.07891]-nodate,
	title = {[1605.07891] {Query} {Expansion} with {Locally}-{Trained} {Word} {Embeddings}},
	url = {https://arxiv.org/abs/1605.07891},
	urldate = {2016-05-28},
	file = {[1605.07891] Query Expansion with Locally-Trained Word Embeddings:/home/user/Zotero/storage/SM452Z5J/1605.html:text/html}
}

@misc{noauthor-[1605.07869]-nodate,
	title = {[1605.07869] {Variational} {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1605.07869},
	urldate = {2016-05-28},
	file = {[1605.07869] Variational Neural Machine Translation:/home/user/Zotero/storage/3DJ99DT7/1605.html:text/html}
}

@misc{noauthor-[1605.07683]-nodate,
	title = {[1605.07683] {Learning} {End}-to-{End} {Goal}-{Oriented} {Dialog}},
	url = {https://arxiv.org/abs/1605.07683},
	urldate = {2016-05-28},
	file = {[1605.07683] Learning End-to-End Goal-Oriented Dialog:/home/user/Zotero/storage/ESTM22PP/1605.html:text/html}
}

@misc{noauthor-[1605.07571]-nodate,
	title = {[1605.07571] {Sequential} {Neural} {Models} with {Stochastic} {Layers}},
	url = {https://arxiv.org/abs/1605.07571},
	urldate = {2016-05-26},
	file = {[1605.07571] Sequential Neural Models with Stochastic Layers:/home/user/Zotero/storage/3ZEBSJRV/1605.html:text/html}
}

@misc{noauthor-[1604.06384]-nodate,
	title = {[1604.06384] {Computation} {Tree} {Logic} for {Synchronization} {Properties}},
	url = {https://arxiv.org/abs/1604.06384},
	urldate = {2016-05-26},
	file = {[1604.06384] Computation Tree Logic for Synchronization Properties:/home/user/Zotero/storage/ZGNXUNE8/1604.html:text/html}
}

@misc{noauthor-[1506.00406]-nodate,
	title = {[1506.00406] {Monolingually} {Derived} {Phrase} {Scores} for {Phrase} {Based} {SMT} {Using} {Neural} {Networks} {Vector} {Representations}},
	url = {https://arxiv.org/abs/1506.00406},
	urldate = {2016-05-26},
	file = {[1506.00406] Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations:/home/user/Zotero/storage/FX6Z98WH/1506.html:text/html}
}

@misc{noauthor-[1605.07427]-nodate,
	title = {[1605.07427] {Hierarchical} {Memory} {Networks}},
	url = {https://arxiv.org/abs/1605.07427},
	urldate = {2016-05-26},
	file = {[1605.07427] Hierarchical Memory Networks:/home/user/Zotero/storage/5CN3S2I9/1605.html:text/html}
}

@misc{noauthor-[1605.07515]-nodate-1,
	title = {[1605.07515] {Neural} {Semantic} {Role} {Labeling} with {Dependency} {Path} {Embeddings}},
	url = {https://arxiv.org/abs/1605.07515},
	urldate = {2016-05-26},
	file = {[1605.07515] Neural Semantic Role Labeling with Dependency Path Embeddings:/home/user/Zotero/storage/6P84VX96/1605.html:text/html}
}

@misc{noauthor-[1605.06069]-nodate-1,
	title = {[1605.06069] {A} {Hierarchical} {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Generating} {Dialogues}},
	url = {https://arxiv.org/abs/1605.06069},
	urldate = {2016-05-24},
	file = {[1605.06069] A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues:/home/user/Zotero/storage/DZE45XSJ/1605.html:text/html}
}

@misc{noauthor-[1605.05573]-nodate,
	title = {[1605.05573] {Modelling} {Interaction} of {Sentence} {Pair} with coupled-{LSTMs}},
	url = {https://arxiv.org/abs/1605.05573},
	urldate = {2016-05-24},
	file = {[1605.05573] Modelling Interaction of Sentence Pair with coupled-LSTMs:/home/user/Zotero/storage/H64RRHMQ/1605.html:text/html}
}

@misc{noauthor-[1604.04562]-nodate,
	title = {[1604.04562] {A} {Network}-based {End}-to-{End} {Trainable} {Task}-oriented {Dialogue} {System}},
	url = {https://arxiv.org/abs/1604.04562},
	urldate = {2016-05-24},
	file = {[1604.04562] A Network-based End-to-End Trainable Task-oriented Dialogue System:/home/user/Zotero/storage/U6SPGSBF/1604.html:text/html}
}

@misc{noauthor-[1605.06304]-nodate,
	title = {[1605.06304] {Local} communities obstruct global consensus: {Naming} game on multi-local-world networks},
	url = {https://arxiv.org/abs/1605.06304},
	urldate = {2016-05-24},
	file = {[1605.06304] Local communities obstruct global consensus\: Naming game on multi-local-world networks:/home/user/Zotero/storage/B8VTFHDA/1605.html:text/html}
}

@misc{noauthor-[1603.02514]-nodate,
	title = {[1603.02514] {Semi}-supervised {Variational} {Autoencoders} for {Sequence} {Classification}},
	url = {https://arxiv.org/abs/1603.02514},
	urldate = {2016-05-24},
	file = {[1603.02514] Semi-supervised Variational Autoencoders for Sequence Classification:/home/user/Zotero/storage/RP2TS75T/1603.html:text/html}
}

@misc{noauthor-[1508.03040]-nodate-1,
	title = {[1508.03040] {Syntax} {Evolution}: {Problems} and {Recursion}},
	url = {https://arxiv.org/abs/1508.03040},
	urldate = {2016-05-24},
	file = {[1508.03040] Syntax Evolution\: Problems and Recursion:/home/user/Zotero/storage/TK6354BS/1508.html:text/html}
}

@misc{noauthor-[1605.07133]-nodate,
	title = {[1605.07133] {Towards} {Multi}-{Agent} {Communication}-{Based} {Language} {Learning}},
	url = {https://arxiv.org/abs/1605.07133},
	urldate = {2016-05-24},
	file = {[1605.07133] Towards Multi-Agent Communication-Based Language Learning:/home/user/Zotero/storage/SRP2JPAV/1605.html:text/html}
}

@misc{noauthor-[1605.07094]-nodate,
	title = {[1605.07094] {Optimal} {Coding} in {Biological} and {Artificial} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1605.07094},
	urldate = {2016-05-24},
	file = {[1605.07094] Optimal Coding in Biological and Artificial Neural Networks:/home/user/Zotero/storage/6CK7Q2IK/1605.html:text/html}
}

@misc{noauthor-[1605.06900]-nodate,
	title = {[1605.06900] {Fast} {Stochastic} {Methods} for {Nonsmooth} {Nonconvex} {Optimization}},
	url = {https://arxiv.org/abs/1605.06900},
	urldate = {2016-05-24},
	file = {[1605.06900] Fast Stochastic Methods for Nonsmooth Nonconvex Optimization:/home/user/Zotero/storage/D933KA47/1605.html:text/html}
}

@misc{noauthor-[1605.07110]-nodate,
	title = {[1605.07110] {Deep} {Learning} without {Poor} {Local} {Minima}},
	url = {https://arxiv.org/abs/1605.07110},
	urldate = {2016-05-24},
	file = {[1605.07110] Deep Learning without Poor Local Minima:/home/user/Zotero/storage/VA6XBZT5/1605.html:text/html}
}

@misc{noauthor-[1603.00810]-nodate,
	title = {[1603.00810] {Character}-based {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1603.00810},
	urldate = {2016-05-20},
	file = {[1603.00810] Character-based Neural Machine Translation:/home/user/Zotero/storage/38V3T3ED/1603.html:text/html}
}

@article{phansalkar-local-1995,
	title = {Local and {Global} {Optimization} {Algorithms} for {Generalized} {Learning} {Automata}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1995.7.5.950},
	doi = {10.1162/neco.1995.7.5.950},
	language = {en},
	number = {5},
	urldate = {2016-05-19},
	journal = {Neural Computation},
	author = {Phansalkar, V. V. and Thathachar, M. A. L.},
	month = sep,
	year = {1995},
	pages = {950--973},
	file = {neco.1995.7.5.950:/home/user/Zotero/storage/4CC375VR/neco.1995.7.5.pdf:application/pdf}
}

@misc{noauthor-[1506.05860]-nodate,
	title = {[1506.05860] {Variational} {Gaussian} {Copula} {Inference}},
	url = {https://arxiv.org/abs/1506.05860},
	urldate = {2016-05-19},
	file = {[1506.05860] Variational Gaussian Copula Inference:/home/user/Zotero/storage/JGTVS9C3/1506.html:text/html}
}

@misc{noauthor-[1605.05433]-nodate,
	title = {[1605.05433] {Relations} such as {Hypernymy}: {Identifying} and {Exploiting} {Hearst} {Patterns} in {Distributional} {Vectors} for {Lexical} {Entailment}},
	url = {https://arxiv.org/abs/1605.05433},
	urldate = {2016-05-19},
	file = {[1605.05433] Relations such as Hypernymy\: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment:/home/user/Zotero/storage/QDQ9FGUX/1605.html:text/html}
}

@misc{noauthor-[1605.05087]-nodate,
	title = {[1605.05087] {Word}2Vec is only a special case of {Kernel} {Correspondence} {Analysis} and {Kernels} for {Natural} {Language} {Processing}},
	url = {https://arxiv.org/abs/1605.05087},
	urldate = {2016-05-18},
	file = {[1605.05087] Word2Vec is only a special case of Kernel Correspondence Analysis and Kernels for Natural Language Processing:/home/user/Zotero/storage/UD3Z2ZG9/1605.html:text/html}
}

@misc{noauthor-[1605.05101]-nodate,
	title = {[1605.05101] {Recurrent} {Neural} {Network} for {Text} {Classification} with {Multi}-{Task} {Learning}},
	url = {https://arxiv.org/abs/1605.05101},
	urldate = {2016-05-18},
	file = {[1605.05101] Recurrent Neural Network for Text Classification with Multi-Task Learning:/home/user/Zotero/storage/4UJQTKQM/1605.html:text/html}
}

@misc{noauthor-[1605.04655]-nodate,
	title = {[1605.04655] {Joint} {Learning} of {Sentence} {Embeddings} for {Relevance} and {Entailment}},
	url = {https://arxiv.org/abs/1605.04655},
	urldate = {2016-05-17},
	file = {[1605.04655] Joint Learning of Sentence Embeddings for Relevance and Entailment:/home/user/Zotero/storage/7N4NA9VP/1605.html:text/html}
}

@misc{noauthor-[1605.04569]-nodate,
	title = {[1605.04569] {Syntactically} {Guided} {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1605.04569},
	urldate = {2016-05-17},
	file = {[1605.04569] Syntactically Guided Neural Machine Translation:/home/user/Zotero/storage/6GE3RIHG/1605.html:text/html}
}

@misc{noauthor-ieee-nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=892332},
	urldate = {2016-05-16},
	file = {IEEE Xplore Full-Text PDF\::/home/user/Zotero/storage/6FQK4X8U/stamp.html:text/html}
}

@misc{noauthor-semantic-nodate,
	title = {Semantic {Scholar}},
	url = {https://www.semanticscholar.org/paper/On-the-existence-of-prime-decompositions-Han-Salomaa/17d5fbcb502df444070f437b2c327b3e671da38e},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	urldate = {2016-05-16},
	file = {Snapshot:/home/user/Zotero/storage/HC5BQX6B/17d5fbcb502df444070f437b2c327b3e671da38e.html:text/html}
}

@article{kufleitner-languages-2011,
	title = {Languages of {Dot}-depth {One} over {Infinite} {Words}},
	url = {http://arxiv.org/abs/1101.4152},
	abstract = {Over finite words, languages of dot-depth one are expressively complete for alternation-free first-order logic. This fragment is also known as the Boolean closure of existential first-order logic. Here, the atomic formulas comprise order, successor, minimum, and maximum predicates. Knast (1983) has shown that it is decidable whether a language has dot-depth one. We extend Knast's result to infinite words. In particular, we describe the class of languages definable in alternation-free first-order logic over infinite words, and we give an effective characterization of this fragment. This characterization has two components. The first component is identical to Knast's algebraic property for finite words and the second component is a topological property, namely being a Boolean combination of Cantor sets. As an intermediate step we consider finite and infinite words simultaneously. We then obtain the results for infinite words as well as for finite words as special cases. In particular, we give a new proof of Knast's Theorem on languages of dot-depth one over finite words.},
	urldate = {2016-05-16},
	journal = {arXiv:1101.4152 [cs]},
	author = {Kufleitner, Manfred and Lauser, Alexander},
	month = jan,
	year = {2011},
	note = {arXiv: 1101.4152},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, F.4.1, F.4.3, 03D05, 68Q45},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9DPH2PXG/1101.html:text/html;Kufleitner_Lauser_2011_Languages of Dot-depth One over Infinite Words.pdf:/home/user/Zotero/storage/KPVDN7J3/Kufleitner_Lauser_2011_Languages of Dot-depth One over Infinite Words.pdf:application/pdf}
}

@article{kufleitner-around-2011,
	title = {Around {Dot}-depth {One}},
	url = {http://arxiv.org/abs/1103.1353},
	abstract = {The dot-depth hierarchy is a classification of star-free languages. It is related to the quantifier alternation hierarchy of first-order logic over finite words. We consider fragments of languages with dot-depth 1/2 and dot-depth 1 obtained by prohibiting the specification of prefixes or suffixes. As it turns out, these language classes are in one-to-one correspondence with fragments of existential first-order logic without min- or max-predicate. For all fragments, we obtain effective algebraic characterizations. Moreover, we give new combinatorial proofs for the decidability of the membership problem for dot-depth 1/2 and dot-depth 1.},
	urldate = {2016-05-16},
	journal = {arXiv:1103.1353 [cs]},
	author = {Kufleitner, Manfred and Lauser, Alexander},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.1353},
	keywords = {Computer Science - Formal Languages and Automata Theory, Computer Science - Logic in Computer Science, F.4.1, F.4.3, 68Q45},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/7K8WKDTU/1103.html:text/html;Kufleitner_Lauser_2011_Around Dot-depth One.pdf:/home/user/Zotero/storage/68AI7F7K/Kufleitner_Lauser_2011_Around Dot-depth One.pdf:application/pdf}
}

@article{brzozowski-dot-depth-1978,
	title = {The dot-depth hierarchy of star-free languages is infinite},
	volume = {16},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/0022000078900491},
	doi = {10.1016/0022-0000(78)90049-1},
	abstract = {Let A be a finite alphabet and A∗ the free monoid generated by A. A language is any subset of A∗. Assume that all the languages of the form {a}, where a is either the empty word or a letter in A, are given. Close this basic family of languages under Boolean operations; let B(0) be the resulting Boolean algebra of languages. Next, close B(0) under concatenation and then close the resulting family under Boolean operations. Call this new Boolean algebra B(1), etc. The sequence B(0), B(1),…, Bk,… of Boolean algebras is called the dot-depth hierarchy. The union of all these Boolean algebras is the family A of star-free or aperiodic languages which is the same as the family of noncounting regular languages. Over an alphabet of one letter the hierarchy is finite; in fact, B(2) = B(1). We show in this paper that the hierarchy is infinite for any alphabet with two or more letters.},
	number = {1},
	urldate = {2016-05-16},
	journal = {Journal of Computer and System Sciences},
	author = {Brzozowski, J. A. and Knast, R.},
	month = feb,
	year = {1978},
	pages = {37--55},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/IP9NWE5P/0022000078900491.html:text/html}
}

@misc{noauthor-[1603.00375]-nodate-1,
	title = {[1603.00375] {Easy}-{First} {Dependency} {Parsing} with {Hierarchical} {Tree} {LSTMs}},
	url = {http://arxiv.org/abs/1603.00375},
	urldate = {2016-05-16},
	file = {[1603.00375] Easy-First Dependency Parsing with Hierarchical Tree LSTMs:/home/user/Zotero/storage/NS3EXFH5/1603.html:text/html}
}

@misc{noauthor-[1602.01595]-nodate-1,
	title = {[1602.01595] {Many} {Languages}, {One} {Parser}},
	url = {http://arxiv.org/abs/1602.01595},
	urldate = {2016-05-16},
	file = {[1602.01595] Many Languages, One Parser:/home/user/Zotero/storage/FHDCJDE9/1602.html:text/html}
}

@misc{noauthor-[1511.06018]-nodate,
	title = {[1511.06018] {Segmental} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06018},
	urldate = {2016-05-16},
	file = {[1511.06018] Segmental Recurrent Neural Networks:/home/user/Zotero/storage/MVUKHI8M/1511.html:text/html}
}

@misc{noauthor-[1603.00810]-nodate-1,
	title = {[1603.00810] {Character}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1603.00810},
	urldate = {2016-05-16},
	file = {[1603.00810] Character-based Neural Machine Translation:/home/user/Zotero/storage/38FBUWW9/1603.html:text/html}
}

@misc{noauthor-[1503.05124]-nodate,
	title = {[1503.05124] {A} representation theorem for stratified complete lattices},
	url = {http://arxiv.org/abs/1503.05124},
	urldate = {2016-05-16},
	file = {[1503.05124] A representation theorem for stratified complete lattices:/home/user/Zotero/storage/KEJWSTNX/1503.html:text/html}
}

@misc{noauthor-[1603.00892]-nodate,
	title = {[1603.00892] {Counter}-fitting {Word} {Vectors} to {Linguistic} {Constraints}},
	url = {http://arxiv.org/abs/1603.00892},
	urldate = {2016-05-16},
	file = {[1603.00892] Counter-fitting Word Vectors to Linguistic Constraints:/home/user/Zotero/storage/SD36ESI4/1603.html:text/html}
}

@misc{noauthor-[1603.01547]-nodate,
	title = {[1603.01547] {Text} {Understanding} with the {Attention} {Sum} {Reader} {Network}},
	url = {http://arxiv.org/abs/1603.01547},
	urldate = {2016-05-16},
	file = {[1603.01547] Text Understanding with the Attention Sum Reader Network:/home/user/Zotero/storage/WG3V2E9W/1603.html:text/html}
}

@misc{noauthor-[1511.01158]-nodate,
	title = {[1511.01158] {Distributed} {Deep} {Learning} for {Answer} {Selection}},
	url = {https://arxiv.org/abs/1511.01158},
	urldate = {2016-05-16},
	file = {[1511.01158] Distributed Deep Learning for Answer Selection:/home/user/Zotero/storage/MTHNKM5I/1511.html:text/html}
}

@misc{noauthor-[1605.03284]-nodate,
	title = {[1605.03284] {Machine} {Comprehension} {Based} on {Learning} to {Rank}},
	url = {https://arxiv.org/abs/1605.03284},
	urldate = {2016-05-16},
	file = {[1605.03284] Machine Comprehension Based on Learning to Rank:/home/user/Zotero/storage/BZJFC77N/1605.html:text/html}
}

@misc{noauthor-[1512.08422]-nodate,
	title = {[1512.08422] {Natural} {Language} {Inference} by {Tree}-{Based} {Convolution} and {Heuristic} {Matching}},
	url = {https://arxiv.org/abs/1512.08422},
	urldate = {2016-05-16},
	file = {[1512.08422] Natural Language Inference by Tree-Based Convolution and Heuristic Matching:/home/user/Zotero/storage/AKM6HUBP/1512.html:text/html}
}

@misc{noauthor-[1605.04238]-nodate,
	title = {[1605.04238] {Semantic} {Spaces}},
	url = {https://arxiv.org/abs/1605.04238},
	urldate = {2016-05-16},
	file = {[1605.04238] Semantic Spaces:/home/user/Zotero/storage/ZWQ8WIDP/1605.html:text/html}
}

@misc{noauthor-[1504.00548]-nodate,
	title = {[1504.00548] {Learning} to {Understand} {Phrases} by {Embedding} the {Dictionary}},
	url = {http://arxiv.org/abs/1504.00548},
	urldate = {2016-05-16},
	file = {[1504.00548] Learning to Understand Phrases by Embedding the Dictionary:/home/user/Zotero/storage/TGTX7UD6/1504.html:text/html}
}

@misc{noauthor-[1603.06677]-nodate,
	title = {[1603.06677] {Learning} {Executable} {Semantic} {Parsers} for {Natural} {Language} {Understanding}},
	url = {http://arxiv.org/abs/1603.06677},
	urldate = {2016-05-16},
	file = {[1603.06677] Learning Executable Semantic Parsers for Natural Language Understanding:/home/user/Zotero/storage/2KS3945X/1603.html:text/html}
}

@misc{noauthor-[1603.06571]-nodate,
	title = {[1603.06571] {Bayesian} {Neural} {Word} {Embedding}},
	url = {http://arxiv.org/abs/1603.06571},
	urldate = {2016-05-16},
	file = {[1603.06571] Bayesian Neural Word Embedding:/home/user/Zotero/storage/RP9S4CMG/1603.html:text/html}
}

@misc{noauthor-task-free-nodate,
	title = {Task-free {MRI} predicts individual differences in brain activity during task performance {\textbar} {Science}},
	url = {http://science.sciencemag.org/content/352/6282/216?utm\_campaign=email-sci-toc&et\_rid=35362199&et\_cid=400756},
	urldate = {2016-05-14},
	file = {Task-free MRI predicts individual differences in brain activity during task performance | Science:/home/user/Zotero/storage/UM48D8RP/216.html:text/html}
}

@misc{noauthor-separate-nodate,
	title = {Separate streams or probabilistic inference? {What} the {N}400 can tell us about the comprehension of events - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 5},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1130233},
	urldate = {2016-05-14},
	file = {Separate streams or probabilistic inference? What the N400 can tell us about the comprehension of events - Language, Cognition and Neuroscience - Volume 31, Issue 5:/home/user/Zotero/storage/D9T5MB9W/23273798.2015.html:text/html}
}

@misc{noauthor-prediction-nodate-1,
	title = {Prediction as memory retrieval: timing and mechanisms - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 5},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2016.1160135},
	urldate = {2016-05-14},
	file = {Prediction as memory retrieval\: timing and mechanisms - Language, Cognition and Neuroscience - Volume 31, Issue 5:/home/user/Zotero/storage/4GQ4QEQS/23273798.2016.html:text/html}
}

@misc{noauthor-prediction-nodate-2,
	title = {Prediction during sentence comprehension is more than a sum of lexical associations: the role of event knowledge - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 5},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1102950},
	urldate = {2016-05-14},
	file = {Prediction during sentence comprehension is more than a sum of lexical associations\: the role of event knowledge - Language, Cognition and Neuroscience - Volume 31, Issue 5:/home/user/Zotero/storage/7XMQ9QUH/23273798.2015.html:text/html}
}

@misc{noauthor-bag--arguments-nodate,
	title = {A “bag-of-arguments” mechanism for initial verb predictions - {Language}, {Cognition} and {Neuroscience} - {Volume} 31, {Issue} 5},
	url = {http://www.tandfonline.com/doi/full/10.1080/23273798.2015.1066832},
	urldate = {2016-05-14},
	file = {A “bag-of-arguments” mechanism for initial verb predictions - Language, Cognition and Neuroscience - Volume 31, Issue 5:/home/user/Zotero/storage/T2QBZHRN/23273798.2015.html:text/html}
}

@misc{noauthor-plos-nodate-3,
	title = {{PLOS} {Biology}: {Time} {Slices}: {What} {Is} the {Duration} of a {Percept}?},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002433},
	urldate = {2016-05-14},
	file = {PLOS Biology\: Time Slices\: What Is the Duration of a Percept?:/home/user/Zotero/storage/9GXFNB4B/article.html:text/html}
}

@article{huettig-using-2011,
	title = {Using the visual world paradigm to study language processing: {A} review and critical evaluation},
	volume = {137},
	issn = {00016918},
	shorttitle = {Using the visual world paradigm to study language processing},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0001691810002180},
	doi = {10.1016/j.actpsy.2010.11.003},
	language = {en},
	number = {2},
	urldate = {2016-05-14},
	journal = {Acta Psychologica},
	author = {Huettig, Falk and Rommers, Joost and Meyer, Antje S.},
	month = jun,
	year = {2011},
	pages = {151--171},
	file = {Huettig_Using_the_visual_world_paradigm_Acta_Psychologica_2011.pdf:/home/user/Zotero/storage/PK3KD6QX/Huettig_Using_the_visual_world_paradigm_Acta_Psychologica_2011.pdf:application/pdf}
}

@article{pica-exact-2004,
	title = {Exact and {Approximate} {Arithmetic} in an {Amazonian} {Indigene} {Group}},
	volume = {306},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1102085},
	doi = {10.1126/science.1102085},
	language = {en},
	number = {5695},
	urldate = {2016-05-14},
	journal = {Science},
	author = {Pica, P.},
	month = oct,
	year = {2004},
	pages = {499--503},
	file = {exact-and-approximate-arithmetic-in-an-amazonian-indigene-group-pica-lemer-izard-dehaene-science-2004.pdf:/home/user/Zotero/storage/7Q69F76I/exact-and-approximate-arithmetic-in-an-amazonian-indigene-group-pica-lemer-izard-dehaene-science-2004.pdf:application/pdf}
}

@article{dehaene-localization-1994,
	title = {Localization of a neural system for error detection and compensation},
	volume = {5},
	url = {http://www.jstor.org/stable/40063122},
	number = {5},
	urldate = {2016-05-14},
	journal = {Psychological Science},
	author = {Dehaene, Stanislas and Posner, Michael I. and Tucker, Don M.},
	year = {1994},
	pages = {303--305},
	file = {303.full.pdf:/home/user/Zotero/storage/RZKUKAHB/303.full.pdf:application/pdf}
}

@article{mccandliss-visual-2003,
	title = {The visual word form area: expertise for reading in the fusiform gyrus},
	volume = {7},
	issn = {13646613},
	shorttitle = {The visual word form area},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661303001347},
	doi = {10.1016/S1364-6613(03)00134-7},
	language = {en},
	number = {7},
	urldate = {2016-05-14},
	journal = {Trends in Cognitive Sciences},
	author = {McCandliss, Bruce D. and Cohen, Laurent and Dehaene, Stanislas},
	month = jul,
	year = {2003},
	pages = {293--299},
	file = {McCandliss.pdf:/home/user/Zotero/storage/MZAQW6J8/McCandliss.pdf:application/pdf}
}

@article{dehaene-neural-2015,
	title = {The {Neural} {Representation} of {Sequences}: {From} {Transition} {Probabilities} to {Algebraic} {Patterns} and {Linguistic} {Trees}},
	volume = {88},
	issn = {08966273},
	shorttitle = {The {Neural} {Representation} of {Sequences}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S089662731500776X},
	doi = {10.1016/j.neuron.2015.09.019},
	language = {en},
	number = {1},
	urldate = {2016-05-14},
	journal = {Neuron},
	author = {Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and Wang, Liping and Pallier, Christophe},
	month = oct,
	year = {2015},
	pages = {2--19},
	file = {Sequences_Neuron_proofs3.pdf:/home/user/Zotero/storage/CSRH8CIF/Sequences_Neuron_proofs3.pdf:application/pdf}
}

@article{dehaene-mental-1993,
	title = {The mental representation of parity and number magnitude.},
	volume = {122},
	issn = {0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.122.3.371},
	doi = {10.1037//0096-3445.122.3.371},
	language = {en},
	number = {3},
	urldate = {2016-05-14},
	journal = {Journal of Experimental Psychology: General},
	author = {Dehaene, Stanislas and Bossini, Serge and Giraux, Pascal},
	year = {1993},
	pages = {371--396},
	file = {542fad8a0cf277d58e91fc78.pdf:/home/user/Zotero/storage/VUSVPKPM/542fad8a0cf277d58e91fc78.pdf:application/pdf}
}

@article{dehaene-three-2003,
	title = {{THREE} {PARIETAL} {CIRCUITS} {FOR} {NUMBER} {PROCESSING}},
	volume = {20},
	issn = {0264-3294, 1464-0627},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02643290244000239},
	doi = {10.1080/02643290244000239},
	language = {en},
	number = {3-6},
	urldate = {2016-05-14},
	journal = {Cognitive Neuropsychology},
	author = {Dehaene, Stanislas and Piazza, Manuela and Pinel, Philippe and Cohen, Laurent},
	month = may,
	year = {2003},
	pages = {487--506},
	file = {02643290244000239.pdf:/home/user/Zotero/storage/NZTZNJMC/02643290244000239.pdf:application/pdf}
}

@article{dehaene-towards-2001,
	title = {Towards a cognitive neuroscience of consciousness: basic evidence and a workspace framework},
	volume = {79},
	shorttitle = {Towards a cognitive neuroscience of consciousness},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027700001232},
	number = {1},
	urldate = {2016-05-14},
	journal = {Cognition},
	author = {Dehaene, Stanislas and Naccache, Lionel},
	year = {2001},
	pages = {1--37},
	file = {Dehaene_Cognition_2001.pdf:/home/user/Zotero/storage/IUK5X8EU/Dehaene_Cognition_2001.pdf:application/pdf}
}

@article{feigenson-core-2004,
	title = {Core systems of number},
	volume = {8},
	issn = {13646613},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661304001317},
	doi = {10.1016/j.tics.2004.05.002},
	language = {en},
	number = {7},
	urldate = {2016-05-14},
	journal = {Trends in Cognitive Sciences},
	author = {Feigenson, Lisa and Dehaene, Stanislas and Spelke, Elizabeth},
	month = jul,
	year = {2004},
	pages = {307--314},
	file = {00b495165d26e2e847000000.pdf:/home/user/Zotero/storage/MI493ECN/00b495165d26e2e847000000.pdf:application/pdf}
}

@article{dehaene-conscious-2006,
	title = {Conscious, preconscious, and subliminal processing: a testable taxonomy},
	volume = {10},
	issn = {13646613},
	shorttitle = {Conscious, preconscious, and subliminal processing},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661306000799},
	doi = {10.1016/j.tics.2006.03.007},
	language = {en},
	number = {5},
	urldate = {2016-05-14},
	journal = {Trends in Cognitive Sciences},
	author = {Dehaene, Stanislas and Changeux, Jean-Pierre and Naccache, Lionel and Sackur, Jérôme and Sergent, Claire},
	month = may,
	year = {2006},
	pages = {204--211},
	file = {550c10fa0cf2063799398318.pdf:/home/user/Zotero/storage/TJCXTGX2/550c10fa0cf2063799398318.pdf:application/pdf}
}

@article{van-roy-generalization-2014,
	title = {Generalization and exploration via randomized value functions},
	url = {http://arxiv.org/abs/1402.0635},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1402.0635},
	author = {Van Roy, Benjamin and Wen, Zheng},
	year = {2014},
	file = {1307.4847v3.pdf:/home/user/Zotero/storage/V8K6RN77/1307.4847v3.pdf:application/pdf}
}

@article{eldan-power-2015-1,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.03965},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1512.03965},
	author = {Eldan, Ronen and Shamir, Ohad},
	year = {2015},
	file = {1512.03965v4.pdf:/home/user/Zotero/storage/ZUW4HMEJ/1512.03965v4.pdf:application/pdf}
}

@article{eldan-power-2015-2,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.03965},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1512.03965},
	author = {Eldan, Ronen and Shamir, Ohad},
	year = {2015},
	file = {1512.03965v4.pdf:/home/user/Zotero/storage/39GUKR6W/1512.03965v4.pdf:application/pdf}
}

@inproceedings{martens-learning-2011,
	title = {Learning recurrent neural networks with hessian-free optimization},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Martens\_532.pdf},
	urldate = {2016-05-14},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Machine} {Learning} ({ICML}-11)},
	author = {Martens, James and Sutskever, Ilya},
	year = {2011},
	pages = {1033--1040},
	file = {RNN_HF.pdf:/home/user/Zotero/storage/FDJTIIQK/RNN_HF.pdf:application/pdf}
}

@article{marcheggiani-discrete-state-2016,
	title = {Discrete-state variational autoencoders for joint discovery and factorization of relations},
	volume = {4},
	url = {http://www.ivan-titov.org/papers/tacl16diego.pdf},
	urldate = {2016-05-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Marcheggiani, Diego and Titov, Ivan},
	year = {2016},
	file = {tacl16diego.pdf:/home/user/Zotero/storage/Z447U62I/tacl16diego.pdf:application/pdf}
}

@article{suster-bilingual-2016-1,
	title = {Bilingual {Learning} of {Multi}-sense {Embeddings} with {Discrete} {Autoencoders}},
	url = {http://arxiv.org/abs/1603.09128},
	urldate = {2016-05-14},
	journal = {arXiv preprint arXiv:1603.09128},
	author = {Šuster, Simon and Titov, Ivan and van Noord, Gertjan},
	year = {2016},
	file = {naacl16.pdf:/home/user/Zotero/storage/CWTAFTWM/naacl16.pdf:application/pdf}
}

@article{xu-optimizing-2016,
	title = {Optimizing statistical machine translation for text simplification},
	volume = {4},
	url = {http://www.cis.upenn.edu/~xwe/publications/tacl2016-smt-simplification.pdf},
	urldate = {2016-05-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
	year = {2016},
	file = {tacl2016-smt-simplification.pdf:/home/user/Zotero/storage/4MBUZ3I5/tacl2016-smt-simplification.pdf:application/pdf}
}

@article{hahn-henkin-2015,
	title = {Henkin {Semantics} for {Reasoning} with {Natural} {Language}},
	volume = {3},
	number = {2},
	journal = {Journal of Language Modeling},
	author = {Hahn, Michael and Richter, Frank},
	year = {2015},
	pages = {513--568}
}

@misc{noauthor-[1608.04355]-nodate,
	title = {[1608.04355] {Polynomial} {Representations} of {Threshold} {Functions} and {Algorithmic} {Applications}},
	url = {https://arxiv.org/abs/1608.04355},
	urldate = {2016-08-16},
	file = {[1608.04355] Polynomial Representations of Threshold Functions and Algorithmic Applications:/home/user/Zotero/storage/CCKNTX95/1608.html:text/html}
}

@article{koplenig-analyzing-2016,
	title = {Analyzing lexical change in diachronic corpora},
	copyright = {http://creativecommons.org/licenses/by-nc/3.0/de/deed.de},
	url = {http://dok.ids-mannheim.de/handle/10932/00-02EC-1772-EC3F-FF01-D},
	abstract = {Qualifikationsarbeit},
	language = {en},
	urldate = {2016-08-15},
	author = {Koplenig, Alexander},
	month = may,
	year = {2016},
	file = {Koplenig_2016_Analyzing lexical change in diachronic corpora.pdf:/home/user/Zotero/storage/ERCP38EE/Koplenig_2016_Analyzing lexical change in diachronic corpora.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/UBD5WBSW/00-02EC-1772-EC3F-FF01-D.html:text/html}
}

@article{koplenig-fully-2015,
	title = {A fully data-driven method to identify (correlated) changes in diachronic corpora},
	url = {http://arxiv.org/abs/1508.06374},
	abstract = {In this paper, a method for measuring synchronic corpus (dis-)similarity put forward by Kilgarriff (2001) is adapted and extended to identify trends and correlated changes in diachronic text data, using the Corpus of Historical American English (Davies 2010a) and the Google Ngram Corpora (Michel et al. 2010a). This paper shows that this fully data-driven method, which extracts word types that have undergone the most pronounced change in frequency in a given period of time, is computationally very cheap and that it allows interpretations of diachronic trends that are both intuitively plausible and motivated from the perspective of information theory. Furthermore, it demonstrates that the method is able to identify correlated linguistic changes and diachronic shifts that can be linked to historical events. Finally, it can help to improve diachronic POS tagging and complement existing NLP approaches. This indicates that the approach can facilitate an improved understanding of diachronic processes in language change.},
	urldate = {2016-08-15},
	journal = {arXiv:1508.06374 [cs, stat]},
	author = {Koplenig, Alexander},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.06374},
	keywords = {Computer Science - Computation and Language, Statistics - Applications, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3TWF8TMN/1508.html:text/html;Koplenig_2015_A fully data-driven method to identify (correlated) changes in diachronic.pdf:/home/user/Zotero/storage/ATJMN97K/Koplenig_2015_A fully data-driven method to identify (correlated) changes in diachronic.pdf:application/pdf}
}

@misc{noauthor-[1608.03587]-nodate,
	title = {[1608.03587] {The} statistical tradeoff between word order and word structure - large-scale evidence for the principle of least effort},
	url = {https://arxiv.org/abs/1608.03587},
	urldate = {2016-08-15},
	file = {[1608.03587] The statistical tradeoff between word order and word structure - large-scale evidence for the principle of least effort:/home/user/Zotero/storage/GPC3329A/1608.html:text/html}
}

@misc{noauthor-frontiers-nodate,
	title = {Frontiers {\textbar} {Differences} in {Sequential} {Eye} {Movement} {Behavior} between {Taiwanese} and {American} {Viewers} {\textbar} {Cognitive} {Science}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00697/full?utm\_source=newsletter&utm\_medium=email&utm\_campaign=Psychology-w27-2016},
	urldate = {2016-08-13},
	file = {Frontiers | Differences in Sequential Eye Movement Behavior between Taiwanese and American Viewers | Cognitive Science:/home/user/Zotero/storage/J95KTMVQ/full.html:text/html}
}

@misc{noauthor-[1606.07711]-nodate,
	title = {[1606.07711] {A} {Game}-{Theoretic} {Approach} to {Word} {Sense} {Disambiguation}},
	url = {https://arxiv.org/abs/1606.07711},
	urldate = {2016-08-13},
	file = {[1606.07711] A Game-Theoretic Approach to Word Sense Disambiguation:/home/user/Zotero/storage/VE9FGJEC/1606.html:text/html}
}

@misc{noauthor-[1508.06451]-nodate,
	title = {[1508.06451] {Crossings} as a side effect of dependency lengths},
	url = {https://arxiv.org/abs/1508.06451},
	urldate = {2016-08-13},
	file = {[1508.06451] Crossings as a side effect of dependency lengths:/home/user/Zotero/storage/8FXNZHPD/1508.html:text/html}
}

@misc{noauthor-[1607.04576]-nodate,
	title = {[1607.04576] {Neural} {Discourse} {Modeling} of {Conversations}},
	url = {https://arxiv.org/abs/1607.04576},
	urldate = {2016-08-13},
	file = {[1607.04576] Neural Discourse Modeling of Conversations:/home/user/Zotero/storage/FMBMD2AN/1607.html:text/html}
}

@misc{noauthor-[1607.04423]-nodate,
	title = {[1607.04423] {Attention}-over-{Attention} {Neural} {Networks} for {Reading} {Comprehension}},
	url = {https://arxiv.org/abs/1607.04423},
	urldate = {2016-08-13},
	file = {[1607.04423] Attention-over-Attention Neural Networks for Reading Comprehension:/home/user/Zotero/storage/PEX24FAW/1607.html:text/html}
}

@misc{noauthor-mit-nodate,
	title = {{MIT} {Press} {Journals} - {Journal} of {Cognitive} {Neuroscience} - {Early} {Access} - {Abstract}},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn\_a\_01016},
	urldate = {2016-08-13},
	file = {MIT Press Journals - Journal of Cognitive Neuroscience - Early Access - Abstract:/home/user/Zotero/storage/3ATV25ZR/jocn_a_01016.html:text/html}
}

@misc{noauthor-mit-nodate-1,
	title = {{MIT} {Press} {Journals} - {Journal} of {Cognitive} {Neuroscience} - {Full} {Text}},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/jocn\_a\_00977},
	urldate = {2016-08-13},
	file = {MIT Press Journals - Journal of Cognitive Neuroscience - Full Text:/home/user/Zotero/storage/CA5KKRBN/jocn_a_00977.html:text/html}
}

@misc{noauthor-[1601.00670]-nodate,
	title = {[1601.00670] {Variational} {Inference}: {A} {Review} for {Statisticians}},
	url = {https://arxiv.org/abs/1601.00670},
	urldate = {2016-08-13},
	file = {[1601.00670] Variational Inference\: A Review for Statisticians:/home/user/Zotero/storage/4S4SNSXF/1601.html:text/html}
}

@misc{noauthor-[1608.00938]-nodate,
	title = {[1608.00938] {Evolutionary} forces in language change},
	url = {https://arxiv.org/abs/1608.00938},
	urldate = {2016-08-13},
	file = {[1608.00938] Evolutionary forces in language change:/home/user/Zotero/storage/BMEIMRBR/1608.html:text/html}
}

@misc{noauthor-[1511.04868]-nodate,
	title = {[1511.04868] {A} {Neural} {Transducer}},
	url = {https://arxiv.org/abs/1511.04868},
	urldate = {2016-08-13},
	file = {[1511.04868] A Neural Transducer:/home/user/Zotero/storage/6CZ4GAME/1511.html:text/html}
}

@misc{noauthor-[1507.08452]-nodate,
	title = {[1507.08452] {Unsupervised} {Sentence} {Simplification} {Using} {Deep} {Semantics}},
	url = {https://arxiv.org/abs/1507.08452},
	urldate = {2016-08-13},
	file = {[1507.08452] Unsupervised Sentence Simplification Using Deep Semantics:/home/user/Zotero/storage/ZGWIJZHC/1507.html:text/html}
}

@misc{noauthor-[1608.02926]-nodate,
	title = {[1608.02926] {A} pragmatic theory of generic language},
	url = {https://arxiv.org/abs/1608.02926},
	urldate = {2016-08-13},
	file = {[1608.02926] A pragmatic theory of generic language:/home/user/Zotero/storage/9M8N43CS/1608.html:text/html}
}

@article{jaeger-echo-2001,
	title = {The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
	volume = {148},
	url = {http://minds.jacobs-university.de/sites/default/files/uploads/papers/EchoStatesTechRep.pdf},
	urldate = {2016-08-13},
	journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
	author = {Jaeger, Herbert},
	year = {2001},
	pages = {34},
	file = {EchoStatesTechRep.pdf:/home/user/Zotero/storage/UCNIPJJX/EchoStatesTechRep.pdf:application/pdf}
}

@article{linial-constant-1993,
	title = {Constant depth circuits, {Fourier} transform, and learnability},
	volume = {40},
	url = {http://dl.acm.org/citation.cfm?id=174138},
	number = {3},
	urldate = {2016-08-13},
	journal = {Journal of the ACM (JACM)},
	author = {Linial, Nathan and Mansour, Yishay and Nisan, Noam},
	year = {1993},
	pages = {607--620},
	file = {Constant depth circuits, Fourier transform, and learnability - LMN.pdf:/home/user/Zotero/storage/SKHVM4GP/LMN.pdf:application/pdf}
}

@incollection{razborov-bounded-1995,
	title = {Bounded arithmetic and lower bounds in {Boolean} complexity},
	url = {http://link.springer.com/chapter/10.1007/978-1-4612-2566-9\_12},
	urldate = {2016-08-13},
	booktitle = {Feasible {Mathematics} {II}},
	publisher = {Springer},
	author = {Razborov, Alexander A.},
	year = {1995},
	pages = {344--386},
	file = {text.dvi - bobo.pdf:/home/user/Zotero/storage/IIDFMM97/bobo.pdf:application/pdf}
}

@article{auer-nonstochastic-2002,
	title = {The nonstochastic multiarmed bandit problem},
	volume = {32},
	url = {http://epubs.siam.org/doi/abs/10.1137/S0097539701398375},
	number = {1},
	urldate = {2016-08-13},
	journal = {SIAM Journal on Computing},
	author = {Auer, Peter and Cesa-Bianchi, Nicolo and Freund, Yoav and Schapire, Robert E.},
	year = {2002},
	pages = {48--77},
	file = {bandits.pdf:/home/user/Zotero/storage/JFUA5PIX/bandits.pdf:application/pdf}
}

@inproceedings{simsek-linear-2013,
	title = {Linear decision rule as aspiration for simple decision heuristics},
	url = {http://papers.nips.cc/paper/4888-linear-decision-rule-as},
	urldate = {2016-08-13},
	booktitle = {Advances in neural information processing systems},
	author = {Şimşek, Özgür},
	year = {2013},
	pages = {2904--2912},
	file = {Linear decision rule as aspiration for simple decision heuristics - 4888-linear-decision-rule-as-aspiration-for-simple-decision-heuristics.pdf:/home/user/Zotero/storage/WVCSZ6AA/4888-linear-decision-rule-as-aspiration-for-simple-decision-heuristics.pdf:application/pdf}
}

@article{luo-learning-2016,
	title = {Learning {Online} {Alignments} with {Continuous} {Rewards} {Policy} {Gradient}},
	url = {http://arxiv.org/abs/1608.01281},
	urldate = {2016-08-13},
	journal = {arXiv preprint arXiv:1608.01281},
	author = {Luo, Yuping and Chiu, Chung-Cheng and Jaitly, Navdeep and Sutskever, Ilya},
	year = {2016},
	file = {1608.01281v1.pdf:/home/user/Zotero/storage/HGU7TMBX/1608.01281v1.pdf:application/pdf}
}

@book{brlek-developments-2016,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Developments in {Language} {Theory}},
	volume = {9840},
	isbn = {978-3-662-53131-0 978-3-662-53132-7},
	url = {http://link.springer.com/10.1007/978-3-662-53132-7},
	urldate = {2016-08-13},
	publisher = {Springer Berlin Heidelberg},
	editor = {Brlek, Srečko and Reutenauer, Christophe},
	year = {2016},
	file = {chp%3A10.1007%2F978-3-662-53132-7_6.pdf:/home/user/Zotero/storage/GZPTCPQM/chp%3A10.1007%2F978-3-662-53132-7_6.pdf:application/pdf}
}

@book{han-implementation-2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Implementation and {Application} of {Automata}},
	volume = {9705},
	isbn = {978-3-319-40945-0 978-3-319-40946-7},
	url = {http://link.springer.com/10.1007/978-3-319-40946-7},
	urldate = {2016-08-13},
	publisher = {Springer International Publishing},
	editor = {Han, Yo-Sub and Salomaa, Kai},
	year = {2016},
	file = {chp%3A10.1007%2F978-3-319-40946-7_8.pdf:/home/user/Zotero/storage/5U3V9DPT/chp%3A10.1007%2F978-3-319-40946-7_8.pdf:application/pdf}
}

@book{dinh-computing-2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computing and {Combinatorics}},
	volume = {9797},
	isbn = {978-3-319-42633-4 978-3-319-42634-1},
	url = {http://link.springer.com/10.1007/978-3-319-42634-1},
	urldate = {2016-08-13},
	publisher = {Springer International Publishing},
	editor = {Dinh, Thang N. and Thai, My T.},
	year = {2016},
	file = {chp%3A10.1007%2F978-3-319-42634-1_47.pdf:/home/user/Zotero/storage/SPZ583CJ/chp%3A10.1007%2F978-3-319-42634-1_47.pdf:application/pdf}
}

@article{futrell-large-scale-2015-1,
	title = {Large-scale evidence of dependency length minimization in 37 languages},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1502134112},
	doi = {10.1073/pnas.1502134112},
	language = {en},
	number = {33},
	urldate = {2016-08-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
	month = aug,
	year = {2015},
	pages = {10336--10341},
	file = {1509.03295v3.pdf:/home/user/Zotero/storage/SUNZCRMX/1509.03295v3.pdf:application/pdf}
}

@misc{noauthor-bcbl-nodate-14,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2166/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/FHZP3G8Z/2166.html:text/html}
}

@misc{noauthor-bcbl-nodate-15,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2055/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/PKESQC4D/2055.html:text/html}
}

@misc{noauthor-bcbl-nodate-16,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2118/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/RG5JKHDP/2118.html:text/html}
}

@misc{noauthor-bcbl-nodate-17,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2061/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/TBX7QAWT/2061.html:text/html}
}

@misc{noauthor-bcbl-nodate-18,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2064/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/GB754E9J/2064.html:text/html}
}

@misc{noauthor-bcbl-nodate-19,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2396/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/INC7N73K/2396.html:text/html}
}

@misc{noauthor-bcbl-nodate-20,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2119/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/X474FBEZ/2119.html:text/html}
}

@misc{noauthor-bcbl-nodate-21,
	title = {{BCBL} {Scientific} {Opening} {Ceremony} {Congress}},
	url = {http://www.bcbl.eu/events/amlap2016/en/speakers/desde0/ver/2258/},
	urldate = {2016-08-13},
	file = {BCBL Scientific Opening Ceremony Congress:/home/user/Zotero/storage/XGFQRJX6/2258.html:text/html}
}

@misc{noauthor-[1610.04211]-nodate,
	title = {[1610.04211] {Gated} {End}-to-{End} {Memory} {Networks}},
	url = {https://arxiv.org/abs/1610.04211},
	urldate = {2016-10-17},
	file = {[1610.04211] Gated End-to-End Memory Networks:/home/user/Zotero/storage/AR9P827C/1610.html:text/html}
}

@misc{noauthor-brenden-nodate,
	title = {Brenden {Lake} - {Data} {Science} {Fellow} at {NYU}},
	url = {http://cims.nyu.edu/~brenden/},
	urldate = {2016-10-17},
	file = {Brenden Lake - Data Science Fellow at NYU:/home/user/Zotero/storage/2A3MK4K2/~brenden.html:text/html}
}

@misc{noauthor-[1511.06038]-nodate,
	title = {[1511.06038] {Neural} {Variational} {Inference} for {Text} {Processing}},
	url = {https://arxiv.org/abs/1511.06038},
	urldate = {2016-10-17},
	file = {[1511.06038] Neural Variational Inference for Text Processing:/home/user/Zotero/storage/GGTNKFF7/1511.html:text/html}
}

@misc{noauthor-[1609.07317]-nodate,
	title = {[1609.07317] {Language} as a {Latent} {Variable}: {Discrete} {Generative} {Models} for {Sentence} {Compression}},
	url = {https://arxiv.org/abs/1609.07317},
	urldate = {2016-10-17},
	file = {[1609.07317] Language as a Latent Variable\: Discrete Generative Models for Sentence Compression:/home/user/Zotero/storage/CMXHXBGW/1609.html:text/html}
}

@misc{noauthor-[1610.04533]-nodate,
	title = {[1610.04533] {A} {Comprehensive} {Comparative} {Study} of {Word} and {Sentence} {Similarity} {Measures}},
	url = {https://arxiv.org/abs/1610.04533},
	urldate = {2016-10-17},
	file = {[1610.04533] A Comprehensive Comparative Study of Word and Sentence Similarity Measures:/home/user/Zotero/storage/IHB77SXG/1610.html:text/html}
}

@misc{noauthor-[1606.03391]-nodate,
	title = {[1606.03391] {Simple} {Question} {Answering} by {Attentive} {Convolutional} {Neural} {Network}},
	url = {https://arxiv.org/abs/1606.03391},
	urldate = {2016-10-15},
	file = {[1606.03391] Simple Question Answering by Attentive Convolutional Neural Network:/home/user/Zotero/storage/IHMM5T8T/1606.html:text/html}
}

@misc{noauthor-[1610.03164]-nodate,
	title = {[1610.03164] {Navigational} {Instruction} {Generation} as {Inverse} {Reinforcement} {Learning} with {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1610.03164},
	urldate = {2016-10-15},
	file = {[1610.03164] Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation:/home/user/Zotero/storage/NNPTXQZA/1610.html:text/html}
}

@misc{noauthor-[1610.03321]-nodate,
	title = {[1610.03321] {Keystroke} dynamics as signal for shallow syntactic parsing},
	url = {https://arxiv.org/abs/1610.03321},
	urldate = {2016-10-15},
	file = {[1610.03321] Keystroke dynamics as signal for shallow syntactic parsing:/home/user/Zotero/storage/HFTA4KVT/1610.html:text/html}
}

@misc{noauthor-[1606.04052]-nodate,
	title = {[1606.04052] {Dialog} state tracking, a machine reading approach using {Memory} {Network}},
	url = {https://arxiv.org/abs/1606.04052},
	urldate = {2016-10-15},
	file = {[1606.04052] Dialog state tracking, a machine reading approach using Memory Network:/home/user/Zotero/storage/PDN5BR5N/1606.html:text/html}
}

@misc{noauthor-[1605.07826]-nodate,
	title = {[1605.07826] {Asymptotically} exact inference in likelihood-free models},
	url = {https://arxiv.org/abs/1605.07826},
	urldate = {2016-10-15},
	file = {[1605.07826] Asymptotically exact inference in likelihood-free models:/home/user/Zotero/storage/RJDUVDSI/1605.html:text/html}
}

@misc{noauthor-[1506.00059]-nodate,
	title = {[1506.00059] {Saddle}-free {Hessian}-free {Optimization}},
	url = {https://arxiv.org/abs/1506.00059},
	urldate = {2016-10-15},
	file = {[1506.00059] Saddle-free Hessian-free Optimization:/home/user/Zotero/storage/2HTX9U8G/1506.html:text/html}
}

@misc{noauthor-[1606.01700]-nodate,
	title = {[1606.01700] {Gated} {Word}-{Character} {Recurrent} {Language} {Model}},
	url = {https://arxiv.org/abs/1606.01700},
	urldate = {2016-10-14},
	file = {[1606.01700] Gated Word-Character Recurrent Language Model:/home/user/Zotero/storage/JSTV4QC2/1606.html:text/html}
}

@misc{noauthor-[1409.7275]-nodate,
	title = {[1409.7275] {The} meaning-frequency law in {Zipfian} optimization models of communication},
	url = {https://arxiv.org/abs/1409.7275},
	urldate = {2016-10-14},
	file = {[1409.7275] The meaning-frequency law in Zipfian optimization models of communication:/home/user/Zotero/storage/KFKI6NJK/1409.html:text/html}
}

@misc{noauthor-[1610.03946]-nodate,
	title = {[1610.03946] {A} {Neural} {Network} for {Coordination} {Boundary} {Prediction}},
	url = {https://arxiv.org/abs/1610.03946},
	urldate = {2016-10-14},
	file = {[1610.03946] A Neural Network for Coordination Boundary Prediction:/home/user/Zotero/storage/ZXJV9EMB/1610.html:text/html}
}

@inproceedings{doyle-nonparametric-2014-1,
	title = {Nonparametric {Learning} of {Phonological} {Constraints} in {Optimality} {Theory}.},
	url = {http://idiom.ucsd.edu/~rlevy/papers/doyle-bicknell-levy-2014-acl.pdf},
	urldate = {2016-10-14},
	booktitle = {{ACL} (1)},
	author = {Doyle, Gabriel and Bicknell, Klinton and Levy, Roger},
	year = {2014},
	pages = {1094--1103},
	file = {Nonparametric Learning of Phonological Constraints in Optimality Theory - zotero\://attachment/8431/:/home/user/Zotero/storage/86DGHZDQ/8431.pdf:application/pdf}
}

@article{lassiter-graded-2014-1,
	title = {Graded modality},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.722.7992&rep=rep1&type=pdf},
	urldate = {2016-10-14},
	journal = {Companion to Semantics, Wiley},
	author = {Lassiter, Daniel},
	year = {2014},
	file = {Lassiter-graded-modality-draft.pdf:/home/user/Zotero/storage/WCIITDF3/Lassiter-graded-modality-draft.pdf:application/pdf}
}

@article{galeazzi-smart-nodate,
	title = {Smart {Representations}: {Rationality} and {Evolution} in a {Richer} {Environment}},
	shorttitle = {Smart {Representations}},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/GaleazziFranke\_2016\_SmartRepresentations.pdf},
	urldate = {2016-10-14},
	author = {Galeazzi, Paolo and Franke, Michael},
	file = {GaleazziFranke_2016_SmartRepresentations.pdf:/home/user/Zotero/storage/B7UGUTBM/GaleazziFranke_2016_SmartRepresentations.pdf:application/pdf}
}

@article{franke-vagueness-nodate-1,
	title = {Vagueness and imprecise imitation in signaling games},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/Papers/FrankeCorreia\_2016\_Vagueness\_and\_imprecise\_imitation\_in\_signalling\_games.pdf},
	urldate = {2016-10-14},
	journal = {British Journal for the Philosophy of Science, to appear},
	author = {Franke, Michael and Correia, José Pedro},
	file = {FrankeCorreia_2016_Vagueness_and_imprecise_imitation_in_signalling_games.pdf:/home/user/Zotero/storage/BUB7U5Q5/FrankeCorreia_2016_Vagueness_and_imprecise_imitation_in_signalling_games.pdf:application/pdf}
}

@book{gall-educational-1996,
	title = {Educational research: {An} introduction .},
	shorttitle = {Educational research},
	url = {http://psycnet.apa.org/psycinfo/1996-97171-000},
	urldate = {2016-10-14},
	publisher = {Longman Publishing},
	author = {Gall, Meredith Damien and Borg, Walter R. and Gall, Joyce P.},
	year = {1996},
	file = {bicknell_diss.pdf:/home/user/Zotero/storage/WSMZ7HKF/bicknell_diss.pdf:application/pdf}
}

@inproceedings{ng-pegasus:-2000,
	title = {{PEGASUS}: {A} policy search method for large {MDPs} and {POMDPs}},
	shorttitle = {{PEGASUS}},
	url = {http://dl.acm.org/citation.cfm?id=2073994},
	urldate = {2016-10-14},
	booktitle = {Proceedings of the {Sixteenth} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ng, Andrew Y. and Jordan, Michael},
	year = {2000},
	pages = {406--415},
	file = {uai00-pegasus.pdf:/home/user/Zotero/storage/CAE9VWUS/uai00-pegasus.pdf:application/pdf}
}

@inproceedings{bicknell-why-2011-1,
	title = {Why readers regress to previous words: {A} statistical analysis},
	shorttitle = {Why readers regress to previous words},
	url = {https://pdfs.semanticscholar.org/3a9b/960587d1e007433ee107e8f70f7cbbf5161b.pdf},
	urldate = {2016-10-14},
	booktitle = {Proceedings of the 33rd annual meeting of the {Cognitive} {Science} {Society}},
	author = {Bicknell, Klinton and Levy, Roger},
	year = {2011},
	pages = {931--936},
	file = {bicknell-levy-2011-cogsci.pdf:/home/user/Zotero/storage/8R8NVPMP/bicknell-levy-2011-cogsci.pdf:application/pdf}
}

@inproceedings{doyle-nonparametric-2014-2,
	title = {Nonparametric {Learning} of {Phonological} {Constraints} in {Optimality} {Theory}.},
	url = {http://idiom.ucsd.edu/~rlevy/papers/doyle-bicknell-levy-2014-acl.pdf},
	urldate = {2016-10-14},
	booktitle = {{ACL} (1)},
	author = {Doyle, Gabriel and Bicknell, Klinton and Levy, Roger},
	year = {2014},
	pages = {1094--1103},
	file = {P14-1103.pdf:/home/user/Zotero/storage/JI2RZB9I/P14-1103.pdf:application/pdf}
}

@article{schotter-task-2014-1,
	title = {Task effects reveal cognitive flexibility responding to frequency and predictability: {Evidence} from eye movements in reading and proofreading},
	volume = {131},
	issn = {0010-0277},
	shorttitle = {Task effects reveal cognitive flexibility responding to frequency and predictability},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027713002369},
	doi = {10.1016/j.cognition.2013.11.018},
	abstract = {It is well-known that word frequency and predictability affect processing time. These effects change magnitude across tasks, but studies testing this use tasks with different response types (e.g., lexical decision, naming, and fixation time during reading; Schilling, Rayner, \&amp; Chumbley, 1998), preventing direct comparison. Recently, Kaakinen and Hyönä (2010) overcame this problem, comparing fixation times in reading for comprehension and proofreading, showing that the frequency effect was larger in proofreading than in reading. This result could be explained by readers exhibiting substantial cognitive flexibility, and qualitatively changing how they process words in the proofreading task in a way that magnifies effects of word frequency. Alternatively, readers may not change word processing so dramatically, and instead may perform more careful identification generally, increasing the magnitude of many word processing effects (e.g., both frequency and predictability). We tested these possibilities with two experiments: subjects read for comprehension and then proofread for spelling errors (letter transpositions) that produce nonwords (e.g., trcak for track as in Kaakinen \&amp; Hyönä) or that produce real but unintended words (e.g., trial for trail) to compare how the task changes these effects. Replicating Kaakinen and Hyönä, frequency effects increased during proofreading. However, predictability effects only increased when integration with the sentence context was necessary to detect errors (i.e., when spelling errors produced words that were inappropriate in the sentence; trial for trail). The results suggest that readers adopt sophisticated word processing strategies to accommodate task demands.},
	number = {1},
	urldate = {2016-10-14},
	journal = {Cognition},
	author = {Schotter, Elizabeth R. and Bicknell, Klinton and Howard, Ian and Levy, Roger and Rayner, Keith},
	month = apr,
	year = {2014},
	keywords = {Eye Movements, Reading, Task effects, Frequency, Predictability},
	pages = {1--27},
	file = {Schotter et al_2014_Task effects reveal cognitive flexibility responding to frequency and.pdf:/home/user/Zotero/storage/6B7MZ4UU/Schotter et al_2014_Task effects reveal cognitive flexibility responding to frequency and.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/QK89JH7V/S0010027713002369.html:text/html}
}

@misc{noauthor-psycnet-nodate,
	title = {{PsycNET} - {DOI} {Landing} page},
	url = {http://psycnet.apa.org/?&fa=main.doiLanding&doi=10.1037/a0033580},
	urldate = {2016-10-14},
	file = {PsycNET - DOI Landing page:/home/user/Zotero/storage/FRTF5GCA/psycnet.apa.org.html:text/html}
}

@misc{noauthor-psycnet-nodate-1,
	title = {{PsycNET} - {DOI} {Landing} page},
	url = {http://psycnet.apa.org/?&fa=main.doiLanding&doi=10.1037/xhp0000054},
	urldate = {2016-10-14},
	file = {PsycNET - DOI Landing page:/home/user/Zotero/storage/KRQHCMVC/psycnet.apa.org.html:text/html}
}

@article{levy-eye-2009-1,
	title = {Eye movement evidence that readers maintain and act on uncertainty about past linguistic input},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/106/50/21086},
	doi = {10.1073/pnas.0907664106},
	abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
	language = {en},
	number = {50},
	urldate = {2016-10-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
	month = dec,
	year = {2009},
	pmid = {19965371},
	keywords = {psycholinguistics, language comprehension, probabilistic models of cognition},
	pages = {21086--21090},
	file = {Levy et al_2009_Eye movement evidence that readers maintain and act on uncertainty about past.pdf:/home/user/Zotero/storage/QMJW2339/Levy et al_2009_Eye movement evidence that readers maintain and act on uncertainty about past.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/PXIE55MQ/21086.html:text/html}
}

@misc{noauthor-michael-nodate,
	title = {Michael {Franke}},
	url = {http://www.sfs.uni-tuebingen.de/~mfranke/},
	urldate = {2016-10-14},
	file = {Michael Franke:/home/user/Zotero/storage/B3PNZCGE/~mfranke.html:text/html}
}

@misc{noauthor-github-nodate,
	title = {{GitHub} - michael-franke/game\_theoretic\_pragmatics\_ORE: {Code} accompanying overview article "{Game} {Theory} in {Pragmatics}: {Evolution}, {Rationality} \& {Reasoning}"},
	url = {https://github.com/michael-franke/game\_theoretic\_pragmatics\_ORE},
	urldate = {2016-10-14},
	file = {GitHub - michael-franke/game_theoretic_pragmatics_ORE\: Code accompanying overview article "Game Theory in Pragmatics\: Evolution, Rationality & Reasoning":/home/user/Zotero/storage/78SK55H2/game_theoretic_pragmatics_ORE.html:text/html}
}

@article{lin-why-2016,
	title = {Why does deep and cheap learning work so well?},
	url = {https://arxiv.org/abs/1608.08225},
	urldate = {2016-10-14},
	journal = {arXiv preprint arXiv:1608.08225},
	author = {Lin, Henry W. and Tegmark, Max},
	year = {2016},
	file = {1608.08225v1.pdf:/home/user/Zotero/storage/HSUM4QVH/1608.08225v1.pdf:application/pdf}
}

@book{nipkow-isabelle/hol:-2002,
	title = {Isabelle/{HOL}: a proof assistant for higher-order logic},
	volume = {2283},
	shorttitle = {Isabelle/{HOL}},
	url = {https://books.google.com/books?hl=en&lr=&id=R6ul20M6nTIC&oi=fnd&pg=PR2&dq=%22Norbert+Schirmer+and+Martin+Strecker.+Stephan+Merz+was+also%22+%22Introduction+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.+.%22+&ots=ev9XlJLVOl&sig=D6UzMy5cfmDroPfNums1TOrjw30},
	urldate = {2016-10-14},
	publisher = {Springer Science \& Business Media},
	author = {Nipkow, Tobias and Paulson, Lawrence C. and Wenzel, Markus},
	year = {2002},
	file = {tutorial.pdf:/home/user/Zotero/storage/KNUHSBN5/tutorial.pdf:application/pdf}
}

@book{wenzel-isabelle/jedit-2014,
	title = {Isabelle/{jEdit}},
	url = {http://isabelle.in.tum.de/dist/doc/jedit.pdf},
	urldate = {2016-10-14},
	author = {Wenzel, Makarius},
	year = {2014},
	file = {jedit.pdf:/home/user/Zotero/storage/92FZDDHU/jedit.pdf:application/pdf}
}

@article{lapolla-arguments-1993,
	title = {Arguments against ‘subject’and ‘direct object’as viable concepts in {Chinese}},
	volume = {63},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.1444&rep=rep1&type=pdf},
	number = {4},
	urldate = {2016-10-14},
	journal = {Bulletin of the Institute of History and Philology},
	author = {LaPolla, Randy J.},
	year = {1993},
	pages = {759--813},
	file = {LaPolla_1993_Arguments_Against_Subject_and_Direct_Object_as_Viable_Concepts_in_Chinese.pdf:/home/user/Zotero/storage/2AACD3N2/LaPolla_1993_Arguments_Against_Subject_and_Direct_Object_as_Viable_Concepts_in_Chinese.pdf:application/pdf}
}

@article{blei-variational-2011,
	title = {Variational {Inference}},
	url = {https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf},
	urldate = {2016-10-14},
	journal = {Lecture from Princeton, https://www. cs. princeton. edu/courses/archive/fall11/cos597C/lectures/variational-inference-i. pdf},
	author = {Blei, David M.},
	year = {2011},
	file = {variational-inference-i.pdf:/home/user/Zotero/storage/9VK5AKHH/variational-inference-i.pdf:application/pdf}
}

@article{marin-resolving-2010,
	title = {On resolving the {Savage}–{Dickey} paradox},
	volume = {4},
	url = {http://projecteuclid.org/euclid.ejs/1278682959},
	urldate = {2016-10-14},
	journal = {Electronic Journal of Statistics},
	author = {Marin, Jean-Michel and Robert, Christian P. and {others}},
	year = {2010},
	pages = {643--654},
	file = {On resolving the Savage–Dickey paradox - 0910.1452.pdf:/home/user/Zotero/storage/XDJMQMEM/0910.1452.pdf:application/pdf}
}

@article{ma-bayesian-2006,
	title = {Bayesian inference with probabilistic population codes},
	volume = {9},
	issn = {1097-6256},
	url = {http://www.nature.com/doifinder/10.1038/nn1790},
	doi = {10.1038/nn1790},
	number = {11},
	urldate = {2016-10-14},
	journal = {Nature Neuroscience},
	author = {Ma, Wei Ji and Beck, Jeffrey M and Latham, Peter E and Pouget, Alexandre},
	month = nov,
	year = {2006},
	pages = {1432--1438},
	file = {npgrj_nn_1790 1432..1438 - ppc-06.pdf:/home/user/Zotero/storage/G2AKPWCC/ppc-06.pdf:application/pdf}
}

@article{marin-resolving-2010-1,
	title = {On resolving the {Savage}–{Dickey} paradox},
	volume = {4},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1278682959},
	doi = {10.1214/10-EJS564},
	abstract = {When testing a null hypothesis H0: θ=θ0 in a Bayesian framework, the Savage–Dickey ratio (Dickey, 1971) is known as a specific representation of the Bayes factor (O’Hagan and Forster, 2004) that only uses the posterior distribution under the alternative hypothesis at θ0, thus allowing for a plug-in version of this quantity. We demonstrate here that the Savage–Dickey representation is in fact a generic representation of the Bayes factor and that it fundamentally relies on specific measure-theoretic versions of the densities involved in the ratio, instead of being a special identity imposing some mathematically void constraints on the prior distributions. We completely clarify the measure-theoretic foundations of the Savage–Dickey representation as well as of the later generalisation of Verdinelli and Wasserman (1995). We provide furthermore a general framework that produces a converging approximation of the Bayes factor that is unrelated with the approach of Verdinelli and Wasserman (1995) and propose a comparison of this new approximation with their version, as well as with bridge sampling and Chib’s approaches.},
	language = {EN},
	urldate = {2016-10-14},
	journal = {Electronic Journal of Statistics},
	author = {Marin, Jean-Michel and Robert, Christian P.},
	year = {2010},
	mrnumber = {MR2660536},
	zmnumber = {06166520},
	keywords = {Bayes factor, Bayesian model choice, bridge sampling, conditional distribution, hypothesis testing, Savage–Dickey ratio, zero measure set},
	pages = {643--654},
	file = {Snapshot:/home/user/Zotero/storage/QJIZX74P/1278682959.html:text/html}
}

@misc{noauthor-dickey-savage-nodate,
	title = {Dickey-{Savage} ratio {\textbar} {Xi}'an's {Og}},
	url = {https://xianblog.wordpress.com/tag/dickey-savage-ratio/},
	urldate = {2016-10-14},
	file = {Dickey-Savage ratio | Xi'an's Og:/home/user/Zotero/storage/2BZIS8VK/dickey-savage-ratio.html:text/html}
}

@misc{noauthor-is-nodate-1,
	title = {Is the {Dickey}-{Savage} ratio any valid?! {\textbar} {Xi}'an's {Og}},
	url = {https://xianblog.wordpress.com/2009/09/24/is-the-dickey-savage-ratio-any-valid/},
	urldate = {2016-10-14},
	file = {Is the Dickey-Savage ratio any valid?! | Xi'an's Og:/home/user/Zotero/storage/JRDRBFHM/is-the-dickey-savage-ratio-any-valid.html:text/html}
}

@misc{noauthor-[1603.04136]-nodate,
	title = {[1603.04136] {On} the {Influence} of {Momentum} {Acceleration} on {Online} {Learning}},
	url = {https://arxiv.org/abs/1603.04136},
	urldate = {2016-10-13},
	file = {[1603.04136] On the Influence of Momentum Acceleration on Online Learning:/home/user/Zotero/storage/UASBFRDW/1603.html:text/html}
}

@misc{noauthor-[1511.06464]-nodate,
	title = {[1511.06464] {Unitary} {Evolution} {Recurrent} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1511.06464},
	urldate = {2016-10-13},
	file = {[1511.06464] Unitary Evolution Recurrent Neural Networks:/home/user/Zotero/storage/Q73G9SFE/1511.html:text/html}
}

@misc{noauthor-[1602.07776]-nodate,
	title = {[1602.07776] {Recurrent} {Neural} {Network} {Grammars}},
	url = {https://arxiv.org/abs/1602.07776},
	urldate = {2016-10-13},
	file = {[1602.07776] Recurrent Neural Network Grammars:/home/user/Zotero/storage/J24HBUUG/1602.html:text/html}
}

@misc{noauthor-[1610.03585]-nodate,
	title = {[1610.03585] {A} {Paradigm} for {Situated} and {Goal}-{Driven} {Language} {Learning}},
	url = {https://arxiv.org/abs/1610.03585},
	urldate = {2016-10-13},
	file = {[1610.03585] A Paradigm for Situated and Goal-Driven Language Learning:/home/user/Zotero/storage/VIX6J8S3/1610.html:text/html}
}

@article{hoppe-learning-2016,
	title = {Learning rational temporal eye movement strategies},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/29/8332},
	doi = {10.1073/pnas.1601305113},
	abstract = {During active behavior humans redirect their gaze several times every second within the visual environment. Where we look within static images is highly efficient, as quantified by computational models of human gaze shifts in visual search and face recognition tasks. However, when we shift gaze is mostly unknown despite its fundamental importance for survival in a dynamic world. It has been suggested that during naturalistic visuomotor behavior gaze deployment is coordinated with task-relevant events, often predictive of future events, and studies in sportsmen suggest that timing of eye movements is learned. Here we establish that humans efficiently learn to adjust the timing of eye movements in response to environmental regularities when monitoring locations in the visual scene to detect probabilistically occurring events. To detect the events humans adopt strategies that can be understood through a computational model that includes perceptual and acting uncertainties, a minimal processing time, and, crucially, the intrinsic costs of gaze behavior. Thus, subjects traded off event detection rate with behavioral costs of carrying out eye movements. Remarkably, based on this rational bounded actor model the time course of learning the gaze strategies is fully explained by an optimal Bayesian learner with humans’ characteristic uncertainty in time estimation, the well-known scalar law of biological timing. Taken together, these findings establish that the human visual system is highly efficient in learning temporal regularities in the environment and that it can use these regularities to control the timing of eye movements to detect behaviorally relevant events.},
	language = {en},
	number = {29},
	urldate = {2016-10-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hoppe, David and Rothkopf, Constantin A.},
	month = jul,
	year = {2016},
	pmid = {27382164},
	keywords = {Computational Modeling, Eye Movements, Learning, Visual attention, decision making},
	pages = {8332--8337},
	file = {Hoppe_Rothkopf_2016_Learning rational temporal eye movement strategies.pdf:/home/user/Zotero/storage/B829FCVW/Hoppe_Rothkopf_2016_Learning rational temporal eye movement strategies.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/M3G2CNAX/8332.html:text/html}
}

@article{tessler-pragmatic-2016,
	title = {A pragmatic theory of generic language},
	url = {http://arxiv.org/abs/1608.02926},
	abstract = {Generalizations about categories are central to human understanding, and generic language (e.g. "Dogs bark.") provides a simple and ubiquitous way to communicate these generalizations. Yet the meaning of generic language is philosophically puzzling and has resisted precise formalization. We explore the idea that the core meaning of a generic sentence is simple but underspecified, and that general principles of pragmatic reasoning are responsible for establishing the precise meaning in context. Building on recent probabilistic models of language understanding, we provide a formal model for the evaluation and comprehension of generic sentences. This model explains the puzzling flexibility in usage of generics in terms of diverse prior beliefs about properties. We elicit these priors experimentally and show that the resulting model predictions explain almost all of the variance in human judgments for both common and novel generics. We probe the theory in more detail, and find that generic language depends in a fundamental way on subjective beliefs, not mere frequency. This theory provides the mathematical bridge between the words we use and the concepts they describe.},
	urldate = {2016-10-11},
	journal = {arXiv:1608.02926 [cs]},
	author = {Tessler, Michael Henry and Goodman, Noah D.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.02926},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/UHX4QWGG/1608.html:text/html;Tessler_Goodman_2016_A pragmatic theory of generic language.pdf:/home/user/Zotero/storage/ABZ5H7X4/Tessler_Goodman_2016_A pragmatic theory of generic language.pdf:application/pdf}
}

@misc{noauthor-[1606.05320]-nodate,
	title = {[1606.05320] {Increasing} the {Interpretability} of {Recurrent} {Neural} {Networks} {Using} {Hidden} {Markov} {Models}},
	url = {https://arxiv.org/abs/1606.05320},
	urldate = {2016-10-10},
	file = {[1606.05320] Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models:/home/user/Zotero/storage/FSHB3WQ3/1606.html:text/html}
}

@misc{noauthor-[1610.00479]-nodate,
	title = {[1610.00479] {Nonsymbolic} {Text} {Representation}},
	url = {https://arxiv.org/abs/1610.00479},
	urldate = {2016-10-10},
	file = {[1610.00479] Nonsymbolic Text Representation:/home/user/Zotero/storage/AAZAP7D5/1610.html:text/html}
}

@misc{noauthor-[1610.00388]-nodate,
	title = {[1610.00388] {Learning} to {Translate} in {Real}-time with {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1610.00388},
	urldate = {2016-10-10},
	file = {[1610.00388] Learning to Translate in Real-time with Neural Machine Translation:/home/user/Zotero/storage/JUT4BFQK/1610.html:text/html}
}

@misc{noauthor-timothy-nodate,
	title = {Timothy {O}'{Donnell}},
	url = {http://web.mit.edu/timod/www/},
	urldate = {2016-10-10},
	file = {Timothy O'Donnell:/home/user/Zotero/storage/9XCPV7NU/www.html:text/html}
}

@misc{noauthor-[1605.06444]-nodate,
	title = {[1605.06444] {Unreasonable} {Effectiveness} of {Learning} {Neural} {Networks}: {From} {Accessible} {States} and {Robust} {Ensembles} to {Basic} {Algorithmic} {Schemes}},
	url = {https://arxiv.org/abs/1605.06444},
	urldate = {2016-10-10},
	file = {[1605.06444] Unreasonable Effectiveness of Learning Neural Networks\: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes:/home/user/Zotero/storage/2IGVZ5GP/1605.html:text/html}
}

@misc{noauthor-[1610.01945]-nodate,
	title = {[1610.01945] {Connecting} {Generative} {Adversarial} {Networks} and {Actor}-{Critic} {Methods}},
	url = {https://arxiv.org/abs/1610.01945},
	urldate = {2016-10-10},
	file = {[1610.01945] Connecting Generative Adversarial Networks and Actor-Critic Methods:/home/user/Zotero/storage/28WUTJ2E/1610.html:text/html}
}

@misc{noauthor-smoothsort-nodate,
	title = {Smoothsort {Demystified}},
	url = {http://www.keithschwarz.com/smoothsort/},
	urldate = {2016-10-09},
	file = {Smoothsort Demystified:/home/user/Zotero/storage/AAN5KF3U/smoothsort.html:text/html}
}

@inproceedings{holzl-three-2011,
	title = {Three chapters of measure theory in {Isabelle}/{HOL}},
	url = {http://link.springer.com/10.1007%2F978-3-642-22863-6\_12},
	urldate = {2016-10-09},
	booktitle = {International {Conference} on {Interactive} {Theorem} {Proving}},
	publisher = {Springer},
	author = {Hölzl, Johannes and Heller, Armin},
	year = {2011},
	pages = {135--151},
	file = {hoelzl2011measuretheory.pdf:/home/user/Zotero/storage/M8DZT9S6/hoelzl2011measuretheory.pdf:application/pdf}
}

@article{kuncar-types-2016,
	title = {From types to sets by local type definitions in higher-order logic},
	url = {http://www21.in.tum.de/~kuncar/documents/kuncar-popescu-t2s2016-extended.pdf},
	urldate = {2016-10-09},
	journal = {Proc. ITP},
	author = {Kuncar, Ondrej and Popescu, Andrei},
	year = {2016},
	file = {From Types to Sets by Local Type Definitions in Higher-Order Logic - kuncar-popescu-t2s2016.pdf:/home/user/Zotero/storage/TFR4VF47/kuncar-popescu-t2s2016.pdf:application/pdf}
}

@book{haftmann-haskell-style-2013,
	title = {Haskell-style type classes with {Isabelle}/{Isar}},
	url = {http://isabelle.in.tum.de/website-Isabelle2015/dist/Isabelle2015/doc/classes.pdf},
	urldate = {2016-10-09},
	author = {Haftmann, Florian},
	year = {2013},
	file = {classes.pdf:/home/user/Zotero/storage/B7WHF99J/classes.pdf:application/pdf}
}

@book{krauss-defining-2008,
	title = {Defining recursive functions in {Isabelle}/{HOL}},
	url = {http://isabelle.in.tum.de/website-Isabelle2016/dist/Isabelle2016/doc/functions.pdf},
	urldate = {2016-10-09},
	author = {Krauss, Alexander},
	year = {2008},
	file = {functions.pdf:/home/user/Zotero/storage/PKNNPHK7/functions.pdf:application/pdf}
}

@book{wenzel-isabelle/isar-2004,
	title = {The isabelle/isar reference manual},
	url = {http://www4.informatik.tu-muenchen.de/~isabelle/html-data/doc/isar-ref.pdf},
	urldate = {2016-10-09},
	author = {Wenzel, Makarius and {others}},
	year = {2004},
	file = {isar-ref.pdf:/home/user/Zotero/storage/8NPXKEFQ/isar-ref.pdf:application/pdf}
}

@article{damonte-incremental-2016,
	title = {An {Incremental} {Parser} for {Abstract} {Meaning} {Representation}},
	url = {http://arxiv.org/abs/1608.06111},
	abstract = {Abstract Meaning Representation (AMR) is a semantic representation for natural language that involves an easy annotation process, hence favoring the development of large datasets. It embeds traditional NLP tasks such as named entity recognition, semantic role labeling, word sense disambiguation and coreference resolution. We describe a transition system that parses the sentence left-to-right, in time linear in the size of the input. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers and show that our left-to-right parser is competitive with the state of the art on the LDC2015E86 dataset and it is superior in recovering reentrancies and handling polarity.},
	urldate = {2016-10-09},
	journal = {arXiv:1608.06111 [cs]},
	author = {Damonte, Marco and Cohen, Shay B. and Satta, Giorgio},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.06111},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/5U9FF8AR/1608.html:text/html;Damonte et al_2016_An Incremental Parser for Abstract Meaning Representation.pdf:/home/user/Zotero/storage/2TBFIJKS/Damonte et al_2016_An Incremental Parser for Abstract Meaning Representation.pdf:application/pdf}
}

@article{brzozowski-complexity-2016,
	title = {Complexity of {Left}-{Ideal}, {Suffix}-{Closed} and {Suffix}-{Free} {Regular} {Languages}},
	url = {http://arxiv.org/abs/1610.00728},
	abstract = {A language \$L\$ over an alphabet \$\Sigma\$ is suffix-convex if, for any words \$x,y,z\in\Sigma{\textasciicircum}*\$, whenever \$z\$ and \$xyz\$ are in \$L\$, then so is \$yz\$. Suffix-convex languages include three special cases: left-ideal, suffix-closed, and suffix-free languages. We examine complexity properties of these three special classes of suffix-convex regular languages. In particular, we study the quotient/state complexity of boolean operations, product (concatenation), star, and reversal on these languages, as well as the size of their syntactic semigroups, and the quotient complexity of their atoms.},
	urldate = {2016-10-09},
	journal = {arXiv:1610.00728 [cs]},
	author = {Brzozowski, Janusz and Sinnamom, Corwin},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.00728},
	keywords = {Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/37D7PMSW/1610.html:text/html;Brzozowski_Sinnamom_2016_Complexity of Left-Ideal, Suffix-Closed and Suffix-Free Regular Languages.pdf:/home/user/Zotero/storage/ZVBPVK2I/Brzozowski_Sinnamom_2016_Complexity of Left-Ideal, Suffix-Closed and Suffix-Free Regular Languages.pdf:application/pdf}
}

@article{junczys-dowmunt-is-2016,
	title = {Is {Neural} {Machine} {Translation} {Ready} for {Deployment}? {A} {Case} {Study} on 30 {Translation} {Directions}},
	shorttitle = {Is {Neural} {Machine} {Translation} {Ready} for {Deployment}?},
	url = {http://arxiv.org/abs/1610.01108},
	abstract = {In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.},
	urldate = {2016-10-09},
	journal = {arXiv:1610.01108 [cs]},
	author = {Junczys-Dowmunt, Marcin and Dwojak, Tomasz and Hoang, Hieu},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.01108},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/4W3BDX5J/1610.html:text/html;Junczys-Dowmunt et al_2016_Is Neural Machine Translation Ready for Deployment.pdf:/home/user/Zotero/storage/A5D6GBQX/Junczys-Dowmunt et al_2016_Is Neural Machine Translation Ready for Deployment.pdf:application/pdf}
}

@misc{noauthor-[1610.01549]-nodate,
	title = {[1610.01549] {A} {Novel} {Representation} of {Neural} {Networks}},
	url = {https://arxiv.org/abs/1610.01549},
	urldate = {2016-10-09},
	file = {[1610.01549] A Novel Representation of Neural Networks:/home/user/Zotero/storage/ZVFDTGAS/1610.html:text/html}
}

@article{gu-learning-2016,
	title = {Learning to {Translate} in {Real}-time with {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1610.00388},
	abstract = {Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.},
	urldate = {2016-10-09},
	journal = {arXiv:1610.00388 [cs]},
	author = {Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O. K.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.00388},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1610.00388 PDF:/home/user/Zotero/storage/P9UUJTHT/Gu et al. - 2016 - Learning to Translate in Real-time with Neural Mac.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/EV423D65/1610.html:text/html}
}

@inproceedings{hahn-modeling-2016,
	title = {Modeling {Human} {Reading} with {Neural} {Attention}},
	url = {https://arxiv.org/pdf/1608.05604v1.pdf},
	booktitle = {Proceedings of {EMNLP}},
	author = {Hahn, Michael and Keller, Frank},
	year = {2016}
}

@misc{noauthor-publications-nodate,
	title = {Publications {\textbar} {Colin} {Phillips}},
	url = {http://www.colinphillips.net/?page\_id=202},
	urldate = {2016-09-15},
	file = {Publications | Colin Phillips:/home/user/Zotero/storage/67AXVA7C/www.colinphillips.net.html:text/html}
}

@article{blasi-soundmeaning-2016,
	title = {Sound–meaning association biases evidenced across thousands of languages},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2016/09/06/1605782113},
	doi = {10.1073/pnas.1605782113},
	abstract = {It is widely assumed that one of the fundamental properties of spoken language is the arbitrary relation between sound and meaning. Some exceptions in the form of nonarbitrary associations have been documented in linguistics, cognitive science, and anthropology, but these studies only involved small subsets of the 6,000+ languages spoken in the world today. By analyzing word lists covering nearly two-thirds of the world’s languages, we demonstrate that a considerable proportion of 100 basic vocabulary items carry strong associations with specific kinds of human speech sounds, occurring persistently across continents and linguistic lineages (linguistic families or isolates). Prominently among these relations, we find property words (“small” and i, “full” and p or b) and body part terms (“tongue” and l, “nose” and n). The areal and historical distribution of these associations suggests that they often emerge independently rather than being inherited or borrowed. Our results therefore have important implications for the language sciences, given that nonarbitrary associations have been proposed to play a critical role in the emergence of cross-modal mappings, the acquisition of language, and the evolution of our species’ unique communication system.},
	language = {en},
	urldate = {2016-09-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Blasi, Damián E. and Wichmann, Søren and Hammarström, Harald and Stadler, Peter F. and Christiansen, Morten H.},
	month = sep,
	year = {2016},
	pmid = {27621455},
	keywords = {Linguistics, iconicity, Language evolution, cognitive sciences, sound symbolism},
	pages = {201605782},
	file = {Snapshot:/home/user/Zotero/storage/VQ3QTHDR/1605782113.html:text/html}
}

@article{blasi-soundmeaning-2016-1,
	title = {Sound–meaning association biases evidenced across thousands of languages},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2016/09/06/1605782113},
	doi = {10.1073/pnas.1605782113},
	abstract = {It is widely assumed that one of the fundamental properties of spoken language is the arbitrary relation between sound and meaning. Some exceptions in the form of nonarbitrary associations have been documented in linguistics, cognitive science, and anthropology, but these studies only involved small subsets of the 6,000+ languages spoken in the world today. By analyzing word lists covering nearly two-thirds of the world’s languages, we demonstrate that a considerable proportion of 100 basic vocabulary items carry strong associations with specific kinds of human speech sounds, occurring persistently across continents and linguistic lineages (linguistic families or isolates). Prominently among these relations, we find property words (“small” and i, “full” and p or b) and body part terms (“tongue” and l, “nose” and n). The areal and historical distribution of these associations suggests that they often emerge independently rather than being inherited or borrowed. Our results therefore have important implications for the language sciences, given that nonarbitrary associations have been proposed to play a critical role in the emergence of cross-modal mappings, the acquisition of language, and the evolution of our species’ unique communication system.},
	language = {en},
	urldate = {2016-09-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Blasi, Damián E. and Wichmann, Søren and Hammarström, Harald and Stadler, Peter F. and Christiansen, Morten H.},
	month = sep,
	year = {2016},
	pmid = {27621455},
	keywords = {Linguistics, iconicity, Language evolution, cognitive sciences, sound symbolism},
	pages = {201605782},
	file = {Snapshot:/home/user/Zotero/storage/EVGFX9QM/1605782113.html:text/html}
}

@misc{noauthor-isabelle/hol-nodate,
	title = {Isabelle/{HOL} {Exercises}},
	url = {http://isabelle.in.tum.de/exercises/},
	urldate = {2016-09-09},
	file = {Isabelle/HOL Exercises:/home/user/Zotero/storage/WGD63925/exercises.html:text/html}
}

@misc{noauthor-course-nodate,
	title = {Course {Material} - {Isabelle} {Community} {Wiki}},
	url = {https://isabelle.in.tum.de/community/Course\_Material},
	urldate = {2016-09-09},
	file = {Course Material - Isabelle Community Wiki:/home/user/Zotero/storage/UQ2FIQIZ/Course_Material.html:text/html}
}

@article{neri-elementary-2015-3,
	title = {The {Elementary} {Operations} of {Human} {Vision} {Are} {Not} {Reducible} to {Template}-{Matching}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004499},
	doi = {10.1371/journal.pcbi.1004499},
	language = {en},
	number = {11},
	urldate = {2016-09-03},
	journal = {PLOS Computational Biology},
	author = {Neri, Peter},
	editor = {Baker, Daniel Hart},
	month = nov,
	year = {2015},
	pages = {e1004499},
	file = {journal.pcbi.1004499.pdf:/home/user/Zotero/storage/C997PUZK/journal.pcbi.1004499.pdf:application/pdf}
}

@book{huang-syntax-2009,
	address = {Cambridge; New York},
	title = {The syntax of {Chinese}},
	isbn = {978-0-511-71927-1 978-0-511-51498-2 978-0-511-51626-9 978-1-139-16693-5 978-1-282-53916-7},
	url = {http://dx.doi.org/10.1017/CBO9781139166935},
	abstract = {A guide to Chinese syntax covering a broad variety of topics including categories, argument structure, passives and anaphora.},
	language = {English},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Huang, Cheng-Teh James and Li, Yen-hui Audrey and Li, Yafei},
	year = {2009},
	note = {OCLC: 647766603},
	file = {[C.-T._James_Huang,_Y.-H._Audrey_Li,_Yafei_Li]_The(BookZZ.org).pdf:/home/user/Zotero/storage/KPTUBKZ4/[C.-T._James_Huang,_Y.-H._Audrey_Li,_Yafei_Li]_The(BookZZ.org).pdf:application/pdf}
}

@book{aoun-syntax-2009-7,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A011.pdf:/home/user/Zotero/storage/GIH3HDXW/CBO9780511691775A011.pdf:application/pdf}
}

@book{aoun-syntax-2009-8,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A019.pdf:/home/user/Zotero/storage/5AJFU5ZU/CBO9780511691775A019.pdf:application/pdf}
}

@book{aoun-syntax-2009-9,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A026.pdf:/home/user/Zotero/storage/GUBG4CAC/CBO9780511691775A026.pdf:application/pdf}
}

@book{aoun-syntax-2009-10,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A026 (1).pdf:/home/user/Zotero/storage/SN3RK9KM/CBO9780511691775A026 (1).pdf:application/pdf}
}

@book{aoun-syntax-2009-11,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A033.pdf:/home/user/Zotero/storage/IBWV5GKG/CBO9780511691775A033.pdf:application/pdf}
}

@book{aoun-syntax-2009-12,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A056.pdf:/home/user/Zotero/storage/2V3F7T7U/CBO9780511691775A056.pdf:application/pdf}
}

@book{aoun-syntax-2009-13,
	address = {Cambridge},
	title = {The {Syntax} of {Arabic}},
	isbn = {978-0-511-69177-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511691775},
	urldate = {2016-09-03},
	publisher = {Cambridge University Press},
	author = {Aoun, Joseph E. and Benmamoun, Elabbas and Choueiri, Lina},
	year = {2009},
	file = {CBO9780511691775A062.pdf:/home/user/Zotero/storage/7GWMC6AW/CBO9780511691775A062.pdf:application/pdf}
}

@book{painter-history-2010,
	address = {New York},
	title = {The history of {White} people},
	isbn = {978-0-393-04934-3},
	publisher = {W.W. Norton},
	author = {Painter, Nell Irvin},
	year = {2010},
	keywords = {History, Race identity, Race relations, United States, Whites},
	file = {[Nell_Irvin_Painter]_The_History_of_White_People(BookZZ.org).pdf:/home/user/Zotero/storage/AMBSAGKB/[Nell_Irvin_Painter]_The_History_of_White_People(BookZZ.org).pdf:application/pdf}
}

@article{cohen-products-2011-2,
	title = {Products of weighted logic programs},
	volume = {11},
	issn = {1471-0684, 1475-3081},
	url = {http://www.journals.cambridge.org/abstract\_S1471068410000529},
	doi = {10.1017/S1471068410000529},
	language = {en},
	number = {2-3},
	urldate = {2016-09-03},
	journal = {Theory and Practice of Logic Programming},
	author = {Cohen, Shay B. and Simmons, Robert J. and Smith, Noah A.},
	month = mar,
	year = {2011},
	pages = {263--296},
	file = {tplp11product.pdf:/home/user/Zotero/storage/KPI2J559/tplp11product.pdf:application/pdf}
}

@book{ariely-predictably-2008,
	address = {New York, NY},
	edition = {1st ed},
	title = {Predictably irrational: the hidden forces that shape our decisions},
	isbn = {978-0-06-135323-9},
	shorttitle = {Predictably irrational},
	abstract = {An evaluation of the sources of illogical decisions explores the reasons why irrational thought often overcomes level-headed practices, offering insight into the structural patterns that cause people to make the same mistakes repeatedly},
	publisher = {Harper},
	author = {Ariely, Dan},
	year = {2008},
	note = {OCLC: ocn182521026},
	keywords = {Reasoning, decision making, Economics, Consumer behavior, Consumers, Psychological aspects, Thought and thinking},
	file = {Ariely_-_Predictably_Irrational__The_Hidden_Forces_That_Shape_Our_Decisions.pdf:/home/user/Zotero/storage/MXGXWQ86/Ariely_-_Predictably_Irrational__The_Hidden_Forces_That_Shape_Our_Decisions.pdf:application/pdf}
}

@book{ariely-predictably-2008-1,
	address = {New York, NY},
	edition = {1st ed},
	title = {Predictably irrational: the hidden forces that shape our decisions},
	isbn = {978-0-06-135323-9},
	shorttitle = {Predictably irrational},
	abstract = {An evaluation of the sources of illogical decisions explores the reasons why irrational thought often overcomes level-headed practices, offering insight into the structural patterns that cause people to make the same mistakes repeatedly},
	publisher = {Harper},
	author = {Ariely, Dan},
	year = {2008},
	note = {OCLC: ocn182521026},
	keywords = {Reasoning, decision making, Economics, Consumer behavior, Consumers, Psychological aspects, Thought and thinking},
	file = {[Dan_Ariely]_Predictably_Irrational_The_Hidden_Fo(BookZZ.org).pdf:/home/user/Zotero/storage/QPE5P7MF/[Dan_Ariely]_Predictably_Irrational_The_Hidden_Fo(BookZZ.org).pdf:application/pdf}
}

@article{djamouri-particules-1991,
	title = {Particules de négation dans les inscriptions sur bronze de la dynastie des {Zhou}},
	volume = {20},
	issn = {0153-3320},
	url = {http://www.persee.fr/web/revues/home/prescript/article/clao\_0153-3320\_1991\_num\_20\_1\_1337},
	doi = {10.3406/clao.1991.1337},
	language = {fr},
	number = {1},
	urldate = {2016-09-03},
	journal = {Cahiers de linguistique - Asie orientale},
	author = {Djamouri, Redouane},
	year = {1991},
	pages = {5--76},
	file = {article_clao_0153-3320_1991_num_20_1_1337.pdf:/home/user/Zotero/storage/FDHWD54H/article_clao_0153-3320_1991_num_20_1_1337.pdf:application/pdf}
}

@article{peyraube-word-1997,
	title = {On word order in {Archaïc} {Chinese}},
	volume = {26},
	issn = {0153-3320},
	url = {http://www.persee.fr/web/revues/home/prescript/article/clao\_0153-3320\_1997\_num\_26\_1\_1502},
	doi = {10.3406/clao.1997.1502},
	language = {fr},
	number = {1},
	urldate = {2016-09-03},
	journal = {Cahiers de linguistique - Asie orientale},
	author = {Peyraube, Alain},
	year = {1997},
	pages = {3--20},
	file = {article_clao_0153-3320_1997_num_26_1_1502.pdf:/home/user/Zotero/storage/APZKQHIM/article_clao_0153-3320_1997_num_26_1_1502.pdf:application/pdf}
}

@article{takashima-focus-1997,
	title = {Focus and explanation in copulative-type sentences in a genuine {Classical} {Chinese} text},
	volume = {26},
	issn = {0153-3320},
	url = {http://www.persee.fr/web/revues/home/prescript/article/clao\_0153-3320\_1997\_num\_26\_2\_1513},
	doi = {10.3406/clao.1997.1513},
	language = {fr},
	number = {2},
	urldate = {2016-09-03},
	journal = {Cahiers de linguistique - Asie orientale},
	author = {Takashima, Ken-ichi},
	year = {1997},
	pages = {177--199},
	file = {article_clao_0153-3320_1997_num_26_2_1513.pdf:/home/user/Zotero/storage/HVKXVFSU/article_clao_0153-3320_1997_num_26_2_1513.pdf:application/pdf}
}

@book{yip-chinese:-2004,
	address = {London ; New York},
	series = {Routledge comprehensive grammars},
	title = {Chinese: a comprehensive grammar},
	isbn = {978-0-415-15031-6 978-0-415-15032-3},
	shorttitle = {Chinese},
	publisher = {Routledge},
	author = {Yip, Po-Ching},
	year = {2004},
	keywords = {grammar, Chinese language},
	file = {chinese Comprehensive Grammar.pdf:/home/user/Zotero/storage/QJJDS32R/chinese Comprehensive Grammar.pdf:application/pdf}
}

@article{zeschel-introduction-2008-2,
	title = {Introduction},
	volume = {19},
	issn = {0936-5907, 1613-3641},
	url = {http://www.degruyter.com/view/j/cogl.2008.19.issue-3/cogl.2008.013/cogl.2008.013.xml},
	doi = {10.1515/COGL.2008.013},
	number = {3},
	urldate = {2016-09-03},
	journal = {Cognitive Linguistics},
	author = {Zeschel, Arne},
	month = jan,
	year = {2008},
	file = {08AmbridgeGoldberg-islands.pdf:/home/user/Zotero/storage/RW9K8D2K/08AmbridgeGoldberg-islands.pdf:application/pdf}
}

@article{schuklenk-should-2009,
	title = {Should we use the criminal law to punish {HIV} transmission?},
	volume = {4},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract\_id=1331054},
	number = {3},
	urldate = {2016-09-03},
	journal = {International Journal of Law in Context},
	author = {Schuklenk, Udo},
	year = {2009},
	pages = {277--284},
	file = {11JCLBoydGoldbergConservative-1.pdf:/home/user/Zotero/storage/T3CCESCI/11JCLBoydGoldbergConservative-1.pdf:application/pdf}
}

@article{allen-distinguishing-2012-4,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X12001599},
	number = {3},
	urldate = {2016-09-03},
	journal = {Brain and language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	year = {2012},
	pages = {174--182},
	file = {12MVPA-final.pdf:/home/user/Zotero/storage/6IAJAUBU/12MVPA-final.pdf:application/pdf}
}

@article{wonnacott-input-2012-2,
	title = {Input effects on the acquisition of a novel phrasal construction in 5year olds},
	volume = {66},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X11001148},
	doi = {10.1016/j.jml.2011.11.004},
	language = {en},
	number = {3},
	urldate = {2016-09-03},
	journal = {Journal of Memory and Language},
	author = {Wonnacott, Elizabeth and Boyd, Jeremy K. and Thomson, Jennifer and Goldberg, Adele E.},
	month = apr,
	year = {2012},
	pages = {458--478},
	file = {12WonnacottBoydGoldbergJML.pdf:/home/user/Zotero/storage/RE3DD2SI/12WonnacottBoydGoldbergJML.pdf:application/pdf}
}

@article{johnson-evidence-2013-2,
	title = {Evidence for automatic accessing of constructional meaning: {Jabberwocky} sentences prime associated verbs},
	volume = {28},
	issn = {0169-0965, 1464-0732},
	shorttitle = {Evidence for automatic accessing of constructional meaning},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2012.717632},
	doi = {10.1080/01690965.2012.717632},
	language = {en},
	number = {10},
	urldate = {2016-09-03},
	journal = {Language and Cognitive Processes},
	author = {Johnson, Matt A. and Goldberg, Adele E.},
	month = dec,
	year = {2013},
	pages = {1439--1452},
	file = {13Jabberwocky-published.pdf:/home/user/Zotero/storage/9SGJ28UB/13Jabberwocky-published.pdf:application/pdf}
}

@article{goldberg-argument-2013-2,
	title = {Argument structure constructions versus lexical rules or derivational verb templates},
	volume = {28},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/mila.12026/full},
	number = {4},
	urldate = {2016-09-03},
	journal = {Mind \& Language},
	author = {Goldberg, Adele E.},
	year = {2013},
	pages = {435--465},
	file = {13Mind-Language-asc-not-rules.pdf:/home/user/Zotero/storage/7RFEU4M2/13Mind-Language-asc-not-rules.pdf:application/pdf}
}

@article{perek-generalizing-2015-2,
	title = {Generalizing beyond the input: {The} functions of the constructions matter},
	volume = {84},
	issn = {0749596X},
	shorttitle = {Generalizing beyond the input},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X15000601},
	doi = {10.1016/j.jml.2015.04.006},
	language = {en},
	urldate = {2016-09-03},
	journal = {Journal of Memory and Language},
	author = {Perek, Florent and Goldberg, Adele E.},
	month = oct,
	year = {2015},
	pages = {108--127},
	file = {15JMLPerek-Goldberg.pdf:/home/user/Zotero/storage/965JWH3F/15JMLPerek-Goldberg.pdf:application/pdf}
}

@article{johnson-neural-2016-2,
	title = {Neural systems involved in processing novel linguistic constructions and their visual referents},
	volume = {31},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2015.1055280},
	number = {1},
	urldate = {2016-09-03},
	journal = {Language, Cognition and Neuroscience},
	author = {Johnson, Matthew A. and Turk-Browne, Nicholas B. and Goldberg, Adele E.},
	year = {2016},
	pages = {129--144},
	file = {15LCN-Prediction-Cx-Neuro.pdf:/home/user/Zotero/storage/SUAATR76/15LCN-Prediction-Cx-Neuro.pdf:application/pdf}
}

@article{goldberg--adjectives-nodate,
	title = {A-adjectives, statistical preemption, and the evidence: {Reply} to {Yang}},
	shorttitle = {A-adjectives, statistical preemption, and the evidence},
	url = {http://www.jeremyboyd.org/wp-content/uploads/2015/06/Yang-response-150618.pdf},
	urldate = {2016-09-03},
	author = {Goldberg, Adele E. and Boyd, Jeremy K. and Hall, Green},
	file = {15Yang-response-Final.pdf:/home/user/Zotero/storage/UAACCA24/15Yang-response-Final.pdf:application/pdf}
}

@article{goldberg-subtle-2015-1,
	title = {Subtle implicit language facts emerge from the functions of constructions},
	volume = {6},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729932/},
	urldate = {2016-09-03},
	journal = {Frontiers in psychology},
	author = {Goldberg, Adele E.},
	year = {2015},
	file = {16Frontiers-UG-Goldberg-final.pdf:/home/user/Zotero/storage/IZCZAXIF/16Frontiers-UG-Goldberg-final.pdf:application/pdf}
}

@article{hahn-henkin-2016,
	title = {Henkin semantics for reasoning with natural language},
	volume = {3},
	url = {http://jlm.ipipan.waw.pl/index.php/JLM/article/view/113},
	number = {2},
	urldate = {2016-09-03},
	journal = {Journal of Language Modelling},
	author = {Hahn, Michael and Richter, Frank},
	year = {2016},
	pages = {513--568},
	file = {113-970-1-PB.pdf:/home/user/Zotero/storage/IA346H7R/113-970-1-PB.pdf:application/pdf}
}

@article{hahn-henkin-2016-1,
	title = {Henkin semantics for reasoning with natural language},
	volume = {3},
	url = {http://jlm.ipipan.waw.pl/index.php/JLM/article/view/113},
	number = {2},
	urldate = {2016-09-03},
	journal = {Journal of Language Modelling},
	author = {Hahn, Michael and Richter, Frank},
	year = {2016},
	pages = {513--568},
	file = {113-979-1-PB.pdf:/home/user/Zotero/storage/ZF4BWDGU/113-979-1-PB.pdf:application/pdf}
}

@article{xu-robustness-2012,
	title = {Robustness and generalization},
	volume = {86},
	url = {http://link.springer.com/article/10.1007/s10994-011-5268-1},
	number = {3},
	urldate = {2016-09-03},
	journal = {Machine learning},
	author = {Xu, Huan and Mannor, Shie},
	year = {2012},
	pages = {391--423},
	file = {1005.2243v1.pdf:/home/user/Zotero/storage/W47UXPFB/1005.2243v1.pdf:application/pdf}
}

@article{anandkumar-tensor-2014-1,
	title = {Tensor decompositions for learning latent variable models.},
	volume = {15},
	url = {http://www.jmlr.org/papers/volume15/anandkumar14b/anandkumar14b.pdf},
	number = {1},
	urldate = {2016-09-03},
	journal = {Journal of Machine Learning Research},
	author = {Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	year = {2014},
	pages = {2773--2832},
	file = {1210.7559v4.pdf:/home/user/Zotero/storage/WG9DFMG8/1210.7559v4.pdf:application/pdf}
}

@article{ioffe-batch-2015-2,
	title = {Batch normalization: {Accelerating} deep network training by reducing internal covariate shift},
	shorttitle = {Batch normalization},
	url = {http://arxiv.org/abs/1502.03167},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1502.03167},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	file = {1502.03167v3.pdf:/home/user/Zotero/storage/ATAHRXM5/1502.03167v3.pdf:application/pdf}
}

@inproceedings{steinhardt-learning-2015-3,
	title = {Learning fast-mixing models for structured prediction},
	url = {http://www.jmlr.org/proceedings/papers/v37/steinhardtb15-supp.pdf},
	urldate = {2016-09-03},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Steinhardt, Jacob and Liang, Percy},
	year = {2015},
	file = {1502.06668.pdf:/home/user/Zotero/storage/XJRWGP5A/1502.06668.pdf:application/pdf}
}

@article{mei-listen-2015,
	title = {Listen, attend, and walk: {Neural} mapping of navigational instructions to action sequences},
	shorttitle = {Listen, attend, and walk},
	url = {http://arxiv.org/abs/1506.04089},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1506.04089},
	author = {Mei, Hongyuan and Bansal, Mohit and Walter, Matthew R.},
	year = {2015},
	file = {1506.04089v4.pdf:/home/user/Zotero/storage/N38C9CQ5/1506.04089v4.pdf:application/pdf}
}

@article{burda-importance-2015-2,
	title = {Importance weighted autoencoders},
	url = {http://arxiv.org/abs/1509.00519},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1509.00519},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1509.00519v2.pdf:/home/user/Zotero/storage/2566DTGF/1509.00519v2.pdf:application/pdf}
}

@article{wilson-deep-2015-2,
	title = {Deep kernel learning},
	url = {http://www.jmlr.org/proceedings/papers/v51/wilson16.pdf},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1511.02222},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2015},
	file = {1511.02222v1.pdf:/home/user/Zotero/storage/JUUZSZI6/1511.02222v1.pdf:application/pdf}
}

@article{mansimov-generating-2015-3,
	title = {Generating images from captions with attention},
	url = {http://arxiv.org/abs/1511.02793},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1511.02793},
	author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.02793v1.pdf:/home/user/Zotero/storage/2MQXIFKS/1511.02793v1.pdf:application/pdf}
}

@article{sharma-action-2015-2,
	title = {Action recognition using visual attention},
	url = {http://arxiv.org/abs/1511.04119},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1511.04119},
	author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
	year = {2015},
	file = {1511.04119v1.pdf:/home/user/Zotero/storage/XEDXJSA6/1511.04119v1.pdf:application/pdf}
}

@book{cohen-proceedings-2006-2,
	address = {New York, NY},
	title = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}: [{June} 25 - 29, 2006, {Pittsburgh}, {PA}, {USA}]},
	isbn = {978-1-59593-383-6},
	shorttitle = {Proceedings / {Twenty}-{Third} {International} {Conference} on {Machine} {Learning}},
	language = {eng},
	publisher = {ACM},
	editor = {Cohen, William W. and Moore, Andrew and {International Conference on Machine Learning}},
	year = {2006},
	note = {OCLC: 255363072},
	keywords = {Kongress, Maschinelles Lernen},
	file = {1511.08400v3.pdf:/home/user/Zotero/storage/XTURW8IA/1511.08400v3.pdf:application/pdf}
}

@incollection{bonchi-brzozowskis-2012,
	title = {Brzozowski’s algorithm (co) algebraically},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-29485-3\_2},
	urldate = {2016-09-03},
	booktitle = {Logic and {Program} {Semantics}},
	publisher = {Springer},
	author = {Bonchi, Filippo and Bonsangue, Marcello M. and Rutten, Jan JMM and Silva, Alexandra},
	year = {2012},
	pages = {12--23},
	file = {2011_SEN1114.pdf:/home/user/Zotero/storage/68BXZBS3/2011_SEN1114.pdf:application/pdf}
}

@article{korattikara-bayesian-2015,
	title = {Bayesian dark knowledge},
	url = {http://arxiv.org/abs/1506.04416},
	urldate = {2016-09-03},
	journal = {arXiv preprint arXiv:1506.04416},
	author = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},
	year = {2015},
	file = {5965-bayesian-dark-knowledge.pdf:/home/user/Zotero/storage/F8XB2S6U/5965-bayesian-dark-knowledge.pdf:application/pdf}
}

@article{gemmer-quantum-2001,
	title = {Quantum approach to a derivation of the second law of thermodynamics},
	volume = {86},
	url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.86.1927},
	number = {10},
	urldate = {2016-09-03},
	journal = {Physical review letters},
	author = {Gemmer, Jochen and Otte, Alexander and Mahler, Günter},
	year = {2001},
	pages = {1927},
	file = {0101140v1.pdf:/home/user/Zotero/storage/NEAH6WIW/0101140v1.pdf:application/pdf}
}

@article{goldberg-are-2011,
	title = {Are a-adjectives like afraid prepositional phrases underlying and does it matter from a learnability perspective},
	url = {http://www.princeton.edu/~adele/Princeton\_Construction\_Site/pcs.sites2/a-adjsnotpps.pdf},
	urldate = {2016-09-03},
	journal = {Princeton, NJ: Princeton University, MS. Online: https://adele. princeton. edu/pubsbytopic/\# Statistical},
	author = {Goldberg, Adele E.},
	year = {2011},
	file = {a-adjsnotpps.pdf:/home/user/Zotero/storage/NFJZZKDC/a-adjsnotpps.pdf:application/pdf}
}

@article{yang-negative-2015,
	title = {Negative knowledge from positive evidence},
	volume = {91},
	url = {https://muse.jhu.edu/article/604080/summary},
	number = {4},
	urldate = {2016-09-03},
	journal = {Language},
	author = {Yang, Charles},
	year = {2015},
	pages = {938--953},
	file = {aa-final-2.pdf:/home/user/Zotero/storage/KA7QNKVS/aa-final-2.pdf:application/pdf}
}

@inproceedings{cohen-variational-2009-1,
	title = {Variational inference for grammar induction with prior knowledge},
	url = {http://dl.acm.org/citation.cfm?id=1667585},
	urldate = {2016-09-03},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 {Conference} {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Cohen, Shay B. and Smith, Noah A.},
	year = {2009},
	pages = {1--4},
	file = {aclshort09constraints.pdf:/home/user/Zotero/storage/95I42CEP/aclshort09constraints.pdf:application/pdf}
}

@article{berant-imitation-2015-3,
	title = {Imitation learning of agenda-based semantic parsers},
	volume = {3},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/646},
	urldate = {2016-09-03},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2015},
	pages = {545--558},
	file = {agenda-tacl2015.pdf:/home/user/Zotero/storage/4PHWMI2D/agenda-tacl2015.pdf:application/pdf}
}

@article{bennett-notes-2003-2,
	title = {Notes on {Landauer}'s principle, reversible computation, and {Maxwell}'s {Demon}},
	volume = {34},
	issn = {13552198},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S135521980300039X},
	doi = {10.1016/S1355-2198(03)00039-X},
	language = {en},
	number = {3},
	urldate = {2016-09-03},
	journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
	author = {Bennett, Charles H.},
	month = sep,
	year = {2003},
	pages = {501--510},
	file = {bennett03.pdf:/home/user/Zotero/storage/R4JR52DU/bennett03.pdf:application/pdf}
}

@article{blevins-entropy-based-2008-1,
	title = {An entropy-based measure of morphological information},
	url = {http://www.academia.edu/download/7029073/qitl3\_pre-proceedings.pdf#page=19},
	urldate = {2016-09-03},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Blevins, James P. and Ackerman, Farrell and Malouf, Robert},
	year = {2008},
	pages = {17},
	file = {Blevins_Ackerman.pdf:/home/user/Zotero/storage/GCSQGHSC/Blevins_Ackerman.pdf:application/pdf}
}

@incollection{nareyek-choosing-2003-2,
	title = {Choosing search heuristics by non-stationary reinforcement learning},
	url = {http://link.springer.com/chapter/10.1007/978-1-4757-4137-7\_25},
	urldate = {2016-09-03},
	booktitle = {Metaheuristics: {Computer} decision-making},
	publisher = {Springer},
	author = {Nareyek, Alexander},
	year = {2003},
	pages = {523--544},
	file = {book2015oct.pdf:/home/user/Zotero/storage/H5M656Z6/book2015oct.pdf:application/pdf}
}

@article{boyd-learning-2011-2,
	title = {Learning what not to say: {The} role of statistical preemption and categorization in a-adjective production},
	volume = {87},
	shorttitle = {Learning what not to say},
	url = {https://muse.jhu.edu/article/422023/summary},
	number = {1},
	urldate = {2016-09-03},
	journal = {Language},
	author = {Boyd, Jeremy K. and Goldberg, Adele E.},
	year = {2011},
	pages = {55--83},
	file = {BoydGoldberg2011-official.pdf:/home/user/Zotero/storage/B72PM6RS/BoydGoldberg2011-official.pdf:application/pdf}
}

@article{boyd-learning-2011-3,
	title = {Learning what not to say: {The} role of statistical preemption and categorization in a-adjective production},
	volume = {87},
	shorttitle = {Learning what not to say},
	url = {https://muse.jhu.edu/article/422023/summary},
	number = {1},
	urldate = {2016-09-03},
	journal = {Language},
	author = {Boyd, Jeremy K. and Goldberg, Adele E.},
	year = {2011},
	pages = {55--83},
	file = {BoydGoldberg2011-official1.pdf:/home/user/Zotero/storage/2THXJXCS/BoydGoldberg2011-official1.pdf:application/pdf}
}

@article{allen-distinguishing-2012-5,
	title = {Distinguishing grammatical constructions with {fMRI} pattern analysis},
	volume = {123},
	issn = {0093934X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0093934X12001599},
	doi = {10.1016/j.bandl.2012.08.005},
	language = {en},
	number = {3},
	urldate = {2016-09-03},
	journal = {Brain and Language},
	author = {Allen, Kachina and Pereira, Francisco and Botvinick, Matthew and Goldberg, Adele E.},
	month = dec,
	year = {2012},
	pages = {174--182},
	file = {BrainLanguagepaper.pdf:/home/user/Zotero/storage/WIDAF5WP/BrainLanguagepaper.pdf:application/pdf}
}

@inproceedings{vogel-emergence-2013,
	title = {Emergence of {Gricean} {Maxims} from {Multi}-{Agent} {Decision} {Theory}.},
	url = {http://www.aclweb.org/anthology/N13-1127},
	urldate = {2016-09-03},
	booktitle = {{HLT}-{NAACL}},
	author = {Vogel, Adam and Bodoia, Max and Potts, Christopher and Jurafsky, Daniel},
	year = {2013},
	pages = {1072--1081},
	file = {cards-naacl2013.pdf:/home/user/Zotero/storage/IWKRHA9W/cards-naacl2013.pdf:application/pdf}
}

@article{sellgren-exploring-2008,
	title = {Exploring competing patterns of verb complementation: {Prevent} in the {British} {National} {Corpus}},
	shorttitle = {Exploring competing patterns of verb complementation},
	url = {http://www.ling.helsinki.fi/sky/tapahtumat/qitl/Abstracts/QITL3\_Pre-Proceedings.pdf#page=109},
	urldate = {2016-09-03},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Sellgren, Elina},
	year = {2008},
	pages = {107},
	file = {Sellgren.pdf:/home/user/Zotero/storage/UWVHEGIQ/Sellgren.pdf:application/pdf}
}

@inproceedings{gens-learning-2013-1,
	title = {Learning the {Structure} of {Sum}-{Product} {Networks}.},
	url = {http://www.jmlr.org/proceedings/papers/v28/gens13.pdf},
	urldate = {2016-09-03},
	booktitle = {{ICML} (3)},
	author = {Gens, Robert and Domingos, Pedro M.},
	year = {2013},
	pages = {873--880},
	file = {slspn.pdf:/home/user/Zotero/storage/DU5GN89T/slspn.pdf:application/pdf}
}

@article{speelman-putting-2008-1,
	title = {Putting the (in) direct causation hypothesis to the test: a quantitative study of {Dutch} doen ‘make’and laten ‘let’},
	shorttitle = {Putting the (in) direct causation hypothesis to the test},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.2137&rep=rep1&type=pdf#page=64},
	urldate = {2016-09-03},
	journal = {Quantitative Investigations In Theoretical Linguistics (QITL3)},
	author = {Speelman, Dirk and Geeraerts, Dirk},
	year = {2008},
	pages = {62},
	file = {Speelman_Geeraerts.pdf:/home/user/Zotero/storage/9WWPJMHS/Speelman_Geeraerts.pdf:application/pdf}
}

@article{srivastava-dropout:-2014-1,
	title = {Dropout: a simple way to prevent neural networks from overfitting.},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf},
	number = {1},
	urldate = {2016-09-03},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {srivastava14a.pdf:/home/user/Zotero/storage/EGV8ACBM/srivastava14a.pdf:application/pdf}
}

@inproceedings{stanley-evolving-2004,
	title = {Evolving a roving eye for go},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-24855-2\_130},
	urldate = {2016-09-03},
	booktitle = {Genetic and {Evolutionary} {Computation} {Conference}},
	publisher = {Springer},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	year = {2004},
	pages = {1226--1238},
	file = {stanley.gecco04.pdf:/home/user/Zotero/storage/ZMXT838D/stanley.gecco04.pdf:application/pdf}
}

@misc{noauthor-[1607.07698]-nodate,
	title = {[1607.07698] {Domains} and {Random} {Variables}},
	url = {https://arxiv.org/abs/1607.07698},
	urldate = {2016-09-03},
	file = {[1607.07698] Domains and Random Variables:/home/user/Zotero/storage/DUEIU3GH/1607.html:text/html}
}

@misc{noauthor-[1601.06579]-nodate,
	title = {[1601.06579] {A} {Kernel} {Independence} {Test} for {Geographical} {Language} {Variation}},
	url = {https://arxiv.org/abs/1601.06579},
	urldate = {2016-09-03},
	file = {[1601.06579] A Kernel Independence Test for Geographical Language Variation:/home/user/Zotero/storage/NUUW8RMS/1601.html:text/html}
}

@misc{noauthor-[1608.07881]-nodate,
	title = {[1608.07881] {Debugging} of {Markov} {Decision} {Processes} ({MDPs}) {Models}},
	url = {https://arxiv.org/abs/1608.07881},
	urldate = {2016-09-03},
	file = {[1608.07881] Debugging of Markov Decision Processes (MDPs) Models:/home/user/Zotero/storage/WMVGNGET/1608.html:text/html}
}

@misc{noauthor-[1608.08188]-nodate,
	title = {[1608.08188] {Visual} {Question}: {Predicting} {If} a {Crowd} {Will} {Agree} on the {Answer}},
	url = {https://arxiv.org/abs/1608.08188},
	urldate = {2016-09-03},
	file = {[1608.08188] Visual Question\: Predicting If a Crowd Will Agree on the Answer:/home/user/Zotero/storage/UIP2KZEV/1608.html:text/html}
}

@misc{noauthor-[1608.07639]-nodate,
	title = {[1608.07639] {Learning} to generalize to new compositions in image understanding},
	url = {https://arxiv.org/abs/1608.07639},
	urldate = {2016-09-03},
	file = {[1608.07639] Learning to generalize to new compositions in image understanding:/home/user/Zotero/storage/I6DRSZQZ/1608.html:text/html}
}

@misc{noauthor-[1608.07187]-nodate,
	title = {[1608.07187] {Semantics} derived automatically from language corpora necessarily contain human biases},
	url = {https://arxiv.org/abs/1608.07187},
	urldate = {2016-09-03},
	file = {[1608.07187] Semantics derived automatically from language corpora necessarily contain human biases:/home/user/Zotero/storage/7QAQMK2G/1608.html:text/html}
}

@misc{noauthor-[1607.08723]-nodate,
	title = {[1607.08723] {Cognitive} {Science} in the era of {Artificial} {Intelligence}: {A} roadmap for reverse-engineering the infant language-learner},
	url = {https://arxiv.org/abs/1607.08723},
	urldate = {2016-09-03},
	file = {[1607.08723] Cognitive Science in the era of Artificial Intelligence\: A roadmap for reverse-engineering the infant language-learner:/home/user/Zotero/storage/XWGTJ2CB/1607.html:text/html}
}

@misc{noauthor-[1608.08974]-nodate,
	title = {[1608.08974] {Interpreting} {Visual} {Question} {Answering} {Models}},
	url = {https://arxiv.org/abs/1608.08974},
	urldate = {2016-09-03},
	file = {[1608.08974] Interpreting Visual Question Answering Models:/home/user/Zotero/storage/VGAWVF8P/1608.html:text/html}
}

@misc{noauthor-[1608.08716]-nodate,
	title = {[1608.08716] {Measuring} {Machine} {Intelligence} {Through} {Visual} {Question} {Answering}},
	url = {https://arxiv.org/abs/1608.08716},
	urldate = {2016-09-03},
	file = {[1608.08716] Measuring Machine Intelligence Through Visual Question Answering:/home/user/Zotero/storage/BI5FKDC9/1608.html:text/html}
}

@misc{noauthor-[1607.07698]-nodate-1,
	title = {[1607.07698] {Domains} and {Random} {Variables}},
	url = {https://arxiv.org/abs/1607.07698},
	urldate = {2016-09-03},
	file = {[1607.07698] Domains and Random Variables:/home/user/Zotero/storage/ZTRC4MW8/1607.html:text/html}
}

@misc{noauthor-[1601.06579]-nodate-1,
	title = {[1601.06579] {A} {Kernel} {Independence} {Test} for {Geographical} {Language} {Variation}},
	url = {https://arxiv.org/abs/1601.06579},
	urldate = {2016-09-03},
	file = {[1601.06579] A Kernel Independence Test for Geographical Language Variation:/home/user/Zotero/storage/UEPMQX36/1601.html:text/html}
}

@misc{noauthor-[1608.07881]-nodate-1,
	title = {[1608.07881] {Debugging} of {Markov} {Decision} {Processes} ({MDPs}) {Models}},
	url = {https://arxiv.org/abs/1608.07881},
	urldate = {2016-09-03},
	file = {[1608.07881] Debugging of Markov Decision Processes (MDPs) Models:/home/user/Zotero/storage/FK6FUWSP/1608.html:text/html}
}

@misc{noauthor-[1608.08188]-nodate-1,
	title = {[1608.08188] {Visual} {Question}: {Predicting} {If} a {Crowd} {Will} {Agree} on the {Answer}},
	url = {https://arxiv.org/abs/1608.08188},
	urldate = {2016-09-03},
	file = {[1608.08188] Visual Question\: Predicting If a Crowd Will Agree on the Answer:/home/user/Zotero/storage/HBJG39B7/1608.html:text/html}
}

@misc{noauthor-[1608.07639]-nodate-1,
	title = {[1608.07639] {Learning} to generalize to new compositions in image understanding},
	url = {https://arxiv.org/abs/1608.07639},
	urldate = {2016-09-03},
	file = {[1608.07639] Learning to generalize to new compositions in image understanding:/home/user/Zotero/storage/RVZD6U5V/1608.html:text/html}
}

@misc{noauthor-[1608.07187]-nodate-1,
	title = {[1608.07187] {Semantics} derived automatically from language corpora necessarily contain human biases},
	url = {https://arxiv.org/abs/1608.07187},
	urldate = {2016-09-03},
	file = {[1608.07187] Semantics derived automatically from language corpora necessarily contain human biases:/home/user/Zotero/storage/C5J2C77S/1608.html:text/html}
}

@misc{noauthor-[1607.08723]-nodate-1,
	title = {[1607.08723] {Cognitive} {Science} in the era of {Artificial} {Intelligence}: {A} roadmap for reverse-engineering the infant language-learner},
	url = {https://arxiv.org/abs/1607.08723},
	urldate = {2016-09-03},
	file = {[1607.08723] Cognitive Science in the era of Artificial Intelligence\: A roadmap for reverse-engineering the infant language-learner:/home/user/Zotero/storage/JFMZ28FF/1607.html:text/html}
}

@misc{noauthor-[1608.08974]-nodate-1,
	title = {[1608.08974] {Interpreting} {Visual} {Question} {Answering} {Models}},
	url = {https://arxiv.org/abs/1608.08974},
	urldate = {2016-09-03},
	file = {[1608.08974] Interpreting Visual Question Answering Models:/home/user/Zotero/storage/V53QV99X/1608.html:text/html}
}

@misc{noauthor-[1608.08716]-nodate-1,
	title = {[1608.08716] {Measuring} {Machine} {Intelligence} {Through} {Visual} {Question} {Answering}},
	url = {https://arxiv.org/abs/1608.08716},
	urldate = {2016-09-03},
	file = {[1608.08716] Measuring Machine Intelligence Through Visual Question Answering:/home/user/Zotero/storage/NDWURKJG/1608.html:text/html}
}

@misc{noauthor-[1612.09213]-nodate,
	title = {[1612.09213] {Verifying} {Heaps}' law using {Google} {Books} {Ngram} data},
	url = {https://arxiv.org/abs/1612.09213},
	urldate = {2017-01-10},
	file = {[1612.09213] Verifying Heaps' law using Google Books Ngram data:/home/user/Zotero/storage/V9ITTBRR/1612.html:text/html}
}

@article{godwin-deep-2016,
	title = {Deep {Semi}-{Supervised} {Learning} with {Linguistically} {Motivated} {Sequence} {Labeling} {Task} {Hierarchies}},
	url = {http://arxiv.org/abs/1612.09113},
	abstract = {In this paper we present a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by junior downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F1 for Chunking compared to a plausible baseline.},
	urldate = {2017-01-10},
	journal = {arXiv:1612.09113 [cs]},
	author = {Godwin, Jonathan and Stenetorp, Pontus and Riedel, Sebastian},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.09113},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/52HI93RF/1612.html:text/html;Godwin et al_2016_Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling.pdf:/home/user/Zotero/storage/J45PJ9EH/Godwin et al_2016_Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling.pdf:application/pdf}
}

@misc{noauthor-cs-nodate,
	title = {cs daily {Subj}-class mailing 8008 1 - m.hahn29@gmail.com - {Gmail}},
	url = {https://mail.google.com/mail/u/0/#inbox/1594e8446f1692e7},
	urldate = {2017-01-10},
	file = {cs daily Subj-class mailing 8008 1 - m.hahn29@gmail.com - Gmail:/home/user/Zotero/storage/9IIFFRVD/0.html:text/html}
}

@misc{noauthor-[1612.08989]-nodate,
	title = {[1612.08989] {Shamela}: {A} {Large}-{Scale} {Historical} {Arabic} {Corpus}},
	url = {https://arxiv.org/abs/1612.08989},
	urldate = {2017-01-10},
	file = {[1612.08989] Shamela\: A Large-Scale Historical Arabic Corpus:/home/user/Zotero/storage/QXHNM5KI/1612.html:text/html}
}

@article{inan-tying-2016,
	title = {Tying {Word} {Vectors} and {Word} {Classifiers}: {A} {Loss} {Framework} for {Language} {Modeling}},
	shorttitle = {Tying {Word} {Vectors} and {Word} {Classifiers}},
	url = {https://arxiv.org/abs/1611.01462},
	urldate = {2017-01-10},
	journal = {arXiv preprint arXiv:1611.01462},
	author = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
	year = {2016},
	file = {() - 1611.01462v1.pdf:/home/user/Zotero/storage/P29VT94V/1611.01462v1.pdf:application/pdf}
}

@article{agarwal-finding-2016,
	title = {Finding {Approximate} {Local} {Minima} for {Nonconvex} {Optimization} in {Linear} {Time}},
	url = {https://arxiv.org/abs/1611.01146},
	urldate = {2017-01-10},
	journal = {arXiv preprint arXiv:1611.01146},
	author = {Agarwal, Naman and Allen-Zhu, Zeyuan and Bullins, Brian and Hazan, Elad and Ma, Tengyu},
	year = {2016},
	file = {1611.01146v2.pdf:/home/user/Zotero/storage/CNUIVSWD/1611.01146v2.pdf:application/pdf}
}

@inproceedings{schulman-gradient-2015,
	title = {Gradient estimation using stochastic computation graphs},
	url = {http://papers.nips.cc/paper/5899-gradient-estimation-using-stochastic-computation-graphs},
	urldate = {2017-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
	year = {2015},
	pages = {3528--3536},
	file = {1506.05254.pdf:/home/user/Zotero/storage/IIU2JU86/1506.05254.pdf:application/pdf}
}

@article{greensmith-variance-2004-1,
	title = {Variance reduction techniques for gradient estimates in reinforcement learning},
	volume = {5},
	url = {http://www.jmlr.org/papers/v5/greensmith04a.html},
	number = {Nov},
	urldate = {2017-01-10},
	journal = {Journal of Machine Learning Research},
	author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
	year = {2004},
	pages = {1471--1530},
	file = {greensmith04a.dvi - greensmith04a.pdf:/home/user/Zotero/storage/E9853WVG/greensmith04a.pdf:application/pdf}
}

@article{greensmith-variance-2004-2,
	title = {Variance reduction techniques for gradient estimates in reinforcement learning},
	volume = {5},
	url = {http://www.jmlr.org/papers/v5/greensmith04a.html},
	number = {Nov},
	urldate = {2017-01-10},
	journal = {Journal of Machine Learning Research},
	author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
	year = {2004},
	pages = {1471--1530},
	file = {greensmith04a.dvi - greensmith04a.pdf:/home/user/Zotero/storage/TASZ92MA/greensmith04a.pdf:application/pdf}
}

@article{liang-neural-2016,
	title = {Neural symbolic machines: {Learning} semantic parsers on freebase with weak supervision},
	shorttitle = {Neural symbolic machines},
	url = {https://arxiv.org/abs/1611.00020},
	urldate = {2017-01-10},
	journal = {arXiv preprint arXiv:1611.00020},
	author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
	year = {2016},
	file = {1611.00020v3.pdf:/home/user/Zotero/storage/A7Q5E5RQ/1611.00020v3.pdf:application/pdf}
}

@article{bresnan-linguistics:-2016,
	title = {Linguistics: {The} {Garden} and the {Bush}},
	shorttitle = {Linguistics},
	url = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI\_a\_00260},
	urldate = {2017-01-10},
	journal = {Computational Linguistics},
	author = {Bresnan, Joan},
	year = {2016},
	file = {jbLTA.pdf:/home/user/Zotero/storage/U6QZXJND/jbLTA.pdf:application/pdf}
}

@article{wu-googles-2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
	url = {https://arxiv.org/abs/1609.08144},
	urldate = {2017-01-10},
	journal = {arXiv preprint arXiv:1609.08144},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and {others}},
	year = {2016},
	file = {1609.08144v2.pdf:/home/user/Zotero/storage/4EWZJE89/1609.08144v2.pdf:application/pdf}
}

@inproceedings{lau-unsupervised-2015,
	title = {Unsupervised prediction of acceptability judgements},
	url = {http://www.aclweb.org/website/old\_anthology/P/P15/P15-1156.pdf},
	urldate = {2017-01-10},
	booktitle = {Proceedings of the 53rd {Annual} {Conference} of the {Association} for {Computational} {Linguistics}, {Beijing}, {China}},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	year = {2015},
	file = {Unsupervised Prediction of Acceptability Judgements - P15-1156.pdf:/home/user/Zotero/storage/9HJMW8I6/P15-1156.pdf:application/pdf}
}

@inproceedings{teh-hierarchical-2006,
	title = {A hierarchical {Bayesian} language model based on {Pitman}-{Yor} processes},
	url = {http://dl.acm.org/citation.cfm?id=1220299},
	urldate = {2017-01-10},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computational} {Linguistics} and the 44th annual meeting of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Teh, Yee Whye},
	year = {2006},
	pages = {985--992},
	file = {report.dvi - acl2006.pdf:/home/user/Zotero/storage/XDFI55XP/acl2006.pdf:application/pdf}
}

@article{griffiths-indian-2011,
	title = {The indian buffet process: {An} introduction and review},
	volume = {12},
	shorttitle = {The indian buffet process},
	url = {http://www.jmlr.org/papers/v12/griffiths11a.html},
	number = {Apr},
	urldate = {2017-01-10},
	journal = {Journal of Machine Learning Research},
	author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
	year = {2011},
	pages = {1185--1224},
	file = {griffiths11a.dvi - indianbuffet.pdf:/home/user/Zotero/storage/29XW4G5P/indianbuffet.pdf:application/pdf}
}

@inproceedings{fedzechkina-communicative-2013-1,
	title = {Communicative biases shape structures of newly acquired languages},
	url = {https://pdfs.semanticscholar.org/967a/7356ceb448e9993fb44bae9b5284d8106f7b.pdf},
	urldate = {2017-01-10},
	booktitle = {Proceedings of the 35th {Annual} {Meeting} of the {Cognitive} {Science} {Society} ({CogSci}13)},
	author = {Fedzechkina, Maryia and Jaeger, T. Florian and Newport, Elissa L.},
	year = {2013},
	pages = {430--435},
	file = {FedzechkinaJaegerNewport13.pdf:/home/user/Zotero/storage/AC98GI7I/FedzechkinaJaegerNewport13.pdf:application/pdf}
}

@article{fedzechkina-language-2012-1,
	title = {Language learners restructure their input to facilitate efficient communication},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1215776109},
	doi = {10.1073/pnas.1215776109},
	language = {en},
	number = {44},
	urldate = {2017-01-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fedzechkina, M. and Jaeger, T. F. and Newport, E. L.},
	month = oct,
	year = {2012},
	pages = {17897--17902},
	file = {pnas201215776 17897..17902 - 17897.full.pdf:/home/user/Zotero/storage/DSC7J3UR/17897.full.pdf:application/pdf}
}

@inproceedings{fedzechkina-functional-2011-2,
	title = {Functional biases in language learning: {Evidence} from word order and case-marking interaction},
	shorttitle = {Functional biases in language learning},
	url = {http://mfedzech.github.io/docs/FedzechkinaJaegerNewport\_cogsci11.pdf},
	urldate = {2017-01-10},
	booktitle = {Proc. of {CogSci}},
	author = {Fedzechkina, Maryia and Jaeger, T. and Newport, Elissa},
	year = {2011},
	pages = {318--323},
	file = {FedzechkinaJaegerNewport11.pdf:/home/user/Zotero/storage/KW9PRA9N/FedzechkinaJaegerNewport11.pdf:application/pdf}
}

@article{goldberg-subtle-2015-2,
	title = {Subtle implicit language facts emerge from the functions of constructions},
	volume = {6},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729932/},
	urldate = {2017-01-10},
	journal = {Frontiers in psychology},
	author = {Goldberg, Adele E.},
	year = {2015},
	file = {16Frontiers-UG-Goldberg-final.pdf:/home/user/Zotero/storage/VFKRE3T9/16Frontiers-UG-Goldberg-final.pdf:application/pdf}
}

@article{culbertson-word-nodate,
	title = {{WORD} {ORDER} {UNIVERSALS} {REFLECT} {COGNITIVE} {BIASES}: {EVIDENCE} {FROM} {SILENT} {GESTURE}},
	shorttitle = {{WORD} {ORDER} {UNIVERSALS} {REFLECT} {COGNITIVE} {BIASES}},
	url = {http://evolang.org/neworleans/pdf/EVOLANG\_11\_paper\_39.pdf},
	urldate = {2017-01-10},
	author = {CULBERTSON, JENNIFER and SCHOUWSTRA, MARIEKE and KIRBY, SIMON},
	file = {evolang11CulbertsonSchouwstraKirby.pdf:/home/user/Zotero/storage/XVR8KHNV/evolang11CulbertsonSchouwstraKirby.pdf:application/pdf}
}

@article{dunn-time-2013,
	title = {Time and place in the prehistory of the {Aslian} languages},
	volume = {85},
	url = {http://www.bioone.org/doi/abs/10.3378/027.085.0318},
	number = {3},
	urldate = {2017-01-10},
	journal = {Human biology},
	author = {Dunn, Michael and Kruspe, Nicole and Burenhult, Niclas},
	year = {2013},
	pages = {383--400},
	file = {Time and Place in the Prehistory of the Aslian Languages - viewcontent.cgi:/home/user/Zotero/storage/MJQ2N9WK/viewcontent.pdf:application/pdf}
}

@article{christiansen-now-or-never-2015,
	title = {The {Now}-or-{Never} {Bottleneck}: {A} {Fundamental} {Constraint} on {Language}},
	issn = {0140-525X, 1469-1825},
	shorttitle = {The {Now}-or-{Never} {Bottleneck}},
	url = {http://www.journals.cambridge.org/abstract\_S0140525X1500031X},
	doi = {10.1017/S0140525X1500031X},
	language = {en},
	urldate = {2017-01-10},
	journal = {Behavioral and Brain Sciences},
	author = {Christiansen, Morten H. and Chater, Nick},
	month = apr,
	year = {2015},
	pages = {1--52},
	file = {BBS1500031 1..72 - div-class-title-process-and-perish-or-multiple-buffers-with-push-down-stacks-div.pdf:/home/user/Zotero/storage/P32KHQ8F/div-class-title-process-and-perish-or-multiple-buffers-with-push-down-stacks-div.pdf:application/pdf}
}

@article{burnett-soft-nodate,
	title = {Soft {Syntax} and the {Evolution} of {Negative} and {Polarity} {Indefinites} in the {History} of {English}},
	url = {https://sites.google.com/site/heathersusanburnett/Burnett\_Tagliamonte\_NegQs\_250616.pdf},
	urldate = {2017-01-10},
	author = {Burnett, Heather and Tagliamonte, Sali A.},
	file = {Burnett_Tagliamonte_NegQs_250616.pdf:/home/user/Zotero/storage/QQNJKS7X/Burnett_Tagliamonte_NegQs_250616.pdf:application/pdf}
}

@article{burnett-variation-2017,
	title = {Variation as a {Testing} {Ground} for {Grammatical} {Theory}: {Variable} {Negative} {Concord} in {Montréal} {French}},
	shorttitle = {Variation as a {Testing} {Ground} for {Grammatical} {Theory}},
	url = {https://sites.google.com/site/heathersusanburnett/LIV\_Montreal\_French.pdf},
	urldate = {2017-01-10},
	author = {Burnett, Heather},
	year = {2017},
	file = {LIV_Montreal_French.pdf:/home/user/Zotero/storage/746MXQ5E/LIV_Montreal_French.pdf:application/pdf}
}

@book{swart-expression-2010,
	address = {Dordrecht},
	series = {Studies in {Natural} {Language} and {Linguistic} {Theory}},
	title = {Expression and {Interpretation} of {Negation}},
	volume = {77},
	isbn = {978-90-481-3161-7 978-90-481-3162-4},
	url = {http://link.springer.com/10.1007/978-90-481-3162-4},
	urldate = {2017-01-10},
	publisher = {Springer Netherlands},
	author = {Swart, Henriëtte},
	year = {2010},
	file = {bok%3A978-90-481-3162-4.pdf:/home/user/Zotero/storage/Q7JPMVQG/bok%3A978-90-481-3162-4.pdf:application/pdf}
}

@inproceedings{burnett-signalling-2016-1,
	title = {Signalling {Games}, {Sociolinguistic} {Variation} and the {Construction} of {Style}},
	url = {http://www.linguist.univ-paris-diderot.fr/~bcrabbe/S2016/burnett.pdf},
	urldate = {2017-01-10},
	booktitle = {the 40th {Penn} {Linguistics} {Colloquium}, {University} of {Pennsylvania}},
	author = {Burnett, Heather},
	year = {2016},
	file = {SMGs_v1.pdf:/home/user/Zotero/storage/ZBX6H5JF/SMGs_v1.pdf:application/pdf}
}

@article{croft-evolutionary-2008,
	title = {Evolutionary {Linguistics}},
	volume = {37},
	url = {http://dx.doi.org/10.1146/annurev.anthro.37.081407.085156},
	doi = {10.1146/annurev.anthro.37.081407.085156},
	abstract = {Both qualitative concepts and quantitative methods from evolutionary biology have been applied to linguistics. Many linguists have noted the similarity between biological evolution and language change, but usually have employed only selective analogies or metaphors. The development of generalized theories of evolutionary change (Dawkins and Hull) has spawned models of language change on the basis of such generalized theories. These models have led to the positing of new mechanisms of language change and new types of selection that may not have biological parallels. Quantitative methods have been applied to questions of language phylogeny in the past decade. Research has focused on widely accepted families with cognates already established by the comparative method (Indo-European, Bantu, Austronesian). Increasingly sophisticated phylogeny reconstruction models have been applied to these families to resolve questions of subgrouping, contact, and migration. Little progress has been made so far in analyzing sound correspondences in the cognates themselves.},
	number = {1},
	urldate = {2017-01-10},
	journal = {Annual Review of Anthropology},
	author = {Croft, William},
	year = {2008},
	keywords = {comparative method, phylogeny, replicator, selection},
	pages = {219--234},
	file = {Croft_2008_Evolutionary Linguistics.pdf:/home/user/Zotero/storage/AQKTETGC/Croft_2008_Evolutionary Linguistics.pdf:application/pdf}
}

@article{sinnemaki-word-2010,
	title = {Word order in zero-marking languages},
	volume = {34},
	doi = {10.1075/sl.34.4.04sin},
	abstract = {It has often been argued that languages with no morphological marking of core arguments (referred to here as zero-marking languages) should prefer SVO word order. This correlation is tested here by studying the effects of word order, genealogical relatedness, and areal diffusion on the distribution of zero marking with multiple logistic regression. The possible confounding areal and genealogical factors are studied in multiple ways. The results, based on data from 848 languages, suggest that zero marking (morphological simplicity) correlates with SVO (syntactic simplicity), regardless of its areally and genealogically biased distribution. It is argued that this word order preference is affected by functional motivations and language contact.},
	number = {4},
	journal = {Studies in Language},
	author = {Sinnemäki, Kaius},
	month = jan,
	year = {2010},
	pages = {869--912},
	file = {Sinnemäki_2010_Word order in zero-marking languages.pdf:/home/user/Zotero/storage/W454A262/Sinnemäki_2010_Word order in zero-marking languages.pdf:application/pdf}
}

@article{sinnemaki-global-2014,
	title = {Global optimization and complexity trade-offs},
	volume = {50},
	issn = {1897-7499},
	url = {https://www.degruyter.com/view/j/psicl.2014.50.issue-2/psicl-2014-0013/psicl-2014-0013.xml},
	doi = {10.1515/psicl-2014-0013},
	abstract = {It is sometimes assumed in research on language complexity that if languages tended towards some optimal level of global complexity, so that all languages would be roughly equally complex, then local complexity trade-offs should be a general principle in language. Drawing evidence from computer simulations I show that in equally complex systems the proportion of trade-offs (significant negative correlations) is higher than in random systems but far from being a general principle in language. In addition, it may be impossible to determine whether a certain correlation-set comes from random systems or equally complex systems. Based on these results a correlational approach on a handful of typological variables cannot be used to validate, or even falsify, the assumption that all languages are equally complex and, therefore, complexity trade-offs should be kept separate from that assumption. The typological distribution of complexity, drawn from the World Atlas of Language Structures, is further shown to differ from both random systems and equally complex systems.},
	number = {2},
	urldate = {2017-01-10},
	journal = {Poznan Studies in Contemporary Linguistics},
	author = {Sinnemäki, Kaius},
	year = {2014},
	pages = {179--195}
}

@article{culbertson-simplicity-2016,
	title = {Simplicity and {Specificity} in {Language}: {Domain}-{General} {Biases} {Have} {Domain}-{Specific} {Effects}},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Simplicity and {Specificity} in {Language}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01964/abstract},
	doi = {10.3389/fpsyg.2015.01964},
	abstract = {The extent to which the linguistic system—its architecture, the representations it operates on, the constraints it is subject to—is specific to language has broad implications for cognitive science and its relation to evolutionary biology. Importantly, a given property of the linguistic system can be “specific” to the domain of language in several ways. For example, if the property evolved by natural selection under the pressure of the linguistic function it serves then the property is domain­-specific in the sense that its design is tailored for language. Equally though, if that property evolved to serve a different function or if that property is domain-general, it may nevertheless interact with the linguistic system in a way that is unique. This gives a second sense in which a property can be thought of as specific to language. An evolutionary approach to the language faculty might at first blush appear to favor domain­-specificity in the first sense, with individual properties of the language faculty being specifically linguistic adaptations. However, we argue that interactions between learning, culture and biological evolution mean any domain-specific adaptations that evolve will take the form of weak biases rather than hard constraints. Turning to the latter sense of domain-specificity, we highlight a very general bias, simplicity, which operates widely in cognition and yet interacts with linguistic representations in a domain-­specific way.},
	language = {English},
	urldate = {2017-01-10},
	journal = {Frontiers in Psychology},
	author = {Culbertson, Jennifer and Kirby, Simon},
	year = {2016},
	keywords = {word order, Language evolution, compositionality, domain-specificity, regularization, simplicity, typological universals},
	file = {Culbertson_Kirby_2016_Simplicity and Specificity in Language.pdf:/home/user/Zotero/storage/9DXSHXDC/Culbertson_Kirby_2016_Simplicity and Specificity in Language.pdf:application/pdf}
}

@article{jordan-kin-2010,
	title = {Kin term diversity is the result of multilevel, historical processes},
	volume = {33},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlekin-term-diversity-is-the-result-of-multilevel-historical-processesdiv/9C10C23A160A079D2B34CD48A2DC005F},
	doi = {10.1017/S0140525X10001962},
	abstract = {AbstractExplanations in the domain of kinship can be sought on several different levels: Jones addresses online processing, as well as issues of origins and innateness. We argue that his framework can more usefully be applied at the levels of developmental and historical change, the latter especially. A phylogenetic approach to the diversity of kinship terminologies is most urgently required.},
	number = {5},
	urldate = {2017-01-10},
	journal = {Behavioral and Brain Sciences},
	author = {Jordan, Fiona M. and Dunn, Michael},
	month = oct,
	year = {2010},
	pages = {388--388},
	file = {Jordan_Dunn_2010_Kin term diversity is the result of multilevel, historical processes.pdf:/home/user/Zotero/storage/A2BIGCIJ/Jordan_Dunn_2010_Kin term diversity is the result of multilevel, historical processes.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/H7E8E6TE/9C10C23A160A079D2B34CD48A2DC005F.html:text/html}
}

@article{dunn-evolved-2011,
	title = {Evolved structure of language shows lineage-specific trends in word-order universals},
	volume = {473},
	copyright = {© 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v473/n7345/full/nature09923.html},
	doi = {10.1038/nature09923},
	abstract = {Languages vary widely but not without limit. The central goal of linguistics is to describe the diversity of human languages and explain the constraints on that diversity. Generative linguists following Chomsky have claimed that linguistic diversity must be constrained by innate parameters that are set as a child learns a language. In contrast, other linguists following Greenberg have claimed that there are statistical tendencies for co-occurrence of traits reflecting universal systems biases, rather than absolute constraints or parametric variation. Here we use computational phylogenetic methods to address the nature of constraints on linguistic diversity in an evolutionary framework. First, contrary to the generative account of parameter setting, we show that the evolution of only a few word-order features of languages are strongly correlated. Second, contrary to the Greenbergian generalizations, we show that most observed functional dependencies between traits are lineage-specific rather than universal tendencies. These findings support the view that—at least with respect to word order—cultural evolution is the primary factor that determines linguistic structure, with the current state of a linguistic system shaping and constraining future states.},
	language = {en},
	number = {7345},
	urldate = {2017-01-10},
	journal = {Nature},
	author = {Dunn, Michael and Greenhill, Simon J. and Levinson, Stephen C. and Gray, Russell D.},
	month = may,
	year = {2011},
	keywords = {Psychology, evolution},
	pages = {79--82},
	file = {Dunn et al_2011_Evolved structure of language shows lineage-specific trends in word-order.pdf:/home/user/Zotero/storage/7SWGPKWS/Dunn et al_2011_Evolved structure of language shows lineage-specific trends in word-order.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/G67D2K6S/nature09923.html:text/html}
}

@article{levinson-universal-2011,
	title = {Universal typological dependencies should be detectable in the history of language families},
	volume = {15},
	url = {https://www.degruyter.com/view/j/lity.2011.15.issue-2/lity.2011.034/lity.2011.034.xml},
	doi = {10.1515/lity.2011.034},
	number = {2},
	urldate = {2017-01-10},
	journal = {Linguistic Typology},
	author = {Levinson, Stephen C. and Greenhill, Simon J. and Gray, Russell D. and Dunn, Michael},
	year = {2011},
	pages = {509--534}
}

@article{gavin-toward-2013,
	title = {Toward a {Mechanistic} {Understanding} of {Linguistic} {Diversity}},
	volume = {63},
	issn = {0006-3568},
	url = {https://academic.oup.com/bioscience/article/63/7/524/288871/Toward-a-Mechanistic-Understanding-of-Linguistic},
	doi = {10.1525/bio.2013.63.7.6},
	number = {7},
	urldate = {2017-01-10},
	journal = {BioScience},
	author = {Gavin, Michael C. and Botero, Carlos A. and Bowern, Claire and Colwell, Robert K. and Dunn, Michael and Dunn, Robert R. and Gray, Russell D. and Kirby, Kathryn R. and McCarter, Joe and Powell, Adam and Rangel, Thiago F. and Stepp, John R. and Trautwein, Michelle and Verdolin, Jennifer L. and Yanega, Gregor},
	month = jul,
	year = {2013},
	pages = {524--535},
	file = {Gavin et al_2013_Toward a Mechanistic Understanding of Linguistic Diversity.pdf:/home/user/Zotero/storage/3DPRKQJQ/Gavin et al_2013_Toward a Mechanistic Understanding of Linguistic Diversity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZH3GKFT8/bio.2013.63.7.html:text/html}
}

@article{majid-semantic-2015,
	series = {Semantic systems in closely related languages},
	title = {Semantic systems in closely related languages},
	volume = {49},
	issn = {0388-0001},
	url = {http://www.sciencedirect.com/science/article/pii/S0388000114001466},
	doi = {10.1016/j.langsci.2014.11.002},
	abstract = {In each semantic domain studied to date, there is considerable variation in how meanings are expressed across languages. But are some semantic domains more likely to show variation than others? Is the domain of space more or less variable in its expression than other semantic domains, such as containers, body parts, or colours? According to many linguists, the meanings expressed in grammaticised expressions, such as (spatial) adpositions, are more likely to be similar across languages than meanings expressed in open class lexical items. On the other hand, some psychologists predict there ought to be more variation across languages in the meanings of adpositions, than in the meanings of nouns. This is because relational categories, such as those expressed as adpositions, are said to be constructed by language; whereas object categories expressed as nouns are predicted to be “given by the world”. We tested these hypotheses by comparing the semantic systems of closely related languages. Previous cross-linguistic studies emphasise the importance of studying diverse languages, but we argue that a focus on closely related languages is advantageous because domains can be compared in a culturally- and historically-informed manner. Thus we collected data from 12 Germanic languages. Naming data were collected from at least 20 speakers of each language for containers, body-parts, colours, and spatial relations. We found the semantic domains of colour and body-parts were the most similar across languages. Containers showed some variation, but spatial relations expressed in adpositions showed the most variation. The results are inconsistent with the view expressed by most linguists. Instead, we find meanings expressed in grammaticised meanings are more variable than meanings in open class lexical items.},
	urldate = {2017-01-10},
	journal = {Language Sciences},
	author = {Majid, Asifa and Jordan, Fiona and Dunn, Michael},
	month = may,
	year = {2015},
	pages = {1--18},
	file = {Majid et al_2015_Semantic systems in closely related languages.pdf:/home/user/Zotero/storage/7RC3Z7AT/Majid et al_2015_Semantic systems in closely related languages.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/BCEIFIW3/S0388000114001466.html:text/html}
}

@article{wang-learning-2016,
	title = {Learning {Language} {Games} through {Interaction}},
	url = {http://arxiv.org/abs/1606.02447},
	abstract = {We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.},
	urldate = {2017-01-10},
	journal = {arXiv:1606.02447 [cs]},
	author = {Wang, Sida I. and Liang, Percy and Manning, Christopher D.},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.02447},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.6, I.2.7},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/NVWZVHMJ/1606.html:text/html;Wang et al_2016_Learning Language Games through Interaction.pdf:/home/user/Zotero/storage/KP7QIV8I/Wang et al_2016_Learning Language Games through Interaction.pdf:application/pdf}
}

@article{lau-grammaticality-2016,
	title = {Grammaticality, {Acceptability}, and {Probability}: {A} {Probabilistic} {View} of {Linguistic} {Knowledge}},
	issn = {1551-6709},
	shorttitle = {Grammaticality, {Acceptability}, and {Probability}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12414/abstract},
	doi = {10.1111/cogs.12414},
	abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
	language = {en},
	urldate = {2017-01-09},
	journal = {Cognitive Science},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	month = oct,
	year = {2016},
	keywords = {Grammaticality, Probabilistic modeling, Syntactic knowledge},
	pages = {n/a--n/a},
	file = {Lau et al_2016_Grammaticality, Acceptability, and Probability.pdf:/home/user/Zotero/storage/5ZGHMCJF/Lau et al_2016_Grammaticality, Acceptability, and Probability.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/C7GF52NH/abstract.html:text/html}
}

@misc{noauthor-[1608.06043]-nodate,
	title = {[1608.06043] {Context} {Gates} for {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1608.06043},
	urldate = {2016-12-07},
	file = {[1608.06043] Context Gates for Neural Machine Translation:/home/user/Zotero/storage/76CDIFDS/1608.html:text/html}
}

@unpublished{qiuwu-mandarin-2000,
	address = {Rutgers Optimality Archive 454},
	title = {Mandarin {Retroflex} {Suffixation}: {An} {OT} {Account}},
	shorttitle = {Mandarin {Retroflex} {Suffixation}},
	url = {http://roa.rutgers.edu/files/454-0801/454-0801-QIUWU-0-0.PDF.gz},
	urldate = {2016-11-22},
	author = {Qiuwu, Ma},
	year = {2000},
	note = {Rutgers Optimality Archive 454}
}

@article{cheng-typology-2013,
	title = {Typology of the {Formation} of “{X}-{ER}” {Diminutives} in the {{H}}ebei {Dialects}: {An} {Optimality}-{Theoretical} {Perspective}},
	volume = {1},
	copyright = {Copyright (c)},
	issn = {2321 - 2799},
	shorttitle = {Typology of the {Formation} of “{X}-{ER}” {Diminutives} in the {Hebei} {Dialects}},
	url = {http://www.ajouronline.com/index.php/AJHSS/article/view/573},
	abstract = {Previous research of the diminutives in the Hebei dialects usually lacks the connection of language descriptions to linguistic theories. For this reason, this study targets to explore the formation of X-ER diminutives in the Hebei dialects in terms of the framework of optimality theory (OT). The data analyzed in this study come from the fieldwork survey records of related literature and the author’s fieldwork investigation. Four formational indexes (i.e., X-retroflexion, onset insertion, onset selection, and [«r]/[«µ] alternation) play decisive roles in shaping the X-ER diminutives. Constraints are proposed to deal with the formation of X-ER diminutives. Research results show that OT well captures the four formational indexes by a common set of constraints, and well functions in the analysis of the formation of synchronic X-ER diminutives in the Hebei dialects. Furthermore, the interaction of the four formational indexes generates a factorial typology, which can be accounted for by reranking the same set of constraints and can be attested by the synchronic dialects in Hebei Province. To conclude, the formation of X-ER diminutives in the Hebei dialect is well analyzed in OT, and the strength of OT in typological prediction is well supported in this study.},
	language = {en},
	number = {5},
	urldate = {2016-11-14},
	journal = {Asian Journal of Humanities and Social Studies},
	author = {Cheng, Ming-chung},
	year = {2013},
	keywords = {Optimality theory, diminutive, Hebei, language typology},
	file = {Cheng_2013_Typology of the Formation of “X-ER” Diminutives in the Hebei Dialects.pdf:/home/user/Zotero/storage/UEPMGVMK/Cheng_2013_Typology of the Formation of “X-ER” Diminutives in the Hebei Dialects.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QH7D8XIB/573.html:text/html}
}

@article{feng-minimal-2001,
	title = {Minimal word in {Mandarin} {Chinese}},
	url = {http://roa.rutgers.edu/files/507-0302/507-0302-FENG-0-0.PDF.gz},
	urldate = {2016-12-02},
	journal = {Ms., University of Kansas},
	author = {Feng, Shengli},
	year = {2001},
	file = {507-0302-FENG-0-0.PDF:/home/user/Zotero/storage/Q5JUB2XV/507-0302-FENG-0-0.pdf:application/pdf}
}

@article{qiuwu-mandarin-nodate,
	title = {Mandarin {Retroflex} {Suffixation}: {An} {OT} {Account}},
	shorttitle = {Mandarin {Retroflex} {Suffixation}},
	url = {http://roa.rutgers.edu/files/454-0801/454-0801-QIUWU-0-0.PDF.gz},
	urldate = {2016-12-02},
	author = {Qiuwu, Ma},
	file = {454-0801-QIUWU-0-0.PDF:/home/user/Zotero/storage/SEFNCCRR/454-0801-QIUWU-0-0.pdf:application/pdf}
}

@incollection{lin-optimality-theoretic-2000,
	address = {San Diego},
	title = {An optimality-theoretic account of dialect variation in er suffixation: {A} case study of {Zhejiang} {Wu} dialects},
	booktitle = {Chinese {Phonology} in {Generative} {Grammar}},
	author = {Lin, Yen-Hwei},
	editor = {Xu, De Bao},
	year = {2000},
	pages = {193--222}
}

@book{yuan-hanyu-1960,
	address = {Beijing},
	title = {Hanyu {Fangyan} {Gaiyao}},
	editor = {Yuan, Jiahua},
	year = {1960}
}

@book{qian-hanyu-2010,
	address = {Jinan},
	title = {Hanyu {Guanhua} {Fangyan} {Yanjiu}},
	publisher = {Qilu Shushe},
	editor = {Qian, Cengyi},
	year = {2010}
}

@book{chao-grammar-1970,
	address = {Berkeley},
	title = {A {Grammar} of {Spoken} {Chinese}},
	author = {Chao, Yuen Ren},
	year = {1970}
}

@book{duanmu-phonology-2000,
	address = {Oxford},
	title = {The {Phonology} of {Standard} {Chinese}},
	author = {Duanmu, San},
	year = {2000}
}

@article{shain-memory-nodate,
	title = {Memory access during incremental sentence processing causes reading time latency},
	url = {http://www.ling.ohio-state.edu/~van-schijndel.1/resources/uploads/shain\_etal-2016-cl4lc.pdf},
	urldate = {2016-11-22},
	author = {Shain, Cory and van Schijndel, Marten and Futrell, Richard and Gibson, Edward and Schuler, William},
	file = {shain_etal-2016-cl4lc.pdf:/home/user/Zotero/storage/Z76RZEE4/shain_etal-2016-cl4lc.pdf:application/pdf}
}

@article{lin-chinese-2004,
	title = {Chinese affixal phonology: {Some} analytical and theoretical issues},
	volume = {5},
	shorttitle = {Chinese affixal phonology},
	url = {http://intranet.ling.sinica.edu.tw/eip/FILES/journal/2007.3.9.5100649.54425412.pdf},
	number = {4},
	urldate = {2016-11-22},
	journal = {Language and Linguistics},
	author = {Lin, Yen-Hwei},
	year = {2004},
	pages = {1019--1046},
	file = {Microsoft Word - LL5.4-10-Lin 20040820.doc - j2004_4_11_7858.pdf:/home/user/Zotero/storage/B2KDWHTV/j2004_4_11_7858.pdf:application/pdf}
}

@article{feng-minimal-2001-1,
	title = {Minimal word in {Mandarin} {Chinese}},
	url = {http://roa.rutgers.edu/files/507-0302/507-0302-FENG-0-0.PDF.gz},
	urldate = {2016-11-22},
	journal = {Ms., University of Kansas},
	author = {Feng, Shengli},
	year = {2001}
}

@article{tian-optimality-2009,
	title = {An optimality theory analysis of diminutive suffixation of {Beijing} {Chinese}},
	volume = {19},
	url = {https://www.mmduvic.ca/index.php/WPLC/article/viewFile/5663/2188},
	number = {1},
	urldate = {2016-11-22},
	journal = {Working Papers of the Linguistics Circle},
	author = {Tian, Jun},
	year = {2009},
	pages = {217--232},
	file = {Tian_09-NWLC23 - download:/home/user/Zotero/storage/EJB5TGDG/download.pdf:application/pdf}
}

@article{alan-infixing-2004,
	title = {Infixing with a vengeance: {Pingding} {Mandarin} infixation},
	volume = {13},
	shorttitle = {Infixing with a vengeance},
	url = {http://link.springer.com/article/10.1023/B:JEAL.0000007241.87718.69},
	number = {1},
	urldate = {2016-11-22},
	journal = {Journal of East Asian Linguistics},
	author = {Alan, C. L.},
	year = {2004},
	pages = {39--58},
	file = {JEAL13.pdf:/home/user/Zotero/storage/QPD4QCDA/JEAL13.pdf:application/pdf}
}

@misc{noauthor-[1611.01874]-nodate,
	title = {[1611.01874] {Neural} {Machine} {Translation} with {Reconstruction}},
	url = {https://arxiv.org/abs/1611.01874},
	urldate = {2016-11-22},
	file = {[1611.01874] Neural Machine Translation with Reconstruction:/home/user/Zotero/storage/C3Q26M4B/1611.html:text/html}
}

@misc{noauthor-[1606.04212]-nodate-1,
	title = {[1606.04212] {Active} {Discriminative} {Text} {Representation} {Learning}},
	url = {https://arxiv.org/abs/1606.04212},
	urldate = {2016-11-22},
	file = {[1606.04212] Active Discriminative Text Representation Learning:/home/user/Zotero/storage/QEB5TGCH/1606.html:text/html}
}

@misc{noauthor-[1611.06997]-nodate,
	title = {[1611.06997] {Coherent} {Dialogue} with {Attention}-based {Language} {Models}},
	url = {https://arxiv.org/abs/1611.06997},
	urldate = {2016-11-22},
	file = {[1611.06997] Coherent Dialogue with Attention-based Language Models:/home/user/Zotero/storage/98XR4RMB/1611.html:text/html}
}

@misc{noauthor-deb-nodate,
	title = {Deb {Roy} {\textbar} {Laboratory} for {Social} {Machines}},
	url = {http://dkroy.media.mit.edu/},
	urldate = {2016-11-22},
	file = {Deb Roy | Laboratory for Social Machines:/home/user/Zotero/storage/S8CAW8R3/dkroy.media.mit.edu.html:text/html}
}

@misc{noauthor-[1611.06188]-nodate,
	title = {[1611.06188] {Variable} {Computation} in {Recurrent} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1611.06188},
	urldate = {2016-11-21},
	file = {[1611.06188] Variable Computation in Recurrent Neural Networks:/home/user/Zotero/storage/DDP66FZP/1611.html:text/html}
}

@article{liang-neural-2016-1,
	title = {Neural {Symbolic} {Machines}: {Learning} {Semantic} {Parsers} on {Freebase} with {Weak} {Supervision}},
	shorttitle = {Neural {Symbolic} {Machines}},
	url = {http://arxiv.org/abs/1611.00020},
	abstract = {Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.},
	urldate = {2016-11-17},
	journal = {arXiv:1611.00020 [cs]},
	author = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
	month = oct,
	year = {2016},
	note = {arXiv: 1611.00020},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GJVTZKXT/1611.html:text/html;Liang et al_2016_Neural Symbolic Machines.pdf:/home/user/Zotero/storage/E4QFQUF8/Liang et al_2016_Neural Symbolic Machines.pdf:application/pdf}
}

@book{cohen-shay-bayesian-nodate,
	title = {Bayesian},
	author = {Cohen, Shay},
	file = {ebscohost (1).pdf:/home/user/Zotero/storage/2IHTTA3U/ebscohost (1).pdf:application/pdf;ebscohost (2).pdf:/home/user/Zotero/storage/KS73V2JQ/ebscohost (2).pdf:application/pdf;ebscohost.pdf:/home/user/Zotero/storage/8GMFH3E6/ebscohost.pdf:application/pdf;fulltext.pdf:/home/user/Zotero/storage/VISHWVIT/fulltext.pdf:application/pdf}
}

@misc{noauthor-[1611.04558]-nodate,
	title = {[1611.04558] {Google}'s {Multilingual} {Neural} {Machine} {Translation} {System}: {Enabling} {Zero}-{Shot} {Translation}},
	url = {https://arxiv.org/abs/1611.04558},
	urldate = {2016-11-15},
	file = {[1611.04558] Google's Multilingual Neural Machine Translation System\: Enabling Zero-Shot Translation:/home/user/Zotero/storage/SMTZN75E/1611.html:text/html}
}

@misc{noauthor-publications-nodate-1,
	title = {Publications - {Heather} {Burnett}},
	url = {https://sites.google.com/site/heathersusanburnett/home/selected-publications},
	abstract = {Heather Burnett's Website},
	urldate = {2016-11-15},
	file = {Snapshot:/home/user/Zotero/storage/XAQI63FX/selected-publications.html:text/html}
}

@misc{noauthor-download.aspx-nodate,
	title = {Download.aspx},
	url = {http://jntnull.ntnu.edu.tw/jll/Download.aspx?ItemId=1475&files=635483657316943447.pdf&counts=n},
	urldate = {2016-11-14},
	file = {Download.aspx:/home/user/Zotero/storage/CRTPJU9J/Download.aspx:application/octet-stream}
}

@misc{noauthor-erhua-2016,
	title = {Erhua},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Erhua&oldid=743220218},
	abstract = {Erhua (simplified Chinese: 儿化; traditional Chinese: 兒化; pinyin: érhuà); also called erhuayin (simplified Chinese: 儿化音; traditional Chinese: 兒化音; pinyin: érhuàyīn) or erization, refers to a phonological process that adds r-coloring or the "ér" (儿) sound (transcribed in IPA as [ɚ]) to syllables in spoken Mandarin Chinese. It is most common in the speech varieties of North China, especially in the Beijing dialect, as a diminutive suffix for nouns, though some dialects also use it for other grammatical purposes. The Standard Chinese spoken in government-produced educational and examination recordings features erhua to some extent, as in 哪儿 nǎr ("where"), 一点儿 yìdiǎnr ("a little"), and 好玩儿 hǎowánr ("fun"). Colloquial speech in many northern dialects has more extensive erhua than the standardized language. Southwestern Mandarin dialects such as those of Chongqing and Chengdu also have erhua. By contrast, many Southern Chinese who speak non-Mandarin dialects may have difficulty pronouncing the sound or may simply prefer not to pronounce it, and usually avoid words with erhua when speaking Standard Chinese; for example, the three examples listed above may be replaced with the synonyms 哪里 nǎlǐ, 一点 yìdiǎn, 好玩 hǎowán.
Only a small number of words in standardized Mandarin, such as 二 èr "two" and 耳 ěr "ear", have r-colored vowels that do not result from the erhua process. All of the non-erhua r-colored syllables have no initial consonant, and are traditionally pronounced [ɚ] in Beijing dialect and in conservative/old Standard Mandarin varieties. In the recent decades, the vowel in the toned syllable "er" has been lowered in many accents, making the syllable come to approach or acquire a quality like "ar" (i.e. [aɚ̯] with the appropriate tone). In some new accents and some different accents than Beijing, all the non-erhua r-colored syllables (may) use "ar"-like qualities regardless of tones. All other instances of r-colored vowels are a result of erhua being applied to originally non-r-colored syllables.},
	language = {en},
	urldate = {2016-11-14},
	journal = {Wikipedia},
	month = oct,
	year = {2016},
	note = {Page Version ID: 743220218},
	file = {Snapshot:/home/user/Zotero/storage/TZMIG7B7/index.html:text/html}
}

@misc{noauthor-presuppositions-nodate,
	title = {The {Presuppositions} of {Soft} {Triggers} are {Obligatory} {Scalar} {Implicatures}},
	url = {http://jos.oxfordjournals.org/content/32/2/173.full.pdf+html},
	urldate = {2016-11-14},
	file = {The Presuppositions of Soft Triggers are Obligatory Scalar Implicatures:/home/user/Zotero/storage/KU46RTZQ/173.full.html:text/html}
}

@misc{noauthor-[1611.01491]-nodate,
	title = {[1611.01491] {Understanding} {Deep} {Neural} {Networks} with {Rectified} {Linear} {Units}},
	url = {https://arxiv.org/abs/1611.01491},
	urldate = {2016-11-14},
	file = {[1611.01491] Understanding Deep Neural Networks with Rectified Linear Units:/home/user/Zotero/storage/96SRUP5Q/1611.html:text/html}
}

@article{syrett-meaning-2010,
	title = {Meaning and {Context} in {Children}'s {Understanding} of {Gradable} {Adjectives}},
	volume = {27},
	issn = {0167-5133, 1477-4593},
	url = {http://jos.oxfordjournals.org/content/27/1/1},
	doi = {10.1093/jos/ffp011},
	abstract = {This paper explores what children and adults know about three specific ways that meaning and context interact: the interpretation of expressions whose extensions vary in different contexts (semantic context dependence); conditions on the felicitous use of expressions in a discourse context (presupposition accommodation) and informative uses of expressions in contexts in which they strictly speaking do not apply (imprecision). The empirical focus is the use of unmodified (positive form) gradable adjectives (GAs) in definite descriptions to distinguish between two objects that differ in the degree to which they possess the property named by the adjective. We show that by 3 years of age, children are sensitive to all three varieties of context–meaning interaction and that their knowledge of this relation with the definite description is appropriately guided by the semantic representations of the GA appearing in it. These findings suggest that children's semantic representations of the GAs we investigated and the definite determiner the are adult-like and that they are aware of the consequences of these representations when relating meaning and context. Bolstered by adult participant responses, this work provides important experimental support for theoretical claims regarding the semantics of gradable predicates and the nature of different types of ‘interpretive variability’, specifically semantic context dependence v. pragmatic tolerance of imprecision.},
	language = {en},
	number = {1},
	urldate = {2016-11-14},
	journal = {Journal of Semantics},
	author = {Syrett, Kristen and Kennedy, Christopher and Lidz, Jeffrey},
	month = feb,
	year = {2010},
	pages = {1--35},
	file = {Snapshot:/home/user/Zotero/storage/CISUEFM3/1.html:text/html}
}

@article{sordoni-iterative-2016,
	title = {Iterative {Alternating} {Neural} {Attention} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1606.02245},
	abstract = {We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.},
	urldate = {2016-11-10},
	journal = {arXiv:1606.02245 [cs]},
	author = {Sordoni, Alessandro and Bachman, Philip and Trischler, Adam and Bengio, Yoshua},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.02245},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KIV55RS6/1606.html:text/html;Sordoni et al_2016_Iterative Alternating Neural Attention for Machine Reading.pdf:/home/user/Zotero/storage/V9RQCBRR/Sordoni et al_2016_Iterative Alternating Neural Attention for Machine Reading.pdf:application/pdf}
}

@article{merchant-neural-2013,
	title = {Neural {Basis} of the {Perception} and {Estimation} of {Time}},
	volume = {36},
	url = {http://dx.doi.org/10.1146/annurev-neuro-062012-170349},
	doi = {10.1146/annurev-neuro-062012-170349},
	abstract = {Understanding how sensory and motor processes are temporally integrated to control behavior in the hundredths of milliseconds-to-minutes range is a fascinating problem given that the basic electrophysiological properties of neurons operate on a millisecond timescale. Single-unit recording studies in monkeys have identified localized timing circuits, whereas neuropsychological studies of humans who have damage to the basal ganglia have indicated that core structures, such as the cortico-thalamic-basal ganglia circuit, play an important role in timing and time perception. Taken together, these data suggest that a core timing mechanism interacts with context-dependent areas. This idea of a temporal hub with a distributed network is used to investigate the abstract properties of interval tuning as well as temporal illusions and intersensory timing. We conclude by proposing that the interconnections built into this core timing mechanism are designed to provide a form of degeneracy as protection against injury, disease, or age-related decline.},
	number = {1},
	urldate = {2016-11-09},
	journal = {Annual Review of Neuroscience},
	author = {Merchant, Hugo and Harrington, Deborah L. and Meck, Warren H.},
	year = {2013},
	pmid = {23725000},
	pages = {313--336},
	file = {Full Text PDF:/home/user/Zotero/storage/6AD348ZF/Merchant et al. - 2013 - Neural Basis of the Perception and Estimation of T.pdf:application/pdf}
}

@misc{noauthor-[1611.01628]-nodate,
	title = {[1611.01628] {Reference}-{Aware} {Language} {Models}},
	url = {https://arxiv.org/abs/1611.01628},
	urldate = {2016-11-08},
	file = {[1611.01628] Reference-Aware Language Models:/home/user/Zotero/storage/6RZXAJKM/1611.html:text/html}
}

@article{aharoni-sequence-2016,
	title = {Sequence to {Sequence} {Transduction} with {Hard} {Monotonic} {Attention}},
	url = {http://arxiv.org/abs/1611.01487},
	abstract = {We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.},
	urldate = {2016-11-07},
	journal = {arXiv:1611.01487 [cs]},
	author = {Aharoni, Roee and Goldberg, Yoav},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01487},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1611.01487 PDF:/home/user/Zotero/storage/4JT2AP8H/Aharoni and Goldberg - 2016 - Sequence to Sequence Transduction with Hard Monoto.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/AJ25AV9C/1611.html:text/html}
}

@misc{noauthor-[1611.01491]-nodate-1,
	title = {[1611.01491] {Understanding} {Deep} {Neural} {Networks} with {Rectified} {Linear} {Units}},
	url = {https://arxiv.org/abs/1611.01491},
	urldate = {2016-11-07},
	file = {[1611.01491] Understanding Deep Neural Networks with Rectified Linear Units:/home/user/Zotero/storage/AEAUVVWK/1611.html:text/html}
}

@misc{noauthor-[1610.08613]-nodate,
	title = {[1610.08613] {Can} {Active} {Memory} {Replace} {Attention}?},
	url = {https://arxiv.org/abs/1610.08613},
	urldate = {2016-10-28},
	file = {[1610.08613] Can Active Memory Replace Attention?:/home/user/Zotero/storage/IAPIHUH5/1610.html:text/html}
}

@article{sanborn-bayesian-nodate,
	title = {Bayesian {Brains} without {Probabilities}},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661316301565},
	doi = {10.1016/j.tics.2016.10.003},
	abstract = {Bayesian explanations have swept through cognitive science over the past two decades, from intuitive physics and causal learning, to perception, motor control and language. Yet people flounder with even the simplest probability questions. What explains this apparent paradox? How can a supposedly Bayesian brain reason so poorly with probabilities? In this paper, we propose a direct and perhaps unexpected answer: that Bayesian brains need not represent or calculate probabilities at all and are, indeed, poorly adapted to do so. Instead, the brain is a Bayesian sampler. Only with infinite samples does a Bayesian sampler conform to the laws of probability; with finite samples it systematically generates classic probabilistic reasoning errors, including the unpacking effect, base-rate neglect, and the conjunction fallacy.},
	urldate = {2016-10-28},
	journal = {Trends in Cognitive Sciences},
	author = {Sanborn, Adam N. and Chater, Nick},
	keywords = {sampling, Bayesian models of cognition, reasoning biases},
	file = {Sanborn_Chater_Bayesian Brains without Probabilities.pdf:/home/user/Zotero/storage/NP6GC7GB/Sanborn_Chater_Bayesian Brains without Probabilities.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/HZSSBGJI/S1364661316301565.html:text/html}
}

@article{blanchette-hammering-2013,
	title = {Hammering {Away}},
	url = {https://isabelle.in.tum.de/website-Isabelle2012/dist/Isabelle2012/doc/sledgehammer.pdf},
	urldate = {2016-10-28},
	journal = {A User’s Guide to Sledgehammer for Isabelle/HOL. u rl: http://isabelle. in. tum. de/dist/Isabelle2013-2/doc/sledgehammer. pdf},
	author = {Blanchette, Jasmin Christian and Paulson, Lawrence C.},
	year = {2013},
	file = {sledgehammer.pdf:/home/user/Zotero/storage/ZF9A72TW/sledgehammer.pdf:application/pdf}
}

@book{zeevat-bayesian-2015,
	address = {Cham},
	series = {Language, {Cognition}, and {Mind}},
	title = {Bayesian {Natural} {Language} {Semantics} and {Pragmatics}},
	volume = {2},
	isbn = {978-3-319-17063-3 978-3-319-17064-0},
	url = {http://link.springer.com/10.1007/978-3-319-17064-0},
	urldate = {2016-10-28},
	publisher = {Springer International Publishing},
	editor = {Zeevat, Henk and Schmitz, Hans-Christian},
	year = {2015},
	file = {bok%3A978-3-319-17064-0.pdf:/home/user/Zotero/storage/T9DGT36Z/bok%3A978-3-319-17064-0.pdf:application/pdf}
}

@incollection{von-heusinger-definite-1997,
	title = {Definite descriptions and choice functions},
	url = {http://link.springer.com/chapter/10.1007/978-94-011-5638-7\_4},
	urldate = {2016-10-28},
	booktitle = {Logic, {Language} and {Computation}},
	publisher = {Springer},
	author = {Von Heusinger, Klaus},
	year = {1997},
	pages = {61--91},
	file = {chp%3A10.1007%2F978-94-011-5638-7_4.pdf:/home/user/Zotero/storage/PWHKERC2/chp%3A10.1007%2F978-94-011-5638-7_4.pdf:application/pdf}
}

@article{harper-ambiguous-1992,
	title = {Ambiguous noun phrases in logical form},
	volume = {18},
	url = {http://dl.acm.org/citation.cfm?id=176315},
	number = {4},
	urldate = {2016-10-28},
	journal = {Computational Linguistics},
	author = {Harper, Mary P.},
	year = {1992},
	pages = {419--465},
	file = {Ambiguous Noun Phrases in Logical Form - J92-4002:/home/user/Zotero/storage/CPFQATXC/J92-4002.pdf:application/pdf}
}

@article{harper-ambiguous-1992-1,
	title = {Ambiguous noun phrases in logical form},
	volume = {18},
	url = {http://dl.acm.org/citation.cfm?id=176315},
	number = {4},
	urldate = {2016-10-28},
	journal = {Computational Linguistics},
	author = {Harper, Mary P.},
	year = {1992},
	pages = {419--465},
	file = {Ambiguous Noun Phrases in Logical Form - J92-4002:/home/user/Zotero/storage/JQE67AV6/J92-4002.pdf:application/pdf}
}

@inproceedings{leu-donkey-2005,
	title = {Donkey pronouns: void descriptions?},
	volume = {35},
	shorttitle = {Donkey pronouns},
	url = {http://www.semanticsarchive.net/Archive/jkyNWY1O/leu-05-NELS-donkey-pronouns.pdf},
	urldate = {2016-10-28},
	booktitle = {{PROCEEDINGS}-{NELS}},
	author = {Leu, Tom},
	year = {2005},
	pages = {379},
	file = {Leu-NELS-35-donkey-paper-final.dvi - leu-05-NELS-donkey-pronouns.pdf:/home/user/Zotero/storage/BCQ38PUK/leu-05-NELS-donkey-pronouns.pdf:application/pdf}
}

@article{abrusan-disappearing-2014,
	title = {Disappearing acts of presuppositions: {Cancelling} the soft-hard distinction},
	shorttitle = {Disappearing acts of presuppositions},
	journal = {Unpublished manuscript CNRS},
	author = {Abrusán, Márta},
	year = {2014},
	file = {PresuppositionCancellation.NALSdraft.pdf:/home/user/Zotero/storage/KSAKJN2X/PresuppositionCancellation.NALSdraft.pdf:application/pdf}
}

@article{kong-look-2013,
	title = {A {Look} into the {Triggering} of {Presuppositions} in {Chinese} and {English}},
	volume = {3},
	issn = {1799-2591},
	url = {http://www.academypublication.com/issues/past/tpls/vol03/11/08.pdf},
	doi = {10.4304/tpls.3.11.1988-1995},
	number = {11},
	urldate = {2016-10-28},
	journal = {Theory and Practice in Language Studies},
	author = {Kong, Lei},
	month = nov,
	year = {2013},
	file = {tpls0311.pdf - 08.pdf:/home/user/Zotero/storage/SQUKV548/08.pdf:application/pdf}
}

@article{kriz-two-2015,
	title = {Two methods to find truth-value gaps and their application to the projection problem of homogeneity},
	volume = {23},
	url = {http://link.springer.com/article/10.1007/s11050-015-9114-z},
	number = {3},
	urldate = {2016-10-27},
	journal = {Natural Language Semantics},
	author = {Križ, Manuel and Chemla, Emmanuel},
	year = {2015},
	pages = {205--248},
	file = {Kriz Chemla 2014 Finding Gaps.pdf:/home/user/Zotero/storage/SQQS38SK/Kriz Chemla 2014 Finding Gaps.pdf:application/pdf}
}

@inproceedings{alxatib-psychology-2009,
	title = {On the psychology of truth-gaps},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-18446-8\_2},
	urldate = {2016-10-27},
	booktitle = {International {Workshop} on {Vagueness} in {Communication}},
	publisher = {Springer},
	author = {Alxatib, Sam and Pelletier, Jeff},
	year = {2009},
	pages = {13--36},
	file = {Title - chp%3A10.1007%2F978-3-642-18446-8_2.pdf:/home/user/Zotero/storage/BXVGUN7B/chp%3A10.1007%2F978-3-642-18446-8_2.pdf:application/pdf}
}

@inproceedings{alxatib-psychology-2009-1,
	title = {On the psychology of truth-gaps},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-18446-8\_2},
	urldate = {2016-10-27},
	booktitle = {International {Workshop} on {Vagueness} in {Communication}},
	publisher = {Springer},
	author = {Alxatib, Sam and Pelletier, Jeff},
	year = {2009},
	pages = {13--36},
	file = {SamLNAIproofs.pdf:/home/user/Zotero/storage/H8ZFABRK/SamLNAIproofs.pdf:application/pdf}
}

@article{schwarz-false-2015,
	title = {False but {Slow}: {Evaluating} {Statements} with {Non}-referring {Definites}},
	issn = {0167-5133, 1477-4593},
	shorttitle = {False but {Slow}},
	url = {http://jos.oxfordjournals.org/lookup/doi/10.1093/jos/ffu019},
	doi = {10.1093/jos/ffu019},
	language = {en},
	urldate = {2016-10-27},
	journal = {Journal of Semantics},
	author = {Schwarz, Florian},
	month = jan,
	year = {2015},
	pages = {ffu019},
	file = {OP-SEMA140036 177..214 - 177.full.pdf:/home/user/Zotero/storage/9ENHMXV6/177.full.pdf:application/pdf}
}

@article{abrusan-experimenting-2013,
	title = {Experimenting with the king of {France}: {Topics}, verifiability and definite descriptions},
	volume = {6},
	copyright = {Copyright (c) 2014 Márta Abrusán, Kriszta Szendrői},
	issn = {1937-8912},
	shorttitle = {Experimenting with the king of {France}},
	url = {http://semprag.org/article/view/sp.6.10},
	doi = {10.3765/sp.6.10},
	abstract = {Definite descriptions with reference failure have been argued to give rise to different truth-value intuitions depending on the local linguistic context in which they appear. We conducted an experiment to investigate these alleged differences, thereby contributing new data to the debate. We have found that pragmatic strategies dependent on verification and topicalisation, suggested in the context of trivalent/partial theories, indeed play a role in people's subjective judgments. We discuss the consequences of these findings for all major approaches to definite descriptions (i.e. Russellian, Strawsonian, pragmatic). Finally, we offer a discussion of the relative contribution of verificational and topicality effects on truth values, reaching the conclusion that verification is primarily relevant and topicality is dependent on that. We thus support von Fintel's (2004) position on the primacy of verification, but not his dismissal of topicality as a factor.

http://dx.doi.org/10.3765/sp.6.10

BibTeX info},
	language = {en},
	number = {0},
	urldate = {2016-10-27},
	journal = {Semantics and Pragmatics},
	author = {Abrusán, Márta and Szendrői, Kriszta},
	month = oct,
	year = {2013},
	keywords = {experimental pragmatics, presupposition, definite descriptions, topics, verifiability},
	pages = {10--1--43},
	file = {Abrusán_Szendrői_2013_Experimenting with the king of France.pdf:/home/user/Zotero/storage/FRGTI595/Abrusán_Szendrői_2013_Experimenting with the king of France.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CESWB7R5/sp.6.html:text/html}
}

@article{politzer-betting-2010,
	title = {Betting on conditionals},
	volume = {16},
	url = {https://jeannicod.ccsd.cnrs.fr/ijn\_00512473/document},
	abstract = {A study is reported testing two hypotheses about a close parallel relation between indicative conditionals, if A then B, and conditional bets, I bet you that if A then B. The first is that both the indicative conditional and the conditional bet are related to the conditional probability, P(B{\textbar}A). The second is that de Finetti's three-valued truth table has psychological reality for both types of conditional – true, false, or void for indicative conditionals and win, lose or void for conditional bets. The participants were presented with an array of chips in two different colours and two different shapes, and an indicative conditional or a conditional bet about a random chip. They had to make judgments in two conditions: either about the chances of making the indicative conditional true or false or about the chances of winning or losing the conditional bet. The observed distributions of responses in the two conditions were generally related to the conditional probability, supporting the first hypothesis. In addition, a majority of participants in further conditions chose the third option, “void”, when the antecedent of the conditional was false, supporting the second hypothesis.},
	language = {en},
	number = {3},
	urldate = {2016-10-27},
	journal = {Thinking and Reasoning},
	author = {Politzer, Guy and Over, David P. and Baratgin, Jean},
	year = {2010},
	pages = {172--197},
	file = {Politzer et al_2010_Betting on conditionals.pdf:/home/user/Zotero/storage/Q37AMTEJ/Politzer et al_2010_Betting on conditionals.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/692N7Q9B/fr.html:text/html}
}

@misc{noauthor-scalar-nodate,
	title = {Scalar {Implicatures} {Versus} {Presuppositions}: {The} {View} from {Acquisition} {\textbar} {SpringerLink}},
	url = {http://link.springer.com/article/10.1007%2Fs11245-014-9276-1},
	urldate = {2016-10-27},
	file = {Scalar Implicatures Versus Presuppositions\: The View from Acquisition | SpringerLink:/home/user/Zotero/storage/3XHADF4I/10.html:text/html}
}

@article{he-deep-2015-1,
	title = {Deep {Reinforcement} {Learning} with a {Natural} {Language} {Action} {Space}},
	url = {http://arxiv.org/abs/1511.04636},
	abstract = {This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.},
	urldate = {2016-10-27},
	journal = {arXiv:1511.04636 [cs]},
	author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04636},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/D8D5J8S8/1511.html:text/html;He et al_2015_Deep Reinforcement Learning with a Natural Language Action Space.pdf:/home/user/Zotero/storage/SFEJHCIS/He et al_2015_Deep Reinforcement Learning with a Natural Language Action Space.pdf:application/pdf}
}

@misc{noauthor-[1610.05654]-nodate,
	title = {[1610.05654] {The} infochemical core},
	url = {https://arxiv.org/abs/1610.05654},
	urldate = {2016-10-22},
	file = {[1610.05654] The infochemical core:/home/user/Zotero/storage/EJHMGMK8/1610.html:text/html}
}

@article{vogt-communicative-2015,
	title = {Communicative intentions of child-directed speech in three different learning environments: {Observations} from the {Netherlands}, and rural and urban {Mozambique}},
	volume = {35},
	shorttitle = {Communicative intentions of child-directed speech in three different learning environments},
	url = {http://journals.sagepub.com/doi/abs/10.1177/0142723715596647},
	number = {4-5},
	urldate = {2017-03-15},
	journal = {First Language},
	author = {Vogt, Paul and Mastin, J. Douglas and Schots, Diede MA},
	year = {2015},
	pages = {341--358},
	file = {Vogt&Mastin-FL2015.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Vogt&Mastin-FL2015.pdf:application/pdf}
}

@misc{noauthor-frontiers-nodate-1,
	title = {Frontiers {\textbar} {The} integration hypothesis of human language evolution and the nature of contemporary languages {\textbar} {Language} {Sciences}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00564/full},
	urldate = {2017-03-15},
	file = {Frontiers | The integration hypothesis of human language evolution and the nature of contemporary languages | Language Sciences:/home/user/Zotero/storage/H6EJEZ38/full.html:text/html}
}

@misc{noauthor-frontiers-nodate-2,
	title = {Frontiers {\textbar} {The} precedence of syntax in the rapid emergence of human language in evolution as defined by the integration hypothesis {\textbar} {Language} {Sciences}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.00271/full},
	urldate = {2017-03-15},
	file = {Frontiers | The precedence of syntax in the rapid emergence of human language in evolution as defined by the integration hypothesis | Language Sciences:/home/user/Zotero/storage/W6RJNISK/full.html:text/html}
}

@misc{noauthor-shigeru-nodate,
	title = {Shigeru {Miyagawa} {\textbar} {Linguistics}},
	url = {http://linguistics.mit.edu/user/miyagawa/},
	urldate = {2017-03-15},
	file = {Shigeru Miyagawa | Linguistics:/home/user/Zotero/storage/E933TWKB/miyagawa.html:text/html}
}

@article{csibra-recognizing-2010,
	title = {Recognizing communicative intentions in infancy},
	volume = {25},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0017.2009.01384.x/full},
	number = {2},
	urldate = {2017-03-15},
	journal = {Mind \& Language},
	author = {Csibra, Gergely},
	year = {2010},
	pages = {141--168},
	file = {mila_1384.dvi - download:/home/user/Zotero/storage/VFDBAXZX/download.pdf:application/pdf}
}

@article{adamson-affect-1985,
	title = {Affect and {Attention}: {Infants} {Observed} with {Mothers} and {Peers}},
	volume = {56},
	issn = {0009-3920},
	shorttitle = {Affect and {Attention}},
	url = {http://www.jstor.org/stable/1129748},
	doi = {10.2307/1129748},
	abstract = {This study documents the rate, mean duration, and mode of infants' affective displays. Using a cross-sequential design, infants were observed in their homes from 6 to 18 months playing with their mothers, with peers, and alone. Affect rates were higher with mothers than peers. With increasing age, affect rates as well as the vocal mode increased, while mean durations and facial and motoric modes decreased. Affect was most likely when infants were engaged with mothers or peers in person play. It also occurred often when infants first became engaged with the same object their partner was manipulating; with mothers (but not peers) affect continued to be expressed throughout these periods of shared object play. Rates were elevated when mothers moved objects repetitively. Discussion focuses on how infants' earlier-developing affective communication skills may continue to be used as they begin to explore the world of objects and on how adults may support this integration of expressive and referential communication.},
	number = {3},
	urldate = {2017-03-15},
	journal = {Child Development},
	author = {Adamson, Lauren B. and Bakeman, Roger},
	year = {1985},
	pages = {582--593},
	file = {Adamson_Bakeman_1985_Affect and Attention.pdf:/home/user/Zotero/storage/9ST22WNA/Adamson_Bakeman_1985_Affect and Attention.pdf:application/pdf}
}

@article{baron-cohen-theory-2001,
	title = {Theory of mind in normal development and autism},
	volume = {34},
	url = {http://www.academia.edu/download/46565761/Theory\_of\_Mind\_in\_normal\_development\_and20160617-19916-vr34ya.pdf},
	number = {1},
	urldate = {2017-03-15},
	journal = {Prisme},
	author = {Baron-Cohen, Simon},
	year = {2001},
	pages = {74--183},
	file = {Theory-of-Mind-in-normal-development-and-autism.pdf:/home/user/Zotero/storage/9Q3V36KT/Theory-of-Mind-in-normal-development-and-autism.pdf:application/pdf}
}

@article{ezell-imitation-2012,
	title = {Imitation {Effects} on {Joint} {Attention} {Behaviors} of {Children} with {Autism}},
	volume = {03},
	issn = {2152-7180, 2152-7199},
	url = {http://www.scirp.org/journal/PaperDownload.aspx?DOI=10.4236/psych.2012.39103},
	doi = {10.4236/psych.2012.39103},
	number = {09},
	urldate = {2017-03-15},
	journal = {Psychology},
	author = {Ezell, Shauna and Field, Tiffany and Nadel, Jacqueline and Newton, Rae and Murrey, Greg and Siddalingappa, Vijaya and Allender, Susan and Grace, Ava},
	year = {2012},
	pages = {681--685}
}

@article{csibra-recognizing-2010-1,
	title = {Recognizing communicative intentions in infancy},
	abstract = {Abstract: I make three related proposals concerning the development of receptive communication in human infants. First, I propose that the presence of communicative intentions can be recognized in others ’ behaviour before the content of these intentions is accessed or inferred. Second, I claim that such recognition can be achieved by decoding specialized ostensive signals. Third, I argue on empirical bases that, by decoding ostensive signals, human infants are capable of recognizing communicative intentions addressed to them. Thus, learning about actual modes of communication benefits from, and is guided by, infants ’ preparedness to detect infant-directed ostensive communication. 1. Ostensive Signals According to the Gricean analysis of meaning (1989), a communicative act intends to fulfil two (or perhaps three) intentions simultaneously. By producing the act, the communicator intends (1) to generate a certain response in the audience, (2) to let the audience recognize the intention specified in (1), and (3) to make the audience fulfil the first intention on the basis of fulfilling the second one. This analysis proposes that, unlike instrumental actions, which can normally be explained by},
	journal = {Mind \& Language},
	author = {Csibra, Gergely},
	year = {2010},
	pages = {141--168},
	file = {Citeseer - Snapshot:/home/user/Zotero/storage/T25P45NV/summary.html:text/html;Csibra_2010_Recognizing communicative intentions in infancy.pdf:/home/user/Zotero/storage/BPIS867Q/Csibra_2010_Recognizing communicative intentions in infancy.pdf:application/pdf}
}

@article{scontras-subjectivity-2017,
	title = {Subjectivity {Predicts} {Adjective} {Ordering} {Preferences}},
	volume = {1},
	issn = {2470-2986},
	url = {http://www.mitpressjournals.org/doi/10.1162/OPMI\_a\_00005},
	doi = {10.1162/OPMI\_a\_00005},
	language = {en},
	number = {1},
	urldate = {2017-03-15},
	journal = {Open Mind},
	author = {Scontras, Gregory and Degen, Judith and Goodman, Noah D.},
	month = feb,
	year = {2017},
	pages = {53--66},
	file = {OPMI_a_00005:/home/user/Zotero/storage/2EW2T588/OPMI_a_00005.pdf:application/pdf}
}

@article{li-adversarial-2017,
	title = {Adversarial {Learning} for {Neural} {Dialogue} {Generation}},
	url = {https://arxiv.org/abs/1701.06547},
	urldate = {2017-03-15},
	journal = {arXiv preprint arXiv:1701.06547},
	author = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Ritter, Alan and Jurafsky, Dan},
	year = {2017},
	file = {() - 1701.06547.pdf:/home/user/Zotero/storage/XBX3VGE4/1701.06547.pdf:application/pdf}
}

@article{tobin-effects-2010,
	title = {Effects of anticipatory coarticulation on lexical access},
	url = {http://palm.mindmodeling.org/cogsci2010/papers/0517/paper0517.pdf},
	urldate = {2017-03-15},
	journal = {Proc. Mtgs. Cognitive Sci},
	author = {Tobin, Stephen J. and Cho, Pyeong Whan and Jennett, Patrick M. and Magnuson, James S.},
	year = {2010},
	pages = {2200--2205},
	file = {Tobin_etal_CogSci_2010_final - paper0517.pdf:/home/user/Zotero/storage/MSTTNBID/paper0517.pdf:application/pdf}
}

@article{scontras-subjectivity-2017-1,
	title = {Subjectivity {Predicts} {Adjective} {Ordering} {Preferences}},
	volume = {1},
	issn = {2470-2986},
	url = {http://www.mitpressjournals.org/doi/10.1162/OPMI\_a\_00005},
	doi = {10.1162/OPMI\_a\_00005},
	language = {en},
	number = {1},
	urldate = {2017-03-15},
	journal = {Open Mind},
	author = {Scontras, Gregory and Degen, Judith and Goodman, Noah D.},
	month = feb,
	year = {2017},
	pages = {53--66},
	file = {OPMI_a_00005:/home/user/Zotero/storage/U7JAZCZZ/OPMI_a_00005.pdf:application/pdf}
}

@inproceedings{chen-stochastic-2014,
	title = {Stochastic {Gradient} {Hamiltonian} {Monte} {Carlo}.},
	url = {http://www.jmlr.org/proceedings/papers/v32/cheni14.pdf},
	urldate = {2017-03-15},
	booktitle = {{ICML}},
	author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
	year = {2014},
	pages = {1683--1691},
	file = {Stochastic Gradient Hamiltonian Monte Carlo - 1402.4102.pdf:/home/user/Zotero/storage/5UUDU2CB/1402.4102.pdf:application/pdf}
}

@inproceedings{welling-bayesian-2011,
	title = {Bayesian learning via stochastic gradient {Langevin} dynamics},
	url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Welling\_398.pdf},
	urldate = {2017-03-15},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Machine} {Learning} ({ICML}-11)},
	author = {Welling, Max and Teh, Yee W.},
	year = {2011},
	pages = {681--688},
	file = {Bayesian Learning via Stochastic Gradient Langevin Dynamics - stoclangevin_v6.pdf:/home/user/Zotero/storage/5VC4MBQ7/stoclangevin_v6.pdf:application/pdf}
}

@article{brown-metrical-2015,
	title = {Metrical expectations from preceding prosody influence perception of lexical stress},
	volume = {41},
	issn = {0096-1523},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4380594/},
	doi = {10.1037/a0038689},
	abstract = {Two visual-world experiments tested the hypothesis that expectations based on preceding prosody influence the perception of suprasegmental cues to lexical stress. The results demonstrate that listeners’ consideration of competing alternatives with different stress patterns (e.g., ‘jury/gi’raffe) can be influenced by the fundamental frequency and syllable timing patterns across material preceding a target word. When preceding stressed syllables distal to the target word shared pitch and timing characteristics with the first syllable of the target word, pictures of alternatives with primary lexical stress on the first syllable (e.g., jury) initially attracted more looks than alternatives with unstressed initial syllables (e.g., giraffe). This effect was modulated when preceding unstressed syllables had pitch and timing characteristics similar to the initial syllable of the target word, with more looks to alternatives with unstressed initial syllables (e.g., giraffe) than to those with stressed initial syllables (e.g., jury). These findings suggest that expectations about the acoustic realization of upcoming speech include information about metrical organization and lexical stress, and that these expectations constrain the initial interpretation of suprasegmental stress cues. These distal prosody effects implicate on-line probabilistic inferences about the sources of acoustic-phonetic variation during spoken-word recognition.},
	number = {2},
	urldate = {2017-03-15},
	journal = {Journal of experimental psychology. Human perception and performance},
	author = {Brown, Meredith and Salverda, Anne Pier and Dilley, Laura C. and Tanenhaus, Michael K.},
	month = apr,
	year = {2015},
	pmid = {25621583},
	pmcid = {PMC4380594},
	pages = {306--323},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/X38QUHNK/Brown et al. - 2015 - Metrical expectations from preceding prosody influ.pdf:application/pdf}
}

@misc{noauthor-limits-nodate,
	title = {Limits on prediction in language comprehension: {A} multi-lab failure to replicate evidence for probabilistic pre-activation of phonology {\textbar} {bioRxiv}},
	url = {http://www.biorxiv.org/content/early/2017/02/25/111807},
	urldate = {2017-03-15},
	file = {Limits on prediction in language comprehension\: A multi-lab failure to replicate evidence for probabilistic pre-activation of phonology | bioRxiv:/home/user/Zotero/storage/DN9ZSCJT/111807.html:text/html}
}

@article{baese-berk-revisiting-2016,
	title = {Revisiting {Neil} {Armstrongs} {Moon}-{Landing} {Quote}: {Implications} for {Speech} {Perception}, {Function} {Word} {Reduction}, and {Acoustic} {Ambiguity}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Revisiting {Neil} {Armstrongs} {Moon}-{Landing} {Quote}},
	url = {http://dx.plos.org/10.1371/journal.pone.0155975},
	doi = {10.1371/journal.pone.0155975},
	language = {en},
	number = {9},
	urldate = {2017-03-14},
	journal = {PLOS ONE},
	author = {Baese-Berk, Melissa M. and Dilley, Laura C. and Schmidt, Stephanie and Morrill, Tuuli H. and Pitt, Mark A.},
	editor = {McLoughlin, Ian},
	month = sep,
	year = {2016},
	pages = {e0155975},
	file = {Revisiting Neil Armstrongs Moon-Landing Quote\: Implications for Speech Perception, Function Word Reduction, and Acoustic Ambiguity - Baese-Berk, Dilley, Schmidt, Morrill, Mark 2016.pdf:/home/user/Zotero/storage/WX2MATZ9/Baese-Berk, Dilley, Schmidt, Morrill, Mark 2016.pdf:application/pdf}
}

@article{banzina-role-2016,
	title = {The {Role} of {Secondary}-{Stressed} and {Unstressed}–{Unreduced} {Syllables} in {Word} {Recognition}: {Acoustic} and {Perceptual} {Studies} with {Russian} {Learners} of {English}},
	volume = {45},
	issn = {0090-6905, 1573-6555},
	shorttitle = {The {Role} of {Secondary}-{Stressed} and {Unstressed}–{Unreduced} {Syllables} in {Word} {Recognition}},
	url = {http://link.springer.com/10.1007/s10936-015-9377-z},
	doi = {10.1007/s10936-015-9377-z},
	language = {en},
	number = {4},
	urldate = {2017-03-14},
	journal = {Journal of Psycholinguistic Research},
	author = {Banzina, Elina and Dilley, Laura C. and Hewitt, Lynne E.},
	month = aug,
	year = {2016},
	pages = {813--831},
	file = {The Role of Secondary-Stressed and Unstressed–Unreduced Syllables in Word Recognition\: Acoustic and Perceptual Studies with Russian Learners of English - Banzina Dilley Hewitt 2016.pdf:/home/user/Zotero/storage/QCSCB3EP/Banzina Dilley Hewitt 2016.pdf:application/pdf}
}

@article{pitt-rate-2016,
	title = {Rate dependent speech processing can be speech specific: {Evidence} from the perceptual disappearance of words under changes in context speech rate},
	volume = {78},
	issn = {1943-3921, 1943-393X},
	shorttitle = {Rate dependent speech processing can be speech specific},
	url = {http://link.springer.com/10.3758/s13414-015-0981-7},
	doi = {10.3758/s13414-015-0981-7},
	language = {en},
	number = {1},
	urldate = {2017-03-14},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Pitt, Mark A. and Szostak, Christine and Dilley, Laura C.},
	month = jan,
	year = {2016},
	pages = {334--345},
	file = {13414_2015_981_Article 334..345 - Pitt Szostak Dilley 2016.pdf:/home/user/Zotero/storage/RU429H23/Pitt Szostak Dilley 2016.pdf:application/pdf}
}

@article{morrill-interactions-2015,
	title = {Interactions between distal speech rate, linguistic knowledge, and speech environment},
	volume = {22},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-015-0820-9},
	doi = {10.3758/s13423-015-0820-9},
	language = {en},
	number = {5},
	urldate = {2017-03-14},
	journal = {Psychonomic Bulletin \& Review},
	author = {Morrill, Tuuli and Baese-Berk, Melissa and Heffner, Christopher and Dilley, Laura},
	month = oct,
	year = {2015},
	pages = {1451--1457},
	file = {13423_2015_820_Article 1451..1457 - Morrill Baese-Berk Heffner Dilley 2015.pdf:/home/user/Zotero/storage/X2RXSQ9M/Morrill Baese-Berk Heffner Dilley 2015.pdf:application/pdf}
}

@article{morrill-distal-2015,
	title = {Distal prosody affects learning of novel words in an artificial language},
	volume = {22},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-014-0733-z},
	doi = {10.3758/s13423-014-0733-z},
	language = {en},
	number = {3},
	urldate = {2017-03-14},
	journal = {Psychonomic Bulletin \& Review},
	author = {Morrill, Tuuli H. and McAuley, J. Devin and Dilley, Laura C. and Zdziarska, Patrycja A. and Jones, Katherine B. and Sanders, Lisa D.},
	month = jun,
	year = {2015},
	pages = {815--823},
	file = {Morrill McAuley Dilley Zdziarska Jones Sanders 2015.pdf:/home/user/Zotero/storage/VAV2ASIR/Morrill McAuley Dilley Zdziarska Jones Sanders 2015.pdf:application/pdf}
}

@inproceedings{brown-metrical-2012,
	title = {Metrical expectations from preceding prosody influence spoken word recognition.},
	url = {https://mindmodeling.org/cogsci2012/papers/0246/paper0246.pdf},
	urldate = {2017-03-14},
	booktitle = {{CogSci}},
	author = {Brown, Meredith and Salverda, Anne Pier and Dilley, Laura and Tanenhaus, Michael K.},
	year = {2012},
	file = {Brown, Salverda, Dilley, & Tanenhaus 2012.pdf:/home/user/Zotero/storage/8J6J22K7/Brown, Salverda, Dilley, & Tanenhaus 2012.pdf:application/pdf}
}

@article{baese-berk-long-term-2014,
	title = {Long-{Term} {Temporal} {Tracking} of {Speech} {Rate} {Affects} {Spoken}-{Word} {Recognition}},
	volume = {25},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797614533705},
	doi = {10.1177/0956797614533705},
	language = {en},
	number = {8},
	urldate = {2017-03-14},
	journal = {Psychological Science},
	author = {Baese-Berk, Melissa M. and Heffner, Christopher C. and Dilley, Laura C. and Pitt, Mark A. and Morrill, Tuuli H. and McAuley, J. Devin},
	month = aug,
	year = {2014},
	pages = {1546--1553},
	file = {Baese-Berk, Heffner, Dilley, Pitt, Morrill, & McAuley 2014.pdf:/home/user/Zotero/storage/IGZ2GZWC/Baese-Berk, Heffner, Dilley, Pitt, Morrill, & McAuley 2014.pdf:application/pdf}
}

@article{morrill-prosodic-2014,
	title = {Prosodic patterning in distal speech context: {Effects} of list intonation and f0 downtrend on perception of proximal prosodic structure},
	volume = {46},
	issn = {00954470},
	shorttitle = {Prosodic patterning in distal speech context},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0095447014000552},
	doi = {10.1016/j.wocn.2014.06.001},
	language = {en},
	urldate = {2017-03-14},
	journal = {Journal of Phonetics},
	author = {Morrill, Tuuli H. and Dilley, Laura C. and McAuley, J. Devin},
	month = sep,
	year = {2014},
	pages = {68--85},
	file = {Prosodic patterning in distal speech context_ Effects of list intonation and f0 downtrend on perception of proximal prosodic structure - Morrill, Dilley, & McAuley 2014.pdf:/home/user/Zotero/storage/KZMHBTXV/Morrill, Dilley, & McAuley 2014.pdf:application/pdf}
}

@article{morrill-distal-2014,
	title = {Distal rhythm influences whether or not listeners hear a word in continuous speech: {Support} for a perceptual grouping hypothesis},
	volume = {131},
	issn = {00100277},
	shorttitle = {Distal rhythm influences whether or not listeners hear a word in continuous speech},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027713002503},
	doi = {10.1016/j.cognition.2013.12.006},
	language = {en},
	number = {1},
	urldate = {2017-03-14},
	journal = {Cognition},
	author = {Morrill, Tuuli H. and Dilley, Laura C. and McAuley, J. Devin and Pitt, Mark A.},
	month = apr,
	year = {2014},
	pages = {69--74},
	file = {Distal rhythm influences whether or not listeners hear a word in continuous speech\: Support for a perceptual grouping hypothesis - Morrill, Dilley, McAuley, & Pitt 2014.pdf:/home/user/Zotero/storage/8AE6GJHP/Morrill, Dilley, McAuley, & Pitt 2014.pdf:application/pdf}
}

@article{dilley-new-2013,
	title = {New tests of the distal speech rate effect: examining cross-linguistic generalization},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {New tests of the distal speech rate effect},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.01002/abstract},
	doi = {10.3389/fpsyg.2013.01002},
	urldate = {2017-03-14},
	journal = {Frontiers in Psychology},
	author = {Dilley, Laura C. and Morrill, Tuuli H. and Banzina, Elina},
	year = {2013},
	file = {New tests of the distal speech rate effect\: examining cross-linguistic generalization - Dilley, Morrill, Banzina 2013.pdf:/home/user/Zotero/storage/3C5NPXDT/Dilley, Morrill, Banzina 2013.pdf:application/pdf}
}

@article{heffner-when-2013,
	title = {When cues combine: {How} distal and proximal acoustic cues are integrated in word segmentation},
	volume = {28},
	issn = {0169-0965, 1464-0732},
	shorttitle = {When cues combine},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2012.672229},
	doi = {10.1080/01690965.2012.672229},
	language = {en},
	number = {9},
	urldate = {2017-03-14},
	journal = {Language and Cognitive Processes},
	author = {Heffner, Christopher C. and Dilley, Laura C. and McAuley, J. Devin and Pitt, Mark A.},
	month = nov,
	year = {2013},
	pages = {1275--1302},
	file = {When cues combine\: How distal and proximal acoustic cues are integrated in word segmentation - Heffner, Dilley, McAuley, & Pitt 2013.pdf:/home/user/Zotero/storage/HN797AVM/Heffner, Dilley, McAuley, & Pitt 2013.pdf:application/pdf}
}

@article{brown-expectations-2011,
	title = {Expectations from preceding prosody influence segmentation in online sentence processing},
	volume = {18},
	issn = {1069-9384, 1531-5320},
	url = {http://www.springerlink.com/index/10.3758/s13423-011-0167-9},
	doi = {10.3758/s13423-011-0167-9},
	language = {en},
	number = {6},
	urldate = {2017-03-14},
	journal = {Psychonomic Bulletin \& Review},
	author = {Brown, Meredith and Salverda, Anne Pier and Dilley, Laura C. and Tanenhaus, Michael K.},
	month = dec,
	year = {2011},
	pages = {1189--1196},
	file = {Brown Salverda Dilley Tanenhaus PBR 2011.pdf:/home/user/Zotero/storage/T4293B5K/Brown Salverda Dilley Tanenhaus PBR 2011.pdf:application/pdf}
}

@article{pitt-exploring-2011,
	title = {Exploring the role of exposure frequency in recognizing pronunciation variants},
	volume = {39},
	issn = {00954470},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0095447010000586},
	doi = {10.1016/j.wocn.2010.07.004},
	language = {en},
	number = {3},
	urldate = {2017-03-14},
	journal = {Journal of Phonetics},
	author = {Pitt, Mark A. and Dilley, Laura and Tat, Michael},
	month = jul,
	year = {2011},
	pages = {304--311},
	file = {Exploring the role of exposure frequency in recognizing pronunciation variants - Pitt, Dilley & Tat 2011.pdf:/home/user/Zotero/storage/P4SSB3GJ/Pitt, Dilley & Tat 2011.pdf:application/pdf}
}

@article{peramunage-phonological-2010,
	title = {Phonological {Neighborhood} {Effects} in {Spoken} {Word} {Production}: {An} {fMRI} {Study}},
	volume = {23},
	issn = {0898-929X},
	shorttitle = {Phonological {Neighborhood} {Effects} in {Spoken} {Word} {Production}},
	url = {http://dx.doi.org/10.1162/jocn.2010.21489},
	doi = {10.1162/jocn.2010.21489},
	abstract = {The current study examined the neural systems underlying lexically conditioned phonetic variation in spoken word production. Participants were asked to read aloud singly presented words, which either had a voiced minimal pair (MP) neighbor (e.g., cape) or lacked a minimal pair (NMP) neighbor (e.g., cake). The voiced neighbor never appeared in the stimulus set. Behavioral results showed longer voice-onset time for MP target words, replicating earlier behavioral results [Baese-Berk, M., \& Goldrick, M. Mechanisms of interaction in speech production. Language and Cognitive Processes, 24, 527–554, 2009]. fMRI results revealed reduced activation for MP words compared to NMP words in a network including left posterior superior temporal gyrus, the supramarginal gyrus, inferior frontal gyrus, and precentral gyrus. These findings support cascade models of spoken word production and show that neural activation at the lexical level modulates activation in those brain regions involved in lexical selection, phonological planning, and, ultimately, motor plans for production. The facilitatory effects for words with MP neighbors suggest that competition effects reflect the overlap inherent in the phonological representation of the target word and its MP neighbor.},
	number = {3},
	urldate = {2017-03-14},
	journal = {Journal of Cognitive Neuroscience},
	author = {Peramunage, Dasun and Blumstein, Sheila E. and Myers, Emily B. and Goldrick, Matthew and Baese-Berk, Melissa},
	month = mar,
	year = {2010},
	pages = {593--603},
	file = {Journal of Cognitive Neuroscience Snapshot:/home/user/Zotero/storage/HX3G3HCE/jocn.2010.html:text/html;Peramunage et al_2010_Phonological Neighborhood Effects in Spoken Word Production.pdf:/home/user/Zotero/storage/4NX9GMM4/Peramunage et al_2010_Phonological Neighborhood Effects in Spoken Word Production.pdf:application/pdf}
}

@article{salverda-immediate-2014,
	title = {Immediate effects of anticipatory coarticulation in spoken-word recognition},
	volume = {71},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X13001113},
	doi = {10.1016/j.jml.2013.11.002},
	language = {en},
	number = {1},
	urldate = {2017-03-14},
	journal = {Journal of Memory and Language},
	author = {Salverda, Anne Pier and Kleinschmidt, Dave and Tanenhaus, Michael K.},
	month = feb,
	year = {2014},
	pages = {145--163},
	file = {Salverda_etal_2014.pdf:/home/user/Zotero/storage/FQSJWWWI/Salverda_etal_2014.pdf:application/pdf}
}

@article{yu-rapid-2007,
	title = {Rapid {Word} {Learning} {Under} {Uncertainty} via {Cross}-{Situational} {Statistics}},
	volume = {18},
	issn = {0956-7976},
	url = {http://pss.sagepub.com/lookup/doi/10.1111/j.1467-9280.2007.01915.x},
	doi = {10.1111/j.1467-9280.2007.01915.x},
	language = {en},
	number = {5},
	urldate = {2017-03-08},
	journal = {Psychological Science},
	author = {Yu, C. and Smith, L. B.},
	month = may,
	year = {2007},
	pages = {414--420},
	file = {Yu&Smith-PsySci2007.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Yu&Smith-PsySci2007.pdf:application/pdf}
}

@inproceedings{scott-infant-2013,
	title = {Infant contributions to joint attention predict vocabulary development},
	url = {http://pubman.mpdl.mpg.de/pubman/faces/viewItemOverviewPage.jsp?itemId=escidoc:1824896},
	urldate = {2017-03-08},
	booktitle = {{CogSci} 2013: the 35th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	publisher = {Cognitive Science Society},
	author = {Scott, Katherine and Sakkalou, Elena and Ellis-Davies, Kate and Hilbrink, Elma and Hahn, Ulrike and Gattis, Merideth},
	year = {2013},
	pages = {3384--3389},
	file = {Scott&-CogSciSoc2013.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Scott&-CogSciSoc2013.pdf:application/pdf}
}

@article{perra-attention-2012,
	title = {Attention engagement in early infancy},
	volume = {35},
	issn = {0163-6383},
	url = {http://www.sciencedirect.com/science/article/pii/S0163638312000641},
	doi = {10.1016/j.infbeh.2012.06.004},
	abstract = {We report a longitudinal study investigating developmental changes in the structure of attention engagement during early infancy. Forty-three infants were observed monthly from 2 to 4 months. Attention engagement was assessed from play interactions with parents, using a coding system developed by Bakeman and Adamson (1984). The results indicated a developmental transition in attention engagement at 3 months: after this age infants engaged for longer periods and in a wider variety of states. Most infants displayed person engagement at 2 months, passive joint engagement at 3 months, and object engagement at 4 months. To address whether emerging abilities of attention engagement allow infants to follow the attention of social partners, we compared attention engagement to performance on an experimental measure of attention control (reported by Perra \&amp; Gattis, 2010). Analyses revealed a positive relation between passive joint engagement and checking back, suggesting that changes in passive joint engagement reflect the development in attention control.},
	number = {4},
	urldate = {2017-03-08},
	journal = {Infant Behavior and Development},
	author = {Perra, Oliver and Gattis, Merideth},
	month = dec,
	year = {2012},
	keywords = {Attention, Development, infancy, Attention control, Attention engagement, Early development, Longitudinal study, Social attention},
	pages = {635--644},
	file = {Perra_Gattis_2012_Attention engagement in early infancy.pdf:/home/user/Zotero/storage/UMCPGHJW/Perra_Gattis_2012_Attention engagement in early infancy.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/9CJIP8EP/S0163638312000641.html:text/html}
}

@article{childers-joint-2007,
	title = {Joint {Attention} and {Word} {Learning} in {Ngas}-speaking {Toddlers} in {Nigeria}},
	volume = {34},
	issn = {0305-0009},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3652572/},
	abstract = {This study examines infants’ joint attention behavior and language development in a rural village in Nigeria. Participants included 8 younger (1;0 to 1;5, M age = 1;2) and 8 older toddlers (1;7 to 2;7, M age = 2;1). Joint attention behaviors in social interaction contexts were recorded and coded at two time points 6 months apart. Analyses revealed that these toddlers were producing more high level joint attention behaviors than less complex behaviors. In addition, the quality and quantity of behaviors produced by these Nigerian children was similar to those found in other cultures. In analyses of children’s noun and verb comprehension and production (in relation to the number of nouns or verbs on a parental checklist), parents reported proportionally more verbs than nouns, perhaps because Ngas has some linguistic characteristics that are similar to languages in which a noun bias is not seen (e.g., Mandarin Chinese). An examination of the interrelations of joint attention and language development revealed that joint attention behaviors were related to both noun and verb development at different times. The set of results is important for understanding the emergence of joint attention in traditional cultures, the comprehension and production of nouns and verbs given the specific linguistic properties of a language, and the importance that early social contexts may have for language development.},
	number = {2},
	urldate = {2017-03-08},
	journal = {Journal of child language},
	author = {Childers, Jane B. and Vaughan, Julie and Burquest, Donald A.},
	month = may,
	year = {2007},
	pmid = {17542156},
	pmcid = {PMC3652572},
	pages = {199--225},
	file = {Childers et al_2007_Joint Attention and Word Learning in Ngas-speaking Toddlers in Nigeria.pdf:/home/user/Zotero/storage/6VBXIX6V/Childers et al_2007_Joint Attention and Word Learning in Ngas-speaking Toddlers in Nigeria.pdf:application/pdf}
}

@article{baldwin-infants-1991,
	title = {Infants' contribution to the achievement of joint reference},
	volume = {62},
	issn = {0009-3920},
	abstract = {This research examines whether infants actively contribute to the achievement of joint reference. One possibility is that infants tend to link a a label with whichever object they are focused on when they hear the label. If so, infants would make a mapping error when an adult labels a different object than the one occupying their focus. Alternatively, infants may be able to use a speaker's nonverbal cues (e.g., line of regard) to interpret the reference of novel labels. This ability would allow infants to avoid errors when adult labels conflict with infants' focus. 64 16-19-month-olds were taught new labels for novel toys in 2 situations. In follow-in labeling, the experimenter looked at and labeled a toy at which infants were already looking. In discrepant labeling, the experimenter looked at and labeled a different toy than the one occupying infants' focus. Infants' responses to subsequent comprehension questions revealed that they (a) successfully learned the labels introduced during follow-in labeling, and (b) displayed no tendency to make mapping errors after discrepant labeling. Thus infants of only 16 to 19 months understand that a speaker's nonverbal cues are relevant to the reference of object labels; they already can contribute to the social coordination involved in achieving joint reference.},
	language = {eng},
	number = {5},
	journal = {Child Development},
	author = {Baldwin, D. A.},
	month = oct,
	year = {1991},
	pmid = {1756664},
	keywords = {Humans, Language Development, psycholinguistics, Attention, Vocabulary, Achievement, Infant, Psychology, Child, Verbal Learning},
	pages = {875--890}
}

@article{tomasello-cultural-1993,
	title = {Cultural learning},
	volume = {16},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/cultural-learning/C1CB7EA0CF7D53D14AA31633D7AF82BF},
	doi = {10.1017/S0140525X0003123X},
	abstract = {AbstractThis target article presents a theory of human cultural learning. Cultural learning is identified with those instances of social learning in which intersubjectivity or perspective-taking plays a vital role, both in the original learning process and in the resulting cognitive product. Cultural learning manifests itself in three forms during human ontogeny: imitative learning, instructed learning, and collaborative learning – in that order. Evidence is provided that this progression arises from the developmental ordering of the underlying social-cognitive concepts and processes involved. Imitative learning relies on a concept of intentional agent and involves simple perspective-taking. Instructed learning relies on a concept of mental agent and involves alternating/coordinated perspective-taking (intersubjectivity). Collaborative learning relies on a concept of reflective agent and involves integrated perspective-taking (reflective intersubjectivity). A comparison of normal children, autistic children and wild and enculturated chimpanzees provides further evidence for these correlations between social cognition and cultural learning. Cultural learning is a uniquely human form of social learning that allows for a fidelity of transmission of behaviors and information among conspecifics not possible in other forms of social learning, thereby providing the psychological basis for cultural evolution.},
	number = {3},
	urldate = {2017-03-08},
	journal = {Behavioral and Brain Sciences},
	author = {Tomasello, Michael and Kruger, Ann Cale and Ratner, Hilary Horn},
	month = sep,
	year = {1993},
	keywords = {Attention, Cognitive development, culture, theory of mind, animal cognition, collaboration, cultural learning, imitation, instruction, intentionality, intersubjectivity, social learning},
	pages = {495--511},
	file = {Snapshot:/home/user/Zotero/storage/P67KD4DR/C1CB7EA0CF7D53D14AA31633D7AF82BF.html:text/html}
}

@article{adamson-affect-1985-1,
	title = {Affect and {Attention}: {Infants} {Observed} with {Mothers} and {Peers}},
	volume = {56},
	issn = {0009-3920},
	shorttitle = {Affect and {Attention}},
	url = {http://www.jstor.org/stable/1129748},
	doi = {10.2307/1129748},
	abstract = {This study documents the rate, mean duration, and mode of infants' affective displays. Using a cross-sequential design, infants were observed in their homes from 6 to 18 months playing with their mothers, with peers, and alone. Affect rates were higher with mothers than peers. With increasing age, affect rates as well as the vocal mode increased, while mean durations and facial and motoric modes decreased. Affect was most likely when infants were engaged with mothers or peers in person play. It also occurred often when infants first became engaged with the same object their partner was manipulating; with mothers (but not peers) affect continued to be expressed throughout these periods of shared object play. Rates were elevated when mothers moved objects repetitively. Discussion focuses on how infants' earlier-developing affective communication skills may continue to be used as they begin to explore the world of objects and on how adults may support this integration of expressive and referential communication.},
	number = {3},
	urldate = {2017-03-08},
	journal = {Child Development},
	author = {Adamson, Lauren B. and Bakeman, Roger},
	year = {1985},
	pages = {582--593}
}

@inproceedings{pusiol-discovering-2014,
	title = {Discovering the {Signatures} of {Joint} {Attention} in {Child}-{Caregiver} {Interaction}.},
	url = {http://langcog.stanford.edu/papers/PSFF-underreview.pdf},
	urldate = {2017-03-08},
	booktitle = {{CogSci}},
	author = {Pusiol, Guido and Soriano, Laura and Frank, Michael C. and Fei-Fei, Li},
	year = {2014},
	file = {guido14.pdf:/home/user/Zotero/storage/FZW4RHBR/guido14.pdf:application/pdf}
}

@article{woodward-infants-1999,
	title = {Infants' learning about words and sounds in relation to objects},
	volume = {70},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-8624.00006/full},
	number = {1},
	urldate = {2017-03-08},
	journal = {Child development},
	author = {Woodward, Amanda and Hoyne, Karen},
	year = {1999},
	pages = {65--77},
	file = {Infants' Learning about Words and Sounds in Relation to Objects - Woodward&Hoyne.pdf:/home/user/Zotero/storage/UFCNFFU5/Woodward&Hoyne.pdf:application/pdf}
}

@inproceedings{yu-investigating-2010,
	title = {Investigating multimodal real-time patterns of joint attention in an hri word learning task},
	url = {http://dl.acm.org/citation.cfm?id=1734561},
	urldate = {2017-03-08},
	booktitle = {Proceedings of the 5th {ACM}/{IEEE} international conference on {Human}-robot interaction},
	publisher = {IEEE Press},
	author = {Yu, Chen and Scheutz, Matthias and Schermerhorn, Paul},
	year = {2010},
	pages = {309--316},
	file = {chenscheutzschermerhorn10hri.pdf:/home/user/Zotero/storage/7PC2QXZA/chenscheutzschermerhorn10hri.pdf:application/pdf}
}

@article{williams-joint-2016,
	title = {Joint {Attention} in {Toddler} {Vocabulary} {Acquisition}},
	url = {http://scholar.colorado.edu/honr\_theses/1257/},
	urldate = {2017-03-08},
	author = {Williams, Jessica},
	year = {2016},
	file = {Joint Attention in Toddler Vocabulary Acquisition - viewcontent.cgi:/home/user/Zotero/storage/HC25SQKA/viewcontent.pdf:application/pdf}
}

@article{flom-nine-month-olds-2004,
	title = {Nine-month-olds’ shared visual attention as a function of gesture and object location},
	volume = {27},
	issn = {01636383},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0163638304000177},
	doi = {10.1016/j.infbeh.2003.09.007},
	language = {en},
	number = {2},
	urldate = {2017-03-08},
	journal = {Infant Behavior and Development},
	author = {Flom, Ross and Deák, Gedeon O and Phill, Christina G and Pick, Anne D},
	month = may,
	year = {2004},
	pages = {181--194},
	file = {doi\:10.1016/j.infbeh.2003.09.007 - FlometalIBD2004.pdf:/home/user/Zotero/storage/3KCT7MJJ/FlometalIBD2004.pdf:application/pdf}
}

@article{flom-increasing-2007,
	title = {Increasing specificity and the development of joint visual attention},
	url = {http://www.academia.edu/download/46816526/Increasing\_Specificity\_and\_the\_Developme20160626-3189-1tiljin.pdf},
	urldate = {2017-03-08},
	journal = {Gaze-following: Its development and significance},
	author = {Flom, Ross and Pick, Anne D.},
	year = {2007},
	pages = {95--111},
	file = {ch05_8048_Flom_LEA - Flom Pick 2007.pdf:/home/user/Zotero/storage/9KRD3HWN/Flom Pick 2007.pdf:application/pdf}
}

@article{moll-12-and-2004,
	title = {12-and 18-month-old infants follow gaze to spaces behind barriers},
	volume = {7},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-7687.2004.00315.x/full},
	number = {1},
	urldate = {2017-03-08},
	journal = {Developmental science},
	author = {Moll, Henrike and Tomasello, Michael},
	year = {2004},
	pages = {F1--F9},
	file = {DESC_315.fm - Moll_Tomasello04.pdf:/home/user/Zotero/storage/MFDXCCKG/Moll_Tomasello04.pdf:application/pdf}
}

@article{butterworth-what-1991,
	title = {What minds have in common is space: {Spatial} mechanisms serving joint visual attention in infancy},
	volume = {9},
	issn = {2044-835X},
	shorttitle = {What minds have in common is space},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2044-835X.1991.tb00862.x/abstract},
	doi = {10.1111/j.2044-835X.1991.tb00862.x},
	abstract = {A series of experiments is reported which show that three successive mechanisms are involved in the first 18 months of life in ‘looking where someone else is looking’. The earliest ‘ecological’ mechanism enables the infant to detect the direction of the adult's visual gaze within the baby's visual field but the mother's signal alone does not allow the precise localization of the target. Joint attention to the same physical object also depends on the intrinsic, attention-capturing properties of the object in the environment. By about 12 months, we have evidence for presence of a new ‘geometric’ mechanism. The infant extrapolates from the orientation of the mother's head and eyes, the intersection of the mother's line of sight within a relatively precise zone of the infant's own visual space. A third ‘representational’ mechanism emerges between 12 and 18 months, with an extension of joint reference to places outside the infant's visual field. None of these mechanisms require the infant to have a theory that others have minds; rather the perceptual systems of different observers ‘meet’ in encountering the same objects and events in the world. Such a ‘realist’ basis for interpersonal knowledge may offer an alternative starting point for development of intrapersonal knowledge, rather than the view that mental events can only be known by construction of a theory.},
	language = {en},
	number = {1},
	urldate = {2017-03-08},
	journal = {British Journal of Developmental Psychology},
	author = {Butterworth, George and Jarrett, Nicholas},
	month = mar,
	year = {1991},
	pages = {55--72},
	file = {Butterworth_Jarrett_1991_What minds have in common is space.pdf:/home/user/Zotero/storage/B4ZMSCHG/Butterworth_Jarrett_1991_What minds have in common is space.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/PAHPDK9V/abstract.html:text/html}
}

@article{scaife-capacity-1975,
	title = {The capacity for joint visual attention in the infant},
	volume = {253},
	issn = {0028-0836},
	language = {eng},
	number = {5489},
	journal = {Nature},
	author = {Scaife, M. and Bruner, J. S.},
	month = jan,
	year = {1975},
	pmid = {1113842},
	keywords = {Humans, Eye Movements, Attention, Orientation, Infant, Child Development, Vision, Ocular},
	pages = {265--266}
}

@article{corkum-origins-1998,
	title = {The origins of joint visual attention in infants},
	volume = {34},
	issn = {0012-1649},
	abstract = {Two experiments examined the origins of joint visual attention with a training procedure. In Experiment 1, infants aged 6-11 months were tested for a gaze-following (joint visual attention) response under feedback and no feedback conditions. In Experiment 2, infants 8-9 months received feedback for either following the experimenter's gaze (natural group) or looking to the opposite side (unnatural group). Results of the 2 experiments indicate that (a) joint visual attention does not reliably appear prior to 10 months of age, (b) from about 8 months of age, a gaze-following response can be learned, and (c) simple learning is not sufficient as the mechanism through which joint attention cues acquire their signal value.},
	language = {eng},
	number = {1},
	journal = {Developmental Psychology},
	author = {Corkum, V. and Moore, C.},
	month = jan,
	year = {1998},
	pmid = {9471002},
	keywords = {Humans, Female, Male, Attention, visual perception, Infant, Age Factors, Conditioning (Psychology), Feedback},
	pages = {28--38}
}

@article{brooks-development-2005,
	title = {The development of gaze following and its relation to language},
	volume = {8},
	issn = {1363-755X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3640988/},
	doi = {10.1111/j.1467-7687.2005.00445.x},
	abstract = {We examined the ontogeny of gaze following by testing infants at 9, 10 and 11 months of age. Infants (N = 96) watched as an adult turned her head toward a target with either open or closed eyes. The 10- and 11-month-olds followed adult turns significantly more often in the open-eyes than the closed-eyes condition, but the 9-month-olds did not respond differentially. Although 9-month-olds may view others as ‘body orienters’, older infants begin to register whether others are ‘visually connected’ to the external world and, hence, understand adult looking in a new way. Results also showed a strong positive correlation between gaze-following behavior at 10–11 months and subsequent language scores at 18 months. Implications for social cognition are discussed in light of the developmental shift in gaze following between 9 and 11 months of age.},
	number = {6},
	urldate = {2017-03-08},
	journal = {Developmental science},
	author = {Brooks, Rechele and Meltzoff, Andrew N.},
	month = nov,
	year = {2005},
	pmid = {16246245},
	pmcid = {PMC3640988},
	pages = {535--543},
	file = {Brooks_Meltzoff_2005_The development of gaze following and its relation to language.pdf:/home/user/Zotero/storage/C9MXGTIF/Brooks_Meltzoff_2005_The development of gaze following and its relation to language.pdf:application/pdf}
}

@article{lord-language-1996,
	title = {Language level and nonverbal social-communicative behaviors in autistic and language-delayed children},
	volume = {35},
	issn = {0890-8567},
	doi = {10.1097/00004583-199611000-00024},
	abstract = {OBJECTIVE: To explore the relationship of general expressive language delay to nonverbal social-communicative deficits and repetitive behaviors associated with autism in preschool children.
METHOD: Interviews of the parents of 51 autistic and 43 nonautistic 3- to 5-year-olds with language impairments were compared.
RESULTS: Main effects of the children's language level occurred for the majority of social and nonverbal communication items but not for restricted, repetitive behaviors or showing and directing attention, socially directed gaze, or range of facial expression.
CONCLUSIONS: In addition to the specific impairments of autism, a broader conceptualization of communication handicap is urged for preschool children with developmental delays.},
	language = {eng},
	number = {11},
	journal = {Journal of the American Academy of Child and Adolescent Psychiatry},
	author = {Lord, C. and Pickles, A.},
	month = nov,
	year = {1996},
	pmid = {8936922},
	keywords = {Humans, Female, Male, Attention, Autistic Disorder, Child, Preschool, Facial Expression, Language Development Disorders, Nonverbal Communication, Personality Assessment, Social Behavior, Stereotyped Behavior},
	pages = {1542--1550}
}

@article{travis-links-2001,
	title = {Links between social understanding and social behavior in verbally able children with autism},
	volume = {31},
	issn = {0162-3257},
	abstract = {This study investigated the relations between various measures of social understanding and social interaction competence in verbally able children with autism. Measures of social understanding included measures of verbalizable knowledge (false belief understanding, affective perspective taking), as well as measures of more intuitive forms of social responsiveness (empathy, concern to distress, and initiating joint attention). Two measures of social interaction competence were employed: level of engagement with peers on the playground, and prosocial behavior in a structured laboratory task. For children with autism, initiating joint attention and empathy were strongly related to both measures of social interaction competence. No understanding-behavior links were identified for a language-age matched comparison sample of developmentally delayed children. Several accounts of these understanding-behavior links are considered, including the possibility that for children with autism, more impaired forms of understanding are more closely linked to behavior because they serve as limits on competence.},
	language = {eng},
	number = {2},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Travis, L. and Sigman, M. and Ruskin, E.},
	month = apr,
	year = {2001},
	pmid = {11450811},
	keywords = {Humans, Adolescent, Female, Male, Verbal Behavior, Autistic Disorder, Social Behavior, Child, Developmental Disabilities, Empathy, Intelligence, Interpersonal Relations, Peer Group, Social Perception},
	pages = {119--130}
}

@article{stone-predicting-2001,
	title = {Predicting spoken language level in children with autism spectrum disorders},
	volume = {5},
	issn = {1362-3613},
	doi = {10.1177/1362361301005004002},
	abstract = {Thirty-five children who received an autism spectrum diagnosis at the age of 2 years (24 with autism, 11 with PDD-NOS) were re-evaluated 2 years later to examine factors related to the development of spoken language. Child variables (play level, motor imitation ability and joint attention) and environmental variables (socioeconomic status and hours of speech/language therapy between ages 2 and 3) were used to predict an aggregate measure of language outcome at age 4. After controlling for age 2 language skills, the only significant predictors were motor imitation and number of hours of speech/language therapy. Implications of these results for understanding the early developmental course of autism spectrum disorders and the effects of intervention are discussed.},
	language = {eng},
	number = {4},
	journal = {Autism: The International Journal of Research and Practice},
	author = {Stone, W. L. and Yoder, P. J.},
	month = dec,
	year = {2001},
	pmid = {11777253},
	keywords = {Humans, Female, Male, Time Factors, Autistic Disorder, Child, Preschool, Early Intervention (Education), Language Disorders, Language Therapy, Prognosis, Socioeconomic Factors},
	pages = {341--361}
}

@article{tomasello-linguistic-1986,
	title = {Linguistic environment of 1- to 2-year-old twins},
	volume = {22},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {1939-0599 0012-1649},
	doi = {10.1037/0012-1649.22.2.169},
	abstract = {Investigated differences in the language learning environments of singletons and twins, with special reference to pragmatic factors that might be expected to differ in dyadic and triadic interactive situations. Six twin pairs and 12 singleton children (all firstborn) were observed in natural interactions with their mothers, once at 15 mo of age and again at 21 mo of age. Findings reveal that twins were lower than singletons on all measures of language development and that their language learning environments were significantly different. Although twin mothers spoke and interacted with their children as much as singleton mothers when twins were analyzed together, when analyzed as individuals, twin children received less speech directed specifically to them, participated in fewer and shorter episodes of joint attentional focus, and had fewer and shorter conversations with their mothers. Twin mothers were also more directive in their interactional styles. Correlational analyses indicated that variation of these language learning environment factors for the sample as a whole, and variation for some of these factors within the twin group itself, were related to early language growth. It is proposed that both the quantitative and qualitative differences observed in the language learning environments of singletons and twins derive from the nature of the triadic situation and that these differences have important effects on the child's early language development. (23 ref)},
	language = {English},
	number = {2},
	journal = {Developmental Psychology},
	author = {Tomasello, Michael and Mannle, Sara and Kruger, Ann C.},
	year = {1986},
	keywords = {*Language Development, *Mother Child Communication, *Twins, Birth Order},
	pages = {169--176},
	file = {Tomasello et al_1986_Linguistic environment of 1- to 2-year-old twins.pdf:/home/user/Zotero/storage/H64DDPVK/Tomasello et al_1986_Linguistic environment of 1- to 2-year-old twins.pdf:application/pdf}
}

@article{tomasello-joint-1983,
	title = {Joint attention and lexical acquisition style},
	volume = {4},
	issn = {0142-7237},
	url = {http://journals.sagepub.com/doi/abs/10.1177/014272378300401202},
	doi = {10.1177/014272378300401202},
	abstract = {Recent research has documented systematic individual differences in early lexical development. The current study investigated the relation ship of these differences to differences in the way mothers and children regulate each other's attentional states. Mothers of 6 one-year-olds kept diary records and were videotaped with their children at monthly intervals as well. Language measures from the diary were related to measures of attention manipulation and maintenance derived from a coding of the videotaped interactions. Results showed that when mothers initiated interactions by directing their child's attention, rather than by following into it, their child learned fewer object labels and more personal-social words. Dyads who maintained sustained bouts of joint attentional focus had children with larger vocabularies overall. It was concluded that the way mothers and children regulate each other's attention is an important factor in children's early lexical development.},
	language = {en},
	number = {12},
	urldate = {2017-03-08},
	journal = {First Language},
	author = {Tomasello, Michael and Todd, Jody},
	month = oct,
	year = {1983},
	pages = {197--211},
	file = {Tomasello_Todd_1983_Joint attention and lexical acquisition style.pdf:/home/user/Zotero/storage/4HEKRMT3/Tomasello_Todd_1983_Joint attention and lexical acquisition style.pdf:application/pdf}
}

@article{morales-responding-2000,
	title = {Responding to joint attention across the 6- through 24-month age period and early language acquisition},
	volume = {21},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {0193-3973},
	doi = {10.1016/S0193-3973(99)00040-4},
	abstract = {This study examined individual differences in the development of the capacity of infants (22 Ss; followed from age 6 mos to 30 mos) to respond to the joint attention bids of others (e.g., gaze shift, pointing, and vocalizing) across the first and second year. The primary aim of the study was to determine if responding to joint attention (RJA) in the first and second year was related to subsequent vocabulary acquisition and whether a specific period of development during the first 2 yrs was optimal for the assessment of individual differences in this skill. The study was also designed to determine if RJA provided unique predictive information about language development over and above that provided by parent reports of early vocabulary acquisition. Findings indicated that RJA at 6, 8, 10, 12, and 18 mos was positively related to individual differences in vocabulary development. Furthermore, both a 6- to 18-mo aggregate measure of RJA and a parent report measure of language development made unique contributions to the predictions of vocabulary acquisition. Finally, individual differences in RJA measured at 21 and 24 mos were not related to language development.},
	language = {English},
	number = {3},
	journal = {Journal of Applied Developmental Psychology},
	author = {Morales, Michael and Mundy, Peter and F, E. and Yale, Marygrace and Messinger, Daniel and Neal, Rebecca and Schwartz, Heidi K.},
	year = {2000},
	keywords = {individual differences, *Language Development, *Attention, *Interpersonal Interaction, *Nonverbal Communication, *Verbal Communication, Early Childhood Development},
	pages = {283--298},
	file = {1-s2.0-S0193397399000404-main.pdf:/home/user/Zotero/storage/7F9BK4FS/1-s2.0-S0193397399000404-main.pdf:application/pdf;APA PsycNET Snapshot:/home/user/Zotero/storage/BCHGPEK6/2001-17649-003.html:text/html}
}

@article{markus-individual-2000,
	title = {Individual {Differences} in {Infant} {Skills} as {Predictors} of {Child}-{Caregiver} {Joint} {Attention} and {Language}},
	volume = {9},
	issn = {1467-9507},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9507.00127/abstract},
	doi = {10.1111/1467-9507.00127},
	abstract = {Current research suggests that the extent to which child-caregiver dyads engage in interactions involving episodes of joint or coordinated attention can have a significant impact on early lexical acquisition. In this regard it has been recognized that individual differences in early developing child communication skills, such as capacity to follow gaze and early infant language, may contribute to these child-caregiver interactional patterns, as well as to subsequent language development. To address this expectation, 21 infant-parent dyads were recruited for participation in a longitudinal study. Early infant language, responding to joint attention skill, and cognitive development were assessed at 12 months of age. Child-caregiver joint attention episodes, as well as responding to joint attention skill and child language, were assessed at 18 months of age. Developmental outcome, using the MacArthur Communicative Development Inventories and the Bayley Scales of Infant Development-II, was assessed at 21 and 24 months of age. Consistent with previous findings, results indicated that individual differences in child-caregiver episodes of joint attention were related to language at 18 months. In addition, though, 12 month vocabulary and responding to joint attention skill were associated with some aspects of 18 month child-caregiver interaction, as well as subsequent language development. In general, 12 month child measures and 18 month child-caregiver interaction measures appeared to make unique contributions to language development in this sample. These results suggest the need to further consider the role of infant skills in the connections between child-caregiver joint attention episodes and language development.},
	language = {en},
	number = {3},
	urldate = {2017-03-08},
	journal = {Social Development},
	author = {Markus, Jessica and Mundy, Peter and Morales, Michael and Delgado, Christine E. F. and Yale, Marygrace},
	month = aug,
	year = {2000},
	keywords = {Language Development, joint attention, mother-child interaction},
	pages = {302--315},
	file = {Markus et al_2000_Individual Differences in Infant Skills as Predictors of Child-Caregiver Joint.pdf:/home/user/Zotero/storage/PBHFVJZ8/Markus et al_2000_Individual Differences in Infant Skills as Predictors of Child-Caregiver Joint.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EN9IT9FI/abstract.html:text/html}
}

@article{carpenter-how-2013,
	title = {How joint is the joint attention of apes and human infants},
	url = {https://books.google.com/books?hl=en&lr=&id=IjFpAgAAQBAJ&oi=fnd&pg=PA49&dq=%22for+much+of+the+disagreement+in+both+areas.+Thus,+in+the+hopes+of%22+%22and+other+related+abilities+such+as+joint%22+%22define+joint+attention+as+%E2%80%9Cthe+intentional+co-orientation+of+two+or+more+organisms+to%22+&ots=9ecDrQvspV&sig=Ok3OOeypBBD2Vw9k0qMLouyEZUk},
	urldate = {2017-03-08},
	journal = {Agency and joint attention},
	author = {Carpenter, Malinda and Call, Josep},
	year = {2013},
	pages = {49--61},
	file = {Carpenter_and_Call_(in press)_joint_attention_in_chimpanzees.pdf:/home/user/Zotero/storage/253D54VM/Carpenter_and_Call_(in press)_joint_attention_in_chimpanzees.pdf:application/pdf}
}

@article{smith-interactional-1988,
	title = {Interactional predictors of early language},
	volume = {8},
	issn = {0142-7237},
	url = {http://journals.sagepub.com/doi/abs/10.1177/014272378800802304},
	doi = {10.1177/014272378800802304},
	abstract = {In this report we ask how mother-infant interaction affects the rate of early language acquisition. Mothers and 15-month-old infants were videotaped playing at home. Coders described (a) infants' attention to people and/or objects, (b) mothers' use of literal and conventional acts to direct infants' attention and (c) functions of mothers' utterances. Taken together, these aspects of mother-infant play predicted 40\% of the variance in infants' vocabulary size at 18 months. Significant unique contributions to this prediction were made by mothers' conven tional object-marking and metalingual use of language. The more mothers highlighted both (a) shared objects using conventional means, and (b) the linguistic code, the greater the variety of words their infants used at 18 months of age.},
	language = {en},
	number = {23},
	urldate = {2017-03-08},
	journal = {First Language},
	author = {Smith, Connie B. and Adamson, Lauren B. and Bakeman, Roger},
	month = jun,
	year = {1988},
	pages = {143--156},
	file = {Smith et al_1988_Interactional predictors of early language.pdf:/home/user/Zotero/storage/A8DHBWMH/Smith et al_1988_Interactional predictors of early language.pdf:application/pdf}
}

@article{laakso-early-1999,
	title = {Early intentional communication as a predictor of language development in young toddlers},
	volume = {19},
	issn = {0142-7237},
	url = {http://journals.sagepub.com/doi/abs/10.1177/014272379901905604},
	doi = {10.1177/014272379901905604},
	abstract = {Interrelations between various types of early intentional communi cation measures, and their relations to children's concurrent and subsequent language skills and maternal interactional sensitivity were studied in a sample of 111 mother-infant pairs. Intentional communication was assessed at 14 months of age using a composite of early actions and gestures derived from parental reports (MacArthur Communicative Development Inventories, MCDI), and measures of early joint attentional behaviours obtained via observations of parent-child play interaction. The sum of actions and gestures and the measures of joint attentional behaviours correlated significantly with each other suggesting that the measures obtained using different techniques and data sources partly tap the same social-cognitive skills. However, the inter relations between various types of joint attentional behaviours did not indicate a single coherent structure. Whereas the parental ratings of intentional communication significantly predicted both later language comprehension and production, the relations between observed joint attentional behaviours and language skills varied depending on the specific aspects of these behaviours that were measured. Both sets of measures of intentional communi cation were related to concurrent maternal interactional sensitivity, which in turn predicted children's language comprehension at},
	language = {en},
	number = {56},
	urldate = {2017-03-08},
	journal = {First Language},
	author = {Laakso, M.-L. and Poikkeus, A.-M. and Katajamäki, J. and Lyytinen, P.},
	month = jun,
	year = {1999},
	pages = {207--231},
	file = {Laakso et al_1999_Early intentional communication as a predictor of language development in young.pdf:/home/user/Zotero/storage/69DRFH89/Laakso et al_1999_Early intentional communication as a predictor of language development in young.pdf:application/pdf}
}

@article{adamson-development-1977,
	title = {The {Development} of {Social} {Reciprocity} between a {Sighted} {Infant} and {Her} {Blind} {Parents}},
	volume = {16},
	issn = {0002-7138},
	url = {http://www.jaacap.com/article/S0002-7138(09)60036-4/abstract},
	doi = {10.1016/S0002-7138(09)60036-4},
	language = {English},
	number = {2},
	urldate = {2017-03-08},
	journal = {Journal of the American Academy of Child Psychiatry},
	author = {Adamson, Lauren and Als, Heidelise and Tronick, Edward and Brazelton, T. Berry},
	month = mar,
	year = {1977},
	pages = {194--207},
	file = {Adamson et al_1977_The Development of Social Reciprocity between a Sighted Infant and Her Blind.pdf:/home/user/Zotero/storage/N5QSE9C8/Adamson et al_1977_The Development of Social Reciprocity between a Sighted Infant and Her Blind.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QTXJ668C/abstract.html:text/html}
}

@article{posner-attention-1980,
	title = {Attention and the detection of signals},
	volume = {109},
	issn = {0022-1015},
	abstract = {Detection of a visual signal requires information to reach a system capable of eliciting arbitrary responses required by the experimenter. Detection latencies are reduced when subjects receive a cue that indicates where in the visual field the signal will occur. This shift in efficiency appears to be due to an alignment (orienting) of the central attentional system with the pathways to be activated by the visual input. It would also be possible to describe these results as being due to a reduced criterion at the expected target position. However, this description ignores important constraints about the way in which expectancy improves performance. First, when subjects are cued on each trial, they show stronger expectancy effects than when a probable position is held constant for a block, indicating the active nature of the expectancy. Second, while information on spatial position improves performance, information on the form of the stimulus does not. Third, expectancy may lead to improvements in latency without a reduction in accuracy. Fourth, there appears to be little ability to lower the criterion at two positions that are not spatially contiguous. A framework involving the employment of a limited-capacity attentional mechanism seems to capture these constraints better than the more general language of criterion setting. Using this framework, we find that attention shifts are not closely related to the saccadic eye movement system. For luminance detection the retina appears to be equipotential with respect to attention shifts, since costs to unexpected stimuli are similar whether foveal or peripheral. These results appear to provide an important model system for the study of the relationship between attention and the structure of the visual system.},
	language = {eng},
	number = {2},
	journal = {Journal of Experimental Psychology},
	author = {Posner, M. I. and Snyder, C. R. and Davidson, B. J.},
	month = jun,
	year = {1980},
	pmid = {7381367},
	keywords = {Humans, Attention, visual perception, Orientation, Space Perception, Cues},
	pages = {160--174},
	file = {37783907c53cdc281dae182cb305364bd71b.pdf:/home/user/Zotero/storage/BKMSEZ55/37783907c53cdc281dae182cb305364bd71b.pdf:application/pdf}
}

@article{akhtar-joint-2007,
	title = {Joint {Attention} and {Vocabulary} {Development}: {A} {Critical} {Look}},
	volume = {1},
	issn = {1749-818X},
	shorttitle = {Joint {Attention} and {Vocabulary} {Development}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4258841/},
	doi = {10.1111/j.1749-818X.2007.00014.x},
	abstract = {Joint attention – parents’ and children’s coordinated attention to each other and to a third object or event – is believed to play a causal and critical role in early word learning. However, joint attention, as conventionally defined and measured, relies only on overt indicators of attention, is studied predominantly in the visual modality, and varies by culture. Moreover, word learning can occur without joint attention in typical development, in autistic development, and in Williams syndrome, and joint attention can occur without commensurate word learning in Down syndrome. Thus, the assumption that joint attention is a necessary and sufficient precursor to vocabulary learning is not universally supported.},
	number = {3},
	urldate = {2017-03-08},
	journal = {Language and linguistics compass},
	author = {Akhtar, Nameera and Gernsbacher, Morton Ann},
	month = may,
	year = {2007},
	pmid = {25505491},
	pmcid = {PMC4258841},
	pages = {195--207},
	file = {Akhtar_Gernsbacher_2007_Joint Attention and Vocabulary Development.pdf:/home/user/Zotero/storage/B5RRTXU4/Akhtar_Gernsbacher_2007_Joint Attention and Vocabulary Development.pdf:application/pdf}
}

@article{van-hecke-infant-2007,
	title = {Infant {Joint} {Attention}, {Temperament}, and {Social} {Competence} in {Preschool} {Children}},
	volume = {78},
	issn = {0009-3920},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2662688/},
	doi = {10.1111/j.1467-8624.2007.00985.x},
	abstract = {Infant joint attention has been observed to be related to social-emotional outcomes in at-risk children. To address whether this relation is also evident in typically developing children, 52 children were tested at 12, 15, 24, and 30 months to examine associations between infant joint attention and social outcomes. Twelve-month initiating and responding to joint attention were related to 30-month social competence and externalizing behavior, even when accounting for 15-month temperament ratings, 24-month cognition and language, and demographic variables. These results suggest that, in addition to associations with language and cognition, infant joint attention reflects robust aspects of development that are related to individual differences in the emergence of social and behavioral competence in childhood.},
	number = {1},
	urldate = {2017-03-08},
	journal = {Child development},
	author = {Van Hecke, Amy Vaughan and Mundy, Peter C. and Acra, C. Françoise and Block, Jessica J. and Delgado, Christine E. F. and Parlade, Meaghan V. and Neal, A. Rebecca and Meyer, Jessica A. and Pomares, Yuly B.},
	year = {2007},
	pmid = {17328693},
	pmcid = {PMC2662688},
	pages = {53--69},
	file = {Van Hecke et al_2007_Infant Joint Attention, Temperament, and Social Competence in Preschool Children.pdf:/home/user/Zotero/storage/C538IDA4/Van Hecke et al_2007_Infant Joint Attention, Temperament, and Social Competence in Preschool Children.pdf:application/pdf}
}

@article{mundy-attention-2007,
	title = {Attention, {Joint} {Attention}, and {Social} {Cognition}},
	volume = {16},
	issn = {0963-7214},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2663908/},
	doi = {10.1111/j.1467-8721.2007.00518.x},
	abstract = {Before social cognition there is joint processing of information about the attention of self and others. This joint attention requires the integrated activation of a distributed cortical network involving the anterior and posterior attention systems. In infancy, practice with the integrated activation of this distributed attention network is a major contributor to the development of social cognition. Thus, the functional neuroanatomies of social cognition and the anterior–posterior attention systems have much in common. These propositions have implications for understanding joint attention, social cognition, and autism.},
	number = {5},
	urldate = {2017-03-08},
	journal = {Current directions in psychological science},
	author = {Mundy, Peter and Newell, Lisa},
	month = oct,
	year = {2007},
	pmid = {19343102},
	pmcid = {PMC2663908},
	pages = {269--274},
	file = {Mundy_Newell_2007_Attention, Joint Attention, and Social Cognition.pdf:/home/user/Zotero/storage/D5FAVWE5/Mundy_Newell_2007_Attention, Joint Attention, and Social Cognition.pdf:application/pdf}
}

@article{bigelow-role-2004,
	title = {The role of joint attention in the development of infants’ play with objects},
	volume = {7},
	issn = {1467-7687},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-7687.2004.00375.x/abstract},
	doi = {10.1111/j.1467-7687.2004.00375.x},
	abstract = {Year-old infants’ play was scored within and outside joint attention with mother and when alone for four levels of maturity: stereotypical, inappropriate relational, appropriate relational, functional. Maternal sensitivity within joint attention was rated on two measures: following infants’ interests and scaffolding infants’ activities. Infants’ play was more advanced with mother than when alone. With mother, infants had more functional and appropriate relational play within joint attention and more stereotypical play outside joint attention, indicating more advanced play within joint attention and more immature play outside joint attention. Functional play within joint attention, but not outside joint attention, correlated with functional play when alone. Mothers’ ability to scaffold infants’ activities within joint attention may be particularly facilitative to infants’ advanced play.},
	language = {en},
	number = {5},
	urldate = {2017-03-08},
	journal = {Developmental Science},
	author = {Bigelow, Ann E. and MacLean, Kim and Proctor, Jane},
	month = nov,
	year = {2004},
	pages = {518--526},
	file = {Bigelow et al_2004_The role of joint attention in the development of infants’ play with objects.pdf:/home/user/Zotero/storage/5EM752NK/Bigelow et al_2004_The role of joint attention in the development of infants’ play with objects.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GEUENNDF/abstract.html:text/html}
}

@article{bigelow-development-2003,
	title = {The development of joint attention in blind infants},
	volume = {15},
	issn = {0954-5794, 1469-2198},
	url = {http://www.journals.cambridge.org/abstract\_S0954579403000142},
	doi = {10.1017/S0954579403000142},
	language = {en},
	number = {02},
	urldate = {2017-03-08},
	journal = {Development and Psychopathology},
	author = {Bigelow, Ann E.},
	month = jun,
	year = {2003},
	file = {Development-of-joint-attention-in-blind-infants.pdf:/home/user/Zotero/storage/BUXI4ISX/Development-of-joint-attention-in-blind-infants.pdf:application/pdf}
}

@misc{noauthor--nodate,
	title = {{{}} - {Heydon}.pdf},
	url = {https://www.uwo.ca/fhs/csd/ebp/reviews/2008-09/Heydon.pdf},
	urldate = {2017-03-08}
}

@article{tomasello-joint-1986,
	title = {Joint {Attention} and {Early} {Language}},
	volume = {57},
	issn = {0009-3920},
	url = {http://www.jstor.org/stable/1130423},
	doi = {10.2307/1130423},
	abstract = {This paper reports 2 studies that explore the role of joint attentional processes in the child's acquisition of language. In the first study, 24 children were videotaped at 15 and 21 months of age in naturalistic interaction with their mothers. Episodes of joint attentional focus between mother and child-for example, joint play with an object-were identified. Inside, as opposed to outside, these episodes both mothers and children produced more utterances, mothers used shorter sentences and more comments, and dyads engaged in longer conversations. Inside joint episodes maternal references to objects that were already the child's focus of attention were positively correlated with the child's vocabulary at 21 months, while object references that attempted to redirect the child's attention were negatively correlated. No measures from outside these episodes related to child language. In an experimental study, an adult attempted to teach novel words to 10 17-month-old children. Words referring to objects on which the child's attention was already focused were learned better than words presented in an attempt to redirect the child's attentional focus.},
	number = {6},
	urldate = {2017-03-08},
	journal = {Child Development},
	author = {Tomasello, Michael and Farrar, Michael Jeffrey},
	year = {1986},
	pages = {1454--1463},
	file = {Tomasello_Farrar_1986_Joint Attention and Early Language.pdf:/home/user/Zotero/storage/TMQWNXV6/Tomasello_Farrar_1986_Joint Attention and Early Language.pdf:application/pdf}
}

@misc{noauthor-hlld-nodate,
	title = {{HLLD} 5(04).book({HLLD}\_A\_400283.fm) - {Shneidman} et al 2009.pdf},
	url = {http://woodwardlab.uchicago.edu/Publications\_files/Shneidman%20et%20al%202009.pdf},
	urldate = {2017-03-08}
}

@incollection{nakano-how-2013,
	address = {London},
	title = {How {Eye} {Gaze} {Feedback} {Changes} {Parent}-{Child} {Joint} {Attention} in {Shared} {Storybook} {Reading}?},
	isbn = {978-1-4471-4783-1 978-1-4471-4784-8},
	url = {http://link.springer.com/10.1007/978-1-4471-4784-8\_2},
	language = {en},
	urldate = {2017-03-08},
	booktitle = {Eye {Gaze} in {Intelligent} {User} {Interfaces}},
	publisher = {Springer London},
	author = {Guo, Jia and Feng, Gary},
	editor = {Nakano, Yukiko I. and Conati, Cristina and Bader, Thomas},
	year = {2013},
	doi = {10.1007/978-1-4471-4784-8\_2},
	pages = {9--21},
	file = {9781447147831-c2.pdf:/home/user/Zotero/storage/AKTPG4MD/9781447147831-c2.pdf:application/pdf}
}

@misc{noauthor-carpenter-liebal-in-press-final-uncorrected-proofs.pdf-nodate,
	title = {Carpenter\_Liebal\_in\_press\_final\_uncorrected\_proofs.pdf},
	url = {http://www.eva.mpg.de/psycho/staff/carpenter/pdf/Carpenter\_Liebal\_in\_press\_final\_uncorrected\_proofs.pdf},
	urldate = {2017-03-08}
}

@misc{noauthor-department-nodate,
	title = {The {Department} of {Linguistics}: {Research}: {Corpus}},
	url = {http://www.linguistics.hku.hk/hkucorpus/},
	urldate = {2017-02-23},
	file = {The Department of Linguistics\: Research\: Corpus:/home/user/Zotero/storage/ZVFGUAFT/hkucorpus.html:text/html}
}

@misc{noauthor--nodate-1,
	title = {香港中文大學 粵語研究中心},
	url = {http://www.cuhk.edu.hk/chi/rcc/rcc\_resources\_chi.html},
	urldate = {2017-02-23},
	file = {香港中文大學 粵語研究中心:/home/user/Zotero/storage/DB2KA4DI/rcc_resources_chi.html:text/html}
}

@article{bybee-linguistic-2001,
	title = {of linguistic structure},
	volume = {45},
	url = {https://books.google.com/books?hl=en&lr=&id=Tt-w2gAYFRAC&oi=fnd&pg=PA1&dq=%22and+theoreticians+accept:+unmarked+members+of+categories+are+more%22+%22aspect+of+Zipf%27s+work+is+often+criticized+(see,+for+example,+Miller+1965),%22+%22focused+their+attention+on+the+theoretical+questions+of+how+to+define%22+&ots=V\_RPkyrt4P&sig=YXwCLRUMiK1W2\_LcyQyjo3zag7M},
	urldate = {2017-02-23},
	journal = {Frequency and the emergence of linguistic structure},
	author = {Bybee, Joan and Hopper, Paul},
	year = {2001},
	pages = {1},
	file = {BybeeHopper2001IntroFreqEmergence.pdf:/home/user/Zotero/storage/Z6QIX2PN/BybeeHopper2001IntroFreqEmergence.pdf:application/pdf}
}

@article{dunn-review-2014,
	title = {Review of the book {Evolutionary} {Linguistics} by {April} {McMahon} and {Robert} {McMahon}},
	volume = {116},
	url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:2058139/component/escidoc:2058138/book\_review\_bydunn.pdf},
	number = {3},
	urldate = {2017-02-23},
	journal = {American Anthropologist},
	author = {Dunn, Michael},
	year = {2014},
	pages = {690--691},
	file = {book_review_bydunn.pdf:/home/user/Zotero/storage/3E87DZMD/book_review_bydunn.pdf:application/pdf}
}

@article{dunn-time-2013-1,
	title = {Time and place in the prehistory of the {Aslian} languages},
	volume = {85},
	url = {http://www.bioone.org/doi/abs/10.3378/027.085.0318},
	number = {3},
	urldate = {2017-02-23},
	journal = {Human biology},
	author = {Dunn, Michael and Kruspe, Nicole and Burenhult, Niclas},
	year = {2013},
	pages = {383--400},
	file = {Dunn_Kruspe_Burenhult_2013.pdf:/home/user/Zotero/storage/VVMQF5GM/Dunn_Kruspe_Burenhult_2013.pdf:application/pdf}
}

@article{noauthor-toward-2013,
	title = {Toward a {Mechanistic} {Understanding} of {Linguistic} {Diversity}},
	volume = {63},
	issn = {00063568, 15253244},
	url = {https://academic.oup.com/bioscience/article-lookup/doi/10.1525/bio.2013.63.7.6},
	doi = {10.1525/bio.2013.63.7.6},
	language = {en},
	number = {7},
	urldate = {2017-02-23},
	journal = {BioScience},
	month = jul,
	year = {2013},
	pages = {524--535},
	file = {Gavin_2013_bioscience.pdf:/home/user/Zotero/storage/MI6N6FS6/Gavin_2013_bioscience.pdf:application/pdf}
}

@article{dunn-assessing-2012,
	title = {Assessing the lexical evidence for a {Central} {Solomons} {Papuan} family using the {Oswalt} {Monte} {Carlo} {Test}},
	volume = {29},
	issn = {0176-4225, 1569-9714},
	url = {http://www.jbe-platform.com/content/journals/10.1075/dia.29.1.01dun},
	doi = {10.1075/dia.29.1.01dun},
	language = {en},
	number = {1},
	urldate = {2017-02-23},
	journal = {Diachronica},
	author = {Dunn, Michael and Terrill, Angela},
	year = {2012},
	pages = {1--27},
	file = {Dunn_Terrill_Diachronica_2012.pdf:/home/user/Zotero/storage/3ZWJZWSZ/Dunn_Terrill_Diachronica_2012.pdf:application/pdf}
}

@incollection{reesink-systematic-2012,
	title = {Systematic typological comparison as a tool for investigating language history},
	url = {http://scholarspace.manoa.hawaii.edu/handle/10125/4560},
	urldate = {2017-02-23},
	publisher = {University of Hawai'i Press},
	author = {Reesink, Ger and Dunn, Michael},
	year = {2012},
	file = {Reesink_2012_systematic.pdf:/home/user/Zotero/storage/R8B4ZK96/Reesink_2012_systematic.pdf:application/pdf}
}

@article{dunn-aslian-2011,
	title = {Aslian linguistic prehistory: {A} case study in computational phylogenetics},
	volume = {28},
	issn = {0176-4225, 1569-9714},
	shorttitle = {Aslian linguistic prehistory},
	url = {http://www.jbe-platform.com/content/journals/10.1075/dia.28.3.01dun},
	doi = {10.1075/dia.28.3.01dun},
	language = {en},
	number = {3},
	urldate = {2017-02-23},
	journal = {Diachronica},
	author = {Dunn, Michael and Burenhult, Niclas and Kruspe, Nicole and Tufvesson, Sylvia and Becker, Neele},
	year = {2011},
	pages = {291--323},
	file = {Dunn_Asian_Linguistic_Prehistory_2011_Diachronica.pdf:/home/user/Zotero/storage/JTEPEG43/Dunn_Asian_Linguistic_Prehistory_2011_Diachronica.pdf:application/pdf}
}

@article{dunn-evolved-2011-1,
	title = {Evolved structure of language shows lineage-specific trends in word-order universals},
	volume = {473},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/nature09923},
	doi = {10.1038/nature09923},
	number = {7345},
	urldate = {2017-02-23},
	journal = {Nature},
	author = {Dunn, Michael and Greenhill, Simon J. and Levinson, Stephen C. and Gray, Russell D.},
	month = may,
	year = {2011},
	pages = {79--82},
	file = {Dunn_2011_Evolved structure of language_Nature.pdf:/home/user/Zotero/storage/HBMZGENA/Dunn_2011_Evolved structure of language_Nature.pdf:application/pdf}
}

@article{levinson-universal-2011-1,
	title = {Universal typological dependencies should be detectable in the history of language families},
	volume = {15},
	issn = {1430-0532, 1613-415X},
	url = {http://www.degruyter.com/view/j/lity.2011.15.issue-2/lity.2011.034/lity.2011.034.xml},
	doi = {10.1515/lity.2011.034},
	number = {2},
	urldate = {2017-02-23},
	journal = {Linguistic Typology},
	author = {Levinson, Stephen C. and Greenhill, Simon J. and Gray, Russell D. and Dunn, Michael},
	month = jan,
	year = {2011},
	file = {Levinson_et_al_2011_Universal_Typological_Dependencies_Linguistic_Typology.pdf:/home/user/Zotero/storage/HCIIQZ69/Levinson_et_al_2011_Universal_Typological_Dependencies_Linguistic_Typology.pdf:application/pdf}
}

@article{friedlaender-linguistics-2009,
	title = {Linguistics more robust than genetics [{Letter} to the editor]},
	volume = {324},
	url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:64694/component/escidoc:64695/Friedlaender-464-c.pdf},
	urldate = {2017-02-23},
	journal = {Science},
	author = {Friedlaender, Jonathan and Hunley, Keith and Dunn, Michael and Terrill, Angela and Lindström, Eva and Reesink, Ger and Friedlaender, Françoise},
	year = {2009},
	pages = {464--465},
	file = {Friedlaender-464-c.pdf:/home/user/Zotero/storage/JD9CPTH6/Friedlaender-464-c.pdf:application/pdf}
}

@article{reesink-explaining-2009,
	title = {Explaining the linguistic diversity of {Sahul} using population models},
	volume = {7},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000241},
	number = {11},
	urldate = {2017-02-23},
	journal = {PLoS Biol},
	author = {Reesink, Ger and Singer, Ruth and Dunn, Michael},
	year = {2009},
	pages = {e1000241},
	file = {journal.pbio.1000241.pdf:/home/user/Zotero/storage/W6EG66J3/journal.pbio.1000241.pdf:application/pdf}
}

@article{hunley-genetic-2008,
	title = {Genetic and {Linguistic} {Coevolution} in {Northern} {Island} {Melanesia}},
	volume = {4},
	issn = {1553-7404},
	url = {http://dx.plos.org/10.1371/journal.pgen.1000239},
	doi = {10.1371/journal.pgen.1000239},
	language = {en},
	number = {10},
	urldate = {2017-02-23},
	journal = {PLoS Genetics},
	author = {Hunley, Keith and Dunn, Michael and Lindström, Eva and Reesink, Ger and Terrill, Angela and Healy, Meghan E. and Koki, George and Friedlaender, Françoise R. and Friedlaender, Jonathan S.},
	editor = {Pritchard, Jonathan K.},
	month = oct,
	year = {2008},
	pages = {e1000239},
	file = {Hunley_2008_genetic.pdf:/home/user/Zotero/storage/2W7U45SM/Hunley_2008_genetic.pdf:application/pdf}
}

@article{dunn-four-2007,
	title = {Four languages from the lower end of the typology of locative predication},
	volume = {45},
	issn = {0024-3949, 1613-396X},
	url = {http://www.degruyter.com/view/j/ling.2007.45.issue-5part6/ling.2007.026/ling.2007.026.xml},
	doi = {10.1515/LING.2007.026},
	number = {5part6},
	urldate = {2017-02-23},
	journal = {Linguistics},
	author = {Dunn, Michael and Margetts, Anna and Meira, Sergio and Terrill, Angela},
	month = jan,
	year = {2007},
	file = {Dunn_2007_four languages.pdf:/home/user/Zotero/storage/DZ9M3GNZ/Dunn_2007_four languages.pdf:application/pdf}
}

@misc{noauthor-semantic-nodate-1,
	title = {Semantic systems in closely related languages — {Max} {Planck} {Institute} for {Psycholinguistics}},
	url = {http://www.mpi.nl/publications/escidoc-2070758},
	urldate = {2017-02-23},
	file = {Semantic systems in closely related languages — Max Planck Institute for Psycholinguistics:/home/user/Zotero/storage/DQ7NBWFE/escidoc-2070758.html:text/html}
}

@misc{noauthor-combined-nodate,
	title = {A combined comparative and phylogenetic analysis of the {Chapacuran} language family — {Max} {Planck} {Institute} for {Psycholinguistics}},
	url = {http://www.mpi.nl/publications/escidoc-2332314},
	urldate = {2017-02-23},
	file = {A combined comparative and phylogenetic analysis of the Chapacuran language family — Max Planck Institute for Psycholinguistics:/home/user/Zotero/storage/6UIMMU5T/escidoc-2332314.html:text/html}
}

@unpublished{macwhinney-emergence-nodate,
	title = {emergence grammar perspective},
	url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&ik=a90995961a&view=att&th=15a68ea1e8b07292&attid=0.1&disp=safe&zw&saddbat=ANGjdJ8X2TD9XYrU-J77giZR5ve2V4PH-lMrYpSP0GwzXr2VYkOJFyOzVxf8LtGmNDCLfn7BVvdh70HOlSLdXZY-9gqHjVCDE0mvxOKdK5RFXrLigHSQNGOpxrKiywdKylzYu5at4JCxTcRiCsQXLW8T6BQr8qi1FCeDGcbDB\_9i\_ne5WXTwrEklX5rm0CJrL0NNgPCnqp8r9s0tGwYzFMt6HoKSUQc7KA1DldmJ6KajjkQ4-FKMOqAzT8c2aVLo4nkyAqP94qZWMujBF2mM9jwjeVb20wuyB6kkVw11pq4ifOHV83BZRL636C-KtW27LBgZJKEv5a2vKUKzlnje--NWiLHzRC-n1v\_MwOJ\_oTdRag9YTlxj96w-YIcFONq5xMpxyXOnX0N4pnoppeMIAV\_0673UYxDWhr\_0JqJ9kBSelTOoY1UkLYYsl6\_havuTAZdLbswOADAVnnMJyKHVIN0HZKwQj0c7uVdTDkI-2yU5Pj7SBNbucB1uW3foxZfadLG0Xb1BHn47MUxftZHBXqOyxz5zk2OxEFbem9BeOcRjbigWCqsoPGVV\_iHuO7TgBWSk0PnKOCo6rp6jPS\_8-Rq77gyBKpypCPIjsfB5382jWtdRvIzGFHQfs5uo-Z2ZqPsjyWyo5hw4rsXnHjPODqlWY18oTaredB1YeSnXUQ},
	urldate = {2017-02-23},
	author = {MacWhinney, Brian},
	file = {0.pdf:/home/user/Zotero/storage/QDTX2IFJ/0.pdf:application/pdf}
}

@article{brown-alignment-2007,
	title = {The alignment of form and function: {Corpus}-based evidence from {Russian}},
	volume = {12},
	issn = {1384-6655, 1569-9811},
	shorttitle = {The alignment of form and function},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ijcl.12.4.04bro},
	doi = {10.1075/ijcl.12.4.04bro},
	abstract = {This paper analyses constraints on inflectional syncretism and inflectional allomorphy using frequency information. Syncretism arises where one form is associated with more than one function, whereas inflectional allomorphy occurs where there is more than one inflectional class, and a single function is associated with two or more forms. If high frequency is associated with more differentiation on both sides, we expect, on the one hand, that a frequent function will have a high number of forms and, on the other, that a frequent form will have a high number of functions. Our study focuses on Russian nominals, in particular nouns, which exhibit both syncretism and inflectional allomorphy. We find that there is a relationship between frequency and differentiation, but that it is not exceptionless, and that the exceptions can be understood in terms of the use of referrals as default rules.},
	number = {4},
	urldate = {2017-02-16},
	journal = {International Journal of Corpus Linguistics},
	author = {Brown, Dunstan and Tiberius, Carole and Corbett, Greville G.},
	month = jan,
	year = {2007},
	pages = {511--534},
	file = {Snapshot:/home/user/Zotero/storage/V9Z95MUI/ijcl.12.4.html:text/html}
}

@incollection{nubling-development-2001,
	series = {Yearbook of {Morphology}},
	title = {The development of “junk”. {Irregularization} strategies of have and say in the {Germanic} languages},
	copyright = {©2001 Springer Science+Business Media Dordrecht},
	isbn = {978-90-481-5582-8 978-94-017-3722-7},
	url = {http://link.springer.com/chapter/10.1007/978-94-017-3722-7\_3},
	abstract = {Although it is a wellknown fact that the most frequent verbs are the most irregular ones (if not suppletive), it is rarely asked how they became irregular. This article deals with the irregularization process of two originally regular (weak) verbs, HAVE and SAY in the Germanic languages, e.g. have, but has/’s and had/’d (instead of regular *haves/*haved) or say [sei], but says [sez] and said [sed] in English. Other verbs, such as DO, GO, STAND, BE, COME, and so on, also tend to irregularizations again and again without any apparent reason. In contrast to HAVE and SAY these verbs have always been rather irregular, at least dating from their first written records.},
	language = {en},
	urldate = {2017-02-16},
	booktitle = {Yearbook of {Morphology} 1999},
	publisher = {Springer Netherlands},
	author = {Nübling, Damaris},
	editor = {Booij, Geert and Marle, Jaap van},
	year = {2001},
	doi = {10.1007/978-94-017-3722-7\_3},
	keywords = {psycholinguistics, historical linguistics, Phonology, Theoretical Linguistics},
	pages = {53--74},
	file = {Nuebling_2001a.pdf:/home/user/Zotero/storage/72TV7M6K/Nuebling_2001a.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/48VVQVTH/10.html:text/html}
}

@article{haspelmath-against-2006,
	title = {Against markedness (and what to replace it with)},
	volume = {42},
	issn = {1469-7742, 0022-2267},
	url = {https://www.cambridge.org/core/journals/journal-of-linguistics/article/against-markedness-and-what-to-replace-it-with/24EAC648FEED82E204539D5F952690B7},
	doi = {10.1017/S0022226705003683},
	abstract = {This paper first provides an overview of the various senses in which the terms ‘marked’ and ‘unmarked’ have been used in 20th-century linguistics. Twelve different senses, related only by family resemblances, are distinguished, grouped into four larger classes: markedness as complexity, as difficulty, as abnormality, and as a multidimensional correlation. In the second part of the paper, it is argued that the term ‘markedness’ is superfluous, because some of the concepts that it denotes are not helpful, and others are better expressed by more straightforward, less ambiguous terms. In a great many cases, frequency asymmetries can be shown to lead to a direct explanation of observed structural asymmetries, and in other cases additional concrete, substantive factors such as phonetic difficulty and pragmatic inferences can replace reference to an abstract notion of ‘markedness’.},
	number = {1},
	urldate = {2017-02-16},
	journal = {Journal of Linguistics},
	author = {Haspelmath, Martin},
	month = mar,
	year = {2006},
	pages = {25--70},
	file = {Haspelmath_2006_Against markedness (and what to replace it with).pdf:/home/user/Zotero/storage/3CIBEATJ/Haspelmath_2006_Against markedness (and what to replace it with).pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BUAZVPUB/24EAC648FEED82E204539D5F952690B7.html:text/html}
}

@incollection{bybee-frequency-2001,
	address = {Amsterdam},
	series = {Typological studies in language ; v. 45},
	title = {Frequency and the emergence of linguistic structure},
	copyright = {Access limited to residents of owning communities and students of owning institutions.},
	isbn = {978-90-272-9803-4 978-1-58811-028-2 978-1-58811-027-5 978-90-272-2947-2 978-90-272-2948-9},
	abstract = {A mainstay of functional linguistics has been the claim that linguistic elements and patterns that are frequently used in discourse become conventionalized as grammar. This book addresses the two issues that are basic to this claim: first, the question of what types of elements are frequently used in discourse and second, the question of how frequency of use affects cognitive representations. Reporting on evidence from natural conversation, diachronic change, variability, child language acquisition and psycholinguistic experimentation the original articles in this book support two major princi.},
	language = {eng},
	publisher = {J. Benjamins},
	author = {Bybee, Joan L. and Hopper, Paul J.},
	year = {2001},
	keywords = {Linguistics, Grammar, Comparative and general, Syntax, FrÃ©quence des mots, Frequency (Linguistics), Frequentie, GramÃ¡tica comparada, Grammaire comparÃ©e et gÃ©nÃ©rale, Grammar \& Punctuation, Grammaticalisering, LANGUAGE ARTS \& DISCIPLINES, Taalstructuur, Taalverandering}
}

@article{tiersma-local-1982-1,
	title = {Local and {General} {Markedness}},
	volume = {58},
	issn = {0097-8507},
	url = {http://www.jstor.org/stable/413959},
	doi = {10.2307/413959},
	abstract = {The work of Jakobson-and later, of Greenberg-has developed the notion of MORPHOLOGICAL MARKEDNESS. In this view, certain morphological categories (e.g. noun singulars) are considered unmarked or more basic in relation to others (e.g. noun plurals). It is shown here that there are some principled exceptions to the general markedness conventions. One example of such LOCAL MARKEDNESS, as this type of markedness reversal will be called, is that nouns whose referents naturally occur in groups or pairs show many of the effects of being unmarked in the plural. The consequences of such local markedness in terms of language acquisition, paradigm regularization, lexical borrowing, and the formation of double morphology are explored in some detail.},
	number = {4},
	urldate = {2017-02-16},
	journal = {Language},
	author = {Tiersma, Peter Meijes},
	year = {1982},
	pages = {832--849},
	file = {Tiersma_1982_Local and General Markedness.pdf:/home/user/Zotero/storage/P5IKCIKQ/Tiersma_1982_Local and General Markedness.pdf:application/pdf}
}

@article{finley-artificial-2009,
	title = {Artificial language learning and feature-based generalization},
	volume = {61},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X09000564},
	doi = {10.1016/j.jml.2009.05.002},
	language = {en},
	number = {3},
	urldate = {2017-03-19},
	journal = {Journal of Memory and Language},
	author = {Finley, Sara and Badecker, William},
	month = oct,
	year = {2009},
	pages = {423--437},
	file = {Artificial language learning and feature-based generalization - 1-s2.0-S0749596X09000564-main.pdf:/home/user/Zotero/storage/IT46MVCU/1-s2.0-S0749596X09000564-main.pdf:application/pdf}
}

@article{gallagher-learning-2013,
	title = {Learning the identity effect as an artificial language: bias and generalisation},
	volume = {30},
	issn = {0952-6757, 1469-8188},
	shorttitle = {},
	url = {https://www.cambridge.org/core/journals/phonology/article/learning-the-identity-effect-as-an-artificial-language-bias-and-generalisation/A902BA44C31EBBBC4E994202B26A406C},
	doi = {10.1017/S0952675713000134},
	abstract = {The results of two artificial grammar experiments show that individuals learn a distinction between identical and non-identical consonant pairs better than an arbitrary distinction, and that they generalise the distinction to novel segmental pairs. These results have implications for inductive models of learning, because they necessitate an explicit representation of identity. While identity has previously been represented as root-node sharing in autosegmental representations (Goldsmith 1976, McCarthy 1986), or implicitly assumed to be a property that constraints can reference (MacEachern 1999, Coetzee \& Pater 2008), the model of inductive learning proposed by Hayes \& Wilson (2008) assumes strictly feature-based representations, and is unable to reference identity directly. This paper explores the predictions of the Hayes \& Wilson model and compares it to a modification of the model where identity is represented (Colavin et al.2010). The results of both experiments support a model incorporating direct reference to identity.},
	number = {2},
	urldate = {2017-03-19},
	journal = {Phonology},
	author = {Gallagher, Gillian},
	month = aug,
	year = {2013},
	pages = {253--295},
	file = {Gallagher_2013_Learning the identity effect as an artificial language.pdf:/home/user/Zotero/storage/3N6TC42X/Gallagher_2013_Learning the identity effect as an artificial language.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HG92Q994/A902BA44C31EBBBC4E994202B26A406C.html:text/html}
}

@inproceedings{brooks-learnability-2013,
	title = {Learnability of complex phonological interactions: an artificial language learning experiment},
	volume = {4},
	shorttitle = {Learnability of complex phonological interactions},
	url = {http://idiom.ucsd.edu/~bakovic/work/LSA-BPB-EA-2013.pdf},
	urldate = {2017-03-19},
	booktitle = {{LSA} {Annual} {Meeting} {Extended} {Abstracts}},
	author = {Brooks, K. Michael and Pajak, Bozena and Baković, Eric},
	year = {2013},
	pages = {8--1},
	file = {LSA-BPB-EA-2013.pdf:/home/user/Zotero/storage/DRE5C6DM/LSA-BPB-EA-2013.pdf:application/pdf}
}

@article{dols-salas-phonology-2012,
	title = {Phonology and morphology and the limits of freedom in an artificial language},
	url = {https://150.254.65.83/handle/10593/9809},
	urldate = {2017-03-19},
	author = {Dols Salas, Nicolau},
	year = {2012},
	file = {Język Komunikacja Informacja tom 7.indd - 1288_dols_salas_1.pdf:/home/user/Zotero/storage/KS2WAN6R/1288_dols_salas_1.pdf:application/pdf}
}

@article{cristia-similarity-2013,
	title = {Similarity in the generalization of implicitly learned sound patterns},
	volume = {4},
	issn = {1868-6354, 1868-6346},
	url = {http://www.degruyter.com/view/j/lp.2013.4.issue-2/lp-2013-0010/lp-2013-0010.xml},
	doi = {10.1515/lp-2013-0010},
	number = {2},
	urldate = {2017-03-19},
	journal = {Laboratory Phonology},
	author = {Cristia, Alejandrina and Mielke, Jeff and Daland, Robert and Peperkamp, Sharon},
	month = jan,
	year = {2013},
	file = {Cristia_Mielke_Daland_Peperkamp_(2013)_Similarity_in_the_generalization_of_implicitly_learned_sound_patterns.pdf:/home/user/Zotero/storage/G7G5H2AT/Cristia_Mielke_Daland_Peperkamp_(2013)_Similarity_in_the_generalization_of_implicitly_learned_so.pdf:application/pdf}
}

@article{peperkamp-learning-2007,
	title = {Learning the mapping from surface to underlying representations in an artificial language},
	volume = {9},
	url = {http://www.lscp.net/persons/dupoux/papers/Peperkamp\_Dupoux\_2007\_Learning\_mapping\_surface\_underlying\_artificial\_language.LabPhon9.pdf},
	urldate = {2017-03-19},
	journal = {Laboratory phonology},
	author = {Peperkamp, Sharon and Dupoux, Emmanuel},
	year = {2007},
	pages = {315--338},
	file = {Learning the mapping from surface to underlying representations in an artificial language - download:/home/user/Zotero/storage/H8NWEDZG/download.pdf:application/pdf}
}

@inproceedings{ozburn-learning-2016,
	title = {Learning vowel harmony with transparency in an artificial language},
	url = {https://blogs.uoregon.edu/nowphon2016/files/2016/05/NoWPhon2\_paper\_31-1vrf0jb.pdf},
	urldate = {2017-03-19},
	booktitle = {Talk {Presented} at the 2016 {NOWCAM} {Meeting}: {Eugene} {Oregon}},
	author = {Ozburn, Avery and Hansson, G.},
	year = {2016},
	file = {Learning vowel harmony with transparency in an artificial language - NoWPhon2_paper_31-1vrf0jb.pdf:/home/user/Zotero/storage/6KI8KJDV/NoWPhon2_paper_31-1vrf0jb.pdf:application/pdf}
}

@inproceedings{peperkamp12-role-2006,
	title = {The role of phonetic naturalness in phonological rule acquisition},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.7529&rep=rep1&type=pdf},
	urldate = {2017-03-19},
	booktitle = {Proceedings of the 30th annual {Boston} {University} {Conference} on {Language} {Development}. 1 (2006)},
	publisher = {Citeseer},
	author = {Peperkamp12, Sharon and Skoruppa, Katrin and Dupoux, Emmanuel},
	year = {2006},
	pages = {464},
	file = {The-Role-of-Phonetic-Naturalness-in-Phonological-Rule-Acquisition.pdf:/home/user/Zotero/storage/AZWE63AR/The-Role-of-Phonetic-Naturalness-in-Phonological-Rule-Acquisition.pdf:application/pdf}
}

@article{brooks-learnability-2013-1,
	title = {Learnability of complex phonological interactions: an artificial language learning experiment},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {2377-3367},
	shorttitle = {Learnability of complex phonological interactions},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/ExtendedAbs/article/view/799},
	doi = {10.3765/exabs.v0i0.799},
	abstract = {Learnability of complex phonological interactions: an artificial language learning experiment},
	number = {0},
	urldate = {2017-03-19},
	journal = {LSA Annual Meeting Extended Abstracts},
	author = {Brooks, K. Michael and Pajak, Bozena and Baković, Eric},
	month = may,
	year = {2013},
	pages = {8--1--5},
	file = {Brooks et al_2013_Learnability of complex phonological interactions.pdf:/home/user/Zotero/storage/3DXJ27AM/Brooks et al_2013_Learnability of complex phonological interactions.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BK9SW78E/799.html:text/html}
}

@article{mei-artificial-2014,
	title = {Artificial {Language} {Training} {Reveals} the {Neural} {Substrates} {Underlying} {Addressed} and {Assembled} {Phonologies}},
	volume = {9},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0093548},
	doi = {10.1371/journal.pone.0093548},
	abstract = {Although behavioral and neuropsychological studies have suggested two distinct routes of phonological access, their neural substrates have not been clearly elucidated. Here, we designed an artificial language (based on Korean Hangul) that can be read either through addressed (i.e., whole word mapping) or assembled (i.e., grapheme-to-phoneme mapping) phonology. Two matched groups of native English-speaking participants were trained in one of the two conditions, one hour per day for eight days. Behavioral results showed that both groups correctly named more than 90\% of the trained words after training. At the neural level, we found a clear dissociation of the neural pathways for addressed and assembled phonologies: There was greater involvement of the anterior cingulate cortex, posterior cingulate cortex, right orbital frontal cortex, angular gyrus and middle temporal gyrus for addressed phonology, but stronger activation in the left precentral gyrus/inferior frontal gyrus and supramarginal gyrus for assembled phonology. Furthermore, we found evidence supporting the strategy-shift hypothesis, which postulates that, with practice, reading strategy shifts from assembled to addressed phonology. Specifically, compared to untrained words, trained words in the assembled phonology group showed stronger activation in the addressed phonology network and less activation in the assembled phonology network. Our results provide clear brain-imaging evidence for the dual-route models of reading.},
	number = {3},
	urldate = {2017-03-19},
	journal = {PLOS ONE},
	author = {Mei, Leilei and Xue, Gui and Lu, Zhong-Lin and He, Qinghua and Zhang, Mingxia and Wei, Miao and Xue, Feng and Chen, Chuansheng and Dong, Qi},
	month = mar,
	year = {2014},
	keywords = {language, Learning, vision, Phonology, Cingulate cortex, Functional magnetic resonance imaging, Natural language, Neural pathways},
	pages = {e93548},
	file = {Mei et al_2014_Artificial Language Training Reveals the Neural Substrates Underlying Addressed.pdf:/home/user/Zotero/storage/3HQ9ACRI/Mei et al_2014_Artificial Language Training Reveals the Neural Substrates Underlying Addressed.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/W8MUH7JS/article.html:text/html}
}

@article{finley-artificial-2009-1,
	title = {Artificial language learning and feature-based generalization},
	volume = {61},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X09000564},
	doi = {10.1016/j.jml.2009.05.002},
	abstract = {representations such as subsegmental phonological features play such a vital role in explanations of phonological processes that many assume that these representations play an equally prominent role in the learning process. This assumption is tested in three artificial grammar experiments involving a mini language with morpho-phonological alternations based on back vowel harmony. In Experiments 1 and 2, adult participants were trained using positive data from four vowels in a six-vowel inventory: the two remaining vowels appeared at test only. If participants use subsegmental phonological features and natural classes for learning, they should generalize to the novel test segments. Results support a subsegmental feature-based learning strategy that makes use of phonetic information and knowledge of phonological principles. A third experiment (Experiment 3) tests for generalizations to novel suffixes, providing further evidence for the generality of learning.},
	number = {3},
	urldate = {2017-03-19},
	journal = {Journal of Memory and Language},
	author = {Finley, Sara and Badecker, William},
	month = oct,
	year = {2009},
	keywords = {Artificial grammar learning, Phonological features, Vowel harmony},
	pages = {423--437},
	file = {Finley_Badecker_2009_Artificial language learning and feature-based generalization.pdf:/home/user/Zotero/storage/BDS9WXID/Finley_Badecker_2009_Artificial language learning and feature-based generalization.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/4T4F8AT4/S0749596X09000564.html:text/html}
}

@article{adriaans-adding-2010,
	title = {Adding generalization to statistical learning: {The} induction of phonotactics from continuous speech},
	volume = {62},
	issn = {0749-596X},
	shorttitle = {Adding generalization to statistical learning},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X09001120},
	doi = {10.1016/j.jml.2009.11.007},
	abstract = {Emerging phonotactic knowledge facilitates the development of the mental lexicon, as demonstrated by studies showing that infants use the phonotactic patterns of their native language to extract words from continuous speech. The present study provides a computational account of how infants might induce phonotactics from their immediate language environment, which consists of unsegmented speech. Our model, StaGe, implements two learning mechanisms that are available to infant language learners: statistical learning and generalization. StaGe constructs phonotactic generalizations on the basis of statistically learned biphone constraints. In a series of computer simulations, we show that such generalizations improve the segmentation performance of the learner, as compared to models that rely solely on statistical learning. Our study thus provides an explicit proposal for a combined role of statistical learning and generalization in the induction of phonotactics by infants. Furthermore, our simulations demonstrate a previously unexplored potential role for phonotactic generalizations in speech segmentation.},
	number = {3},
	urldate = {2017-03-19},
	journal = {Journal of Memory and Language},
	author = {Adriaans, Frans and Kager, René},
	month = apr,
	year = {2010},
	keywords = {Computational Modeling, statistical learning, Phonotactics, Phonological features, Constraint induction, Speech segmentation},
	pages = {311--331},
	file = {Adriaans_Kager_2010_Adding generalization to statistical learning.pdf:/home/user/Zotero/storage/XUGXN79R/Adriaans_Kager_2010_Adding generalization to statistical learning.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/C3UD7AWG/S0749596X09001120.html:text/html}
}

@article{linzen-timecourse-2014,
	title = {The {Timecourse} of {Generalization} in {Phonotactic} {Learning}},
	volume = {1},
	copyright = {Copyright (c)},
	issn = {2377-3324},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/amphonology/article/view/18},
	doi = {10.3765/amp.v1i1.18},
	abstract = {There is considerable evidence that speakers show sensitivity to the phonotactic patterns of their language. These patterns can involve specific sound sequences (e.g. the consonant combination b-b) or more general classes of sequences (e.g. two identical consonants). In some models of phonotactic learning, generalizations can only be formed once some of their specific instantiations have been acquired (the specific-before-general assumption). To test this assumption, we designed an artificial language with both general and specific phonotactic patterns, and gave participants different amounts of exposure to the language. Contrary to the predictions of specific-before-general models, the general pattern required less exposure to be learned than did its specific instantiations. These results are most straightforwardly predicted by learning models that learn general and specific patterns simultaneously. We discuss the importance of modeling learners’ sensitivity to the amount of evidence supporting each phonotactic generalization, and show how specific-before-general models can be adapted to accommodate the results.},
	language = {en},
	number = {1},
	urldate = {2017-03-19},
	journal = {Proceedings of the Annual Meetings on Phonology},
	author = {Linzen, Tal and Gallagher, Gillian},
	month = mar,
	year = {2014},
	keywords = {Computational Modeling, Generalization, Learning, Phonotactics, Identity},
	file = {Linzen_Gallagher_2014_The Timecourse of Generalization in Phonotactic Learning.pdf:/home/user/Zotero/storage/FNJBC7SG/Linzen_Gallagher_2014_The Timecourse of Generalization in Phonotactic Learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/832ASSRG/18.html:text/html}
}

@inproceedings{bergelson-structural-2009,
	title = {Structural biases in phonology: {Infant} and adult evidence from artificial language learning},
	shorttitle = {Structural biases in phonology},
	url = {http://174.143.211.73/~idsardi/papers/2009bucld.pdf},
	urldate = {2017-03-19},
	booktitle = {{BUCLD} 33: {Proceedings} of the 33rd annual {Boston} university conference on language development},
	author = {Bergelson, Elika and Idsardi, William J.},
	year = {2009},
	pages = {85--96},
	file = {Bergelson-IdsardiBUCLD2008-2 - 2009bucld.pdf:/home/user/Zotero/storage/V5KMCXNW/2009bucld.pdf:application/pdf}
}

@misc{whalen-hl0777.pdf-1991,
	title = {{HL}0777.pdf},
	url = {http://www.haskins.yale.edu/Reprints/HL0777.pdf},
	urldate = {2017-03-19},
	author = {Whalen},
	year = {1991},
	file = {HL0777.pdf:/home/user/Zotero/storage/D98J7EIG/HL0777.pdf:application/pdf}
}

@article{streeter-role-1979,
	title = {The role of medial consonant transitions in word perception},
	volume = {65},
	issn = {0001-4966},
	abstract = {In VCV nonsense forms (such as /epsilondepsilon/, while both the CV transition and the VC transition are perceptible in isolation, the CV transition dominates identification of the stop consonant. Thus, the question arises, what role, if any, do VC transitions play in word perception? Stimuli were two-syllable English words in which the medial consonant was either a stop or a fricative (e.g., "feeding" and "gravy"). Each word was constructed in three ways: (1) the VC transition was incompatible with the CV in either place, manner of articulation, or both; (2) the VC transition was eliminated and the steady-state portion of first vowel was substituted in its place; and (3) the original word. All versions of a particular word were identical with respect to duration, pitch contour, and amplitude envelope. While an intelligibility test revealed no differences among the three conditions, data from a paired comparison preference task and an unspeeded lexical decision task indicated that incompatible VC transitions hindered word perception, but lack of VC transitions did not. However, there were clear differences among the three conditions in the speeded lexical decision task for word stimuli, but not for nonword stimuli that were constructed in an analogous fashion. We discuss the use of lexical tasks for speech quality assessment and possible processes by which listeners recognize spoken words.},
	language = {eng},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Streeter, L. A. and Nigro, G. N.},
	month = jun,
	year = {1979},
	pmid = {489823},
	keywords = {Humans, Phonetics, speech perception, Adolescent, Adult, Reaction Time, Cues, Speech Intelligibility},
	pages = {1533--1541}
}

@article{barber-dealing-2016,
	title = {Dealing with a large number of classes -- {Likelihood}, {Discrimination} or {Ranking}?},
	url = {http://arxiv.org/abs/1606.06959},
	abstract = {We consider training probabilistic classifiers in the case of a large number of classes. The number of classes is assumed too large to perform exact normalisation over all classes. To account for this we consider a simple approach that directly approximates the likelihood. We show that this simple approach works well on toy problems and is competitive with recently introduced alternative non-likelihood based approximations. Furthermore, we relate this approach to a simple ranking objective. This leads us to suggest a specific setting for the optimal threshold in the ranking objective.},
	urldate = {2017-03-19},
	journal = {arXiv:1606.06959 [stat]},
	author = {Barber, David and Botev, Aleksandar},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06959},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/34T78U78/1606.html:text/html;Barber_Botev_2016_Dealing with a large number of classes -- Likelihood, Discrimination or Ranking.pdf:/home/user/Zotero/storage/XTFG4HKF/Barber_Botev_2016_Dealing with a large number of classes -- Likelihood, Discrimination or Ranking.pdf:application/pdf}
}

@article{anselmi-young-1986,
	title = {Young children's responses to neutral and specific contingent queries},
	volume = {13},
	issn = {1469-7602, 0305-0009},
	url = {https://www.cambridge.org/core/journals/journal-of-child-language/article/div-classtitleyoung-childrenandaposs-responses-to-neutral-and-specific-contingent-queriesa-hreffn01-ref-typefnadiv/A073579C4F77016EE11C12A7A6377D7F},
	doi = {10.1017/S0305000900000349},
	abstract = {ABSTRACTThis study examined young children's responses to adult contingent queries. Each of 22 children in Language Stages II–V conversed alone with their mother and alone with an adult experimenter. The adults queried the child's multi-word utterances with either a neutral or a specific query. Children at all stages responded differently to the two types of query. In response to the neutral query children tended to repeat their entire utterance, whereas in response to the specific query they most often replied with only the asked-for information. Some children found it easier to differentiate the query types when their mother was the listener. These findings suggest that very young children can comprehend the linguistic structure of specific queries and that they can make pragmatically appropriate responses.},
	number = {1},
	urldate = {2017-03-18},
	journal = {Journal of Child Language},
	author = {Anselmi, Dina and Tomasello, Michael and Acunzo, Mary},
	month = feb,
	year = {1986},
	pages = {135--144},
	file = {Snapshot:/home/user/Zotero/storage/GXNFU7T3/A073579C4F77016EE11C12A7A6377D7F.html:text/html}
}

@incollection{tomasello-joint-1995,
	address = {Hillsdale},
	title = {Joint attention as social cognition},
	url = {https://books.google.com/books?hl=de&lr=&id=-SIBAwAAQBAJ&oi=fnd&pg=PA103&dq=joint+attention+as+social+cognition&ots=Kb9CbXYsYT&sig=IEigZbEiZoHHC-LE6pzjp400R88},
	urldate = {2017-03-15},
	booktitle = {Joint attention: {Its} origins and role in development},
	publisher = {Erlbaum},
	author = {Tomasello, Michael},
	editor = {Moore, C. and Dunham, P. J.},
	year = {1995},
	pages = {103--130},
	file = {Snapshot:/home/user/Zotero/storage/TF456U7A/books.html:text/html;Tomasello-JA1995.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Tomasello-JA1995.pdf:application/pdf}
}

@article{akhtar-directive-1991,
	title = {Directive interactions and early vocabulary development: the role of joint attentional focus},
	volume = {18},
	issn = {1469-7602, 0305-0009},
	shorttitle = {},
	url = {https://www.cambridge.org/core/journals/journal-of-child-language/article/div-classtitledirective-interactions-and-early-vocabulary-development-the-role-of-joint-attentional-focusa-hreffn01-ref-typefnadiv/C25524B80E94DFE15D5322AB2034322E},
	doi = {10.1017/S0305000900013283},
	abstract = {ABSTRACTMaternal directiveness, assessed by the mother's use of prescriptives, is correlated with slow vocabulary development. As prescriptives are most often used to redirect a child's attention to a different object or activity, it is hypothesized that attentional regulation underlies this negative relationship. In the present study, twelve mothers were videotaped interacting with their children aged 1;1, and 100 maternal utterances were coded for pragmatic intent. Prescriptives were coded as either changing (LEADING) or FOLLOWING the child's focus of attention. Only the frequency of mothers' FOLLOW-prescriptives correlated significantly with a productive vocabulary measure taken at 1;10. This correlation was high and positive, indicating that, given joint focus, directing a 13-month-old's behaviour can have beneficial effects on subsequent vocabulary development.},
	number = {1},
	urldate = {2017-03-18},
	journal = {Journal of Child Language},
	author = {Akhtar, Nameera and Dunham, Frances and Dunham, Philip J.},
	month = feb,
	year = {1991},
	pages = {41--49},
	file = {Snapshot:/home/user/Zotero/storage/6SJA7TAJ/C25524B80E94DFE15D5322AB2034322E.html:text/html}
}

@article{butterworth-towards-1980,
	title = {Towards a {Mechanism} of {Joint} {Visual} {Attention} {In} {Human} {Infancy}},
	volume = {3},
	url = {http://journals.sagepub.com/doi/pdf/10.1177/016502548000300303},
	urldate = {2017-03-08},
	journal = {International Journal of Behavioral Development},
	author = {Butterworth, George and Cochran, Edward},
	year = {1980},
	pages = {253--272},
	file = {016502548000300303:/home/user/Zotero/storage/8PHC6DHD/016502548000300303.pdf:application/pdf}
}

@article{moore-social-1994,
	title = {Social {Understanding} at the {End} of the {First} {Year} of {Life}},
	volume = {14},
	url = {http://www.sciencedirect.com/science/article/pii/S0273229784710148},
	urldate = {2017-03-18},
	journal = {Developmental Review},
	author = {Moore, Chris and Corkum, Valerie},
	year = {1994},
	pages = {349--372},
	file = {Social Understanding at the End of the First Year of Life - ScienceDirect:/home/user/Zotero/storage/GJ7AQ7NR/S0273229784710148.html:text/html;wike304.tmp - 1-s2.0-S0273229784710148-main.pdf:/home/user/Zotero/storage/27AUBAMS/1-s2.0-S0273229784710148-main.pdf:application/pdf}
}

@article{charman-testing-2000,
	title = {Testing joint attention, imitation, and play as infancy precursors to language and theory of mind},
	volume = {15},
	url = {http://www.sciencedirect.com/science/article/pii/S0885201401000375},
	number = {4},
	urldate = {2017-03-18},
	journal = {Cognitive development},
	author = {Charman, Tony and Baron-Cohen, Simon and Swettenham, John and Baird, Gillian and Cox, Antony and Drew, Auriol},
	year = {2000},
	pages = {481--498},
	file = {PII\: S0885-2014(01)00037-5 - 1-s2.0-S0885201401000375-main.pdf:/home/user/Zotero/storage/3JESXDC5/1-s2.0-S0885201401000375-main.pdf:application/pdf}
}

@article{barresi-sharing-1993,
	title = {Sharing a perspective precedes the understanding of that perspective},
	volume = {16},
	issn = {0140-525X, 1469-1825},
	url = {http://www.journals.cambridge.org/abstract\_S0140525X00031265},
	doi = {10.1017/S0140525X00031265},
	language = {en},
	number = {03},
	urldate = {2017-03-18},
	journal = {Behavioral and Brain Sciences},
	author = {Barresi, John and Moore, Chris},
	month = sep,
	year = {1993},
	pages = {513},
	file = {Sharing-a-perspective-precedes-the-understanding-of-that-perspective.pdf:/home/user/Zotero/storage/H7P534R7/Sharing-a-perspective-precedes-the-understanding-of-that-perspective.pdf:application/pdf}
}

@article{barresi-sharing-1993-1,
	title = {Sharing a {Perspective} {Precedes} the {Understanding} of that {Perspective}},
	issn = {0140-525X},
	url = {http://DalSpace.library.dal.ca/handle/10222/32492},
	abstract = {No abstract available.},
	urldate = {2017-03-18},
	author = {Barresi, J. and Moore, C.},
	month = sep,
	year = {1993},
	file = {Snapshot:/home/user/Zotero/storage/DZSNRWSD/32492.html:text/html}
}

@book{vygotsky-mind-1978,
	address = {Cambridge, Mass.},
	title = {Mind in society: {The} development of higher psychological processes},
	url = {http://www.hup.harvard.edu/catalog.php?isbn=9780674576292},
	urldate = {2017-03-18},
	publisher = {Harvard University Press},
	author = {Vygotsky, Lev},
	year = {1978},
	file = {Snapshot:/home/user/Zotero/storage/KTIV7KAM/catalog.html:text/html}
}

@misc{noauthor-citing-nodate,
	title = {citing {Vygotsky}},
	url = {http://www.psy.gla.ac.uk/~steve/courses/cereRefsVyg.html},
	urldate = {2017-03-18},
	file = {citing Vygotsky:/home/user/Zotero/storage/GJM2D6NH/cereRefsVyg.html:text/html}
}

@article{benjamin-effect-2015,
	title = {Effect of speaker gaze on word learning in fragile {X} syndrome: a comparison with nonsyndromic autism spectrum disorder},
	volume = {58},
	issn = {1558-9102},
	shorttitle = {Effect of speaker gaze on word learning in fragile {X} syndrome},
	doi = {10.1044/2015\_JSLHR-L-14-0136},
	abstract = {PURPOSE: This study examined use of a speaker's direction of gaze during word learning by boys with fragile X syndrome (FXS), boys with nonsyndromic autism spectrum disorder (ASD), and typically developing (TD) boys.
METHOD: A fast-mapping task with follow-in and discrepant labeling conditions was administered. We expected that the use of speaker gaze would lead to participants selecting as the referent of the novel label the object to which they attended in follow-in trials and the object to which the examiner attended in the discrepant labeling trials. Participants were school-aged boys with FXS (n=18) or ASD (n=18) matched on age, intelligence quotient, and nonverbal cognition and younger TD boys (n=18) matched on nonverbal cognition.
RESULTS: All groups performed above chance in both conditions, although the TD boys performed closest to the expected pattern. Boys with FXS performed better during follow-in than in discrepant label trials, whereas TD boys and boys with ASD did equally well in both trial types. The type of trial administered first influenced subsequent responding. Error patterns also distinguished the groups.
CONCLUSION: The ability to utilize a speaker's gaze during word learning is not as well developed in boys with FXS or nonsyndromic ASD as in TD boys of the same developmental level.},
	language = {eng},
	number = {2},
	journal = {Journal of speech, language, and hearing research: JSLHR},
	author = {Benjamin, David P. and McDuffie, Andrea S. and Thurman, Angela J. and Kover, Sara T. and Mastergeorge, Ann M. and Hagerman, Randi J. and Abbeduto, Leonard},
	month = apr,
	year = {2015},
	pmid = {25629603},
	pmcid = {PMC4675125},
	keywords = {Humans, speech perception, Male, Fixation, Ocular, Cognition, Verbal Learning, Child, Preschool, Child, Intelligence, Autism Spectrum Disorder, Child Language, Fragile X Syndrome},
	pages = {383--395}
}

@article{tomasello-eighteen-month-old-1996,
	title = {Eighteen-month-old children learn words in non-ostensive contexts},
	volume = {23},
	issn = {0305-0009},
	abstract = {Previous studies have demonstrated that children aged 2;0 can learn new words in a variety of non-ostensive contexts. The current two studies were aimed at seeing if this was also true of children just beginning to learn words at 1;6. In the first study an adult interacted with 48 children. She used a nonce word to announce her intention to find an object ('Let's find the gazzer'), picked up and rejected an object with obvious disappointment, and then gleefully found the target object (using no language). Children learned the new word as well in this condition as in a condition in which the adult found the object immediately. In the second study the adult first played several rounds of a finding game with each of 60 children, in which it was first established that one of several novel objects was always in a very distinctive hiding place (a toy barn). The adult then used a nonce word to announce her intention to find an object ('Let's find the toma') and then proceeded to the barn. In the key condition the barn was mysteriously 'locked'; the child thus never saw the target object after the nonce word was introduced. Children learned the new word as well in this condition as in a condition in which the adult found the object immediately. The results of these two studies suggest that from very early in language acquisition children learn words not through passive, associative processes, but rather through active attempts to understand adult behaviour in a variety of action and discourse contexts.},
	language = {eng},
	number = {1},
	journal = {Journal of Child Language},
	author = {Tomasello, M. and Strosberg, R. and Akhtar, N.},
	month = feb,
	year = {1996},
	pmid = {8733565},
	keywords = {Humans, Language Development, Adult, Female, Male, Infant, Verbal Learning, Child, Preschool, Child, Child Language},
	pages = {157--176}
}

@article{tomasello-learning-1994,
	title = {Learning words in nonostensive contexts.},
	volume = {30},
	issn = {0012-1649},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0012-1649.30.5.639},
	doi = {10.1037/0012-1649.30.5.639},
	language = {en},
	number = {5},
	urldate = {2017-03-18},
	journal = {Developmental Psychology},
	author = {Tomasello, Michael and Barton, Michelle E.},
	year = {1994},
	pages = {639--650},
	file = {639.pdf:/home/user/Zotero/storage/ESQVDVM5/639.pdf:application/pdf}
}

@article{akhtar-joint-2007-1,
	title = {Joint {Attention} and {Vocabulary} {Development}: {A} {Critical} {Look}},
	volume = {1},
	issn = {1749-818X},
	shorttitle = {Joint {Attention} and {Vocabulary} {Development}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4258841/},
	doi = {10.1111/j.1749-818X.2007.00014.x},
	abstract = {Joint attention – parents’ and children’s coordinated attention to each other and to a third object or event – is believed to play a causal and critical role in early word learning. However, joint attention, as conventionally defined and measured, relies only on overt indicators of attention, is studied predominantly in the visual modality, and varies by culture. Moreover, word learning can occur without joint attention in typical development, in autistic development, and in Williams syndrome, and joint attention can occur without commensurate word learning in Down syndrome. Thus, the assumption that joint attention is a necessary and sufficient precursor to vocabulary learning is not universally supported.},
	number = {3},
	urldate = {2017-03-18},
	journal = {Language and linguistics compass},
	author = {Akhtar, Nameera and Gernsbacher, Morton Ann},
	month = may,
	year = {2007},
	pmid = {25505491},
	pmcid = {PMC4258841},
	pages = {195--207},
	file = {Akhtar_Gernsbacher_2007_Joint Attention and Vocabulary Development.pdf:/home/user/Zotero/storage/WRSZERHS/Akhtar_Gernsbacher_2007_Joint Attention and Vocabulary Development.pdf:application/pdf}
}

@article{floor-can-2006,
	title = {Can 18-{Month}-{Old} {Infants} {Learn} {Words} by {Listening} {In} on {Conversations}?},
	volume = {9},
	issn = {1532-7078},
	url = {http://onlinelibrary.wiley.com/doi/10.1207/s15327078in0903\_4/abstract},
	doi = {10.1207/s15327078in0903\_4},
	abstract = {Previous research has shown that children as young as 2 can learn words from 3rd-party conversations (Akhtar, Jipson, \& Callanan, 2001). The focus of this study was to determine whether younger infants could learn a new word through overhearing. Novel object labels were introduced to 18-month-old infants in 1 of 2 conditions: directly by an experimenter or in the context of overhearing the experimenter use the word while interacting with another adult. The findings suggest that, when memory demands are not too high, 18-month-old infants can learn words through overhearing.},
	language = {en},
	number = {3},
	urldate = {2017-03-18},
	journal = {Infancy},
	author = {Floor, Penelope and Akhtar, Nameera},
	month = may,
	year = {2006},
	pages = {327--339},
	file = {Floor_Akhtar_2006_Can 18-Month-Old Infants Learn Words by Listening In on Conversations.pdf:/home/user/Zotero/storage/MQG2AP97/Floor_Akhtar_2006_Can 18-Month-Old Infants Learn Words by Listening In on Conversations.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/5RRH9VBE/abstract.html:text/html}
}

@article{rogoff-firsthand-2003,
	title = {Firsthand learning through intent participation},
	volume = {54},
	issn = {0066-4308},
	doi = {10.1146/annurev.psych.54.101601.145118},
	abstract = {This article examines how people learn by actively observing and "listening-in" on ongoing activities as they participate in shared endeavors. Keen observation and listening-in are especially valued and used in some cultural communities in which children are part of mature community activities. This intent participation also occurs in some settings (such as early language learning in the family) in communities that routinely segregate children from the full range of adult activities. However, in the past century some industrial societies have relied on a specialized form of instruction that seems to accompany segregation of children from adult settings, in which adults "transmit" information to children. We contrast these two traditions of organizing learning in terms of their participation structure, the roles of more- and less-experienced people, distinctions in motivation and purpose, sources of learning (observation in ongoing activity versus lessons), forms of communication, and the role of assessment.},
	language = {eng},
	journal = {Annual Review of Psychology},
	author = {Rogoff, Barbara and Paradise, Ruth and Arauz, Rebeca Mejía and Correa-Chavez, Maricela and Angelillo, Cathy},
	year = {2003},
	pmid = {12499516},
	keywords = {Humans, speech perception, Learning, Adult, Attention, Infant, Child, Preschool, Child, Communication, Cultural Characteristics, Imitative Behavior, Motivation, Social Environment, Socialization, Teaching},
	pages = {175--203}
}

@article{bakeman-coordinating-1984,
	title = {Coordinating attention to people and objects in mother-infant and peer-infant interaction},
	volume = {55},
	issn = {0009-3920},
	abstract = {In a longitudinal study, infants 6-18 months of age were observed in their homes playing with their mothers and with peers. Of primary concern was how they coordinated their attention to people and objects. Observations were coded using a state-based scheme that included a state of coordinated joint engagement as well as states of person engagement, object engagement, onlooking, and passive joint engagement. All developmental trends observed were similar regardless of partner: person engagement declined with age, while coordinated joint engagement increased. Passive joint engagement, object engagement, and onlooking did not change with age. However, the absolute amount of some engagement states was affected by partner: both passive and coordinated joint engagement were much more likely when infants played with mothers. We conclude that mothers may indeed support or "scaffold" their infants' early attempts to embed objects in social interaction, but that as attentional capabilities develop even quite unskilled peers may be appropriate partners for the exercise of these capacities.},
	language = {eng},
	number = {4},
	journal = {Child Development},
	author = {Bakeman, R. and Adamson, L. B.},
	month = aug,
	year = {1984},
	pmid = {6488956},
	keywords = {Humans, Adult, Female, Male, Attention, Infant, Child Development, Age Factors, Interpersonal Relations, Peer Group, Longitudinal Studies, Mother-Child Relations, Play and Playthings},
	pages = {1278--1289}
}

@article{walden-development-1988,
	title = {The {Development} of {Social} {Referencing}},
	volume = {59},
	issn = {0009-3920},
	url = {http://www.jstor.org/stable/1130486},
	doi = {10.2307/1130486},
	abstract = {The development of social referencing in 40 infants aged 6-9, 10-13, and 14-22 months was investigated in this study. Social referencing was defined broadly to include children's looks toward parents, their instrumental toy behaviors, affective expressions, and other behaviors toward parents. Children's looks at parents were more selective with increasing age, with older infants preferring to look directly at their parents' faces and younger infants showing no preference for looks to faces over looks elsewhere at the parent. Younger infants looked most often when their parents expressed positive affect, whereas older infants looked most often when parents displayed fearful reactions toward a stimulus. Evidence of a behavioral regulatory effect on instrumental toy behaviors was found only among infants 10-13 months of age. However, only infants older than 14 months of age inhibited touching the toy until after referencing the parent. On some measures these older infants showed a preference for toys associated with fearful messages. Affective expressions were in line with positive and negative behavior toward toys. No support for mood modification or simple imitation as explanations for the effects was found. Results indicated that the looking behavior of younger children may function differently than that of older children, and that social referencing involves a number of component skills that develop during the end of the first year and throughout the second year of life.},
	number = {5},
	urldate = {2017-03-18},
	journal = {Child Development},
	author = {Walden, Tedra A. and Ogan, Tamra A.},
	year = {1988},
	pages = {1230--1240},
	file = {Walden_Ogan_1988_The Development of Social Referencing.pdf:/home/user/Zotero/storage/CFUGQGTQ/Walden_Ogan_1988_The Development of Social Referencing.pdf:application/pdf}
}

@article{meltzoff-infant-1988,
	title = {Infant {Imitation} {After} a 1-{Week} {Delay}: {Long}-{Term} {Memory} for {Novel} {Acts} and {Multiple} {Stimuli}},
	volume = {24},
	issn = {0012-1649},
	shorttitle = {Infant {Imitation} {After} a 1-{Week} {Delay}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4137879/},
	doi = {10.1037/0012-1649.24.4.470},
	abstract = {Deferred imitation after a 1-week delay was examined in 14-month-old infants. Six actions, each using a different object, were demonstrated to each infant. One of the six actions was a novel behavior that had a zero probability of occurrence in spontaneous play. In the imitation condition, infants observed the demonstration but were not allowed to touch the objects, thus preventing any immediate imitation. After the 1-week delay, infants returned to the laboratory and their imitation of the adult’s previous actions was scored. Infants in the imitation condition produced significantly more of the target actions than infants in control groups who were not exposed to the modeling; there was also strong evidence for the imitation of the novel act. From a cognitive perspective deferred imitation provides a means of assessing recall memory and representation in children. From a social-developmental viewpoint the findings illustrate that the behavioral repertoire of infants and their knowledge about objects can expand as a result of seeing the actions of others.},
	number = {4},
	urldate = {2017-03-18},
	journal = {Developmental psychology},
	author = {Meltzoff, Andrew N.},
	month = jul,
	year = {1988},
	pmid = {25147404},
	pmcid = {PMC4137879},
	pages = {470--476},
	file = {Meltzoff_1988_Infant Imitation After a 1-Week Delay.pdf:/home/user/Zotero/storage/NJG6GQCC/Meltzoff_1988_Infant Imitation After a 1-Week Delay.pdf:application/pdf}
}

@article{baron-cohen-does-1985,
	title = {Does the autistic child have a "theory of mind"?},
	volume = {21},
	issn = {0010-0277},
	language = {eng},
	number = {1},
	journal = {Cognition},
	author = {Baron-Cohen, S. and Leslie, A. M. and Frith, U.},
	month = oct,
	year = {1985},
	pmid = {2934210},
	keywords = {Humans, Adolescent, Autistic Disorder, Child, Social Perception, Down Syndrome},
	pages = {37--46}
}

@article{happe-role-1995,
	title = {The role of age and verbal ability in the theory of mind task performance of subjects with autism},
	volume = {66},
	issn = {0009-3920},
	abstract = {A number of studies have reported that most children with autism fail theory of mind tasks. It is unclear why certain children with autism pass such tests and what might be different about these subjects. In the present study, the role of age and verbal ability in theory of mind task performance was explored. Data were pooled from 70 autistic, 34 mentally handicapped, and 70 normal young subjects, previously tested for a number of different studies. The analysis suggested that children with autism required far higher verbal mental age to pass false belief tasks than did other subjects. While normally developing children had a 50\% probability of passing both tasks at the verbal mental age of 4 years, autistic subjects took more than twice as long to reach this probability of success (at the advanced verbal mental age of 9-2). Possible causal relations between verbal ability and the ability to represent mental states are discussed.},
	language = {eng},
	number = {3},
	journal = {Child Development},
	author = {Happé, F. G.},
	month = jun,
	year = {1995},
	pmid = {7789204},
	keywords = {Humans, Adolescent, Female, Male, Verbal Behavior, Age Factors, Autistic Disorder, Child, Preschool, Child, Cognition Disorders, Task Performance and Analysis},
	pages = {843--855}
}

@incollection{baron-cohen-precursors-1991,
	address = {Oxford},
	title = {Precursors to a theory of mind: {Understanding} attention in others},
	url = {http://web.media.mit.edu/~cynthiab/Readings/baron-cohen-91.pdf},
	urldate = {2017-03-18},
	booktitle = {Natural theories of mind: {Evolution}, development, and simulation of everyday mindreading},
	publisher = {Blackwell},
	author = {Baron-Cohen, Simon},
	year = {1991},
	pages = {233--251},
	file = {baron-cohen-91.pdf:/home/user/Zotero/storage/PFDKD44Z/baron-cohen-91.pdf:application/pdf}
}

@misc{noauthor-psycnet-nodate-2,
	title = {{PsycNET} - {Display} {Record}},
	url = {http://psycnet.apa.org/psycinfo/1995-01247-001},
	urldate = {2017-03-18},
	file = {PsycNET - Display Record:/home/user/Zotero/storage/B696TMHJ/1995-01247-001.html:text/html}
}

@article{tomasello-eighteen-month-old-1996-1,
	title = {Eighteen-month-old children learn words in non-ostensive contexts},
	volume = {23},
	issn = {0305-0009},
	abstract = {Previous studies have demonstrated that children aged 2;0 can learn new words in a variety of non-ostensive contexts. The current two studies were aimed at seeing if this was also true of children just beginning to learn words at 1;6. In the first study an adult interacted with 48 children. She used a nonce word to announce her intention to find an object ('Let's find the gazzer'), picked up and rejected an object with obvious disappointment, and then gleefully found the target object (using no language). Children learned the new word as well in this condition as in a condition in which the adult found the object immediately. In the second study the adult first played several rounds of a finding game with each of 60 children, in which it was first established that one of several novel objects was always in a very distinctive hiding place (a toy barn). The adult then used a nonce word to announce her intention to find an object ('Let's find the toma') and then proceeded to the barn. In the key condition the barn was mysteriously 'locked'; the child thus never saw the target object after the nonce word was introduced. Children learned the new word as well in this condition as in a condition in which the adult found the object immediately. The results of these two studies suggest that from very early in language acquisition children learn words not through passive, associative processes, but rather through active attempts to understand adult behaviour in a variety of action and discourse contexts.},
	language = {eng},
	number = {1},
	journal = {Journal of Child Language},
	author = {Tomasello, M. and Strosberg, R. and Akhtar, N.},
	month = feb,
	year = {1996},
	pmid = {8733565},
	keywords = {Humans, Language Development, Adult, Female, Male, Infant, Verbal Learning, Child, Preschool, Child, Child Language},
	pages = {157--176}
}

@article{scofield-clarifying-2011,
	title = {Clarifying the role of joint attention in early word learning},
	volume = {31},
	issn = {0142-7237},
	url = {http://journals.sagepub.com/doi/abs/10.1177/0142723710395423},
	doi = {10.1177/0142723710395423},
	abstract = {Four studies examined whether two-year-olds could successfully learn a novel word in conditions in which joint attention was not present. Study 1 examined whether children could learn a novel word while the speaker, but not the child, attended to the target object. Study 2 examined whether children could learn a novel word while the child, but not the speaker, attended to the target object. Study 3 examined whether children could learn a novel word while the child and the speaker attended to two different target objects. Study 4 examined whether children could learn a novel word while neither the child nor the speaker attended to the target object. Findings showed that successful word learning occurred in each of the four studies. These results suggest that joint attention may play an important, though not a necessary, role in young children’s word learning.},
	language = {en},
	number = {3},
	urldate = {2017-03-18},
	journal = {First Language},
	author = {Scofield, Jason and Behrend, Douglas A.},
	month = aug,
	year = {2011},
	pages = {326--341},
	file = {Scofield_Behrend_2011_Clarifying the role of joint attention in early word learning.pdf:/home/user/Zotero/storage/EERAKXUU/Scofield_Behrend_2011_Clarifying the role of joint attention in early word learning.pdf:application/pdf}
}

@article{tomasello-childrens-1984,
	title = {Children's {Speech} {Revisions} for a {Familiar} and an {Unfamiliar} {Adult}},
	volume = {27},
	issn = {1092-4388},
	url = {http://jslhr.pubs.asha.org/article.aspx?articleid=1778914},
	doi = {10.1044/jshr.2703.359},
	number = {3},
	urldate = {2017-03-18},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Tomasello, Michael and Farrar, Michael Jeffrey and Dines, Jennifer},
	month = sep,
	year = {1984},
	pages = {359--363},
	file = {Snapshot:/home/user/Zotero/storage/93G8245V/article.html:text/html;Tomasello et al_1984_Children's Speech Revisions for a Familiar and an Unfamiliar Adult.pdf:/home/user/Zotero/storage/7PRBN58X/Tomasello et al_1984_Children's Speech Revisions for a Familiar and an Unfamiliar Adult.pdf:application/pdf}
}

@misc{noauthor-statistical-nodate,
	title = {Statistical {Machine} {Learning} 10-702/36-702},
	url = {http://www.stat.cmu.edu/~larry/=sml/},
	urldate = {2017-03-18},
	file = {Statistical Machine Learning 10-702/36-702:/home/user/Zotero/storage/85FFGCZN/=sml.html:text/html}
}

@article{sumner-effect-2009,
	title = {The effect of experience on the perception and representation of dialect variants},
	volume = {60},
	issn = {0749-596X},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X09000096},
	doi = {10.1016/j.jml.2009.01.001},
	abstract = {The task of recognizing spoken words is notoriously difficult. Once dialectal variation is considered, the difficulty of this task increases. When living in a new dialect region, however, processing difficulties associated with dialectal variation dissipate over time. Through a series of primed lexical decision tasks (form priming, semantic priming, and long-term repetition priming), we examine the general issue of dialectal variation in spoken word recognition, while investigating the role of experience in perception and representation. The main questions we address are: (1) how are cross-dialect variants recognized and stored, and (2) how are these variants accommodated by listeners with different levels of exposure to the dialect? Three claims are made based on the results: (1) dialect production is not always representative of dialect perception and representation, (2) experience strongly affects a listener’s ability to recognize and represent spoken words, and (3) there is a general benefit for variants that are not regionally-marked.},
	number = {4},
	urldate = {2017-03-17},
	journal = {Journal of Memory and Language},
	author = {Sumner, Meghan and Samuel, Arthur G.},
	month = may,
	year = {2009},
	keywords = {speech perception, Spoken word recognition, Dialect, Experience, r-Dropping, Variation},
	pages = {487--501},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/B3EE3PXI/S0749596X09000096.html:text/html;Sumner_Samuel_2009_The effect of experience on the perception and representation of dialect.pdf:/home/user/Zotero/storage/HZT95KXQ/Sumner_Samuel_2009_The effect of experience on the perception and representation of dialect.pdf:application/pdf}
}

@misc{noauthor-perception-nodate,
	title = {Perception and {Representation} of {Regular} {Variation}: the {Case} of {Final} /t/ {Journal} of {Memory} and {Language} - {Semantic} {Scholar}},
	shorttitle = {Perception and {Representation} of {Regular} {Variation}},
	url = {/paper/Perception-and-Representation-of-Regular-Variation-Sumner-Samuel/58564e1c165e33f7fa53d64ac78ddda8c576d305},
	abstract = {Spoken words exhibit considerable variation from their hypothesized canonical forms. Much of the variation is regular , occurring often in language. The present work examines the immediate and long-term processing consequences for rule-governed final-/t/ variation in English. Two semantic priming experiments demonstrate that variation does not hinder short-term semantic processing, as long as variation is not arbitrary. Two long-term priming experiments with different tasks show that form processing over time is not as lenient as immediate semantic processing: strong priming is found only for the canonical, unchanged form of /t/. Our results suggest that surface information is used in immediate processing and exemplar representations for regular variants are not stored in long-term memory. Listeners are confronted by a remarkably variable signal when they understand spoken language. Variation in the speech signal may be caused by many factors, such as speaker variation, speech rate, speech style, or phono-tactic variation. A central issue in the perception of spoken language is this substantial variation found in the speech signal, and more specifically, the ways in which this variation is accommodated. Research in this area has been divided between studies of phonologically reg-tent, 2001) or by splicing together segments with mis-matching formant information due to cross-splicing (Marslen-Wilson \&amp; Warren, 1994). For both types of variation, a critical theoretical question is how listeners cope with such variation. The current study is concerned with two major issues regarding variation: (1) What effect (if any) does regular variation have on immediate word recognition? and (2) How do listeners represent regular variants in the long term? Understanding variation requires specification of both the recognition process and the longer-term representation of variable forms. The first issue focuses on whether regular phonetic variation has particular costs (or benefits) for word recognition, and the extent to We thank Carol Fowler, Stephen Goldinger, and two anonymous reviewers for their invaluable comments and suggestions. We also thank Donna Kat for help with this project.},
	urldate = {2017-03-17},
	file = {Snapshot:/home/user/Zotero/storage/GSRDUAI4/58564e1c165e33f7fa53d64ac78ddda8c576d305.html:text/html}
}

@misc{noauthor-effect-nodate,
	title = {The effect of experience on the perception and representation of dialect variants},
	url = {https://www.researchgate.net/publication/222821498\_The\_effect\_of\_experience\_on\_the\_perception\_and\_representation\_of\_dialect\_variants},
	abstract = {The effect of experience on the perception and representation of dialect variants on ResearchGate, the professional network for scientists.},
	urldate = {2017-03-17},
	journal = {ResearchGate},
	file = {Snapshot:/home/user/Zotero/storage/4BAW8C2E/222821498_The_effect_of_experience_on_the_perception_and_representation_of_dialect_variants.html:text/html}
}

@article{whalen-subcategorical-1984,
	title = {Subcategorical phonetic mismatches slow phonetic judgments},
	volume = {35},
	issn = {0031-5117, 1532-5962},
	url = {https://link.springer.com/article/10.3758/BF03205924},
	doi = {10.3758/BF03205924},
	abstract = {When an [s] or an [š] fricative noise is combined with vocalic formant transitions appropriate to a different fricative, the resulting consonantal percept is usually that of the noise. To see if the mismatch affects processing time, five experiments were run. Three experiments examined reaction time for identification of [s] and [š], as well as the whole syllable (in one experiment) or only the vowel (in the others). The stimuli contained either appropriate or inappropriate formant transitions, and the vowel information in the noise was either appropriate or not. Subjects were significantly slower in all tasks in identifying stimuli with inappropriate transitions or inappropriate vowel information. Similar results were obtained with stop-vowel syllables in which the release bursts of syllable initials [p] and [k]were transposed in syllables containing the vowels [a] and [u], In the fifth experiment, enough silence was introduced between the initial fricatives and vocalic segment for the vocalic formant transitions to be perceived as a stop (e.g., [stu] from [su]). Mismatched transitions still had an effect on reaction time, as did mismatches of vowel quality. The results indicate that listeners take into account all available cues, even when the phonetic judgment seems to be based on only some of the cues.},
	language = {en},
	number = {1},
	urldate = {2017-03-17},
	journal = {Perception \& Psychophysics},
	author = {Whalen, D. H.},
	month = jan,
	year = {1984},
	pages = {49--64},
	file = {Snapshot:/home/user/Zotero/storage/WT6EFM5G/BF03205924.html:text/html;Whalen_1984_Subcategorical phonetic mismatches slow phonetic judgments.pdf:/home/user/Zotero/storage/B8HRSQD4/Whalen_1984_Subcategorical phonetic mismatches slow phonetic judgments.pdf:application/pdf}
}

@article{whalen-subcategorical-1991,
	title = {Subcategorical phonetic mismatches and lexical access},
	volume = {50},
	issn = {0031-5117, 1532-5962},
	url = {https://link.springer.com/article/10.3758/BF03212227},
	doi = {10.3758/BF03212227},
	abstract = {The place of phonetic analysis in the perception of words is unclear. While some theories assume fully specified phonemic strings as input, other theories assume that little analysis occurs. An earlier experiment by Streeter and Nigro (1979) produced evidence, based on auditorily presented words with misleading acoustic cues, that lexical decisions were based on mostly unanalyzed patterns, since word judgments were delayed by misleading information whereas non word judgments were not. The present studies expand that work to a different set of cues, and to cases in which the overriding cue came first. An additional task, auditory naming, was used to examine the effects when the decision stage is less demanding. For the lexical decision task, misleading information slowed the responses, for both words and nonwords. In the auditory naming task, only the slower responses were affected. These results suggest that phonetic conflicts are resolved prior to lexical access.},
	language = {en},
	number = {4},
	urldate = {2017-03-17},
	journal = {Perception \& Psychophysics},
	author = {Whalen, D. H.},
	month = jul,
	year = {1991},
	pages = {351--360},
	file = {Snapshot:/home/user/Zotero/storage/3A4AT86U/BF03212227.html:text/html;Subcategorical phonetic mismatches and lexical access - art%3A10.3758%2FBF03212227.pdf:/home/user/Zotero/storage/S68PXMC4/art%3A10.3758%2FBF03212227.pdf:application/pdf}
}

@article{marslen-wilson-levels-1994,
	title = {Levels of perceptual representation and process in lexical access: words, phonemes, and features},
	volume = {101},
	issn = {0033-295X},
	shorttitle = {Levels of perceptual representation and process in lexical access},
	abstract = {Three experiments and a simulation study investigate competing featural and phonemic views of the representation of the speech input in access to the mental lexicon. Auditory lexical decision and gating tasks show that the processing consequences of subcategorical mismatches (conflicts between phonetic cues to speech segment identity) depend on the lexical status of the conflicting cues, such that conflicts that only involve nonwords do not disrupt performance. A further study, using a phonetic-decision task with the same stimuli, found the same pattern. A simulation study shows that the interactive activation model TRACE, with top-down feedback to a prelexical phonemic level, does not model these effects successfully. The authors argue instead for a direct access featural model, based on a distributed computational substrate, where featural information is mapped directly onto lexical representations.},
	language = {eng},
	number = {4},
	journal = {Psychological Review},
	author = {Marslen-Wilson, W. and Warren, P.},
	month = oct,
	year = {1994},
	pmid = {7984710},
	keywords = {Humans, Phonetics, speech perception, Adult, Cues, Analysis of Variance, Psychological Theory, Speech Articulation Tests, Speech Discrimination Tests},
	pages = {653--675}
}

@article{norris-prediction-2016-1,
	title = {Prediction, {Bayesian} inference and feedback in speech recognition},
	volume = {31},
	issn = {2327-3798},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4685608/},
	doi = {10.1080/23273798.2015.1081703},
	abstract = {Speech perception involves prediction, but how is that prediction implemented? In cognitive models prediction has often been taken to imply that there is feedback of activation from lexical to pre-lexical processes as implemented in interactive-activation models (IAMs). We show that simple activation feedback does not actually improve speech recognition. However, other forms of feedback can be beneficial. In particular, feedback can enable the listener to adapt to changing input, and can potentially help the listener to recognise unusual input, or recognise speech in the presence of competing sounds. The common feature of these helpful forms of feedback is that they are all ways of optimising the performance of speech recognition using Bayesian inference. That is, listeners make predictions about speech because speech recognition is optimal in the sense captured in Bayesian models.},
	number = {1},
	urldate = {2017-03-17},
	journal = {Language, Cognition and Neuroscience},
	author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
	month = jan,
	year = {2016},
	pmid = {26740960},
	pmcid = {PMC4685608},
	pages = {4--18},
	file = {Norris et al_2016_Prediction, Bayesian inference and feedback in speech recognition.pdf:/home/user/Zotero/storage/W4RFBE5J/Norris et al_2016_Prediction, Bayesian inference and feedback in speech recognition.pdf:application/pdf}
}

@article{dickman-analysis-2012,
	title = {Analysis of an information-theoretic model for communication},
	volume = {2012},
	issn = {1742-5468},
	url = {http://stacks.iop.org/1742-5468/2012/i=12/a=P12022},
	doi = {10.1088/1742-5468/2012/12/P12022},
	abstract = {We study the cost-minimization problem posed by Ferrer i Cancho and Solé in their model of communication that aimed at explaining the origin of Zipf’s law (2003 Proc. Nat. Acad. Sci. 100 788). Direct analysis shows that the minimum cost is minλ,1 − λ, where λ determines the relative weights of speaker’s and hearer’s costs in the total, as shown in several previous works using different approaches. The nature and multiplicity of the minimizing solution change discontinuously at λ = 1/2, being qualitatively different for λ 1/2, and λ = 1/2. Zipf’s law is found only in a vanishing fraction of the minimum-cost solutions at λ = 1/2 and therefore is not explained by this model. Imposing the further condition of equal costs yields distributions substantially closer to Zipf’s law ones, but significant differences persist. We also investigate the solutions reached with the previously used minimization algorithm and find that they correctly recover global minimum states at the transition.},
	language = {en},
	number = {12},
	urldate = {2017-03-17},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Dickman, Ronald and Moloney, Nicholas R. and Altmann, Eduardo G.},
	year = {2012},
	pages = {P12022},
	file = {Dickman et al_2012_Analysis of an information-theoretic model for communication.pdf:/home/user/Zotero/storage/CC5P5I44/Dickman et al_2012_Analysis of an information-theoretic model for communication.pdf:application/pdf}
}

@article{altmann-identifying-2013,
	title = {Identifying {Trends} in {Word} {Frequency} {Dynamics}},
	volume = {151},
	issn = {0022-4715, 1572-9613},
	url = {https://link.springer.com/article/10.1007/s10955-013-0699-7},
	doi = {10.1007/s10955-013-0699-7},
	abstract = {The word-stock of a language is a complex dynamical system in which words can be created, evolve, and become extinct. Even more dynamic are the short-term fluctuations in word usage by individuals in a population. Building on the recent demonstration that word niche is a strong determinant of future rise or fall in word frequency, here we introduce a model that allows us to distinguish persistent from temporary increases in frequency. Our model is illustrated using a 108-word database from an online discussion group and a 1011-word collection of digitized books. The model reveals a strong relation between changes in word dissemination and changes in frequency. Aside from their implications for short-term word frequency dynamics, these observations are potentially important for language evolution as new words must survive in the short term in order to survive in the long term.},
	language = {en},
	number = {1-2},
	urldate = {2017-03-17},
	journal = {Journal of Statistical Physics},
	author = {Altmann, Eduardo G. and Whichard, Zakary L. and Motter, Adilson E.},
	month = apr,
	year = {2013},
	pages = {277--288},
	file = {Altmann et al_2013_Identifying Trends in Word Frequency Dynamics.pdf:/home/user/Zotero/storage/7PI4VE95/Altmann et al_2013_Identifying Trends in Word Frequency Dynamics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/NQDQN29A/10.html:text/html}
}

@article{gerlach-scaling-2014,
	title = {Scaling laws and fluctuations in the statistics of word frequencies},
	volume = {16},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/16/i=11/a=113010},
	doi = {10.1088/1367-2630/16/11/113010},
	abstract = {In this paper, we combine statistical analysis of written texts and simple stochastic models to explain the appearance of scaling laws in the statistics of word frequencies. The average vocabulary of an ensemble of fixed-length texts is known to scale sublinearly with the total number of words (Heaps’ law). Analyzing the fluctuations around this average in three large databases (Google-ngram, English Wikipedia, and a collection of scientific articles), we find that the standard deviation scales linearly with the average (Taylorʼs law), in contrast to the prediction of decaying fluctuations obtained using simple sampling arguments. We explain both scaling laws (Heaps’ and Taylor) by modeling the usage of words using a Poisson process with a fat-tailed distribution of word frequencies (Zipfʼs law) and topic-dependent frequencies of individual words (as in topic models). Considering topical variations lead to quenched averages, turn the vocabulary size a non-self-averaging quantity, and explain the empirical observations. For the numerous practical applications relying on estimations of vocabulary size, our results show that uncertainties remain large even for long texts. We show how to account for these uncertainties in measurements of lexical richness of texts with different lengths.},
	language = {en},
	number = {11},
	urldate = {2017-03-17},
	journal = {New Journal of Physics},
	author = {Gerlach, Martin and Altmann, Eduardo G.},
	year = {2014},
	pages = {113010},
	file = {Gerlach_Altmann_2014_Scaling laws and fluctuations in the statistics of word frequencies.pdf:/home/user/Zotero/storage/PJDKTIH8/Gerlach_Altmann_2014_Scaling laws and fluctuations in the statistics of word frequencies.pdf:application/pdf}
}

@article{ghanbarnejad-extracting-2014,
	title = {Extracting information from {S}-curves of language change},
	volume = {11},
	copyright = {© 2014 The Author(s) Published by the Royal Society. All rights reserved.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/11/101/20141044},
	doi = {10.1098/rsif.2014.1044},
	abstract = {It is well accepted that adoption of innovations are described by S-curves (slow start, accelerating period and slow end). In this paper, we analyse how much information on the dynamics of innovation spreading can be obtained from a quantitative description of S-curves. We focus on the adoption of linguistic innovations for which detailed databases of written texts from the last 200 years allow for an unprecedented statistical precision. Combining data analysis with simulations of simple models (e.g. the Bass dynamics on complex networks), we identify signatures of endogenous and exogenous factors in the S-curves of adoption. We propose a measure to quantify the strength of these factors and three different methods to estimate it from S-curves. We obtain cases in which the exogenous factors are dominant (in the adoption of German orthographic reforms and of one irregular verb) and cases in which endogenous factors are dominant (in the adoption of conventions for romanization of Russian names and in the regularization of most studied verbs). These results show that the shape of S-curve is not universal and contains information on the adoption mechanism.},
	language = {en},
	number = {101},
	urldate = {2017-03-17},
	journal = {Journal of The Royal Society Interface},
	author = {Ghanbarnejad, Fakhteh and Gerlach, Martin and Miotto, José M. and Altmann, Eduardo G.},
	month = dec,
	year = {2014},
	pmid = {25339692},
	pages = {20141044},
	file = {Ghanbarnejad et al_2014_Extracting information from S-curves of language change.pdf:/home/user/Zotero/storage/8PC8NS8D/Ghanbarnejad et al_2014_Extracting information from S-curves of language change.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CSBTHBQ2/20141044.html:text/html}
}

@article{altmann-generalized-2017,
	title = {Generalized entropies and the similarity of texts},
	volume = {2017},
	issn = {1742-5468},
	url = {http://stacks.iop.org/1742-5468/2017/i=1/a=014002},
	doi = {10.1088/1742-5468/aa53f5},
	abstract = {We show how generalized Gibbs–Shannon entropies can provide new insights on the statistical properties of texts. The universal distribution of word frequencies (Zipf’s law) implies that the generalized entropies, computed at the word level, are dominated by words in a specific range of frequencies. Here we show that this is the case not only for the generalized entropies but also for the generalized (Jensen–Shannon) divergences, used to compute the similarity between different texts. This finding allows us to identify the contribution of specific words (and word frequencies) for the different generalized entropies and also to estimate the size of the databases needed to obtain a reliable estimation of the divergences. We test our results in large databases of books (from the google n-gram database) and scientific papers (indexed by Web of Science).},
	language = {en},
	number = {1},
	urldate = {2017-03-17},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Altmann, Eduardo G. and Dias, Laércio and Gerlach, Martin},
	year = {2017},
	pages = {014002},
	file = {Altmann et al_2017_Generalized entropies and the similarity of texts.pdf:/home/user/Zotero/storage/XR4RVKIG/Altmann et al_2017_Generalized entropies and the similarity of texts.pdf:application/pdf}
}

@misc{noauthor-physicists-nodate,
	title = {Physicists' papers on natural language},
	url = {http://www.maths.usyd.edu.au/u/ega/physicist-language/},
	urldate = {2017-03-17},
	file = {Physicists' papers on natural language:/home/user/Zotero/storage/HURSHWFW/physicist-language.html:text/html}
}

@article{xiong-bayesian-2011,
	title = {Bayesian prediction of tissue-regulated splicing using {RNA} sequence and cellular context},
	issn = {1367-4803, 1460-2059},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr444},
	doi = {10.1093/bioinformatics/btr444},
	language = {en},
	urldate = {2017-03-17},
	journal = {Bioinformatics},
	author = {Xiong, H. Y. and Barash, Y. and Frey, B. J.},
	month = jul,
	year = {2011},
	file = {btr444.dvi - btr444.pdf:/home/user/Zotero/storage/27V8JIPA/btr444.pdf:application/pdf}
}

@inproceedings{hernandez-lobato-probabilistic-2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}.},
	url = {http://www.jmlr.org/proceedings/papers/v37/hernandez-lobatoc15.pdf},
	urldate = {2017-03-17},
	booktitle = {{ICML}},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan},
	year = {2015},
	pages = {1861--1869},
	file = {1502.05336.pdf:/home/user/Zotero/storage/TXPAHSG5/1502.05336.pdf:application/pdf}
}

@inproceedings{graves-practical-2011,
	title = {Practical variational inference for neural networks},
	url = {http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks},
	urldate = {2017-03-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Graves, Alex},
	year = {2011},
	pages = {2348--2356},
	file = {nips_2011.pdf:/home/user/Zotero/storage/A7B2X8XB/nips_2011.pdf:application/pdf}
}

@article{wedel-functional-2013,
	title = {Functional load and the lexicon: {Evidence} that syntactic category and frequency relationships in minimal lemma pairs predict the loss of phoneme contrasts in language change},
	volume = {56},
	shorttitle = {Functional load and the lexicon},
	url = {http://journals.sagepub.com/doi/abs/10.1177/0023830913489096},
	number = {3},
	urldate = {2017-03-17},
	journal = {Language and speech},
	author = {Wedel, Andrew and Jackson, Scott and Kaplan, Abby},
	year = {2013},
	pages = {395--417},
	file = {LAS489096.indd - 0023830913489096:/home/user/Zotero/storage/XQUCI6X6/0023830913489096.pdf:application/pdf}
}

@article{wedel-high-2013,
	title = {High functional load inhibits phonological contrast loss: a corpus study},
	volume = {128},
	issn = {1873-7838},
	shorttitle = {High functional load inhibits phonological contrast loss},
	doi = {10.1016/j.cognition.2013.03.002},
	abstract = {For nearly a century, linguists have suggested that diachronic merger is less likely between phonemes with a high functional load--that is, phonemes that distinguish many words in the language in question. However, limitations in data and computational power have made assessing this hypothesis difficult. Here we present the first larger-scale study of the functional load hypothesis, using data from sound changes in a diverse set of languages. Our results support the functional load hypothesis: phoneme pairs undergoing merger distinguish significantly fewer minimal pairs in the lexicon than unmerged phoneme pairs. Furthermore, we show that higher phoneme probability is positively correlated with merger, but that this effect is stronger for phonemes that distinguish no minimal pairs. Finally, within our dataset we find that minimal pair count and phoneme probability better predict merger than change in system entropy at the lexical or phoneme level.},
	language = {eng},
	number = {2},
	journal = {Cognition},
	author = {Wedel, Andrew and Kaplan, Abby and Jackson, Scott},
	month = aug,
	year = {2013},
	pmid = {23685207},
	keywords = {Humans, Linguistics, Phonetics, psycholinguistics, Models, Psychological, Probability},
	pages = {179--186}
}

@article{gaskell-nature-2008,
	title = {The nature of phoneme representation in spoken word recognition},
	volume = {137},
	issn = {0096-3445},
	doi = {10.1037/0096-3445.137.2.282},
	abstract = {Four experiments used the psychological refractory period logic to examine whether integration of multiple sources of phonemic information has a decisional locus. All experiments made use of a dual-task paradigm in which participants made forced-choice color categorization (Task 1) and phoneme categorization (Task 2) decisions at varying stimulus onset asynchronies. In Experiment 1, Task 2 difficulty was manipulated using words containing matching or mismatching coarticulatory cues to the final consonant. The results showed that difficulty and onset asynchrony combined in an underadditive way, suggesting that the phonemic mismatch was resolved prior to a central decisional bottleneck. Similar results were found in Experiment 2 using nonwords. In Experiment 3, the manipulation of task difficulty involved lexical status, which once again revealed an underadditive pattern of response times. Finally, Experiment 4 compared this prebottleneck variable with a decisional variable: response key bias. The latter showed an additive pattern of responses. The experiments show that resolution of phonemic ambiguity can take advantage of cognitive slack time at short asynchronies, indicating that phonemic integration takes place at a relatively early stage of spoken word recognition.},
	language = {eng},
	number = {2},
	journal = {Journal of Experimental Psychology. General},
	author = {Gaskell, M. Gareth and Quinlan, Philip T. and Tamminen, Jakke and Cleland, Alexandra A.},
	month = may,
	year = {2008},
	pmid = {18473660},
	keywords = {Humans, Pattern Recognition, Visual, Phonetics, psycholinguistics, Reading, speech perception, Adult, Female, Male, decision making, Reaction Time, Choice Behavior, Color Perception, Refractory Period, Psychological},
	pages = {282--302},
	file = {Microsoft Word - JEPG-for web.doc - JEPG-for web.pdf:/home/user/Zotero/storage/DAECEVMB/JEPG-for web.pdf:application/pdf}
}

@article{marslen-wilson-levels-1994-1,
	title = {Levels of perceptual representation and process in lexical access: {Words}, phonemes, and features.},
	volume = {101},
	issn = {0033-295X},
	shorttitle = {Levels of perceptual representation and process in lexical access},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.101.4.653},
	doi = {10.1037/0033-295X.101.4.653},
	language = {en},
	number = {4},
	urldate = {2017-03-17},
	journal = {Psychological Review},
	author = {Marslen-Wilson, William and Warren, Paul},
	year = {1994},
	pages = {653--675},
	file = {Levels-of-perceptual-representation-and-process-in-lexical-access-Words-phonemes-and-features.pdf:/home/user/Zotero/storage/I4XSS5U5/Levels-of-perceptual-representation-and-process-in-lexical-access-Words-phonemes-and-features.pdf:application/pdf}
}

@article{dahan-subcategorical-2001,
	title = {Subcategorical mismatches and the time course of lexical access: {Evidence} for lexical competition},
	volume = {16},
	issn = {0169-0965, 1464-0732},
	shorttitle = {Subcategorical mismatches and the time course of lexical access},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690960143000074},
	doi = {10.1080/01690960143000074},
	language = {en},
	number = {5-6},
	urldate = {2017-03-17},
	journal = {Language and Cognitive Processes},
	author = {Dahan, Delphine and Magnuson, James S. and Tanenhaus, Michael K. and Hogan, Ellen M.},
	month = oct,
	year = {2001},
	pages = {507--534},
	file = {Subcategorical mismatches and the time course of lexical access\: Evidence for lexical competition - dahan_lcp01.pdf:/home/user/Zotero/storage/ZP5S87TJ/dahan_lcp01.pdf:application/pdf}
}

@misc{noauthor-incremental-nodate,
	title = {Incremental interpretation at verbs: restricting the domain of subsequent reference},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027799000591},
	urldate = {2017-03-17},
	file = {Incremental interpretation at verbs\: restricting the domain of subsequent reference:/home/user/Zotero/storage/AIG9UK63/S0010027799000591.html:text/html;PII\: S0010-0277(99)00059-1 - 1-s2.0-S0010027799000591-main.pdf:/home/user/Zotero/storage/AT5QZGEI/1-s2.0-S0010027799000591-main.pdf:application/pdf}
}

@article{beddor-time-2013,
	title = {The time course of perception of coarticulation},
	volume = {133},
	url = {http://asa.scitation.org/doi/abs/10.1121/1.4794366},
	number = {4},
	urldate = {2017-03-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Beddor, Patrice Speeter and McGowan, Kevin B. and Boland, Julie E. and Coetzee, Andries W. and Brasher, Anthony},
	year = {2013},
	pages = {2350--2366},
	file = {untitled - BeddorMcGowanBolandCoetzeeBrasher_2013.pdf:/home/user/Zotero/storage/S63BUXUC/BeddorMcGowanBolandCoetzeeBrasher_2013.pdf:application/pdf}
}

@article{kamide-anticipatory-2008,
	title = {Anticipatory {Processes} in {Sentence} {Processing}},
	volume = {2},
	issn = {1749818X, 1749818X},
	url = {http://doi.wiley.com/10.1111/j.1749-818X.2008.00072.x},
	doi = {10.1111/j.1749-818X.2008.00072.x},
	language = {en},
	number = {4},
	urldate = {2017-03-17},
	journal = {Language and Linguistics Compass},
	author = {Kamide, Yuki},
	month = jul,
	year = {2008},
	pages = {647--670},
	file = {Anticipatory Processes in Sentence Processing - levy_reading.pdf:/home/user/Zotero/storage/RIHE3PEG/levy_reading.pdf:application/pdf}
}

@article{kamide-time-course-2003,
	title = {The time-course of prediction in incremental sentence processing: {Evidence} from anticipatory eye movements},
	volume = {49},
	issn = {0749596X},
	shorttitle = {The time-course of prediction in incremental sentence processing},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X03000238},
	doi = {10.1016/S0749-596X(03)00023-8},
	language = {en},
	number = {1},
	urldate = {2017-03-17},
	journal = {Journal of Memory and Language},
	author = {Kamide, Yuki and Altmann, Gerry T.M and Haywood, Sarah L},
	month = jul,
	year = {2003},
	pages = {133--156}
}

@article{gt-language-2011,
	title = {Language can mediate eye movement control within 100 milliseconds, regardless of whether there is anything to move the eyes to., {Language} can mediate eye movement control within 100 milliseconds, regardless of whether there is anything to move the eyes to},
	volume = {137, 137},
	issn = {0001-6918},
	url = {http://europepmc.org/abstract/MED/20965479, http://europepmc.org/articles/PMC3118831/?report=abstract},
	doi = {10.1016/j.actpsy.2010.09.009},
	abstract = {FULL TEXT Abstract: The delay between the signal to move the eyes, and the execution of the corresponding eye movement, is variable, and skewed; with an early peak...},
	language = {eng},
	number = {2, 2},
	urldate = {2017-03-17},
	journal = {Acta psychologica, Acta Psychologica},
	author = {Gt, Altmann},
	month = jun,
	year = {2011},
	pmid = {20965479},
	pages = {190, 190--200},
	file = {Snapshot:/home/user/Zotero/storage/WH2WR6XU/PMC3118831.html:text/html}
}

@article{trottier-visual-2005,
	title = {Visual processing of targets can reduce saccadic latencies},
	volume = {45},
	issn = {00426989},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698905000180},
	doi = {10.1016/j.visres.2004.12.007},
	language = {en},
	number = {11},
	urldate = {2017-03-17},
	journal = {Vision Research},
	author = {Trottier, Leo and Pratt, Jay},
	month = may,
	year = {2005},
	pages = {1349--1354},
	file = {Visual-processing-of-targets-can-reduce-saccadic-latencies.pdf:/home/user/Zotero/storage/FITK6XMU/Visual-processing-of-targets-can-reduce-saccadic-latencies.pdf:application/pdf}
}

@article{dahan-continuous-2004,
	title = {Continuous mapping from sound to meaning in spoken-language comprehension: {Evidence} from immediate effects of verb-based constraints},
	shorttitle = {Continuous mapping from sound to meaning in spoken-language comprehension},
	abstract = {The authors used 2 “visual-world” eye-tracking experiments to examine lexical access using Dutch constructions in which the verb did or did not place semantic constraints on its subsequent subject noun phrase. In Experiment 1, fixations to the picture of a cohort competitor (overlapping with the onset of the referent’s name, the subject) did not differ from fixations to a distractor in the constraining-verb condition. In Experiment 2, cross-splicing introduced phonetic information that temporarily biased the input toward the cohort competitor. Fixations to the cohort competitor temporarily increased in both the neutral and constraining conditions. These results favor models in which mapping from the input onto meaning is continuous over models in which contextual effects follow access of an initial form-based competitor set.},
	journal = {Journal of Experimental Psychology: Learning, Memory \& Cognition},
	author = {Dahan, Delphine and Tanenhaus, Michael K.},
	year = {2004},
	pages = {498--513},
	file = {Citeseer - Snapshot:/home/user/Zotero/storage/I45QMBKG/summary.html:text/html;Dahan_Tanenhaus_2004_Continuous mapping from sound to meaning in spoken-language comprehension.pdf:/home/user/Zotero/storage/4ARWJU7J/Dahan_Tanenhaus_2004_Continuous mapping from sound to meaning in spoken-language comprehension.pdf:application/pdf}
}

@article{norris-perceptual-2003,
	title = {Perceptual learning in speech},
	volume = {47},
	issn = {0010-0285},
	abstract = {This study demonstrates that listeners use lexical knowledge in perceptual learning of speech sounds. Dutch listeners first made lexical decisions on Dutch words and nonwords. The final fricative of 20 critical words had been replaced by an ambiguous sound, between [f] and [s]. One group of listeners heard ambiguous [f]-final words (e.g., [WItlo?], from witlof, chicory) and unambiguous [s]-final words (e.g., naaldbos, pine forest). Another group heard the reverse (e.g., ambiguous [na:ldbo?], unambiguous witlof). Listeners who had heard [?] in [f]-final words were subsequently more likely to categorize ambiguous sounds on an [f]-[s] continuum as [f] than those who heard [?] in [s]-final words. Control conditions ruled out alternative explanations based on selective adaptation and contrast. Lexical information can thus be used to train categorization of speech. This use of lexical information differs from the on-line lexical feedback embodied in interactive models of speech perception. In contrast to on-line feedback, lexical feedback for learning is of benefit to spoken word recognition (e.g., in adapting to a newly encountered dialect).},
	language = {eng},
	number = {2},
	journal = {Cognitive Psychology},
	author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
	month = sep,
	year = {2003},
	pmid = {12948518},
	keywords = {Humans, Semantics, Learning, speech, Perception},
	pages = {204--238}
}

@article{marslen-wilson-levels-1994-2,
	title = {Levels of perceptual representation and process in lexical access: {Words}, phonemes, and features.},
	volume = {101},
	issn = {0033-295X},
	shorttitle = {Levels of perceptual representation and process in lexical access},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.101.4.653},
	doi = {10.1037/0033-295X.101.4.653},
	language = {en},
	number = {4},
	urldate = {2017-03-17},
	journal = {Psychological Review},
	author = {Marslen-Wilson, William and Warren, Paul},
	year = {1994},
	pages = {653--675},
	file = {Levels-of-perceptual-representation-and-process-in-lexical-access-Words-phonemes-and-features.pdf:/home/user/Zotero/storage/KBF5TBPK/Levels-of-perceptual-representation-and-process-in-lexical-access-Words-phonemes-and-features.pdf:application/pdf}
}

@article{fowler-segmentation-1984,
	title = {Segmentation of coarticulated speech in perception},
	volume = {36},
	issn = {0031-5117},
	doi = {10.3758/BF03202790},
	abstract = {The research investigates how listeners segment the acoustic speech signal into phonetic segments and explores implications that the segmentation strategy may have for their perception of the (apparently) context-sensitive allophones of a phoneme. Two manners of segmentation are contrasted. In one, listeners segment the signal into temporally discrete, context-sensitive segments. In the other, which may be consistent with the talker's production of the segments, they partition the signal into separate, but overlapping, segments freed of their contextual influences. Two complementary predictions of the second hypothesis are tested. First, listeners will use anticipatory coarticulatory information for a segment as information for the forthcoming segment. Second, subjects will not hear anticipatory coarticulatory information as part of the phonetic segment with which it co-occurs in time. The first hypothesis is supported by findings on a choice reaction time procedure; the second is supported by findings on a 4IAX discrimination test. Implications of the findings for theories of speech production, perception, and of the relation between the two are considered. © 1985 Psychonomic Society, Inc.},
	language = {English},
	number = {4},
	journal = {Perception \& Psychophysics},
	author = {Fowler, C.A.},
	year = {1984},
	pages = {359--368},
	file = {SCOPUS Snapshot:/home/user/Zotero/storage/5HDFAQXM/display.html:text/html}
}

@article{whalen-subcategorical-1984-1,
	title = {Subcategorical phonetic mismatches slow phonetic judgments},
	volume = {35},
	issn = {0031-5117},
	doi = {10.3758/BF03205924},
	abstract = {When an [s] or an [š] fricative noise is combined with vocalic formant transitions appropriate to a different fricative, the resulting consonantal percept is usually that of the noise. To see if the mismatch affects processing time, five experiments were run. Three experiments examined reaction time for identification of [s] and [š], as well as the whole syllable (in one experiment) or only the vowel (in the others). The stimuli contained either appropriate or inappropriate formant transitions, and the vowel information in the noise was either appropriate or not. Subjects were significantly slower in all tasks in identifying stimuli with inappropriate transitions or inappropriate vowel information. Similar results were obtained with stop-vowel syllables in which the release bursts of syllable initials [p] and [k]were transposed in syllables containing the vowels [a] and [u], In the fifth experiment, enough silence was introduced between the initial fricatives and vocalic segment for the vocalic formant transitions to be perceived as a stop (e.g., [stu] from [su]). Mismatched transitions still had an effect on reaction time, as did mismatches of vowel quality. The results indicate that listeners take into account all available cues, even when the phonetic judgment seems to be based on only some of the cues. © 1984 Psychonomic Society, Inc.},
	language = {English},
	number = {1},
	journal = {Perception \& Psychophysics},
	author = {Whalen, D.H.},
	year = {1984},
	pages = {49--64},
	file = {SCOPUS Snapshot:/home/user/Zotero/storage/GH9QSVAN/display.html:text/html}
}

@article{fowler-parsing-2005,
	title = {Parsing coarticulated speech in perception: effects of coarticulation resistance},
	volume = {33},
	issn = {0095-4470},
	shorttitle = {Parsing coarticulated speech in perception},
	url = {http://www.sciencedirect.com/science/article/pii/S0095447004000634},
	doi = {10.1016/j.wocn.2004.10.003},
	abstract = {A speaker produced schwa-CV disyllables in which consonants were low or high in coarticulation resistance. Articulatory and acoustic measurements verified that the magnitude, but not the extent, of anticipatory coarticulation from the stressed vowel to the schwa was modulated by coarticulation resistance. In a perception experiment, listeners heard schwa-CV disyllables that ended in stressed /i/ or /a/ and made speeded identifications of the stressed vowels. Some disyllables had been cross-spliced so that a schwa vowel that had originally been produced in the context of C/i/ was spliced onto a C/a/ syllable or vice versa. Other disyllables were spliced so that a schwa originally from a C/i/ (or C/a/) context was spliced onto a different C/i/ (C/a/) token. Listeners’ response latencies were slower to cross spliced than to spliced disyllables but only in the context of low rather than high resistant consonants. The outcome is generally consistent with a hypothesis that listeners to speech “parse” the acoustic signal along coarticulatory or phonetic gestural lines and that success in parsing varies with the amount of acoustic evidence talkers provide. However, findings did not suggest “perfect” parsing. Correlations between the amount of acoustic information provided by the speaker and the extent to which listeners were disrupted by cross splicing were nonsignificant.},
	number = {2},
	urldate = {2017-03-17},
	journal = {Journal of Phonetics},
	author = {Fowler, Carol A.},
	month = apr,
	year = {2005},
	pages = {199--213},
	file = {Fowler_2005_Parsing coarticulated speech in perception.pdf:/home/user/Zotero/storage/59TVQW6R/Fowler_2005_Parsing coarticulated speech in perception.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/866TIGJ9/S0095447004000634.html:text/html}
}

@article{martin-perception-1982,
	title = {Perception of anticipatory coarticulation effects in vowel-stop consonant-vowel sequences},
	volume = {8},
	issn = {0096-1523},
	doi = {10.1037/0096-1523.8.3.473},
	abstract = {In 2 experiments with 8 graduate students, a talker produced consonant (C) and vowel (V) sequences in a sentence frame (e.g., "I say pookee") of the form "I say / C V1 C V2/" in which V1 was /u, ae/ and V2 was /i, a/. Each /i, a/ sentence pair was cross-spliced by exchanging the final syllable /C V2/ so that coarticulatory information prior to the crosspoint was inappropriate for the final vowel (V2) in crossed sentences. Recognition time (RT) for V2 in crossed and intact (as spoken) sentences was obtained from practiced listeners. RT was slower in crossed sentences; crossed sentences also attracted more false alarms. The pattern of perceptual results was mirrored in the pattern of precross acoustic differences in experimental sentences. Pretarget variation in the formants jointly predicted amount of RT interference in crossed sentences. Exp III (6 Ss) found interference and also facilitation from exchanges of pretarget coarticulatory information in sentences. Two final experiments (33 Ss) showed that previous results were not dependent on the use of practiced listeners. (25 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). © 1982 American Psychological Association.},
	language = {English},
	number = {3},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Martin, J.G. and Bunnell, H.T.},
	year = {1982},
	keywords = {cross-spliced vowel-stop consonant-vowel sequences, experienced vs novice listeners, perception of anticipatory coarticulation},
	pages = {473--488},
	file = {SCOPUS Snapshot:/home/user/Zotero/storage/UKB42EAV/display.html:text/html}
}

@article{martin-perception-1981,
	title = {Perception of anticipatory coarticulation effects},
	volume = {69},
	issn = {0001-4966},
	doi = {10.1121/1.385484},
	abstract = {Articulatory and acoustic studies have shown that the effects of anticipatory coarticulation may extend across several segments in an utterance. But previous perceptual studies suggest that only the information carried by immediately adjacent segments is used in perception. To show that perception is not so limited, we persuaded ten talkers each to produce 12 sentences (e.g., “I say poozee”) of the form “I say /C V1 z V2/” in which C was /p, t, k/, VI was /u, as/, and V2 was /i, a/. Each /i, a / sentence pair was cross spliced by exchanging the final syllable /z V2/ so that coarticulatory information prior to the crosspoint was inappropriate for the final vowel V2 in crossed sentences. Recognition time (RT) for V2 in crossed and intact (as spoken) sentences was obtained from practiced listeners. The results were slower RT in crossed sentences, and amount of interference depended on both VI and C context. Another experiment varied the location of crosspoints across /C VI/ and found that RT interference increased directly with amount and proximity to target of inappropriate precross acoustics. LPC analysis of the experimental sentences showed pretarget variations in F2 frequency which were jointly dependent on identity of C, VI, and V2. Pretarget F2 variations and C and VI identity jointly predicted amount of RT interference in crossed sentences. Finally, experiments with pretarget F2 variations in synthetic speech repeated and extended the results with real speech. These studies lead to the conclusion that the perceptual significance of coarticulation is not limited to effects on immediately adjacent segments. Listeners appear to be sensitive to many acoustic effects of the mutual influence among the segments in a sequence. © 1981, American Association of Physics Teachers. All rights reserved.},
	language = {English},
	number = {2},
	journal = {Journal of the Acoustical Society of America},
	author = {Martin, J.G. and Bunnell, H.T.},
	year = {1981},
	pages = {559--567},
	file = {SCOPUS Snapshot:/home/user/Zotero/storage/VF3UAVSZ/display.html:text/html}
}

@article{kuehn-perceptual-1972,
	title = {Perceptual {Effects} of {Forward} {Coarticulation}},
	volume = {15},
	issn = {1092-4388},
	url = {http://jslhr.pubs.asha.org/article.aspx?articleid=1773790},
	doi = {10.1044/jshr.1503.654},
	number = {3},
	urldate = {2017-03-17},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Kuehn, David P. and Moll, Kenneth L.},
	month = sep,
	year = {1972},
	pages = {654--664},
	file = {Kuehn_Moll_1972_Perceptual Effects of Forward Coarticulation.pdf:/home/user/Zotero/storage/ZU7DAB9N/Kuehn_Moll_1972_Perceptual Effects of Forward Coarticulation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/B754S3U4/article.html:text/html}
}

@incollection{hardcastle-coarticulation-2006,
	address = {Oxford},
	title = {Coarticulation},
	isbn = {978-0-08-044854-1},
	url = {http://www.sciencedirect.com/science/article/pii/B0080448542005654},
	abstract = {The term ‘coarticulation’ in its original sense refers to the overlapping of articulatory movements associated with separate speech sound segments. One consequence of this ubiquitous overlapping is that sounds vary (both physiologically and acoustically) according to the nature of neighboring sounds, and coarticulation is often used these days in its broader sense to refer to this variation. Coarticulatory effects can be described in terms of their type (anticipatory, or ‘right-to-left,’ versus carryover, or ‘left-to-right’) and their temporal domain (‘begin as early as possible’ versus ‘time-locked’). The article gives examples of coarticulation and offers a brief overview of some of the many theoretical approaches to the subject.},
	urldate = {2017-03-17},
	booktitle = {Encyclopedia of {Language} \& {Linguistics} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Hardcastle, W.},
	editor = {Brown, Keith},
	year = {2006},
	doi = {10.1016/B0-08-044854-2/00565-4},
	keywords = {anticipatory or right-to-left coarticulation, articulation, articulatory gestures, carryover or left-to-right coarticulation, coarticulation, speech production models},
	pages = {501--505},
	file = {Hardcastle_2006_Coarticulation.pdf:/home/user/Zotero/storage/R3P9292B/Hardcastle_2006_Coarticulation.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/DZTFQ2HZ/B0080448542005654.html:text/html}
}

@article{dunham-joint-attentional-1993,
	title = {Joint-attentional states and lexical acquisition at 18 months},
	volume = {29},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {1939-0599 0012-1649},
	doi = {10.1037/0012-1649.29.5.827},
	abstract = {Two groups of 18-mo-old infants were observed during a relatively natural play session with an adult experimenter and several toys. A novel object associated with one of the toys was labeled a dodo by the experimenter using either an attention-following strategy (i.e., introducing the label when the infant was focused on the dodo object) or an attention-switching strategy (i.e., introducing the label when the infant was focused on an alternative object). With factors such as frequency of exposure to the object label and infant compliance equivalent across the groups, infants in the attention-following procedure were more likely to correctly identify the dodo object in a subsequent comprehension task. These experimental data corroborate previous correlational observations suggesting that early lexical development is facilitated during interactions in which the caregiver is following rather than leading the infant's focus of attention.},
	language = {English},
	number = {5},
	journal = {Developmental Psychology},
	author = {Dunham, Philip J. and Dunham, Frances and Curwin, Ann},
	year = {1993},
	keywords = {*Attention, Naming},
	pages = {827--831},
	file = {Dunham et al_1993_Joint-attentional states and lexical acquisition at 18 months.pdf:/home/user/Zotero/storage/HH9B5DKF/Dunham et al_1993_Joint-attentional states and lexical acquisition at 18 months.pdf:application/pdf}
}

@misc{noauthor-directive-nodate,
	title = {Directive interactions and early vocabulary development: the role of joint attentional focus. - {PubMed} - {NCBI}},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/2010504},
	urldate = {2017-03-16},
	file = {Directive interactions and early vocabulary development\: the role of joint attentional focus. - PubMed - NCBI:/home/user/Zotero/storage/4VBRRJA7/2010504.html:text/html}
}

@article{ramenzoni-social-2016,
	title = {The {Social} {Reach} 8-{Month}-{Olds} {Reach} for {Unobtainable} {Objects} in the {Presence} of {Another} {Person}},
	volume = {27},
	url = {http://pss.sagepub.com/content/27/9/1278.short},
	number = {9},
	urldate = {2017-03-16},
	journal = {Psychological Science},
	author = {Ramenzoni, Verónica C. and Liszkowski, Ulf},
	year = {2016},
	pages = {1278--1285},
	file = {0956797616659938:/home/user/Zotero/storage/HW54RP2R/0956797616659938.pdf:application/pdf}
}

@article{esteve-gibert-twelve-month-olds-2017,
	title = {Twelve-{Month}-{Olds} {Understand} {Social} {Intentions} {Based} on {Prosody} and {Gesture} {Shape}},
	volume = {22},
	issn = {15250008},
	url = {http://doi.wiley.com/10.1111/infa.12146},
	doi = {10.1111/infa.12146},
	language = {en},
	number = {1},
	urldate = {2017-03-16},
	journal = {Infancy},
	author = {Esteve-Gibert, Núria and Prieto, Pilar and Liszkowski, Ulf},
	month = jan,
	year = {2017},
	pages = {108--129},
	file = {Twelve&#x2010\;Month&#x2010\;Olds Understand Social Intentions Based on Prosody and Gesture Shape - Este-Gilbert_etal_2016.pdf:/home/user/Zotero/storage/QWJIWVQ3/Este-Gilbert_etal_2016.pdf:application/pdf}
}

@incollection{fawcett-social-2015,
	title = {Social {Referencing} during {Infancy} and {Early} {Childhood} across {Cultures}},
	isbn = {978-0-08-097087-5},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780080970868231693},
	language = {en},
	urldate = {2017-03-16},
	booktitle = {International {Encyclopedia} of the {Social} \& {Behavioral} {Sciences}},
	publisher = {Elsevier},
	author = {Fawcett, Christine and Liszkowski, Ulf},
	year = {2015},
	doi = {10.1016/B978-0-08-097086-8.23169-3},
	pages = {556--562},
	file = {Social Referencing during Infancy and Early Childhood across Cultures - 3-s2.0-B9780080970868231693-main.pdf:/home/user/Zotero/storage/2227KCVS/3-s2.0-B9780080970868231693-main.pdf:application/pdf}
}

@article{baldwin-infants-1991-1,
	title = {Infants' {Contribution} to the {Achievement} of {Joint} {Reference}},
	volume = {62},
	issn = {00093920},
	url = {http://www.jstor.org/stable/1131140?origin=crossref},
	doi = {10.2307/1131140},
	number = {5},
	urldate = {2017-03-15},
	journal = {Child Development},
	author = {Baldwin, Dare A.},
	month = oct,
	year = {1991},
	pages = {875},
	file = {Baldwin-CD1991.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Baldwin-CD1991.pdf:application/pdf}
}

@article{baldwin-interpersonal-2000,
	title = {Interpersonal understanding fuels knowledge acquisition},
	volume = {9},
	url = {http://journals.sagepub.com/doi/abs/10.1111/1467-8721.00057},
	number = {2},
	urldate = {2017-03-15},
	journal = {Current directions in psychological science},
	author = {Baldwin, Dare A.},
	year = {2000},
	pages = {40--45},
	file = {Baldwin-CDPS2000.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Baldwin-CDPS2000.pdf:application/pdf}
}

@article{childers-joint-2007-1,
	title = {Joint attention and word learning in {Ngas}-speaking toddlers in {Nigeria}},
	volume = {34},
	issn = {0305-0009, 1469-7602},
	url = {http://www.journals.cambridge.org/abstract\_S0305000906007835},
	doi = {10.1017/S0305000906007835},
	language = {en},
	number = {02},
	urldate = {2017-03-15},
	journal = {Journal of Child Language},
	author = {Childers, Jane B. and Vaughan, Julie and Burquest, Donald A.},
	month = may,
	year = {2007},
	pages = {199},
	file = {Childers&-JCL2007.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Childers&-JCL2007.pdf:application/pdf}
}

@article{perra-attention-2012-1,
	title = {Attention engagement in early infancy},
	volume = {35},
	issn = {01636383},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0163638312000641},
	doi = {10.1016/j.infbeh.2012.06.004},
	language = {en},
	number = {4},
	urldate = {2017-03-15},
	journal = {Infant Behavior and Development},
	author = {Perra, Oliver and Gattis, Merideth},
	month = dec,
	year = {2012},
	pages = {635--644},
	file = {Attention engagement in early infancy - zotero\://attachment/8960/:/home/user/Zotero/storage/8XB77EAW/8960.pdf:application/pdf}
}

@article{frischen-gaze-2007,
	title = {Gaze cueing of attention: visual attention, social cognition, and individual differences.},
	volume = {133},
	shorttitle = {Gaze cueing of attention},
	url = {http://psycnet.apa.org/psycinfo/2007-09203-007},
	number = {4},
	urldate = {2017-03-15},
	journal = {Psychological bulletin},
	author = {Frischen, Alexandra and Bayliss, Andrew P. and Tipper, Steven P.},
	year = {2007},
	pages = {694},
	file = {[HTML] nih.gov:/home/user/Zotero/storage/5P7Z7BB2/PMC1950440.html:text/html;Snapshot:/home/user/Zotero/storage/2BRPU7SC/2007-09203-007.html:text/html}
}

@article{mundy-attention-2007-1,
	title = {Attention, joint attention, and social cognition},
	volume = {16},
	url = {http://cdp.sagepub.com/content/16/5/269.short},
	number = {5},
	urldate = {2017-03-15},
	journal = {Current directions in psychological science},
	author = {Mundy, Peter and Newell, Lisa},
	year = {2007},
	pages = {269--274},
	file = {[HTML] nih.gov:/home/user/Zotero/storage/JDI4G4KR/PMC2663908.html:text/html;Snapshot:/home/user/Zotero/storage/2V286KFQ/j.1467-8721.2007.00518.html:text/html}
}

@article{meltzoff-role-1993,
	title = {The role of imitation in understanding persons and developing a theory of mind},
	journal = {Understanding other minds: Perspectives from autism},
	author = {Meltzoff, Andrew N. and Gopnik, Alison},
	year = {1993},
	pages = {335--366},
	file = {Meltzoff_Gopnik_Understanding_Other_Minds_1993.tif - 93Meltzoff_Gopnik_RoleImit.pdf:/home/user/Zotero/storage/VP9SREAP/93Meltzoff_Gopnik_RoleImit.pdf:application/pdf}
}

@article{mastin-infant-2016,
	title = {Infant engagement and early vocabulary development: a naturalistic observation study of {Mozambican} infants from 1;1 to 2;1},
	volume = {43},
	issn = {0305-0009, 1469-7602},
	shorttitle = {Infant engagement and early vocabulary development},
	url = {http://www.journals.cambridge.org/abstract\_S0305000915000148},
	doi = {10.1017/S0305000915000148},
	language = {en},
	number = {02},
	urldate = {2017-03-15},
	journal = {Journal of Child Language},
	author = {Mastin, J. Douglas and Vogt, Paul},
	month = mar,
	year = {2016},
	pages = {235--264},
	file = {Mastin&V-JCL2015.pdf:C\:\\Users\\user\\Downloads\\Week6-JointAttention\\Week6-JointAttention\\Mastin&V-JCL2015.pdf:application/pdf}
}

@article{maddison-concrete-2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2017-07-07},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00712},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/MFV66K85/1611.html:text/html;Maddison et al_2016_The Concrete Distribution.pdf:/home/user/Zotero/storage/5E6B7W5S/Maddison et al_2016_The Concrete Distribution.pdf:application/pdf}
}

@article{jang-categorical-2016,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2017-07-07},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01144},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/5GATKBAM/1611.html:text/html;Jang et al_2016_Categorical Reparameterization with Gumbel-Softmax.pdf:/home/user/Zotero/storage/FGQIFCJC/Jang et al_2016_Categorical Reparameterization with Gumbel-Softmax.pdf:application/pdf}
}

@article{mnih-neural-2014-1,
	title = {Neural {Variational} {Inference} and {Learning} in {Belief} {Networks}},
	url = {http://arxiv.org/abs/1402.0030},
	abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
	urldate = {2017-07-07},
	journal = {arXiv:1402.0030 [cs, stat]},
	author = {Mnih, Andriy and Gregor, Karol},
	month = jan,
	year = {2014},
	note = {arXiv: 1402.0030},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/H6645TVQ/1402.html:text/html;Mnih_Gregor_2014_Neural Variational Inference and Learning in Belief Networks.pdf:/home/user/Zotero/storage/MUUW48ZU/Mnih_Gregor_2014_Neural Variational Inference and Learning in Belief Networks.pdf:application/pdf}
}

@article{rolfe-discrete-2016,
	title = {Discrete {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1609.02200},
	abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
	urldate = {2017-07-07},
	journal = {arXiv:1609.02200 [cs, stat]},
	author = {Rolfe, Jason Tyler},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.02200},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/XTHXMB5J/1609.html:text/html;Rolfe_2016_Discrete Variational Autoencoders.pdf:/home/user/Zotero/storage/FZRQ5DC9/Rolfe_2016_Discrete Variational Autoencoders.pdf:application/pdf}
}

@incollection{goodfellow-generative-2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	urldate = {2017-07-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680},
	file = {Goodfellow et al_2014_Generative Adversarial Nets.pdf:/home/user/Zotero/storage/ZZT4I29C/Goodfellow et al_2014_Generative Adversarial Nets.pdf:application/pdf;NIPS Snapshort:/home/user/Zotero/storage/NFPU6F86/5423-generative-adversarial-nets.html:text/html}
}

@article{bresnan-gradient-2008,
	title = {Gradient grammar: {An} effect of animacy on the syntax of give in {New} {Zealand} and {American} {English}},
	volume = {118},
	issn = {00243841},
	shorttitle = {Gradient grammar},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0024384107000289},
	doi = {10.1016/j.lingua.2007.02.007},
	language = {en},
	number = {2},
	urldate = {2017-07-07},
	journal = {Lingua},
	author = {Bresnan, Joan and Hay, Jennifer},
	month = feb,
	year = {2008},
	pages = {245--259},
	file = {bresnan-hay-2008.pdf:/home/user/Zotero/storage/MQMI2F9P/bresnan-hay-2008.pdf:application/pdf}
}

@article{bresnan-few-2007,
	title = {A few lessons from typology},
	volume = {11},
	issn = {1430-0532, 1613-415X},
	url = {https://www.degruyter.com/view/j/lity.2007.11.issue-1/lingty.2007.024/lingty.2007.024.xml},
	doi = {10.1515/LINGTY.2007.024},
	number = {1},
	urldate = {2017-07-07},
	journal = {Linguistic Typology},
	author = {Bresnan, Joan},
	month = jan,
	year = {2007},
	file = {bresnan-typology2007.pdf:/home/user/Zotero/storage/58NMMSB8/bresnan-typology2007.pdf:application/pdf}
}

@inproceedings{bresnan-soft-2001,
	title = {Soft constraints mirror hard constraints: {Voice} and person in {English} and {Lummi}},
	shorttitle = {Soft constraints mirror hard constraints},
	url = {http://web.stanford.edu/group/cslipublications/cslipublications/LFG/6/pdfs/lfg01bresnanetal.pdf},
	urldate = {2017-07-07},
	booktitle = {Proceedings of the {LFG}01 {Conference}},
	publisher = {Stanford: CSLI Publications. E. Verhoeven},
	author = {Bresnan, Joan and Dingare, Shipra and Manning, Christopher D.},
	year = {2001},
	pages = {13--32},
	file = {lfg01.pdf:/home/user/Zotero/storage/D4MAW94S/lfg01.pdf:application/pdf}
}

@article{kuperman-effects-2012,
	title = {The effects of construction probability on word durations during spontaneous incremental sentence production},
	volume = {66},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X12000344},
	doi = {10.1016/j.jml.2012.04.003},
	language = {en},
	number = {4},
	urldate = {2017-07-07},
	journal = {Journal of Memory and Language},
	author = {Kuperman, Victor and Bresnan, Joan},
	month = may,
	year = {2012},
	pages = {588--611},
	file = {kuperman-bresnan-jml.pdf:/home/user/Zotero/storage/HPTPVJID/kuperman-bresnan-jml.pdf:application/pdf}
}

@article{kendall-dative-2011,
	title = {The dative alternation in {African} {American} {English}: {Researching} syntactic variation and change across sociolinguistic datasets},
	volume = {7},
	issn = {1613-7027, 1613-7035},
	shorttitle = {The dative alternation in {African} {American} {English}},
	url = {https://www.degruyter.com/view/j/cllt.2011.7.issue-2/cllt.2011.011/cllt.2011.011.xml},
	doi = {10.1515/cllt.2011.011},
	number = {2},
	urldate = {2017-07-07},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Kendall, Tyler and Bresnan, Joan and van Herk, Gerard},
	month = jan,
	year = {2011},
	file = {kendall.et.al.CLLT.pdf:/home/user/Zotero/storage/6R8AIRKI/kendall.et.al.CLLT.pdf:application/pdf}
}

@article{bresnan-predicting-2010,
	title = {Predicting syntax: {Processing} dative constructions in {American} and {Australian} varieties of {English}},
	volume = {86},
	shorttitle = {Predicting syntax},
	url = {https://muse.jhu.edu/article/376557/summary},
	number = {1},
	urldate = {2017-07-07},
	journal = {Language},
	author = {Bresnan, Joan and Ford, Marilyn},
	year = {2010},
	pages = {168--213},
	file = {LSA86103.qxd\:LSA - 86.1.bresnan.pdf:/home/user/Zotero/storage/BPX8PJFV/86.1.bresnan.pdf:application/pdf}
}

@article{hay-spoken-2006,
	title = {Spoken syntax: {The} phonetics of giving a hand in {New} {Zealand} {English}},
	volume = {23},
	issn = {0167-6318, 1613-3676},
	shorttitle = {Spoken syntax},
	url = {https://www.degruyter.com/view/j/tlir.2006.23.issue-3/tlr.2006.013/tlr.2006.013.xml},
	doi = {10.1515/TLR.2006.013},
	number = {3},
	urldate = {2017-07-07},
	journal = {The Linguistic Review},
	author = {Hay, Jennifer and Bresnan, Joan},
	month = jan,
	year = {2006},
	file = {tlr23-3.dvi - hay-bresnan.pdf:/home/user/Zotero/storage/X5ZM9RAP/hay-bresnan.pdf:application/pdf}
}

@article{eckmann-ergodic-1985,
	title = {Ergodic theory of chaos and strange attractors},
	volume = {57},
	issn = {0034-6861},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.57.617},
	doi = {10.1103/RevModPhys.57.617},
	language = {en},
	number = {3},
	urldate = {2017-07-07},
	journal = {Reviews of Modern Physics},
	author = {Eckmann, J. -P. and Ruelle, D.},
	month = jul,
	year = {1985},
	pages = {617--656},
	file = {Ruelle-D-Ergodic-theory-of-chaos-and-strange-attractors-Rev-Mod-Phys57-617-656.pdf:/home/user/Zotero/storage/B3VUD3TJ/Ruelle-D-Ergodic-theory-of-chaos-and-strange-attractors-Rev-Mod-Phys57-617-656.pdf:application/pdf}
}

@article{evans-equilibrium-1994,
	title = {Equilibrium microstates which generate second law violating steady states},
	volume = {50},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.50.1645},
	doi = {10.1103/PhysRevE.50.1645},
	abstract = {For reversible deterministic N-particle thermostatted systems, we examine the question of why it is so difficult to find initial microstates that will, at long times under the influence of an external dissipative field and a thermostat, lead to second law violating nonequilibrium steady states. We prove that the measure of those phases that generate second law violating phase space trajectories vanishes exponentially with time.},
	number = {2},
	urldate = {2017-07-07},
	journal = {Physical Review E},
	author = {Evans, Denis J. and Searles, Debra J.},
	month = aug,
	year = {1994},
	pages = {1645--1648},
	file = {APS Snapshot:/home/user/Zotero/storage/PNTKIIAK/PhysRevE.50.html:text/html}
}

@article{evans-probability-1993,
	title = {Probability of second law violations in shearing steady states},
	volume = {71},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.71.2401},
	doi = {10.1103/PhysRevLett.71.2401},
	abstract = {We propose a new definition of natural invariant measure for trajectory segments of finite duration for a many-particle system. On this basis we give an expression for the probability of fluctuations in the shear stress of a fluid in a nonequilibrium steady state far from equilibrium. In particular we obtain a formula for the ratio that, for a finite time, the shear stress reverse sign, violating the second law of thermodynamics. Computer simulations support this formula.},
	number = {15},
	urldate = {2017-07-07},
	journal = {Physical Review Letters},
	author = {Evans, Denis J. and Cohen, E. G. D. and Morriss, G. P.},
	month = oct,
	year = {1993},
	pages = {2401--2404},
	file = {APS Snapshot:/home/user/Zotero/storage/NAJF78PQ/PhysRevLett.71.html:text/html}
}

@article{hyman-what-2014,
	title = {What (else) depends on phonology?},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA141&dq=%22Phonology+is+ignored:+There+is+no+mention+of+phonology+in+Chapter+10%22+%222013)+includes+19+chapters+on+phonology+out+of+144+(or+13.2%25).+There%22+%22What+(else)+depends+on%22+&ots=kh8NDrJmJt&sig=Xsz7ZWwz1QNPBUNC\_iPB1I1YiG4},
	urldate = {2017-07-07},
	journal = {Dependencies in},
	author = {Hyman, Larry M.},
	year = {2014},
	pages = {141},
	file = {669-1.pdf:/home/user/Zotero/storage/HIFG2VWM/669-1.pdf:application/pdf}
}

@article{de-marneffe-statistical-2012,
	title = {A statistical model of the grammatical choices in child production of dative sentences},
	volume = {27},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2010.542651},
	number = {1},
	urldate = {2017-07-07},
	journal = {Language and Cognitive Processes},
	author = {De Marneffe, Marie-Catherine and Grimm, Scott and Arnon, Inbal and Kirby, Susannah and Bresnan, Joan},
	year = {2012},
	pages = {25--61},
	file = {NimbusRomNo9L-ReguItal - child.dative.pdf:/home/user/Zotero/storage/Z3HU3PJV/child.dative.pdf:application/pdf}
}

@article{tily-syntactic-2009-1,
	title = {Syntactic probabilities affect pronunciation variation in spontaneous speech},
	volume = {1},
	issn = {1866-9808, 1866-9859},
	url = {http://www.journals.cambridge.org/abstract\_S1866980800000247},
	doi = {10.1515/LANGCOG.2009.008},
	language = {en},
	number = {02},
	urldate = {2017-07-07},
	journal = {Language and Cognition},
	author = {Tily, Harry and Gahl, Susanne and Arnon, Inbal and Snider, Neal and Kothari, Anubha and Bresnan, Joan},
	month = jun,
	year = {2009},
	pages = {147--165},
	file = {2167_1-2_01 147..166 - langcog.2009.008.pdf:/home/user/Zotero/storage/KJRPF28N/langcog.2009.008.pdf:application/pdf}
}

@article{wolk-dative-2013,
	title = {Dative and genitive variability in {Late} {Modern} {English}: {Exploring} cross-constructional variation and change},
	volume = {30},
	shorttitle = {Dative and genitive variability in {Late} {Modern} {English}},
	url = {http://www.jbe-platform.com/content/journals/10.1075/dia.30.3.04wol},
	number = {3},
	urldate = {2017-07-07},
	journal = {Diachronica},
	author = {Wolk, Christoph and Bresnan, Joan and Rosenbach, Anette and Szmrecsanyi, Benedikt},
	year = {2013},
	pages = {382--419},
	file = {Diachronica. International Journal for Historical Linguistics - Pagesfromdia.30-3.2e.kader.pdf:/home/user/Zotero/storage/29VDMRWG/Pagesfromdia.30-3.2e.kader.pdf:application/pdf}
}

@article{szmrecsanyi-culturally-2014,
	title = {Culturally conditioned language change? {A} multi-variate analysis of genitive constructions in {ARCHER}},
	shorttitle = {Culturally conditioned language change?},
	url = {http://www.benszm.net/omnibuslit/SRBW\_July-2013.pdf},
	urldate = {2017-07-07},
	journal = {Late Modern English syntax},
	author = {Szmrecsanyi, Benedikt and Rosenbach, Anette and Bresnan, Joan and Wolk, Christoph},
	year = {2014},
	pages = {133--152},
	file = {Culture change versus grammar change\: genitive constructions in Late Modern English - SRBW_April-2013.pdf:/home/user/Zotero/storage/SQ8TSATP/SRBW_April-2013.pdf:application/pdf}
}

@article{burda-importance-2015-3,
	title = {Importance {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
	urldate = {2017-07-07},
	journal = {arXiv:1509.00519 [cs, stat]},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.00519},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/C8HKZNXH/1509.html:text/html;Burda et al_2015_Importance Weighted Autoencoders.pdf:/home/user/Zotero/storage/JABDDBIS/Burda et al_2015_Importance Weighted Autoencoders.pdf:application/pdf}
}

@article{zhai-generative-2016,
	title = {Generative {Adversarial} {Networks} as {Variational} {Training} of {Energy} {Based} {Models}},
	url = {http://arxiv.org/abs/1611.01799},
	abstract = {In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density \$p(\mathbf{x})\$ is approximated by a variational distribution \$q(\mathbf{x})\$ that is easy to sample from. The training of VGAN takes a two step procedure: given \$p(\mathbf{x})\$, \$q(\mathbf{x})\$ is updated to maximize the lower bound; \$p(\mathbf{x})\$ is then updated one step with samples drawn from \$q(\mathbf{x})\$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where \$p(\mathbf{x})\$ corresponds to the discriminator and \$q(\mathbf{x})\$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions. \footnote{Experimental code is available at https://github.com/Shuangfei/vgan}},
	urldate = {2017-07-07},
	journal = {arXiv:1611.01799 [cs]},
	author = {Zhai, Shuangfei and Cheng, Yu and Feris, Rogerio and Zhang, Zhongfei},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01799},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/KR6KF3E7/1611.html:text/html;Zhai et al_2016_Generative Adversarial Networks as Variational Training of Energy Based Models.pdf:/home/user/Zotero/storage/7CFRQJU7/Zhai et al_2016_Generative Adversarial Networks as Variational Training of Energy Based Models.pdf:application/pdf}
}

@article{sonderby-amortised-2016,
	title = {Amortised {MAP} {Inference} for {Image} {Super}-resolution},
	url = {http://arxiv.org/abs/1610.04490},
	abstract = {Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.},
	urldate = {2017-07-07},
	journal = {arXiv:1610.04490 [cs, stat]},
	author = {Sønderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Huszár, Ferenc},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.04490},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/S5JUE7AC/1610.html:text/html;Sønderby et al_2016_Amortised MAP Inference for Image Super-resolution.pdf:/home/user/Zotero/storage/4XF55U6I/Sønderby et al_2016_Amortised MAP Inference for Image Super-resolution.pdf:application/pdf}
}

@article{mohamed-learning-2016,
	title = {Learning in {Implicit} {Generative} {Models}},
	url = {http://arxiv.org/abs/1610.03483},
	abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	urldate = {2017-07-07},
	journal = {arXiv:1610.03483 [cs, stat]},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.03483},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/T6VV67HZ/1610.html:text/html;Mohamed_Lakshminarayanan_2016_Learning in Implicit Generative Models.pdf:/home/user/Zotero/storage/INZJIAAE/Mohamed_Lakshminarayanan_2016_Learning in Implicit Generative Models.pdf:application/pdf}
}

@article{mescheder-adversarial-2017,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2017-07-07},
	journal = {arXiv:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04722},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/CIA68SKH/1701.html:text/html;Mescheder et al_2017_Adversarial Variational Bayes.pdf:/home/user/Zotero/storage/WPVJN8UQ/Mescheder et al_2017_Adversarial Variational Bayes.pdf:application/pdf}
}

@article{huszar-variational-2017,
	title = {Variational {Inference} using {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1702.08235},
	abstract = {Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising.},
	urldate = {2017-07-07},
	journal = {arXiv:1702.08235 [cs, stat]},
	author = {Huszár, Ferenc},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08235},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/MKD6NEKT/1702.html:text/html;Huszár_2017_Variational Inference using Implicit Distributions.pdf:/home/user/Zotero/storage/7HK9HARU/Huszár_2017_Variational Inference using Implicit Distributions.pdf:application/pdf}
}

@article{goyal-nonparametric-2017,
	title = {Nonparametric {Variational} {Auto}-encoders for {Hierarchical} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1703.07027},
	abstract = {The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.},
	urldate = {2017-07-07},
	journal = {arXiv:1703.07027 [cs, stat]},
	author = {Goyal, Prasoon and Hu, Zhiting and Liang, Xiaodan and Wang, Chenyu and Xing, Eric},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07027},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/J52E5V4S/1703.html:text/html;Goyal et al_2017_Nonparametric Variational Auto-encoders for Hierarchical Representation Learning.pdf:/home/user/Zotero/storage/E7GCJHSR/Goyal et al_2017_Nonparametric Variational Auto-encoders for Hierarchical Representation Learning.pdf:application/pdf}
}

@article{finn-connection-2016,
	title = {A {Connection} between {Generative} {Adversarial} {Networks}, {Inverse} {Reinforcement} {Learning}, and {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/1611.03852},
	abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
	urldate = {2017-07-07},
	journal = {arXiv:1611.03852 [cs]},
	author = {Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03852},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AI26MJZI/1611.html:text/html;Finn et al_2016_A Connection between Generative Adversarial Networks, Inverse Reinforcement.pdf:/home/user/Zotero/storage/IU9RRIHS/Finn et al_2016_A Connection between Generative Adversarial Networks, Inverse Reinforcement.pdf:application/pdf}
}

@article{chen-variational-2016,
	title = {Variational {Lossy} {Autoencoder}},
	url = {http://arxiv.org/abs/1611.02731},
	abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x{\textbar}z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
	urldate = {2017-07-07},
	journal = {arXiv:1611.02731 [cs, stat]},
	author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02731},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3XWPNRGG/1611.html:text/html;Chen et al_2016_Variational Lossy Autoencoder.pdf:/home/user/Zotero/storage/M9566KNF/Chen et al_2016_Variational Lossy Autoencoder.pdf:application/pdf}
}

@article{chen-adversarial-2016,
	title = {Adversarial {Deep} {Averaging} {Networks} for {Cross}-{Lingual} {Sentiment} {Classification}},
	url = {http://arxiv.org/abs/1606.01614},
	abstract = {In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To tackle this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to low-resource languages where only unlabeled data exists. ADAN is a "Y-shaped" network with two discriminative branches: a sentiment classifier and an adversarial language identification scorer. Both branches take input from a shared feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on state-of-the-art Machine Translation.},
	urldate = {2017-07-07},
	journal = {arXiv:1606.01614 [cs]},
	author = {Chen, Xilun and Sun, Yu and Athiwaratkun, Ben and Cardie, Claire and Weinberger, Kilian},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01614},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/HN5NEESS/1606.html:text/html;Chen et al_2016_Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification.pdf:/home/user/Zotero/storage/DU3KV4RC/Chen et al_2016_Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification.pdf:application/pdf}
}

@article{che-maximum-likelihood-2017,
	title = {Maximum-{Likelihood} {Augmented} {Discrete} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1702.07983},
	abstract = {Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.},
	urldate = {2017-07-07},
	journal = {arXiv:1702.07983 [cs]},
	author = {Che, Tong and Li, Yanran and Zhang, Ruixiang and Hjelm, R. Devon and Li, Wenjie and Song, Yangqiu and Bengio, Yoshua},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.07983},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/X6JRFDPT/1702.html:text/html;Che et al_2017_Maximum-Likelihood Augmented Discrete Generative Adversarial Networks.pdf:/home/user/Zotero/storage/7E6AVSXK/Che et al_2017_Maximum-Likelihood Augmented Discrete Generative Adversarial Networks.pdf:application/pdf}
}

@article{arora-generalization-2017,
	title = {Generalization and {Equilibrium} in {Generative} {Adversarial} {Nets} ({GANs})},
	url = {http://arxiv.org/abs/1703.00573},
	abstract = {It is shown that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a natural training objective (Wasserstein) when generator capacity and training set sizes are moderate. This existence of equilibrium inspires mix+gan protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
	urldate = {2017-07-07},
	journal = {arXiv:1703.00573 [cs, stat]},
	author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00573},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Arora et al_2017_Generalization and Equilibrium in Generative Adversarial Nets (GANs).pdf:/home/user/Zotero/storage/K5U2MEEH/Arora et al_2017_Generalization and Equilibrium in Generative Adversarial Nets (GANs).pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MKG9SZ23/1703.html:text/html}
}

@book{cohen-bayesian-2016-2,
	title = {Bayesian analysis in natural language processing},
	isbn = {978-1-62705-421-8},
	url = {http://dx.doi.org/10.2200/S00719ED1V01Y201605HLT035},
	abstract = {Natural language processing (NLP) went through a profound transformation in the mid-1980s when it shifted to make heavy use of corpora and data-driven techniques to analyze language. Since then, the use of statistical techniques in NLP has evolved in several ways. One such example of evolution took place in the late 1990s or early 2000s, when full-fledged Bayesian machinery was introduced to NLP. This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it, especially in the unsupervised setting, where statistical learning is done without target prediction examples. We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area. These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed "in-house" in NLP. We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference, Bayesian estimation, and nonparametric modeling. We also cover fundamental concepts in Bayesian statistics such as prior distributions, conjugacy, and generative modeling. Finally, we cover some of the fundamental modeling techniques in NLP, such as grammar modeling and their use with Bayesian analysis.},
	language = {English},
	urldate = {2017-07-05},
	author = {Cohen, Shay},
	year = {2016},
	note = {OCLC: 959801508},
	file = {s00719ed1v01y201605hlt035.pdf:/home/user/Zotero/storage/QZ34G294/s00719ed1v01y201605hlt035.pdf:application/pdf}
}

@inproceedings{eisner-grammar-2012,
	title = {Grammar induction: {Beyond} local search},
	shorttitle = {Grammar induction},
	url = {http://www.jmlr.org/proceedings/papers/v21/eisner12a/eisner12a.pdf},
	urldate = {2017-07-05},
	booktitle = {International {Conference} on {Grammatical} {Inference}},
	author = {Eisner, Jason},
	year = {2012},
	pages = {112--113},
	file = {eisner12a.pdf:/home/user/Zotero/storage/J82F3NI4/eisner12a.pdf:application/pdf}
}

@inproceedings{stanojevic-reordering-2015,
	title = {Reordering {Grammar} {Induction}.},
	url = {http://www.aclweb.org/anthology/D15-1005},
	urldate = {2017-07-05},
	booktitle = {{EMNLP}},
	author = {Stanojevic, Milos and Sima'an, Khalil},
	year = {2015},
	pages = {44--54},
	file = {D15-1005.pdf:/home/user/Zotero/storage/VR7GVAMX/D15-1005.pdf:application/pdf}
}

@inproceedings{smith-guiding-2005,
	title = {Guiding unsupervised grammar induction using contrastive estimation},
	url = {http://www.cs.jhu.edu/~jason/papers/smith+eisner.gia05.pdf},
	urldate = {2017-07-05},
	booktitle = {Proc. of {IJCAI} {Workshop} on {Grammatical} {Inference} {Applications}},
	author = {Smith, Noah A. and Eisner, Jason},
	year = {2005},
	pages = {73--82},
	file = {smith+eisner.ijcaigia05.pdf:/home/user/Zotero/storage/FPKHUVH4/smith+eisner.ijcaigia05.pdf:application/pdf}
}

@inproceedings{klein-corpus-based-2004,
	title = {Corpus-based induction of syntactic structure: {Models} of dependency and constituency},
	shorttitle = {Corpus-based induction of syntactic structure},
	url = {http://dl.acm.org/citation.cfm?id=1219016},
	urldate = {2017-07-05},
	booktitle = {Proceedings of the 42nd {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Dan and Manning, Christopher D.},
	year = {2004},
	pages = {478},
	file = {factored-induction-camera.pdf:/home/user/Zotero/storage/X9MQ6HDN/factored-induction-camera.pdf:application/pdf}
}

@inproceedings{klein-generative-2002,
	title = {A generative constituent-context model for improved grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1073106},
	urldate = {2017-07-05},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Dan and Manning, Christopher D.},
	year = {2002},
	pages = {128--135},
	file = {KleinManningACL2002.pdf:/home/user/Zotero/storage/FHQQG44X/KleinManningACL2002.pdf:application/pdf}
}

@inproceedings{naseem-using-2010,
	title = {Using universal linguistic knowledge to guide grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1870778},
	urldate = {2017-07-05},
	booktitle = {Proceedings of the 2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Naseem, Tahira and Chen, Harr and Barzilay, Regina and Johnson, Mark},
	year = {2010},
	pages = {1234--1244},
	file = {emnlp2010.pdf:/home/user/Zotero/storage/72XKCVEU/emnlp2010.pdf:application/pdf}
}

@inproceedings{headden-iii-improving-2009,
	title = {Improving unsupervised dependency parsing with richer contexts and smoothing},
	url = {http://dl.acm.org/citation.cfm?id=1620769},
	urldate = {2017-07-05},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Headden III, William P. and Johnson, Mark and McClosky, David},
	year = {2009},
	pages = {101--109},
	file = {N09-1012.pdf:/home/user/Zotero/storage/92ZZCR9A/N09-1012.pdf:application/pdf}
}

@inproceedings{cohen-shared-2009,
	title = {Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1620766},
	urldate = {2017-07-05},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Cohen, Shay B. and Smith, Noah A.},
	year = {2009},
	pages = {74--82},
	file = {N09-1009.pdf:/home/user/Zotero/storage/Z5X4VRH2/N09-1009.pdf:application/pdf}
}

@inproceedings{johnson-bayesian-2007,
	title = {Bayesian {Inference} for {PCFGs} via {Markov} {Chain} {Monte} {Carlo}.},
	url = {http://www.research.ed.ac.uk/portal/files/7695790/N07\_1018.pdf},
	urldate = {2017-07-05},
	booktitle = {{HLT}-{NAACL}},
	author = {Johnson, Mark and Griffiths, Thomas L. and Goldwater, Sharon},
	year = {2007},
	pages = {139--146},
	file = {mcmc-pcfg.pdf:/home/user/Zotero/storage/NKZSPNNC/mcmc-pcfg.pdf:application/pdf}
}

@inproceedings{smith-annealing-2006,
	title = {Annealing structural bias in multilingual weighted grammar induction},
	url = {http://dl.acm.org/citation.cfm?id=1220247},
	urldate = {2017-07-05},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computational} {Linguistics} and the 44th annual meeting of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Smith, Noah A. and Eisner, Jason},
	year = {2006},
	pages = {569--576},
	file = {p569-smith.pdf:/home/user/Zotero/storage/2SB5W5R8/p569-smith.pdf:application/pdf}
}

@article{alonso-parsing-2017,
	title = {Parsing {Universal} {Dependencies} without training},
	url = {https://arxiv.org/abs/1701.03163},
	urldate = {2017-07-05},
	journal = {arXiv preprint arXiv:1701.03163},
	author = {Alonso, Héctor Martínez and Agić, Željko and Plank, Barbara and Søgaard, Anders},
	year = {2017},
	file = {alonso2017-parsing.pdf:/home/user/Zotero/storage/A669DMG5/alonso2017-parsing.pdf:application/pdf}
}

@article{blasi-beyond-2017,
	title = {Beyond binary dependencies in language structure},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA117&dq=%22observations+translates+into+support+for+the+dependency.+In+the+first%22+%22structure.+In+N.+J.+Enfield+(ed.),+Dependencies+in+language,%22+%22exist+several+possible+formalizations+of+the+concept+of+causality%22+&ots=kh8KzuJmFw&sig=ogtgj\_s2vLSo-n6uWssGk3dOKr8},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Blasi, Damián E. and Roberts, Seán G.},
	year = {2017},
	pages = {117},
	file = {667-1.pdf:/home/user/Zotero/storage/HH3A3U44/667-1.pdf:application/pdf}
}

@article{lieven-is-2017,
	title = {Is language development dependent on early communicative development?},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA85&dq=%22of+later+language+development+in+autism+have+come+to+highly%22+%22that+shared+intentionality+is+a+necessary+foundation+for+language%22+%22qualitative+change+in+interactive+behaviour+starting+sometime+around+the%22+&ots=kh8KzuJmFt&sig=6ZOCm67\_kzwSbpZ5Tyu64Fh0Yq8},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Lieven, Elena},
	year = {2017},
	pages = {85},
	file = {665-1.pdf:/home/user/Zotero/storage/B6RWD2F8/665-1.pdf:application/pdf}
}

@article{cristofaro-implicational-2017,
	title = {Implicational universals and dependencies},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA9&dq=%22(ed.),+Dependencies+in+language,+9%E2%80%9323.+Berlin:+Language+Science%22+%22a+dependency+relationship+between+the%22+%22No+functional+principles+leading+to%22+%22overt+marking+for+different+grammatical+categories.+Sometimes,+the%22+&ots=kh8KztSvLr&sig=x5mnKASzCf2JPYsCr\_0-GTgqiIs},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Cristofaro, Sonia},
	year = {2017},
	pages = {9},
	file = {661-1.pdf:/home/user/Zotero/storage/P6R3IHSG/661-1.pdf:application/pdf}
}

@article{sandler-what-2017,
	title = {What comes first in language emergence?},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA63&dq=%22Chomsky+%26+Fitch+2002),+more+recently+described+as+a%22+%22of+years+old,+or+descended+from+old+languages,+with+their+full%22+%22Stokoe+%26+Wilcox+1995%3B+Arbib+2012).+I+do+not+deal+with+that+issue+here,+but+see%22+&ots=kh8KztSvLp&sig=ng5rADgoi76m3heSC4CCGAhrcdk},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Sandler, Wendy},
	year = {2017},
	pages = {63},
	file = {659-1.pdf:/home/user/Zotero/storage/824VISCA/659-1.pdf:application/pdf}
}

@article{culbertson-new-2017,
	title = {New approaches to {Greenbergian} word order dependencies},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA23&dq=%22this+chapter+is+to+highlight+a+strand+of+behavioral+research+which+can%22+%22particular+distribution+of+linguistic+patterns%E2%80%93and+how+we+should%22+%22a+traditional+nativist+view,+typological+universals+are+treated+as+a+source%22+&ots=kh8KztSvKw&sig=LtpDd4jQ3rm\_4ekUy0jjVE4Hs2A},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Culbertson, Jennifer},
	year = {2017},
	pages = {23},
	file = {662-1.pdf:/home/user/Zotero/storage/HMTRCWCU/662-1.pdf:application/pdf}
}

@article{christiansen-language-2017,
	title = {Language intertwined across multiple timescales: {Processing}, acquisition and evolution},
	volume = {14},
	shorttitle = {Language intertwined across multiple timescales},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA53&dq=%22the+mainstream+generative+grammar+tradition,+possible%22+%22For+example,+typological+and+usage-based+approaches+to%22+%22of+language+evolution+usually+pay+little+attention+to+research+on%22+%22The+Now-or-Never%22+&ots=kh8KztSvKr&sig=plRwIVIaKbLHTo8ozhWtrLXgFco},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Christiansen, Morten H.},
	year = {2017},
	pages = {53},
	file = {664-1.pdf:/home/user/Zotero/storage/HWHD9RFP/664-1.pdf:application/pdf}
}

@article{rice-dependencies-2017,
	title = {Dependencies in phonology: hierarchies and variation},
	volume = {14},
	shorttitle = {Dependencies in phonology},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA159&dq=%22if+%E2%80%9Cx,+then+y%E2%80%9D,+x+is+considered+to+be+more+marked+than+y+since+the%22+%22to+be+more+marked+than+y+since+the+presence+of+y+depends+on%22+&ots=kh8KztSvKq&sig=V899pmlPrQpq-qsQ4ykKitOwQoA},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Rice, Keren},
	year = {2017},
	pages = {159},
	file = {670-1.pdf:/home/user/Zotero/storage/6MF99NNB/670-1.pdf:application/pdf}
}

@incollection{dediu-biology-2016,
	title = {From biology to language change and diversity},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA39&dq=%22the+complexity+of+this+literature+and+the+brevity+of+this+chapter,+I%22+%22control+(how+and+when+to+control+for+confounds,+the%22+%22relations),+metaphysical+(what+is+causality,+what+features+must%22+&ots=kh8KztSvIv&sig=CUU6AK6ZZlTqx5KCf-Q-XyiWSpw},
	urldate = {2017-05-29},
	booktitle = {Dependencies in language: {On} the causal ontology of linguistics systems},
	publisher = {Language Science Press},
	author = {Dediu, Dan},
	year = {2016},
	file = {663-1.pdf:/home/user/Zotero/storage/662MFBV6/663-1.pdf:application/pdf}
}

@article{collins-real-2017,
	title = {Real and spurious correlations involving tonal languages},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA129&dq=%22declines+with+distance+from+Africa,+and+number+of+tones+in%22+%221).+They+are+predominantly+found+in+Africa+and+Southeast+Asia,%22+%22Collins.+2017.+Real+and+spurious+correlations+involving+tonal%22+&ots=kh8KztSvIs&sig=BXA\_8mElIyFfrbmkaNggjJ39fvU},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Collins, Jeremy},
	year = {2017},
	pages = {129},
	file = {668-1.pdf:/home/user/Zotero/storage/J3RNT289/668-1.pdf:application/pdf}
}

@article{foley-structural-2017,
	title = {Structural and semantic dependencies in word class},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA179&dq=%22over+3000+noun+roots,+but+only+around+100+verb+roots,+a+skewing%22+%22noun+meaning,+while+with+verbs+this+is+commonly+not+the+case%3B+what%22+%22in+others,+it+shouldn%E2%80%99t,+but+that+is+not+my+concern+here.+I+am%22+&ots=kh8KztSvHx&sig=GG7Rzys1jI8GS5FF1riLNZpDFII},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Foley, William A.},
	year = {2017},
	pages = {179},
	file = {672-1.pdf:/home/user/Zotero/storage/5Q94X9T3/672-1.pdf:application/pdf}
}

@article{enfield-dependencies-2017,
	title = {Dependencies in language},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA1&dq=%22then+it+has+at+least+three+places+of%22+%22pointing+gestures,+then+she+will%22+%22also+observed.+Clear+examples+are+when+B+is+a+medium+for+A.+For%22+&ots=kh8KztSvGv&sig=cFee89guckJQDYIkSrBgImwBvRA},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Enfield, N. J.},
	year = {2017},
	pages = {1},
	file = {660-1.pdf:/home/user/Zotero/storage/KZ6M55WA/660-1.pdf:application/pdf}
}

@article{dingemanse-margins-2017,
	title = {On the margins of language: {Ideophones}, interjections and dependencies in linguistic theory},
	volume = {14},
	shorttitle = {On the margins of language},
	url = {https://books.google.com/books?hl=en&lr=&id=SnXhDgAAQBAJ&oi=fnd&pg=PA195&dq=%22level+of+granularity+(at+the+expense+of+others),+and+they+bring+certain%22+%22are+typologically+unexceptional+phenomena+that+many%22+%22by+inspecting+two+supposed+marginalia:+ideophones+and%22+&ots=kh8KztSvDo&sig=pVvBv6rBXekGnGBA0dsPWfa3ZRM},
	urldate = {2017-05-29},
	journal = {Dependencies in language: On the causal ontology of linguistic systems},
	author = {Dingemanse, Mark},
	year = {2017},
	pages = {195},
	file = {673-1.pdf:/home/user/Zotero/storage/9XZNMNWU/673-1.pdf:application/pdf}
}

@inproceedings{culbertson-artificial-2013,
	title = {Artificial grammar learning of shape-based noun classification.},
	url = {http://mindmodeling.org/cogsci2013/papers/0389/paper0389.pdf},
	urldate = {2017-05-29},
	booktitle = {{CogSci}},
	author = {Culbertson, Jennifer and Wilson, Colin},
	year = {2013},
	file = {0B_8jjfFyRUA5UVRHZ09DV2pZc0k.pdf:/home/user/Zotero/storage/MNZP2KFU/0B_8jjfFyRUA5UVRHZ09DV2pZc0k.pdf:application/pdf}
}

@inproceedings{culbertson-statistical-2012,
	title = {Statistical learning constrained by syntactic biases in an artificial language learning task},
	url = {https://www.researchgate.net/profile/Jennifer\_Culbertson/publication/265282620\_Statistical\_Learning\_Constrained\_by\_Syntactic\_Biases\_in\_an\_Artificial\_Language\_Learning\_Task/links/55cde77e08ae118c85baddd7.pdf},
	urldate = {2017-05-29},
	booktitle = {Proceedings of the 36th {Annual} {Boston} {University} {Conference} on {Language} {Development}},
	author = {Culbertson, Jennifer and Smolensky, Paul and Legendre, Géraldine},
	year = {2012},
	pages = {139--151},
	file = {0B_8jjfFyRUA5bktLdGRlVE5lcEE.pdf:/home/user/Zotero/storage/HX783VM3/0B_8jjfFyRUA5bktLdGRlVE5lcEE.pdf:application/pdf}
}

@article{culbertson-typological-2012,
	title = {Typological {Universals} as {Reflections} of {Biased} {Learning}: {Evidence} from {Artificial} {Language} {Learning}: {Typological} {Universals} as {Reflections} of {Biased} {Learning}},
	volume = {6},
	issn = {1749818X},
	shorttitle = {Typological {Universals} as {Reflections} of {Biased} {Learning}},
	url = {http://doi.wiley.com/10.1002/lnc3.338},
	doi = {10.1002/lnc3.338},
	language = {en},
	number = {5},
	urldate = {2017-05-29},
	journal = {Language and Linguistics Compass},
	author = {Culbertson, Jennifer},
	month = may,
	year = {2012},
	pages = {310--329},
	file = {0B_8jjfFyRUA5Vm8wWGR3WUxPUTA.pdf:/home/user/Zotero/storage/2ANVQXBH/0B_8jjfFyRUA5Vm8wWGR3WUxPUTA.pdf:application/pdf}
}

@article{culbertson-prefixal-2014,
	title = {Prefixal agreement and impersonal ‘il’ in {Spoken} {French}: {Experimental} evidence},
	volume = {24},
	issn = {0959-2695, 1474-0079},
	shorttitle = {Prefixal agreement and impersonal ‘il’ in {Spoken} {French}},
	url = {http://www.journals.cambridge.org/abstract\_S0959269513000380},
	doi = {10.1017/S0959269513000380},
	language = {en},
	number = {01},
	urldate = {2017-05-29},
	journal = {Journal of French Language Studies},
	author = {Culbertson, Jennifer and Legendre, GéRaldine},
	month = mar,
	year = {2014},
	pages = {83--105},
	file = {0B_8jjfFyRUA5N05USDVJR195SUE.pdf:/home/user/Zotero/storage/DMPH9H3F/0B_8jjfFyRUA5N05USDVJR195SUE.pdf:application/pdf}
}

@article{culbertson-language-2014,
	title = {Language learners privilege structured meaning over surface frequency},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1320525111},
	doi = {10.1073/pnas.1320525111},
	language = {en},
	number = {16},
	urldate = {2017-05-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Culbertson, J. and Adger, D.},
	month = apr,
	year = {2014},
	pages = {5842--5847},
	file = {0B_8jjfFyRUA5d0lLVDQwdW9NYVE.pdf:/home/user/Zotero/storage/QSCQTXUQ/0B_8jjfFyRUA5d0lLVDQwdW9NYVE.pdf:application/pdf}
}

@article{gross-revisited-2011,
	title = {Revisited {Linguistic} {Intuitions}},
	volume = {62},
	issn = {0007-0882, 1464-3537},
	url = {https://academic.oup.com/bjps/article-lookup/doi/10.1093/bjps/axr009},
	doi = {10.1093/bjps/axr009},
	language = {en},
	number = {3},
	urldate = {2017-05-29},
	journal = {The British Journal for the Philosophy of Science},
	author = {Gross, S. and Culbertson, J.},
	month = sep,
	year = {2011},
	pages = {639--656},
	file = {0B_8jjfFyRUA5UGswMjhTLVRiOWM.pdf:/home/user/Zotero/storage/Q4MMIG3G/0B_8jjfFyRUA5UGswMjhTLVRiOWM.pdf:application/pdf}
}

@article{legendre-is-2014,
	title = {Is children's comprehension of subject–verb agreement universally late? {Comparative} evidence from {French}, {English}, and {Spanish}},
	volume = {144},
	issn = {00243841},
	shorttitle = {Is children's comprehension of subject–verb agreement universally late?},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0024384113001277},
	doi = {10.1016/j.lingua.2013.05.004},
	language = {en},
	urldate = {2017-05-29},
	journal = {Lingua},
	author = {Legendre, Géraldine and Culbertson, Jennifer and Zaroukian, Erin and Hsin, Lisa and Barrière, Isabelle and Nazzi, Thierry},
	month = may,
	year = {2014},
	pages = {21--39},
	file = {0B_8jjfFyRUA5a1VnbWVRTGYySTA.pdf:/home/user/Zotero/storage/XCNUVDVM/0B_8jjfFyRUA5a1VnbWVRTGYySTA.pdf:application/pdf}
}

@article{culbertson-harmonic-2015,
	title = {Harmonic biases in child learners: {In} support of language universals},
	volume = {139},
	issn = {00100277},
	shorttitle = {Harmonic biases in child learners},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715000372},
	doi = {10.1016/j.cognition.2015.02.007},
	language = {en},
	urldate = {2017-05-29},
	journal = {Cognition},
	author = {Culbertson, Jennifer and Newport, Elissa L.},
	month = jun,
	year = {2015},
	pages = {71--82},
	file = {0B_8jjfFyRUA5TXdjYUExRzVwM2c.pdf:/home/user/Zotero/storage/MFEWWXU9/0B_8jjfFyRUA5TXdjYUExRzVwM2c.pdf:application/pdf}
}

@article{culbertson-cognitive-2013-1,
	title = {Cognitive {Biases}, {Linguistic} {Universals}, and {Constraint}-{Based} {Grammar} {Learning}},
	volume = {5},
	issn = {17568757},
	url = {http://doi.wiley.com/10.1111/tops.12027},
	doi = {10.1111/tops.12027},
	language = {en},
	number = {3},
	urldate = {2017-05-29},
	journal = {Topics in Cognitive Science},
	author = {Culbertson, Jennifer and Smolensky, Paul and Wilson, Colin},
	month = jul,
	year = {2013},
	pages = {392--424},
	file = {0B_8jjfFyRUA5aFlkdTZ6Q3RCQ0k.pdf:/home/user/Zotero/storage/WETDVD5M/0B_8jjfFyRUA5aFlkdTZ6Q3RCQ0k.pdf:application/pdf}
}

@article{culbertson-are-2009,
	title = {Are {Linguists} {Better} {Subjects}?},
	volume = {60},
	issn = {0007-0882, 1464-3537},
	url = {https://academic.oup.com/bjps/article-lookup/doi/10.1093/bjps/axp032},
	doi = {10.1093/bjps/axp032},
	language = {en},
	number = {4},
	urldate = {2017-05-29},
	journal = {The British Journal for the Philosophy of Science},
	author = {Culbertson, J. and Gross, S.},
	month = dec,
	year = {2009},
	pages = {721--736},
	file = {0B_8jjfFyRUA5MEp6aHNsblphVG8.pdf:/home/user/Zotero/storage/UPHPRAXD/0B_8jjfFyRUA5MEp6aHNsblphVG8.pdf:application/pdf}
}

@article{culbertson-learning-2012,
	title = {Learning biases predict a word order universal},
	volume = {122},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027711002745},
	doi = {10.1016/j.cognition.2011.10.017},
	language = {en},
	number = {3},
	urldate = {2017-05-29},
	journal = {Cognition},
	author = {Culbertson, Jennifer and Smolensky, Paul and Legendre, Géraldine},
	month = mar,
	year = {2012},
	pages = {306--329},
	file = {0B_8jjfFyRUA5TXpodVB1TWgtQ3c.pdf:/home/user/Zotero/storage/ACGX2QU2/0B_8jjfFyRUA5TXpodVB1TWgtQ3c.pdf:application/pdf}
}

@misc{noauthor-dependencies-nodate,
	title = {Dependencies in language: {On} the causal ontology of linguistic systems},
	shorttitle = {Dependencies in language},
	url = {http://langsci-press.org/catalog/book/96},
	urldate = {2017-05-29},
	file = {Snapshot:/home/user/Zotero/storage/HXDI74C4/96.html:text/html}
}

@article{enochson-collecting-2015,
	title = {Collecting {Psycholinguistic} {Response} {Time} {Data} {Using} {Amazon} {Mechanical} {Turk}},
	volume = {10},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0116946},
	doi = {10.1371/journal.pone.0116946},
	language = {en},
	number = {3},
	urldate = {2017-05-29},
	journal = {PLOS ONE},
	author = {Enochson, Kelly and Culbertson, Jennifer},
	editor = {Eriksson, Kimmo},
	month = mar,
	year = {2015},
	pages = {e0116946},
	file = {0B_8jjfFyRUA5Q3JPdXJEV01Jd1k.pdf:/home/user/Zotero/storage/P4JIRDDK/0B_8jjfFyRUA5Q3JPdXJEV01Jd1k.pdf:application/pdf}
}

@article{culbertson-simplicity-2016-1,
	title = {Simplicity and {Specificity} in {Language}: {Domain}-{General} {Biases} {Have} {Domain}-{Specific} {Effects}},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Simplicity and {Specificity} in {Language}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01964/full},
	doi = {10.3389/fpsyg.2015.01964},
	abstract = {The extent to which the linguistic system—its architecture, the representations it operates on, the constraints it is subject to—is specific to language has broad implications for cognitive science and its relation to evolutionary biology. Importantly, a given property of the linguistic system can be “specific” to the domain of language in several ways. For example, if the property evolved by natural selection under the pressure of the linguistic function it serves then the property is domain­-specific in the sense that its design is tailored for language. Equally though, if that property evolved to serve a different function or if that property is domain-general, it may nevertheless interact with the linguistic system in a way that is unique. This gives a second sense in which a property can be thought of as specific to language. An evolutionary approach to the language faculty might at first blush appear to favor domain­-specificity in the first sense, with individual properties of the language faculty being specifically linguistic adaptations. However, we argue that interactions between learning, culture and biological evolution mean any domain-specific adaptations that evolve will take the form of weak biases rather than hard constraints. Turning to the latter sense of domain-specificity, we highlight a very general bias, simplicity, which operates widely in cognition and yet interacts with linguistic representations in a domain-­specific way.},
	language = {English},
	urldate = {2017-05-29},
	journal = {Frontiers in Psychology},
	author = {Culbertson, Jennifer and Kirby, Simon},
	year = {2016},
	keywords = {word order, Language evolution, compositionality, domain-specificity, regularization, simplicity, typological universals}
}

@article{culbertson-investigating-2010,
	title = {Investigating the evolution of agreement systems using an artificial language learning paradigm},
	url = {http://repository.library.fresnostate.edu/bitstream/handle/10211.3/174773/WECOL%202010%20VOLUME%2020.pdf?sequence=1#page=51},
	urldate = {2017-05-29},
	journal = {Pragmatic Processing Factors in Negative Island Contexts},
	author = {Culbertson, Jennifer and Legendre, Géraldine},
	year = {2010},
	pages = {46},
	file = {0B_8jjfFyRUA5LTJNLVZwRXdCd2M.pdf:/home/user/Zotero/storage/PN2VNISS/0B_8jjfFyRUA5LTJNLVZwRXdCd2M.pdf:application/pdf}
}

@article{croft-greenbergian-2011,
	title = {Greenbergian universals, diachrony, and statistical analyses},
	volume = {15},
	issn = {1430-0532, 1613-415X},
	url = {https://www.degruyter.com/view/j/lity.2011.15.issue-2/lity.2011.029/lity.2011.029.xml},
	doi = {10.1515/lity.2011.029},
	number = {2},
	urldate = {2017-05-29},
	journal = {Linguistic Typology},
	author = {Croft, William and Bhattacharya, Tanmoy and Kleinschmidt, Dave and Smith, D. Eric and Jaeger, T. Florian},
	month = jan,
	year = {2011},
	file = {Greenbergian universals, diachrony, and statistical analyses - LITY.2011.029.pdf:/home/user/Zotero/storage/SBGMESKW/LITY.2011.029.pdf:application/pdf}
}

@article{hoffman-stochastic-2013-1,
	title = {Stochastic variational inference},
	volume = {14},
	url = {http://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf},
	number = {1},
	urldate = {2017-05-29},
	journal = {The Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347},
	file = {hoffman13a.dvi - hoffman13a.pdf:/home/user/Zotero/storage/6D3VQKW2/hoffman13a.pdf:application/pdf}
}

@inproceedings{korattikara-austerity-2014,
	title = {Austerity in {MCMC} land: {Cutting} the {Metropolis}-{Hastings} budget},
	shorttitle = {Austerity in {MCMC} land},
	url = {http://www.jmlr.org/proceedings/papers/v32/korattikara14.pdf},
	urldate = {2017-05-29},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning} ({ICML}-14)},
	author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
	year = {2014},
	pages = {181--189},
	file = {Austerity in MCMC Land\: Cutting the Metropolis-Hastings Budget - korattikara14.pdf:/home/user/Zotero/storage/ZTGP5KAC/korattikara14.pdf:application/pdf}
}

@inproceedings{maclaurin-firefly-2015,
	title = {Firefly {Monte} {Carlo}: {Exact} {MCMC} with subsets of data},
	shorttitle = {Firefly {Monte} {Carlo}},
	url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/11279},
	urldate = {2017-05-29},
	booktitle = {Twenty-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Maclaurin, Dougal and Adams, Ryan Prescott},
	year = {2015},
	file = {Firefly Monte Carlo\: Exact MCMC with Subsets of Data - 11135:/home/user/Zotero/storage/9G29AVBZ/11135.pdf:application/pdf}
}

@inproceedings{ahn-distributed-2014,
	title = {Distributed {Stochastic} {Gradient} {MCMC}.},
	url = {http://www.jmlr.org/proceedings/papers/v32/ahn14.pdf},
	urldate = {2017-05-29},
	booktitle = {{ICML}},
	author = {Ahn, Sungjin and Shahbaba, Babak and Welling, Max and {others}},
	year = {2014},
	pages = {1044--1052},
	file = {Distributed Stochastic Gradient MCMC - ahn14.pdf:/home/user/Zotero/storage/J6NE5Q6P/ahn14.pdf:application/pdf}
}

@inproceedings{kingma-variational-2015-1,
	title = {Variational dropout and the local reparameterization trick},
	url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick},
	urldate = {2017-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	year = {2015},
	pages = {2575--2583},
	file = {Variational Dropout and the Local Reparameterization Trick - 5666-variational-dropout-and-the-local-reparameterization-trick.pdf:/home/user/Zotero/storage/3UPSKWDH/5666-variational-dropout-and-the-local-reparameterization-trick.pdf:application/pdf}
}

@inproceedings{balan-bayesian-2015,
	title = {Bayesian dark knowledge},
	url = {http://papers.nips.cc/paper/5965-bayesian-dark-knowledge},
	urldate = {2017-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Balan, Anoop Korattikara and Rathod, Vivek and Murphy, Kevin P. and Welling, Max},
	year = {2015},
	pages = {3438--3446},
	file = {Bayesian dark knowledge - 5965-bayesian-dark-knowledge.pdf:/home/user/Zotero/storage/E56G9HBM/5965-bayesian-dark-knowledge.pdf:application/pdf}
}

@inproceedings{xu-distributed-2014,
	title = {Distributed {Bayesian} posterior sampling via moment sharing},
	url = {http://papers.nips.cc/paper/5596-distributed-bayesian-posterior-sampling-via-moment-sharin},
	urldate = {2017-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Xu, Minjie and Lakshminarayanan, Balaji and Teh, Yee Whye and Zhu, Jun and Zhang, Bo},
	year = {2014},
	pages = {3356--3364},
	file = {Distributed Bayesian Posterior Sampling via Moment Sharing - 5596-distributed-bayesian-posterior-sampling-via-moment-sharing.pdf:/home/user/Zotero/storage/2SZMXRW8/5596-distributed-bayesian-posterior-sampling-via-moment-sharing.pdf:application/pdf}
}

@inproceedings{patterson-stochastic-2013,
	title = {Stochastic gradient {Riemannian} {Langevin} dynamics on the probability simplex},
	url = {http://papers.nips.cc/paper/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex},
	urldate = {2017-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Patterson, Sam and Teh, Yee Whye},
	year = {2013},
	pages = {3102--3110},
	file = {Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex - 4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf:/home/user/Zotero/storage/KRTSXZ7E/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf:application/pdf}
}

@article{teh-consistency-2016,
	title = {Consistency and fluctuations for stochastic gradient {Langevin} dynamics},
	volume = {17},
	url = {http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf},
	number = {7},
	urldate = {2017-05-29},
	journal = {Journal of Machine Learning Research},
	author = {Teh, Yee Whye and Thiery, Alexandre H. and Vollmer, Sebastian J.},
	year = {2016},
	pages = {1--33},
	file = {teh16a.pdf:/home/user/Zotero/storage/X3X67NZH/teh16a.pdf:application/pdf}
}

@inproceedings{sato-approximation-2014,
	title = {Approximation {Analysis} of {Stochastic} {Gradient} {Langevin} {Dynamics} by using {Fokker}-{Planck} {Equation} and {Ito} {Process}.},
	url = {http://www.jmlr.org/proceedings/papers/v32/satoa14.pdf},
	urldate = {2017-05-29},
	booktitle = {Icml},
	author = {Sato, Issei and Nakagawa, Hiroshi},
	year = {2014},
	pages = {982--990},
	file = {satoa14.pdf:/home/user/Zotero/storage/ENCAJMUB/satoa14.pdf:application/pdf}
}

@inproceedings{xu-distributed-2014-1,
	title = {Distributed {Bayesian} posterior sampling via moment sharing},
	url = {http://papers.nips.cc/paper/5596-distributed-bayesian-posterior-sampling-via-moment-sharin},
	urldate = {2017-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Xu, Minjie and Lakshminarayanan, Balaji and Teh, Yee Whye and Zhu, Jun and Zhang, Bo},
	year = {2014},
	pages = {3356--3364},
	file = {Distributed Bayesian Posterior Sampling via Moment Sharing - 5596-distributed-bayesian-posterior-sampling-via-moment-sharing.pdf:/home/user/Zotero/storage/WXR8TVFE/5596-distributed-bayesian-posterior-sampling-via-moment-sharing.pdf:application/pdf}
}

@article{ma-complete-2015,
	title = {A {Complete} {Recipe} for {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1506.04696},
	abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
	urldate = {2017-05-29},
	journal = {arXiv:1506.04696 [math, stat]},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04696},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/3JXFUFSR/1506.html:text/html;Ma et al_2015_A Complete Recipe for Stochastic Gradient MCMC.pdf:/home/user/Zotero/storage/6ZVGJ6PH/Ma et al_2015_A Complete Recipe for Stochastic Gradient MCMC.pdf:application/pdf}
}

@article{blei-variational-2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {0},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	url = {http://dx.doi.org/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review VI, a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {ja},
	urldate = {2017-05-29},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = feb,
	year = {2017},
	keywords = {Algorithms, Computationally Intensive Methods, Statistical Computing},
	pages = {0--0},
	file = {Blei et al_2017_Variational Inference.pdf:/home/user/Zotero/storage/NU2IREFS/Blei et al_2017_Variational Inference.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/B6VKUPIM/01621459.2017.html:text/html}
}

@article{neiswanger-asymptotically-2013,
	title = {Asymptotically {Exact}, {Embarrassingly} {Parallel} {MCMC}},
	url = {http://arxiv.org/abs/1311.4780},
	abstract = {Communication costs, resulting from synchronization requirements during learning, can greatly slow down many parallel machine learning algorithms. In this paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in which subsets of data are processed independently, with very little communication. First, we arbitrarily partition data onto multiple machines. Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be used to draw samples from a posterior distribution given the data subset. Finally, the samples from each machine are combined to form samples from the full posterior. This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data (without communication) until the final combination stage. We prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models.},
	urldate = {2017-05-29},
	journal = {arXiv:1311.4780 [cs, stat]},
	author = {Neiswanger, Willie and Wang, Chong and Xing, Eric},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.4780},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Computation, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/GXFQZR3J/1311.html:text/html;Neiswanger et al_2013_Asymptotically Exact, Embarrassingly Parallel MCMC.pdf:/home/user/Zotero/storage/W4UQXREV/Neiswanger et al_2013_Asymptotically Exact, Embarrassingly Parallel MCMC.pdf:application/pdf}
}

@inproceedings{li-preconditioned-2016,
	title = {Preconditioned stochastic gradient {Langevin} dynamics for deep neural networks},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewPaper/11835},
	urldate = {2017-05-29},
	booktitle = {Thirtieth {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Chunyuan and Chen, Changyou and Carlson, David and Carin, Lawrence},
	year = {2016},
	file = {11805.pdf:/home/user/Zotero/storage/M5SJSEXG/11805.pdf:application/pdf}
}

@article{ma-complete-2015-1,
	title = {A {Complete} {Recipe} for {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1506.04696},
	abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
	urldate = {2017-05-29},
	journal = {arXiv:1506.04696 [math, stat]},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04696},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9GIE7F7J/1506.html:text/html;Ma et al_2015_A Complete Recipe for Stochastic Gradient MCMC.pdf:/home/user/Zotero/storage/HVRHQH8H/Ma et al_2015_A Complete Recipe for Stochastic Gradient MCMC.pdf:application/pdf}
}

@article{culbertson-language-2014-1,
	title = {Language learners privilege structured meaning over surface frequency},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/111/16/5842},
	doi = {10.1073/pnas.1320525111},
	abstract = {Although it is widely agreed that learning the syntax of natural languages involves acquiring structure-dependent rules, recent work on acquisition has nevertheless attempted to characterize the outcome of learning primarily in terms of statistical generalizations about surface distributional information. In this paper we investigate whether surface statistical knowledge or structural knowledge of English is used to infer properties of a novel language under conditions of impoverished input. We expose learners to artificial-language patterns that are equally consistent with two possible underlying grammars—one more similar to English in terms of the linear ordering of words, the other more similar on abstract structural grounds. We show that learners’ grammatical inferences overwhelmingly favor structural similarity over preservation of superficial order. Importantly, the relevant shared structure can be characterized in terms of a universal preference for isomorphism in the mapping from meanings to utterances. Whereas previous empirical support for this universal has been based entirely on data from cross-linguistic language samples, our results suggest it may reflect a deep property of the human cognitive system—a property that, together with other structure-sensitive principles, constrains the acquisition of linguistic knowledge.},
	language = {en},
	number = {16},
	urldate = {2017-05-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Culbertson, Jennifer and Adger, David},
	month = apr,
	year = {2014},
	pmid = {24706789},
	keywords = {Learning biases, Typology, Artificial grammar learning, semantic scope, transitional probabilities},
	pages = {5842--5847},
	file = {Culbertson_Adger_2014_Language learners privilege structured meaning over surface frequency.pdf:/home/user/Zotero/storage/IJVVMCHA/Culbertson_Adger_2014_Language learners privilege structured meaning over surface frequency.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XUM2CCUC/5842.html:text/html}
}

@article{kleinschmidt-robust-2015,
	title = {Robust speech perception: {Recognize} the familiar, generalize to the similar, and adapt to the novel},
	volume = {122},
	issn = {0033-295X},
	shorttitle = {Robust speech perception},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4744792/},
	doi = {10.1037/a0038695},
	abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker’s /p/ might be physically indistinguishable from another talker’s /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively non-stationary world and propose that the speech perception system overcomes this challenge by (1) recognizing previously encountered situations, (2) generalizing to other situations based on previous similar experience, and (3) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (1) to (3) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on two critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these two aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension.},
	number = {2},
	urldate = {2017-05-29},
	journal = {Psychological review},
	author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
	month = apr,
	year = {2015},
	pmid = {25844873},
	pmcid = {PMC4744792},
	pages = {148--203},
	file = {Kleinschmidt_Jaeger_2015_Robust speech perception.pdf:/home/user/Zotero/storage/8ASV6VIW/Kleinschmidt_Jaeger_2015_Robust speech perception.pdf:application/pdf}
}

@article{lakens-equivalence-2017,
	title = {Equivalence {Tests}},
	issn = {1948-5506},
	url = {http://dx.doi.org/10.1177/1948550617697177},
	doi = {10.1177/1948550617697177},
	abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
	urldate = {2017-05-29},
	journal = {Social Psychological and Personality Science},
	author = {Lakens, Daniël},
	month = may,
	year = {2017},
	pages = {1948550617697177},
	file = {Lakens_2017_Equivalence Tests.pdf:/home/user/Zotero/storage/8AFPTHCS/Lakens_2017_Equivalence Tests.pdf:application/pdf}
}

@article{liu-dependency-2017,
	title = {Dependency distance: {A} new perspective on syntactic patterns in natural languages},
	issn = {15710645},
	shorttitle = {Dependency distance},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1571064517300532},
	doi = {10.1016/j.plrev.2017.03.002},
	language = {en},
	urldate = {2017-05-10},
	journal = {Physics of Life Reviews},
	author = {Liu, Haitao and Xu, Chunshan and Liang, Junying},
	month = mar,
	year = {2017},
	file = {Dependency distance\: A new perspective on syntactic patterns in natural languages - 1-s2.0-S1571064517300532-main.pdf:/home/user/Zotero/storage/SJPJPZIQ/1-s2.0-S1571064517300532-main.pdf:application/pdf}
}

@article{hawkins-efficiency-2003,
	title = {Efficiency and complexity in grammars: {Three} general principles},
	shorttitle = {Efficiency and complexity in grammars},
	url = {http://www.ling.helsinki.fi/uhlcs/LENCA/LENCA-2/information/John-Hawkins-CSLIpaper.pdf},
	urldate = {2017-05-10},
	journal = {The nature of explanation in linguistic theory},
	author = {Hawkins, John A.},
	year = {2003},
	pages = {121--152},
	file = {Microsoft Word - ch6.doc - download:/home/user/Zotero/storage/N927VNQ3/download.pdf:application/pdf}
}

@article{dryer-evidence-2011,
	title = {The evidence for word order correlations},
	volume = {15},
	issn = {1430-0532, 1613-415X},
	url = {http://www.degruyter.com/view/j/lity.2011.15.issue-2/lity.2011.024/lity.2011.024.xml},
	doi = {10.1515/lity.2011.024},
	number = {2},
	urldate = {2017-05-10},
	journal = {Linguistic Typology},
	author = {Dryer, Matthew S.},
	month = jan,
	year = {2011},
	file = {DryerResponseToDunnEtAl.pdf:/home/user/Zotero/storage/D6GXCSV3/DryerResponseToDunnEtAl.pdf:application/pdf}
}

@article{dryer-problems-2009,
	title = {Problems testing typological correlations with the online {WALS}},
	volume = {13},
	issn = {1430-0532, 1613-415X},
	url = {https://www.degruyter.com/view/j/lity.2009.13.issue-1/lity.2009.007/lity.2009.007.xml},
	doi = {10.1515/LITY.2009.007},
	number = {1},
	urldate = {2017-05-10},
	journal = {Linguistic Typology},
	author = {Dryer, Matthew S.},
	month = jan,
	year = {2009},
	file = {DryerTestingWALS2009.pdf:/home/user/Zotero/storage/BWBIDFDB/DryerTestingWALS2009.pdf:application/pdf}
}

@incollection{dryer-branching-2009,
	title = {The branching direction theory of word order correlations revisited},
	url = {http://link.springer.com/chapter/10.1007/978-1-4020-8825-4\_10},
	urldate = {2017-05-10},
	booktitle = {Universals of language today},
	publisher = {Springer},
	author = {Dryer, Matthew S.},
	year = {2009},
	pages = {185--207},
	file = {DryerBDTrevisited.pdf:/home/user/Zotero/storage/WK8FKRQI/DryerBDTrevisited.pdf:application/pdf}
}

@article{dryer-word-2007,
	title = {Word order},
	volume = {1},
	url = {http://www.sfs.uni-tuebingen.de/~gjaeger/lehre/ws0910/languagesOfTheWorld/dryer2000.pdf},
	urldate = {2017-05-10},
	journal = {Language typology and syntactic description},
	author = {Dryer, Matthew S.},
	year = {2007},
	pages = {61--131},
	file = {DryerShopenWordOrder.pdf:/home/user/Zotero/storage/8DJT3EIZ/DryerShopenWordOrder.pdf:application/pdf}
}

@article{dryer-case-2002,
	title = {Case distinctions, rich verb agreement, and word order type (comments on {Hawkins}’ paper)},
	volume = {28},
	url = {https://www.degruyter.com/dg/viewarticle.fullcontentlink:pdfeventlink/$002fj$002fthli.2002.28.issue-2$002fthli.2002.28.2.151$002fthli.2002.28.2.151.pdf?t:ac=j$002fthli.2002.28.issue-2$002fthli.2002.28.2.151$002fthli.2002.28.2.151.xml},
	number = {2},
	urldate = {2017-05-10},
	journal = {Theoretical Linguistics},
	author = {Dryer, Matthew S.},
	year = {2002},
	pages = {151--158},
	file = {DryerTheoLing02.pdf:/home/user/Zotero/storage/TXBXKEM6/DryerTheoLing02.pdf:application/pdf}
}

@article{dryer-frequency-1995,
	title = {Frequency and pragmatically unmarked word order},
	volume = {30},
	url = {https://books.google.com/books?hl=en&lr=&id=cEDtBaam0vAC&oi=fnd&pg=PA105&dq=%22order,+that+it+is+part+of+what+it+means+to+be+unmarked+that+the%22+%22in+which+claims+of+this+sort+have+been+made.+The+purpose+of%22+&ots=sDU0AxU5Nl&sig=BrFiELth8tb55xrscKXaAL7Y4Bg},
	urldate = {2017-05-10},
	journal = {Word order in discourse},
	author = {Dryer, Matthew S.},
	year = {1995},
	pages = {105},
	file = {DryerFreqUnmarkedWordOrder.pdf:/home/user/Zotero/storage/BGKGC8NZ/DryerFreqUnmarkedWordOrder.pdf:application/pdf}
}

@article{dryer-greenbergian-1992,
	title = {The {Greenbergian} {Word} {Order} {Correlations}},
	volume = {68},
	issn = {0097-8507},
	url = {http://www.jstor.org/stable/416370},
	doi = {10.2307/416370},
	abstract = {This paper reports on the results of a detailed empirical study of word order correlations, based on a sample of 625 languages. The primary result is a determination of exactly what pairs of elements correlate in order with the verb and object. Some pairs of elements that have been claimed to correlate in order with the verb and object do not in fact exhibit any correlation. I argue against the Head-Dependent Theory (HDT), according to which the correlations reflect a tendency towards consistent ordering of heads and dependents. I offer an alternative account, the Branching Direction Theory (BDT), based on consistent ordering of phrasal and nonphrasal elements. According to the BDT, the word order correlations reflect a tendency for languages to be consistently right-branching or consistently left-branching.},
	number = {1},
	urldate = {2017-05-10},
	journal = {Language},
	author = {Dryer, Matthew S.},
	year = {1992},
	pages = {81--138}
}

@article{osband-why-2016,
	title = {Why is {Posterior} {Sampling} {Better} than {Optimism} for {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1607.00215},
	urldate = {2017-05-09},
	journal = {arXiv preprint arXiv:1607.00215},
	author = {Osband, Ian and Van Roy, Benjamin},
	year = {2016},
	file = {0.pdf:/home/user/Zotero/storage/AW5VPIIW/0.pdf:application/pdf}
}

@article{kleinschmidt-robust-2015-1,
	title = {Robust speech perception: {Recognize} the familiar, generalize to the similar, and adapt to the novel.},
	volume = {122},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Robust speech perception},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038695},
	doi = {10.1037/a0038695},
	language = {en},
	number = {2},
	urldate = {2017-05-09},
	journal = {Psychological Review},
	author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
	year = {2015},
	pages = {148--203},
	file = {0.pdf:/home/user/Zotero/storage/UJSMRTAH/0.pdf:application/pdf}
}

@article{hawkins-processing-2007,
	series = {Modern {Approaches} to {Language}},
	title = {Processing typology and why psychologists need to know about it},
	volume = {25},
	issn = {0732-118X},
	url = {http://www.sciencedirect.com/science/article/pii/S0732118X07000219},
	doi = {10.1016/j.newideapsych.2007.02.003},
	abstract = {This paper illustrates an interdisciplinary research program based on cross-linguistic comparison that is of relevance for psychologists working on language processing, so-called “processing typology” [Hawkins, J. A. (1994). A performance theory of order and constituency. Cambridge: Cambridge University Press; (2004). Efficiency and complexity in grammars. Oxford: Oxford University Press]. Its most original feature is the hypothesis that patterns and preferences found in performance in languages with several structures of a given type (e.g. preferences among alternative word orders) are the same patterns and preferences one finds across languages in the fixed conventions of grammars that permit less variation (i.e. in fixed word orders). Data supporting this “performance–grammar correspondence hypothesis” are summarized. One of its consequences is that principles of performance can be used to make predictions for patterns of grammatical variation, while preferences in grammars become relevant for the testing of psycholinguistic ideas. Two proposed principles of ordering in performance, in terms of “end weight” and “memory cost”, are criticized on the basis of cross-linguistic data. Both predict an asymmetry in ordering, whereby some category A precedes B. But end weight is not a valid cross-linguistic asymmetry, and memory cost cannot explain certain asymmetries for which it has been invoked when different language types are considered. The paper argues for greater mutual awareness between processing theorists and language typologists, for more consideration of non-European grammars and language types in psycholinguistics, and for a greater appeal to processing in the explanation of typological variation.},
	number = {2},
	urldate = {2017-04-27},
	journal = {New Ideas in Psychology},
	author = {Hawkins, John A.},
	month = aug,
	year = {2007},
	pages = {87--107},
	file = {Hawkins_2007_Processing typology and why psychologists need to know about it.pdf:/home/user/Zotero/storage/392HTMNQ/Hawkins_2007_Processing typology and why psychologists need to know about it.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/B57WTR8P/S0732118X07000219.html:text/html}
}

@misc{noauthor-performance-nodate,
	title = {A {Performance} {Theory} of {Order} and {Constituency} - {Cambridge} {University} {Press}},
	url = {http://www.cambridge.org/catalogue/catalogue.asp?isbn=0521378672},
	urldate = {2017-04-27},
	file = {A Performance Theory of Order and Constituency - Cambridge University Press:/home/user/Zotero/storage/D3BG6RR4/catalogue.html:text/html}
}

@unpublished{straubing-forest-nodate-1,
	title = {Forest {Categories}},
	url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&ik=a90995961a&view=att&th=154e32c9cf8ff2bd&attid=0.1&disp=safe&realattid=f\_ioljfkxe2&zw&saddbat=ANGjdJ9AIp7OCU-BDeEb2eZSYJeibmynVJ\_zFnRB9mKelLjZGoikpsbyvKmb-yp7hE4WRHfdfGvqr3LoL2xLlsWsgeA-lLoaq46ESTRNX4Vg-DGEL2LxRqgH1n0bk1xnz4sK-xzE7q3lvUMy4zTAWyPGjLdf1xuEtXEeeCAjTfIrBCdAhDUqlwVswXogkCmSQ1eGmpBoF5YLGfQ03bakGtCPNF6KJecpN\_E24QHf40OEI\_uaW\_M2-rpwpVKX879dd4WXVwokClD6aYXlVTr8ktoJjIJC0hR6QSjls9LmlamIVpxTbZkzzoSr3eOjg0qLDzYHMAf4Wd8DqXBIpDPb-6QSjWNTsM3gRd63wcLon\_hHD0r3FzomwcOidj3LBokYZKE48d5NBf5mp9S2r0WhbEEEDERI7XNsTBVkJTLr-xSZDFEOsqFcPlzRpqNMwBlphuluMkmXM2CX9JMQqndSjN26fJgRrPRhpJP9zOL1w4m0F\_mCamBVdpUOW-1pompePRxWMhl48d7SlpASJG8Sz2ZOX6\_\_IHYg3-cVxbT5dPuXQ2XWi04J4Mpu8SqU2otKhmnUgHR9jkNiSWXEbJ9LeGUKU5wgka3PL-1mjdoFX62MXp8VPoilgauK8hycFT-mPzCmzBXXSdUudW3ynwdb},
	urldate = {2017-04-26},
	author = {Straubing, Howard},
	file = {0.pdf:/home/user/Zotero/storage/MZ5UD59Q/0.pdf:application/pdf}
}

@inproceedings{futrell-noisy-context-2017-1,
	title = {Noisy-context surprisal as a human sentence processing cost model},
	url = {https://www.aclweb.org/anthology/E/E17/E17-1065.pdf},
	urldate = {2017-04-25},
	booktitle = {Proceedings of {EACL}},
	author = {Futrell, Richard and Levy, Roger},
	year = {2017},
	file = {Noisy-context surprisal as a human sentence processing cost model - E17-1065.pdf:/home/user/Zotero/storage/ZI2NDCR3/E17-1065.pdf:application/pdf}
}

@article{dautriche-wordform-2016,
	title = {Wordform {Similarity} {Increases} {With} {Semantic} {Similarity}: {An} {Analysis} of 100 {Languages}},
	issn = {03640213},
	shorttitle = {Wordform {Similarity} {Increases} {With} {Semantic} {Similarity}},
	url = {http://doi.wiley.com/10.1111/cogs.12453},
	doi = {10.1111/cogs.12453},
	language = {en},
	urldate = {2017-04-25},
	journal = {Cognitive Science},
	author = {Dautriche, Isabelle and Mahowald, Kyle and Gibson, Edward and Piantadosi, Steven T.},
	month = nov,
	year = {2016},
	file = {untitled - dautriche2016wordform.pdf:/home/user/Zotero/storage/3I55SBWT/dautriche2016wordform.pdf:application/pdf}
}

@article{mahowald-snap-2016,
	title = {{SNAP} judgments: {A} small {N} acceptability paradigm ({SNAP}) for linguistic acceptability judgments},
	volume = {92},
	shorttitle = {{SNAP} judgments},
	url = {https://muse.jhu.edu/article/629764/summary},
	number = {3},
	urldate = {2017-04-25},
	journal = {Language},
	author = {Mahowald, Kyle and Graff, Peter and Hartman, Jeremy and Gibson, Edward},
	year = {2016},
	pages = {619--635},
	file = {Microsoft Word - SNAP_revision1_submit.docx - SNAP.pdf:/home/user/Zotero/storage/JVIMTCMM/SNAP.pdf:application/pdf}
}

@article{mahowald-info/information-2013-1,
	title = {Info/information theory: {Speakers} choose shorter words in predictive contexts},
	volume = {126},
	issn = {00100277},
	shorttitle = {Info/information theory},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027712002107},
	doi = {10.1016/j.cognition.2012.09.010},
	language = {en},
	number = {2},
	urldate = {2017-04-25},
	journal = {Cognition},
	author = {Mahowald, Kyle and Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward},
	month = feb,
	year = {2013},
	pages = {313--318},
	file = {Info/information theory\: Speakers choose shorter words in predictive contexts - mahowald_info.pdf:/home/user/Zotero/storage/QNRJKV6B/mahowald_info.pdf:application/pdf}
}

@article{temperley-information-2015,
	title = {Information {Density} and {Syntactic} {Repetition}},
	volume = {39},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12215/abstract},
	doi = {10.1111/cogs.12215},
	abstract = {In noun phrase (NP) coordinate constructions (e.g., NP and NP), there is a strong tendency for the syntactic structure of the second conjunct to match that of the first; the second conjunct in such constructions is therefore low in syntactic information. The theory of uniform information density predicts that low-information syntactic constructions will be counterbalanced by high information in other aspects of that part of the sentence, and high-information constructions will be counterbalanced by other low-information components. Three predictions follow: (a) lexical probabilities (measured by N-gram probabilities and head-dependent probabilities) will be lower in second conjuncts than first conjuncts; (b) lexical probabilities will be lower in matching second conjuncts (those whose syntactic expansions match the first conjunct) than nonmatching ones; and (c) syntactic repetition should be especially common for low-frequency NP expansions. Corpus analysis provides support for all three of these predictions.},
	language = {en},
	number = {8},
	urldate = {2017-04-25},
	journal = {Cognitive Science},
	author = {Temperley, David and Gildea, Daniel},
	month = nov,
	year = {2015},
	keywords = {Syntax, Information, Language production, Coordination, Probabilistic models},
	pages = {1802--1823},
	file = {Snapshot:/home/user/Zotero/storage/KZGJSVCR/abstract\;jsessionid=3D2CDBD5C02954A0109E5F52234BD7EE.html:text/html;Temperley_Gildea_2015_Information Density and Syntactic Repetition.pdf:/home/user/Zotero/storage/PWFTKPCU/Temperley_Gildea_2015_Information Density and Syntactic Repetition.pdf:application/pdf}
}

@article{gildea-human-2015,
	title = {Human languages order information efficiently},
	url = {http://arxiv.org/abs/1510.02823},
	abstract = {Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that, despite these differences, human languages might constitute different `solutions' to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations.},
	urldate = {2017-04-25},
	journal = {arXiv:1510.02823 [cs]},
	author = {Gildea, Daniel and Jaeger, T. Florian},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.02823},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/BU33WISV/1510.html:text/html;Gildea_Jaeger_2015_Human languages order information efficiently.pdf:/home/user/Zotero/storage/GQ5VCS7W/Gildea_Jaeger_2015_Human languages order information efficiently.pdf:application/pdf;paper.pdf:/home/user/Zotero/storage/NQGZFC4R/paper.pdf:application/pdf}
}

@article{koplenig-statistical-2017,
	title = {The statistical trade-off between word order and word structure -- {Large}-scale evidence for the principle of least effort},
	volume = {12},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0173614},
	doi = {10.1371/journal.pone.0173614},
	abstract = {Languages employ different strategies to transmit structural and grammatical information. While, for example, grammatical dependency relationships in sentences are mainly conveyed by the ordering of the words for languages like Mandarin Chinese, or Vietnamese, the word ordering is much less restricted for languages such as Inupiatun or Quechua, as these languages (also) use the internal structure of words (e.g. inflectional morphology) to mark grammatical relationships in a sentence. Based on a quantitative analysis of more than 1,500 unique translations of different books of the Bible in almost 1,200 different languages that are spoken as a native language by approximately 6 billion people (more than 80\% of the world population), we present large-scale evidence for a statistical trade-off between the amount of information conveyed by the ordering of words and the amount of information conveyed by internal word structure: languages that rely more strongly on word order information tend to rely less on word structure information and vice versa. Or put differently, if less information is carried within the word, more information has to be spread among words in order to communicate successfully. In addition, we find that–despite differences in the way information is expressed–there is also evidence for a trade-off between different books of the biblical canon that recurs with little variation across languages: the more informative the word order of the book, the less informative its word structure and vice versa. We argue that this might suggest that, on the one hand, languages encode information in very different (but efficient) ways. On the other hand, content-related and stylistic features are statistically encoded in very similar ways.},
	number = {3},
	urldate = {2017-04-04},
	journal = {PLOS ONE},
	author = {Koplenig, Alexander and Meyer, Peter and Wolfer, Sascha and Müller-Spitzer, Carolin},
	month = mar,
	year = {2017},
	keywords = {Linguistics, Syntax, Data processing, Languages, Natural language, Cognitive linguistics, Entropy, Linguistic morphology},
	pages = {e0173614},
	file = {Koplenig et al_2017_The statistical trade-off between word order and word structure – Large-scale.pdf:/home/user/Zotero/storage/G9F55Q8F/Koplenig et al_2017_The statistical trade-off between word order and word structure – Large-scale.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ES5CZQ2Q/article.html:text/html}
}

@misc{noauthor-isabelle-nodate,
	title = {Isabelle {Dautriche}},
	url = {http://www.lscp.net/persons/dautriche/publi.php},
	urldate = {2017-04-25},
	file = {Isabelle Dautriche:/home/user/Zotero/storage/B5BF9VVV/publi.html:text/html}
}

@article{dautriche-words-2017,
	title = {Words cluster phonetically beyond phonotactic regularities},
	volume = {163},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027717300331},
	doi = {10.1016/j.cognition.2017.02.001},
	abstract = {Recent evidence suggests that cognitive pressures associated with language acquisition and use could affect the organization of the lexicon. On one hand, consistent with noisy channel models of language (e.g., Levy, 2008), the phonological distance between wordforms should be maximized to avoid perceptual confusability (a pressure for dispersion). On the other hand, a lexicon with high phonological regularity would be simpler to learn, remember and produce (e.g., Monaghan et al., 2011) (a pressure for clumpiness). Here we investigate wordform similarity in the lexicon, using measures of word distance (e.g., phonological neighborhood density) to ask whether there is evidence for dispersion or clumpiness of wordforms in the lexicon. We develop a novel method to compare lexicons to phonotactically-controlled baselines that provide a null hypothesis for how clumpy or sparse wordforms would be as the result of only phonotactics. Results for four languages, Dutch, English, German and French, show that the space of monomorphemic wordforms is clumpier than what would be expected by the best chance model according to a wide variety of measures: minimal pairs, average Levenshtein distance and several network properties. This suggests a fundamental drive for regularity in the lexicon that conflicts with the pressure for words to be as phonologically distinct as possible.},
	urldate = {2017-04-25},
	journal = {Cognition},
	author = {Dautriche, Isabelle and Mahowald, Kyle and Gibson, Edward and Christophe, Anne and Piantadosi, Steven T.},
	month = jun,
	year = {2017},
	keywords = {Linguistics, Phonotactics, Communication, Lexical design},
	pages = {128--145},
	file = {Dautriche et al_2017_Words cluster phonetically beyond phonotactic regularities.pdf:/home/user/Zotero/storage/R8KURTUS/Dautriche et al_2017_Words cluster phonetically beyond phonotactic regularities.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/U9KVNT4I/S0010027717300331.html:text/html}
}

@article{scontras-heritage-2015,
	title = {Heritage language and linguistic theory},
	volume = {6},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01545/full},
	doi = {10.3389/fpsyg.2015.01545},
	abstract = {This paper discusses a common reality in many cases of multilingualism: heritage speakers, or unbalanced bilinguals, simultaneous or sequential, who shifted early in childhood from one language (their heritage language) to their dominant language (the language of their speech community). To demonstrate the relevance of heritage linguistics to the study of linguistic competence more broadly defined, we present a series of case studies on heritage linguistics, documenting some of the deficits and abilities typical of heritage speakers, together with the broader theoretical questions they inform. We consider the reorganization of morphosyntactic feature systems, the reanalysis of atypical argument structure, the attrition of the syntax of relativization, and the simplification of scope interpretations; these phenomena implicate diverging trajectories and outcomes in the development of heritage speakers. The case studies also have practical and methodological implications for the study of multilingualism. We conclude by discussing more general concepts central to linguistic inquiry, in particular, complexity and native speaker competence.},
	language = {English},
	urldate = {2017-04-25},
	journal = {Frontiers in Psychology},
	author = {Scontras, Gregory and Fuchs, Zuzanna and Polinsky, Maria},
	year = {2015},
	keywords = {Syntax, Semantics, Russian, Mandarin Chinese, pragmatics, experimental methods, heritage linguistics, Morphosyntax, Multilingualism, spanish}
}

@book{sampson-language-2009,
	address = {Oxford ; New York},
	series = {Oxford linguistics},
	title = {Language complexity as an evolving variable},
	isbn = {978-0-19-954521-6 978-0-19-954522-3},
	number = {13},
	publisher = {Oxford University Press},
	editor = {Sampson, Geoffrey and Gil, David and Trudgill, Peter},
	year = {2009},
	note = {OCLC: ocn227962299},
	keywords = {Complexity (Linguistics)},
	file = {Language complexity as an evolving variable in SearchWorks:/home/user/Zotero/storage/JZVPTFE4/8481279.html:text/html}
}

@misc{noauthor-sampson-nodate,
	title = {Sampson, {Gil}, and {Trudgill}: {Language} {Complexity} as an {Evolving} {Variable}},
	url = {https://www.grsampson.net/BLCA.html},
	urldate = {2017-04-25},
	file = {Sampson, Gil, and Trudgill\: Language Complexity as an Evolving Variable:/home/user/Zotero/storage/48JK8NUV/BLCA.html:text/html}
}

@incollection{dubey-variance-2016,
	title = {Variance {Reduction} in {Stochastic} {Gradient} {Langevin} {Dynamics}},
	url = {http://papers.nips.cc/paper/6293-variance-reduction-in-stochastic-gradient-langevin-dynamics.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Dubey, Kumar Avinava and J. Reddi, Sashank and Williamson, Sinead A and Poczos, Barnabas and Smola, Alexander J and Xing, Eric P},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1154--1162},
	file = {Variance Reduction in Stochastic Gradient Langevin Dynamics - 6293-variance-reduction-in-stochastic-gradient-langevin-dynamics.pdf:/home/user/Zotero/storage/9FSMCUDM/6293-variance-reduction-in-stochastic-gradient-langevin-dynamics.pdf:application/pdf}
}

@article{gu-trainable-2017,
	title = {Trainable {Greedy} {Decoding} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1702.02429},
	abstract = {Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.},
	urldate = {2017-04-19},
	journal = {arXiv:1702.02429 [cs]},
	author = {Gu, Jiatao and Cho, Kyunghyun and Li, Victor O. K.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.02429},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/2JCJRGRP/1702.html:text/html;Gu et al_2017_Trainable Greedy Decoding for Neural Machine Translation.pdf:/home/user/Zotero/storage/4PZ74R95/Gu et al_2017_Trainable Greedy Decoding for Neural Machine Translation.pdf:application/pdf}
}

@article{le-optimizing-2017,
	title = {Optimizing {Differentiable} {Relaxations} of {Coreference} {Evaluation} {Metrics}},
	url = {http://arxiv.org/abs/1704.04451},
	abstract = {Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.},
	urldate = {2017-04-19},
	journal = {arXiv:1704.04451 [cs]},
	author = {Le, Phong and Titov, Ivan},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04451},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/TX5HMZRA/1704.html:text/html;Le_Titov_2017_Optimizing Differentiable Relaxations of Coreference Evaluation Metrics.pdf:/home/user/Zotero/storage/68GHZUWZ/Le_Titov_2017_Optimizing Differentiable Relaxations of Coreference Evaluation Metrics.pdf:application/pdf}
}

@article{le-optimizing-2017-1,
	title = {Optimizing {Differentiable} {Relaxations} of {Coreference} {Evaluation} {Metrics}},
	url = {https://scirate.com/arxiv/1704.04451},
	abstract = {Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.},
	urldate = {2017-04-19},
	journal = {SciRate},
	author = {Le, Phong and Titov, Ivan},
	month = apr,
	year = {2017},
	file = {Le_Titov_2017_Optimizing Differentiable Relaxations of Coreference Evaluation Metrics.pdf:/home/user/Zotero/storage/RG9NGI5Z/Le_Titov_2017_Optimizing Differentiable Relaxations of Coreference Evaluation Metrics.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/V5T8PEQ7/1704.html:text/html}
}

@article{mandt-variational-2016-1,
	title = {A {Variational} {Analysis} of {Stochastic} {Gradient} {Algorithms}},
	url = {http://arxiv.org/abs/1602.02666},
	abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.},
	urldate = {2017-04-18},
	journal = {arXiv:1602.02666 [cs, stat]},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02666},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/6Q4NUWBK/1602.html:text/html;Mandt et al_2016_A Variational Analysis of Stochastic Gradient Algorithms.pdf:/home/user/Zotero/storage/QPA6NKMZ/Mandt et al_2016_A Variational Analysis of Stochastic Gradient Algorithms.pdf:application/pdf}
}

@article{mandt-stochastic-2017,
	title = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/1704.04289},
	abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
	urldate = {2017-04-18},
	journal = {arXiv:1704.04289 [cs, stat]},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04289},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/RGRBS4AD/1704.html:text/html;Mandt et al_2017_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf:/home/user/Zotero/storage/76SQ6VSG/Mandt et al_2017_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf:application/pdf}
}

@article{mandt-variational-2016-2,
	title = {A {Variational} {Analysis} of {Stochastic} {Gradient} {Algorithms}},
	url = {http://arxiv.org/abs/1602.02666},
	abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.},
	urldate = {2017-04-18},
	journal = {arXiv:1602.02666 [cs, stat]},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02666},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/HF5SQ5ZZ/1602.html:text/html;Mandt et al_2016_A Variational Analysis of Stochastic Gradient Algorithms.pdf:/home/user/Zotero/storage/FH397TSC/Mandt et al_2016_A Variational Analysis of Stochastic Gradient Algorithms.pdf:application/pdf}
}

@article{dhingra-end--end-2016,
	title = {End-to-{End} {Reinforcement} {Learning} of {Dialogue} {Agents} for {Information} {Access}},
	url = {http://arxiv.org/abs/1609.00777},
	abstract = {This paper proposes \emph{KB-InfoBot}---a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced "soft" posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KB-InfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.},
	urldate = {2017-04-18},
	journal = {arXiv:1609.00777 [cs]},
	author = {Dhingra, Bhuwan and Li, Lihong and Li, Xiujun and Gao, Jianfeng and Chen, Yun-Nung and Ahmed, Faisal and Deng, Li},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.00777},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/HFDH8WH6/1609.html:text/html;Dhingra et al_2016_End-to-End Reinforcement Learning of Dialogue Agents for Information Access.pdf:/home/user/Zotero/storage/9B9IF9I5/Dhingra et al_2016_End-to-End Reinforcement Learning of Dialogue Agents for Information Access.pdf:application/pdf}
}

@misc{noauthor-predicting-nodate-1,
	title = {Predicting {NP} forms in vernacular written {Cantonese} - pdf},
	url = {https://muse.jhu.edu/article/581617/pdf},
	urldate = {2017-04-07},
	file = {Predicting NP forms in vernacular written Cantonese - pdf.pdf:/home/user/Zotero/storage/Q54FDBHA/Predicting NP forms in vernacular written Cantonese - pdf.pdf:application/pdf}
}

@misc{noauthor-multisyllabication-nodate,
	title = {Multisyllabication and {Phonological} {Simplification} {Throughout} {Chinese} {History} - pdf},
	url = {https://muse.jhu.edu/article/595228/pdf},
	urldate = {2017-04-07},
	file = {Multisyllabication and Phonological Simplification Throughout Chinese History -.pdf:/home/user/Zotero/storage/888C7G23/Multisyllabication and Phonological Simplification Throughout Chinese History -.pdf:application/pdf}
}

@misc{noauthor-highly-nodate,
	title = {A {Highly} {Improbable} {Data} {Point} - pdf},
	url = {https://muse.jhu.edu/article/595227/pdf},
	urldate = {2017-04-07},
	file = {Full Text PDF:/home/user/Zotero/storage/DVPMZIAH/A Highly Improbable Data Point - pdf.pdf:application/pdf}
}

@misc{noauthor-chinese-nodate-1,
	title = {A {Chinese} phonological enigma - pdf},
	url = {https://muse.jhu.edu/article/595223/pdf},
	urldate = {2017-04-07},
	file = {Full Text PDF:/home/user/Zotero/storage/4BCTWXWF/A Chinese phonological enigma - pdf.pdf:application/pdf}
}

@misc{noauthor-origins-nodate,
	title = {The {Origins} and {Grammaticalization} {Paths} of ￢ﾀﾜ{Le}([{INSERT} {INLINE} {GRAPHIC}]{\textless}sub xmlns:m="http://www.w3.org/1998/{Math}/{MathML}" xmlns:mml="http://www.w3.org/1998/{Math}/{MathML}" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater}2{\textless}/sub{\textgreater})￢ﾀﾝ in the {Northern} and {Southern} {Mandarins} - pdf},
	shorttitle = {The {Origins} and {Grammaticalization} {Paths} of ￢ﾀﾜ{Le}([{INSERT} {INLINE} {GRAPHIC}]{\textless}sub xmlns},
	url = {https://muse.jhu.edu/article/595222/pdf},
	urldate = {2017-04-07},
	file = {The Origins and Grammaticalization Paths of ￢ﾀﾜLe([INSERT INLINE GRAPHIC]sub.pdf:/home/user/Zotero/storage/QQ2GEGTR/The Origins and Grammaticalization Paths of ￢ﾀﾜLe([INSERT INLINE GRAPHIC]sub.pdf:application/pdf}
}

@misc{noauthor-types-nodate,
	title = {Types of falling tones - pdf},
	url = {https://muse.jhu.edu/article/595220/pdf},
	urldate = {2017-04-07},
	file = {Full Text PDF:/home/user/Zotero/storage/D5JQJKJ9/Types of falling tones - pdf.pdf:application/pdf}
}

@article{lien-polyfunctionality-2013,
	title = {{THE} {POLYFUNCTIONALITY} {OF} {KIO} 叫 {IN} {NIRI} {KIANN} {KHI} 荔鏡記: {A} {FRAMENET}-{BASED} {APPROACH} / 荔鏡記“叫" 的多功能性:從框架網絡入手},
	volume = {41},
	issn = {0091-3723},
	shorttitle = {ňͥȄԁЀภ㈅ꛜ볘峠⣉驸宆κ額曍೏ࣃ杔峫},
	url = {http://www.jstor.org/stable/23753858},
	abstract = {This paper explores the polyfunctionality of kio3 叫 in Niri7 kiann3 ki3 'Lì Jìng Jì' 慕鏡言己 (1522-1566 AD), a Ming script of play written in Southern Min. Based on FrameNet (Fillmore, Johnson, and Petruck 2003) it establishes a set of frames to capture the related senses of kio3 叫. Communication frame featuring kio3 叫 constitutes the core in a family of related frames with inheritance relationship such as counterfactive frame, name conferral frame and causation frame. It encompasses a list of roles such as speaker, addressee, message, topic, medium, and code in the framework of frame semantics (Johnson and Fillmore 2000). The paper aims at fleshing out interrelated semantic aspects of kio3 叫 induced by patterns of constructions. A cross-linguistic comparison of kio3 叫 in Southern Min and call in English yields the finding that synonyms in these two languages share partial common grounds, but there are also divergences that can only be explained on historical grounds. 本文探討明代閩南語戯文荔鏡記 （西元1522-1566) 中 “叫” 的多功能 性。文中根據Fillmore, Johnson, and Petruck (2003) 框架網絡建立一套框架來捕捉“叫”的相關語意。“叫” 的溝通框架形成相關框架家族 的核心，這些框架包括悖實框架、命名框架、致使框架，彼此間有傳承的 關係。溝通事件涵蓋框架語意理論（Johnson and Fillmore 2000) 中說 者、聽者、信息内容、話題、憑藉、語碼等角色。主文將就“叫”的各類 構式所引發的相關語意面相作深入的闡明。此外，透過閩南語“叫”和英 語call的比較，得知這兩個同義詞雖有部分雷同處，閩南語所反映的特異 性只能從歷史的演變角度來解釋。},
	number = {1},
	urldate = {2017-04-07},
	journal = {Journal of Chinese Linguistics},
	author = {Lien, Chinfa and {連金發}},
	year = {2013},
	pages = {170--196},
	file = {Lien_連金發_2013_THE POLYFUNCTIONALITY OF KIO 叫 IN NIRI KIANN KHI 荔鏡記.pdf:/home/user/Zotero/storage/XGTFQ86I/Lien_連金發_2013_THE POLYFUNCTIONALITY OF KIO 叫 IN NIRI KIANN KHI 荔鏡記.pdf:application/pdf}
}

@article{--2013,
	title = {關於漢語史和吳方言史中體標記“著”語法化問題的探討 / {ASPECTUAL} {MARKER}： {ISSUES} {OF} {ZHE} {OF} {GRAMMTICALIZATION} {IN} {THE} {HISTORY} {OF} {CHINESE} {AND} {WU} {DIALECTS}},
	volume = {41},
	issn = {0091-3723},
	url = {http://www.jstor.org/stable/23754821},
	abstract = {本文討論在漢語史和吳方言史中有關動詞後面體標記 （aspectualmarker) “著” 的語法化問題。 一般地， 對于漢語史中體標記 “著” 的出現，人們認爲是由狀態補語虛化爲非完成體 （imperfective) (包括持續和進行）， 由結果補語虛化爲完成體 （perfective) 的結果。 本文的研究表明， 此 “著” 在近代漢語中語法化而成爲非完成體， 人們通常所指的宋元時期的 “完成體”， 其實只不過是含完成語義的詞囊體 （aktionsart )， 幷不是語法體 （grammaticalaspects)。 而在吳方言史中， “著” 的語法化過程， 幷不是完成體和非完成體同時産生， 而是先從動相補語發展到非完成體 （表持續）， 再從非完成體發展到完成體和狀態補語標記的過程。 以此爲據， 本文進而認爲，體標記 “著” 的産生源于複句前一小句。 This article discusses the aspectual marker Zhe in the historical texts of Chinese and Wu dialects. Zhe was an imperfective marker in the history of Chinese and was never used as a perfective marker. Historically, Zhe called as perfective marker was actually still a phase complement of lexical compound, but not a grammatical perfective marker in the Song and Yuan dynasties. Whereas the aspectual marker Zhe were imperfective and perfective markers and a complement marker of state in the historical texts of Wu dialects. The perfective developed from its imperfective; i.e., a phase complement into imperfective, then from imperfective into perfective. The article further proposes that the context from which the aspectual marker Zhe was induced is an adverbial clause.},
	number = {2},
	urldate = {2017-04-07},
	journal = {Journal of Chinese Linguistics},
	author = {{龍國富} and {孫朝奮} and Long, Guofu and Sun, Chaofen},
	year = {2013},
	pages = {392--417},
	file = {龍國富 et al_2013_關於漢語史和吳方言史中體標記“著”語法化問題的探討 - ASPECTUAL MARKER： ISSUES OF ZHE OF.pdf:/home/user/Zotero/storage/ANECX4HP/龍國富 et al_2013_關於漢語史和吳方言史中體標記“著”語法化問題的探討 - ASPECTUAL MARKER： ISSUES OF ZHE OF.pdf:application/pdf}
}

@article{--2013-1,
	title = {關於漢語史和吳方言史中體標記“著”語法化問題的探討 / {ASPECTUAL} {MARKER}： {ISSUES} {OF} {ZHE} {OF} {GRAMMTICALIZATION} {IN} {THE} {HISTORY} {OF} {CHINESE} {AND} {WU} {DIALECTS}},
	volume = {41},
	issn = {0091-3723},
	url = {http://www.jstor.org/stable/23754821},
	abstract = {本文討論在漢語史和吳方言史中有關動詞後面體標記 （aspectualmarker) “著” 的語法化問題。 一般地， 對于漢語史中體標記 “著” 的出現，人們認爲是由狀態補語虛化爲非完成體 （imperfective) (包括持續和進行）， 由結果補語虛化爲完成體 （perfective) 的結果。 本文的研究表明， 此 “著” 在近代漢語中語法化而成爲非完成體， 人們通常所指的宋元時期的 “完成體”， 其實只不過是含完成語義的詞囊體 （aktionsart )， 幷不是語法體 （grammaticalaspects)。 而在吳方言史中， “著” 的語法化過程， 幷不是完成體和非完成體同時産生， 而是先從動相補語發展到非完成體 （表持續）， 再從非完成體發展到完成體和狀態補語標記的過程。 以此爲據， 本文進而認爲，體標記 “著” 的産生源于複句前一小句。 This article discusses the aspectual marker Zhe in the historical texts of Chinese and Wu dialects. Zhe was an imperfective marker in the history of Chinese and was never used as a perfective marker. Historically, Zhe called as perfective marker was actually still a phase complement of lexical compound, but not a grammatical perfective marker in the Song and Yuan dynasties. Whereas the aspectual marker Zhe were imperfective and perfective markers and a complement marker of state in the historical texts of Wu dialects. The perfective developed from its imperfective; i.e., a phase complement into imperfective, then from imperfective into perfective. The article further proposes that the context from which the aspectual marker Zhe was induced is an adverbial clause.},
	number = {2},
	urldate = {2017-04-07},
	journal = {Journal of Chinese Linguistics},
	author = {{龍國富} and {孫朝奮} and Long, Guofu and Sun, Chaofen},
	year = {2013},
	pages = {392--417},
	file = {龍國富 et al_2013_關於漢語史和吳方言史中體標記“著”語法化問題的探討 - ASPECTUAL MARKER： ISSUES OF ZHE OF.pdf:/home/user/Zotero/storage/5PA9CJJS/龍國富 et al_2013_關於漢語史和吳方言史中體標記“著”語法化問題的探討 - ASPECTUAL MARKER： ISSUES OF ZHE OF.pdf:application/pdf}
}

@article{kim-learning-2017,
	title = {Learning to {Discover} {Cross}-{Domain} {Relations} with {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.05192},
	abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
	urldate = {2017-04-04},
	journal = {arXiv:1703.05192 [cs]},
	author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon and Kim, Jiwon},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/8X5RMBNU/1703.html:text/html;Kim et al_2017_Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.pdf:/home/user/Zotero/storage/X2UAE4JQ/Kim et al_2017_Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.pdf:application/pdf}
}

@article{duan-one-shot-2017,
	title = {One-{Shot} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1703.07326},
	abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/one-shot-imitation .},
	urldate = {2017-04-04},
	journal = {arXiv:1703.07326 [cs]},
	author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07326},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/IXUQBSPZ/1703.html:text/html;Duan et al_2017_One-Shot Imitation Learning.pdf:/home/user/Zotero/storage/XK6GNSWT/Duan et al_2017_One-Shot Imitation Learning.pdf:application/pdf}
}

@article{brochu-tutorial-2010,
	title = {A {Tutorial} on {Bayesian} {Optimization} of {Expensive} {Cost} {Functions}, with {Application} to {Active} {User} {Modeling} and {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1012.2599},
	abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
	urldate = {2017-04-04},
	journal = {arXiv:1012.2599 [cs]},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	month = dec,
	year = {2010},
	note = {arXiv: 1012.2599},
	keywords = {Computer Science - Learning, G.3, I.2.6, G.1.6},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/9CW7D7QK/1012.html:text/html;Brochu et al_2010_A Tutorial on Bayesian Optimization of Expensive Cost Functions, with.pdf:/home/user/Zotero/storage/VH2GAEPC/Brochu et al_2010_A Tutorial on Bayesian Optimization of Expensive Cost Functions, with.pdf:application/pdf}
}

@article{choi-transfer-2017,
	title = {Transfer learning for music classification and regression tasks},
	url = {http://arxiv.org/abs/1703.09179},
	abstract = {In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.},
	urldate = {2017-04-04},
	journal = {arXiv:1703.09179 [cs]},
	author = {Choi, Keunwoo and Fazekas, György and Sandler, Mark and Cho, Kyunghyun},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.09179},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Sound},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/T662UMRZ/1703.html:text/html;Choi et al_2017_Transfer learning for music classification and regression tasks.pdf:/home/user/Zotero/storage/5R5UKDUQ/Choi et al_2017_Transfer learning for music classification and regression tasks.pdf:application/pdf}
}

@misc{noauthor-dependency-nodate,
	title = {Dependency distance: {A} new perspective on syntactic patterns in natural languages},
	url = {http://www.sciencedirect.com/science/article/pii/S1571064517300532},
	urldate = {2017-04-04},
	file = {Dependency distance\: A new perspective on syntactic patterns in natural languages:/home/user/Zotero/storage/9RHBAMVC/S1571064517300532.html:text/html}
}

@article{mair-script-1992,
	title = {Script and {Word} in {Medieval} {Vernacular} {Sinitic}},
	volume = {112},
	issn = {0003-0279},
	url = {http://www.jstor.org/stable/603705},
	doi = {10.2307/603705},
	abstract = {A review of Robert Henricks' annotated translation of the complete poems of Han-shan reveals that the author has almost completely ignored what is undoubtedly the key to understanding these poems accurately, namely the high proportion of colloquial and vernacular elements in the language used to write them. This obliviousness to the distinctive lexical and grammatical features of Medieval Vernacular Sinitic stems partly from sheer unfamiliarity with the language, but it is also due to overemphasis on script at the expense of word. Fixation on the sinographs not only presents virtually insurmountable obstacles to the correct interpretation of MVS texts, it often seriously interferes with sinological readings of works written even in Classical Chinese, which is itself far from being exclusively monosyllabic. Unsegmented, equidistantly spaced strings of graphic forms merely give the appearance that languages written in the Chinese script consist entirely of monosyllabic words. Nothing could be further from the linguistic truth.},
	number = {2},
	urldate = {2017-03-23},
	journal = {Journal of the American Oriental Society},
	author = {Mair, Victor H.},
	editor = {Henricks, Robert G. and {Han-shan}},
	year = {1992},
	pages = {269--278},
	file = {Mair_1992_Script and Word in Medieval Vernacular Sinitic.pdf:/home/user/Zotero/storage/C68UMFX5/Mair_1992_Script and Word in Medieval Vernacular Sinitic.pdf:application/pdf}
}

@book{hureau-guide-nodate,
	title = {A {Guide} to the {Earliest} {Chinese} {Buddhist} {Translations}. {Texts} from the {Eastern} {Han} and {Three} {Kingdoms} {Periods}},
	volume = {95},
	url = {https://www.academia.edu/8629096/A\_Guide\_to\_the\_Earliest\_Chinese\_Buddhist\_Translations\_Texts\_from\_the\_Eastern\_Han\_and\_Three\_Kingdoms\_Periods},
	abstract = {This OCR'd version was produced by Michael Radich, whose help I would like to gratefully acknowledge here. Please also download the Addenda and Corrigenda page (an updated version is currently being prepared).},
	urldate = {2017-03-23},
	author = {Hureau, Sylvie},
	file = {Snapshot:/home/user/Zotero/storage/6IBJH5FC/A_Guide_to_the_Earliest_Chinese_Buddhist_Translations_Texts_from_the_Eastern_Han_and_Three_King.html:text/html}
}

@book{gelman-data-2007,
	address = {Cambridge ; New York},
	series = {Analytical methods for social research},
	title = {Data analysis using regression and multilevel/hierarchical models},
	isbn = {978-0-521-86706-1 978-0-521-68689-1},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	year = {2007},
	note = {OCLC: ocm67375137},
	keywords = {Regression analysis, Multilevel models (Statistics)},
	file = {v30b03.pdf:/home/user/Zotero/storage/78RVUMCM/v30b03.pdf:application/pdf}
}

@book{noauthor-nonlinear-nodate,
	title = {nonlinear regression},
	file = {refs.pdf:/home/user/Zotero/storage/XPJG98SG/refs.pdf:application/pdf}
}

@inproceedings{soudry-expectation-2014,
	title = {Expectation backpropagation: {Parameter}-free training of multilayer neural networks with continuous or discrete weights},
	shorttitle = {Expectation backpropagation},
	url = {http://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights},
	urldate = {2017-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Soudry, Daniel and Hubara, Itay and Meir, Ron},
	year = {2014},
	pages = {963--971},
	file = {Expectation Backpropagation\: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights - 5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf:/home/user/Zotero/storage/XSNGI7GE/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-cont.pdf:application/pdf}
}

@article{oord-pixel-2016,
	title = {Pixel recurrent neural networks},
	url = {https://arxiv.org/abs/1601.06759},
	urldate = {2017-03-22},
	journal = {arXiv preprint arXiv:1601.06759},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	year = {2016},
	file = {1601.06759.pdf:/home/user/Zotero/storage/MV88BWJX/1601.06759.pdf:application/pdf}
}

@article{ba-layer-2016,
	title = {Layer normalization},
	url = {https://arxiv.org/abs/1607.06450},
	urldate = {2017-03-22},
	journal = {arXiv preprint arXiv:1607.06450},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
	file = {1607.06450.pdf:/home/user/Zotero/storage/5VK9TB9M/1607.06450.pdf:application/pdf}
}

@article{mcclelland-trace-1986,
	title = {The {TRACE} model of speech perception},
	volume = {18},
	issn = {0010-0285},
	language = {eng},
	number = {1},
	journal = {Cognitive Psychology},
	author = {McClelland, J. L. and Elman, J. L.},
	month = jan,
	year = {1986},
	pmid = {3753912},
	keywords = {Humans, Phonetics, speech perception, Semantics, Models, Psychological, Cues, Software},
	pages = {1--86},
	file = {PII\: 0010-0285(86)90015-0 - McClellandElman86.pdf:/home/user/Zotero/storage/C394B6HK/McClellandElman86.pdf:application/pdf}
}

@article{goldinger-echoes-1998,
	title = {Echoes of echoes? {An} episodic theory of lexical access},
	volume = {105},
	issn = {0033-295X},
	shorttitle = {Echoes of echoes?},
	abstract = {In this article the author proposes an episodic theory of spoken word representation, perception, and production. By most theories, idiosyncratic aspects of speech (voice details, ambient noise, etc.) are considered noise and are filtered in perception. However, episodic theories suggest that perceptual details are stored in memory and are integral to later perception. In this research the author tested an episodic model (MINERVA 2; D. L. Hintzman, 1986) against speech production data from a word-shadowing task. The model predicted the shadowing-response-time patterns, and it correctly predicted a tendency for shadowers to spontaneously imitate the acoustic patterns of words and nonwords. It also correctly predicted imitation strength as a function of "abstract" stimulus properties, such as word frequency. Taken together, the data and theory suggest that detailed episodes constitute the basic substrate of the mental lexicon.},
	language = {eng},
	number = {2},
	journal = {Psychological Review},
	author = {Goldinger, S. D.},
	month = apr,
	year = {1998},
	pmid = {9577239},
	keywords = {Humans, memory, computer simulation, speech perception, Vocabulary, speech, Verbal Learning},
	pages = {251--279},
	file = {rev105020251.tif - Goldinger_PsyRev_98.pdf:/home/user/Zotero/storage/CSVNWWWJ/Goldinger_PsyRev_98.pdf:application/pdf}
}

@article{gow-cross-linguistic-2004,
	title = {A cross-linguistic examination of assimilation context effects},
	volume = {51},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X04000580},
	doi = {10.1016/j.jml.2004.05.004},
	language = {en},
	number = {2},
	urldate = {2017-03-21},
	journal = {Journal of Memory and Language},
	author = {Gow, David W. and Im, Aaron M.},
	month = aug,
	year = {2004},
	pages = {279--296},
	file = {doi\:10.1016/j.jml.2004.05.004 - gowim2004-jml.pdf:/home/user/Zotero/storage/EZQJABWW/gowim2004-jml.pdf:application/pdf}
}

@inproceedings{moon-learning-2013,
	title = {The learning and generalization of contrasts consistent or inconsistent with native biases.},
	url = {http://web.stanford.edu/~sumner/Publications/2013\_Moon\_ISCA.pdf},
	urldate = {2017-03-21},
	booktitle = {{INTERSPEECH}},
	author = {Moon, Kyuwon and Sumner, Meghan},
	year = {2013},
	pages = {2103--2107},
	file = {Microsoft Word - moon_sumner_interspeech_revised.docx - 2013_Moon_ISCA.pdf:/home/user/Zotero/storage/8FUK86MN/2013_Moon_ISCA.pdf:application/pdf}
}

@article{deelman-missing-2001,
	title = {Missing information in spoken word recognition: {Nonrelased} stop consonants.},
	volume = {27},
	issn = {0096-1523},
	shorttitle = {Missing information in spoken word recognition},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.27.3.656},
	doi = {10.1037//0096-1523.27.3.656},
	language = {en},
	number = {3},
	urldate = {2017-03-21},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Deelman, Thomas and Connine, Cynthia M.},
	year = {2001},
	pages = {656--663},
	file = {DeelmanConnine2001.pdf:/home/user/Zotero/storage/N82CU64X/DeelmanConnine2001.pdf:application/pdf}
}

@article{connine-phonological-2006,
	title = {Phonological variation in spoken word recognition: {Episodes} and abstractions},
	volume = {23},
	issn = {0167-6318, 1613-3676},
	shorttitle = {Phonological variation in spoken word recognition},
	url = {http://www.degruyter.com/view/j/tlir.2006.23.issue-3/tlr.2006.009/tlr.2006.009.xml},
	doi = {10.1515/TLR.2006.009},
	number = {3},
	urldate = {2017-03-21},
	journal = {The Linguistic Review},
	author = {Connine, Cynthia M and Pinnow, Eleni},
	month = jan,
	year = {2006},
	file = {tlr23-3.dvi - conninePinnow.pdf:/home/user/Zotero/storage/JQSR47XJ/conninePinnow.pdf:application/pdf}
}

@inproceedings{moon-learning-2013-1,
	title = {The learning and generalization of contrasts consistent or inconsistent with native biases.},
	url = {http://web.stanford.edu/~sumner/Publications/2013\_Moon\_ISCA.pdf},
	urldate = {2017-03-21},
	booktitle = {{INTERSPEECH}},
	author = {Moon, Kyuwon and Sumner, Meghan},
	year = {2013},
	pages = {2103--2107},
	file = {Microsoft Word - moon_sumner_interspeech_revised.docx - 2013_Moon_ISCA.pdf:/home/user/Zotero/storage/C5B74Z3N/2013_Moon_ISCA.pdf:application/pdf}
}

@article{dahan-talker-2008,
	title = {Talker adaptation in speech perception: {Adjusting} the signal or the representations?},
	volume = {108},
	issn = {0010-0277},
	shorttitle = {Talker adaptation in speech perception},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2614823/},
	doi = {10.1016/j.cognition.2008.06.003},
	abstract = {Past research has established that listeners can accommodate a wide range of talkers in understanding language. How this adjustment operates, however, is a matter of debate. Here, listeners were exposed to spoken words from a speaker of an American English dialect in which the vowel /æ/ is raised before /g/, but not before /k/. Results from two experiments showed that listeners’ identification of /k/-final words like back (which are unaffected by the dialect) was facilitated by prior exposure to their dialect-affected /g/-final counterparts, e.g., bag. This facilitation occurred because the competition between interpretations, e.g., bag or back, while hearing the initial portion of the input [bæ], was mitigated by the reduced probability for the input to correspond to bag as produced by this talker. Thus, adaptation to an accent is not just a matter of adjusting the speech signal as it is being heard; adaptation involves dynamic adjustment of the representations stored in the lexicon, according to the characteristics of the speaker or the context.},
	number = {3},
	urldate = {2017-03-21},
	journal = {Cognition},
	author = {Dahan, Delphine and Drucker, Sarah J. and Scarborough, Rebecca A.},
	month = sep,
	year = {2008},
	pmid = {18653175},
	pmcid = {PMC2614823},
	pages = {710--718},
	file = {Dahan et al_2008_Talker adaptation in speech perception.pdf:/home/user/Zotero/storage/DT6ZW68M/Dahan et al_2008_Talker adaptation in speech perception.pdf:application/pdf}
}

@article{locasto-rule-governed-2002,
	title = {Rule-governed missing information in spoken word recognition: schwa vowel deletion},
	volume = {64},
	issn = {0031-5117},
	shorttitle = {Rule-governed missing information in spoken word recognition},
	abstract = {Vowel deletion is a phonological process in which an unstressed /inverted e/ (schwa) vowel is deleted during pronunciation. In Experiment 1, vowel-deleted and vowel-reduced versions of two- and three-syllable words rated for pronunciation acceptability showed reduced acceptability for deleted vowel versions with a greater decrement for two-syllable words. Experiments 2 and 3 used vowel-intact and vowel-deleted productions preceded by themselves (repetition prime), their alternative production (variant prime), or a control (unrelated) prime. Lexical decisions to three-syllable vowel-intact and vowel-deleted targets, as well as to two-syllable vowel-intact targets, showed greater priming in the repetition conditions than in the variant conditions. Two-syllable vowel-deleted targets, however, showed comparable repetition and variant priming. The results are discussed in terms of lexical activation and representation of phonological variants. A model is offered in which activation based on similarity triggers utilization of phonological inferences only when required for successful recognition.},
	language = {eng},
	number = {2},
	journal = {Perception \& Psychophysics},
	author = {LoCasto, Paul C. and Connine, Cynthia M.},
	month = feb,
	year = {2002},
	pmid = {12013376},
	keywords = {Humans, Phonetics, speech perception, Speech Acoustics, Attention, Psychoacoustics},
	pages = {208--219}
}

@article{hu-controllable-2017,
	title = {Controllable {Text} {Generation}},
	url = {https://arxiv.org/abs/1703.00955},
	urldate = {2017-03-20},
	journal = {arXiv preprint arXiv:1703.00955},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2017},
	file = {Controllable Text Generation - 1703.00955.pdf:/home/user/Zotero/storage/NI4K2PK3/1703.00955.pdf:application/pdf}
}

@article{semeniuta-hybrid-2017,
	title = {A {Hybrid} {Convolutional} {Variational} {Autoencoder} for {Text} {Generation}},
	url = {https://arxiv.org/abs/1702.02390},
	urldate = {2017-03-20},
	journal = {arXiv preprint arXiv:1702.02390},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	year = {2017},
	file = {1702.02390.pdf:/home/user/Zotero/storage/DIP7MS6P/1702.02390.pdf:application/pdf}
}

@inproceedings{kingma-auto-encoding-2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2015-11-20},
	booktitle = {{ICLR}},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/ZK6QTKM5/1312.html:text/html;Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf:application/pdf}
}

@inproceedings{miao-neural-2016,
	title = {Neural {Variational} {Inference} for {Text} {Processing}},
	url = {http://arxiv.org/abs/1511.06038},
	urldate = {2016-08-04},
	booktitle = {Proceedings of {ICML}},
	author = {Miao, Yishu and Yu, Lei and Blunsom, Phil},
	year = {2016},
	file = {miao16.pdf:/home/user/Zotero/storage/5SZT9X5Q/miao16.pdf:application/pdf}
}

@book{association-for-computational-linguistics-proceedings-2016,
	title = {Proceedings of of the 1st {Workshop} on {Evaluating} {Vector} {Space} {Representations} for {NLP}},
	url = {http://www.aclweb.org/anthology/W/W16/W16-25.pdf},
	urldate = {2017-03-20},
	editor = {Association for Computational Linguistics},
	year = {2016},
	file = {Proceedings of of the 1st Workshop on Evaluating Vector Space Representations for NLP - W16-25.pdf:/home/user/Zotero/storage/EFDW3DWQ/W16-25.pdf:application/pdf}
}

@inproceedings{bowman-generating-2016,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	urldate = {2015-11-20},
	booktitle = {Proceedings of {CoNLL}},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	year = {2016},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/27IHSBSK/1511.html:text/html;Bowman et al_2015_Generating Sentences from a Continuous Space.pdf:/afs/inf.ed.ac.uk/user/s15/s1582047/zotero/Bowman et al_2015_Generating Sentences from a Continuous Space.pdf:application/pdf}
}

@inproceedings{de-marneffe-interaction-2011,
	title = {The interaction of lexical frequency and phonetic variation in the perception of accented speech},
	url = {http://pubman.mpdl.mpg.de/pubman/faces/viewItemOverviewPage.jsp?itemId=escidoc:1795861},
	urldate = {2017-03-20},
	booktitle = {The 33rd {Annual} {Meeting} of the {Cognitive} {Science} {Society} [{CogSci} 2011]},
	publisher = {Cognitive Science Society},
	author = {de Marneffe, Marie-Catherine and Tomlinson Jr, John and Tice, Marisa and Sumner, Meghan},
	year = {2011},
	pages = {3575--3580},
	file = {cogsci11.pdf:/home/user/Zotero/storage/JRN5384E/cogsci11.pdf:application/pdf}
}

@inproceedings{moon-learning-2013-2,
	title = {The learning and generalization of contrasts consistent or inconsistent with native biases.},
	url = {http://web.stanford.edu/~sumner/Publications/2013\_Moon\_ISCA.pdf},
	urldate = {2017-03-20},
	booktitle = {{INTERSPEECH}},
	author = {Moon, Kyuwon and Sumner, Meghan},
	year = {2013},
	pages = {2103--2107},
	file = {0.pdf:/home/user/Zotero/storage/SJ9BGP25/0.pdf:application/pdf}
}

@article{gow-cross-linguistic-2004-1,
	title = {A cross-linguistic examination of assimilation context effects},
	volume = {51},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X04000580},
	doi = {10.1016/j.jml.2004.05.004},
	language = {en},
	number = {2},
	urldate = {2017-03-20},
	journal = {Journal of Memory and Language},
	author = {Gow, David W. and Im, Aaron M.},
	month = aug,
	year = {2004},
	pages = {279--296},
	file = {0.pdf:/home/user/Zotero/storage/24MNSK5X/0.pdf:application/pdf}
}

@inproceedings{sumner-learning-2009,
	title = {Learning and generalization of novel contrastive cues},
	booktitle = {Tenth {Annual} {Conference} of the {International} {Speech} {Communication} {Association}},
	author = {Sumner, Meghan},
	year = {2009},
	file = {0.pdf:/home/user/Zotero/storage/9UIFVRM3/0.pdf:application/pdf}
}

@book{linzen-rapid-2015,
	title = {Rapid generalization in phonotactic learning},
	url = {http://tallinzen.net/media/papers/linzen\_gallagher\_2015.pdf},
	urldate = {2017-03-20},
	author = {Linzen, Tal and Gallagher, Gillian},
	year = {2015},
	file = {My Title - linzen_gallagher_2015.pdf:/home/user/Zotero/storage/EHNRJNT2/linzen_gallagher_2015.pdf:application/pdf}
}

@article{linzen-reliability-2015,
	title = {The reliability of acceptability judgments across languages},
	url = {http://tallinzen.net/media/papers/linzen\_oseki\_acceptability.pdf},
	urldate = {2017-03-20},
	journal = {New York: New York University, ms},
	author = {Linzen, Tal and Oseki, Yohei},
	year = {2015},
	file = {My Title - linzen_oseki_acceptability.pdf:/home/user/Zotero/storage/P8FMDNSJ/linzen_oseki_acceptability.pdf:application/pdf}
}

@article{linzen-uncertainty-2016,
	title = {Uncertainty and {Expectation} in {Sentence} {Processing}: {Evidence} {From} {Subcategorization} {Distributions}},
	volume = {40},
	issn = {03640213},
	shorttitle = {Uncertainty and {Expectation} in {Sentence} {Processing}},
	url = {http://doi.wiley.com/10.1111/cogs.12274},
	doi = {10.1111/cogs.12274},
	language = {en},
	number = {6},
	urldate = {2017-03-20},
	journal = {Cognitive Science},
	author = {Linzen, Tal and Jaeger, T. Florian},
	month = aug,
	year = {2016},
	pages = {1382--1411},
	file = {Uncertainty and Expectation in Sentence Processing\: Evidence From Subcategorization Distributions - download:/home/user/Zotero/storage/XFMD6FZH/download.pdf:application/pdf}
}

@article{levy-surprisal-2013,
	title = {Surprisal, the {PDC}, and the primary locus of processing difficulty in relative clauses},
	volume = {4},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00229/abstract},
	doi = {10.3389/fpsyg.2013.00229},
	urldate = {2017-03-20},
	journal = {Frontiers in Psychology},
	author = {Levy, Roger and Gibson, Edward},
	year = {2013},
	file = {Surprisal, the PDC, and the primary locus of processing difficulty in relative clauses - levy-gibson-2013-frontiers.pdf:/home/user/Zotero/storage/JSZHEXIA/levy-gibson-2013-frontiers.pdf:application/pdf}
}

@inproceedings{linzen-model-2015,
	title = {A model of rapid phonotactic generalization.},
	url = {http://www.aclweb.org/website/old\_anthology/D/D15/D15-1134.pdf},
	urldate = {2017-03-20},
	booktitle = {{EMNLP}},
	author = {Linzen, Tal and O'Donnell, Timothy},
	year = {2015},
	pages = {1126--1131},
	file = {A model of rapid phonotactic generalization - D15-1134.pdf:/home/user/Zotero/storage/HJKI2U7R/D15-1134.pdf:application/pdf}
}

@inproceedings{smith-offline-2017,
	title = {Offline bilingual word vectors, orthogonal transformations and the inverted softmax},
	url = {https://arxiv.org/abs/1702.03859},
	urldate = {2017-03-20},
	booktitle = {{ICLR} 2017},
	author = {Smith, Samuel L. and Turban, David HP and Hamblin, Steven and Hammerla, Nils Y.},
	year = {2017},
	file = {pdf:/home/user/Zotero/storage/IMMS3CN9/pdf.pdf:application/pdf}
}

@inproceedings{shi-does-2016,
	title = {Does {String}-{Based} {Neural} {MT} {Learn} {Source} {Syntax}?},
	url = {http://xingshi.me/data/pdf/EMNLP2016long.pdf},
	urldate = {2017-03-20},
	booktitle = {Proc. of {EMNLP}},
	author = {Shi, Xing and Padhi, Inkit and Knight, Kevin},
	year = {2016},
	file = {emnlp16-nmt-grammar.pdf:/home/user/Zotero/storage/PDN6N368/emnlp16-nmt-grammar.pdf:application/pdf}
}

@article{tanenhaus-integration-1995,
	title = {Integration of visual and linguistic information in spoken language comprehension},
	volume = {268},
	issn = {0036-8075},
	abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
	language = {eng},
	number = {5217},
	journal = {Science (New York, N.Y.)},
	author = {Tanenhaus, M. K. and Spivey-Knowlton, M. J. and Eberhard, K. M. and Sedivy, J. C.},
	month = jun,
	year = {1995},
	pmid = {7777863},
	keywords = {Humans, language, Eye Movements, speech perception, Cognition},
	pages = {1632--1634}
}

@article{mcqueen-lexical-1999,
	title = {Lexical influence in phonetic decision making: {Evidence} from subcategorical mismatches},
	volume = {25},
	doi = {10.1037/0096-1523.25.5.1363},
	language = {English},
	number = {5},
	urldate = {2017-03-17},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {McQueen, James M. and Norris, Dennis and Cutler, Anne},
	year = {1999},
	pages = {1363},
	file = {McQueen et al_1999_Lexical influence in phonetic decision making.pdf:/home/user/Zotero/storage/AMIRJSFQ/McQueen et al_1999_Lexical influence in phonetic decision making.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/U2GMIDR5/1363.html:text/html}
}

@misc{noauthor-frontiers-nodate-3,
	title = {Frontiers {\textbar} {How} language production shapes language form and comprehension {\textbar} {Language} {Sciences}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00226/full},
	urldate = {2017-03-19},
	file = {Frontiers | How language production shapes language form and comprehension | Language Sciences:/home/user/Zotero/storage/AIPZWFTK/full.html:text/html}
}

@misc{noauthor-statistical-nodate-1,
	title = {Statistical learning of language: {Theory}, validity, and predictions of a statistical learning account of language acquisition ({PDF} {Download} {Available})},
	shorttitle = {Statistical learning of language},
	url = {https://www.researchgate.net/publication/280008074\_Statistical\_learning\_of\_language\_Theory\_validity\_and\_predictions\_of\_a\_statistical\_learning\_account\_of\_language\_acquisition},
	abstract = {Official Full-Text Publication: Statistical learning of language: Theory, validity, and predictions of a statistical learning account of language acquisition on ResearchGate, the professional network for scientists.},
	urldate = {2017-03-19},
	journal = {ResearchGate},
	file = {Snapshot:/home/user/Zotero/storage/JNXZ9W53/280008074_Statistical_learning_of_language_Theory_validity_and_predictions_of_a_statistical_lea.html:text/html}
}

@inproceedings{linzen-timecourse-2014-1,
	title = {The timecourse of generalization in phonotactic learning},
	volume = {1},
	url = {http://journals.linguisticsociety.org/proceedings/index.php/amphonology/article/view/18},
	urldate = {2017-03-19},
	booktitle = {Proceedings of the {Annual} {Meetings} on {Phonology}},
	author = {Linzen, Tal and Gallagher, Gillian},
	year = {2014},
	file = {13.pdf:/home/user/Zotero/storage/Q3DPMWPA/13.pdf:application/pdf}
}

@article{moreton-structure-2012,
	title = {Structure and {Substance} in {Artificial}-phonology {Learning}, {Part} {I}: {Structure}},
	volume = {6},
	issn = {1749-818X},
	shorttitle = {Structure and {Substance} in {Artificial}-phonology {Learning}, {Part} {I}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/lnc3.363/abstract},
	doi = {10.1002/lnc3.363},
	abstract = {Artificial analogues of natural-language phonological patterns can often be learned in the lab from small amounts of training or exposure. The difficulty of a featurally-defined pattern has been hypothesized to be affected by two main factors, its formal structure (the abstract logical configuration of the defining features) and its phonetic substance (the concrete phonetic interpretation of the pattern). This paper, the first of a two-part series, reviews the experimental literature on structural effects. The principal finding is a robust complexity effect: Patterns which depend on more features are reliably harder to learn.},
	language = {en},
	number = {11},
	urldate = {2017-03-19},
	journal = {Language and Linguistics Compass},
	author = {Moreton, Elliott and Pater, Joe},
	month = nov,
	year = {2012},
	pages = {686--701},
	file = {Moreton_Pater_2012_Structure and Substance in Artificial-phonology Learning, Part I.pdf:/home/user/Zotero/storage/GICKD9Z9/Moreton_Pater_2012_Structure and Substance in Artificial-phonology Learning, Part I.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/28K46MEF/abstract.html:text/html}
}

@article{moreton-structure-2012-1,
	title = {Structure and {Substance} in {Artificial}-{Phonology} {Learning}, {Part} {II}: {Substance}},
	volume = {6},
	issn = {1749-818X},
	shorttitle = {Structure and {Substance} in {Artificial}-{Phonology} {Learning}, {Part} {II}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/lnc3.366/abstract},
	doi = {10.1002/lnc3.366},
	abstract = {Artificial analogues of natural-language phonological patterns can often be learned in the lab from small amounts of training or exposure. The difficulty of a featurally-defined pattern has been hypothesized to be affected by two main factors, its formal structure (the abstract logical relationships between the defining features) and its phonetic substance (the concrete phonetic interpretation of the pattern). This paper, the second of a two-part series, reviews the experimental literature on phonetic substance, which is hypothesized to facilitate the acquisition of phonological patterns that resemble naturally-occurring phonetic patterns. The effects of phonetic substance on pattern learning turn out to be elusive and unreliable in comparison with the robust effects of formal complexity (reviewed in Part I). If natural-language acquisition is guided by the same inductive biases as are found in the lab, these results support a theory in which inductive bias shapes only the form, and channel bias shapes the content, of the sound patterns of the worlds languages.},
	language = {en},
	number = {11},
	urldate = {2017-03-19},
	journal = {Language and Linguistics Compass},
	author = {Moreton, Elliott and Pater, Joe},
	month = nov,
	year = {2012},
	pages = {702--718},
	file = {Moreton_Pater_2012_Structure and Substance in Artificial-Phonology Learning, Part II.pdf:/home/user/Zotero/storage/67PCC6ZT/Moreton_Pater_2012_Structure and Substance in Artificial-Phonology Learning, Part II.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Z26MRRBH/abstract.html:text/html}
}

@misc{noauthor-language-nodate-2,
	title = {Language {Study}},
	url = {file:///mhahn/repo2/mhahn\_files/stnfclasses/qp/experiment/9-artificial/order-preference.html},
	urldate = {2017-09-27},
	file = {Language Study:/home/user/Zotero/storage/BFA6QTJG/order-preference.html:text/html}
}

@article{bojanowski-unsupervised-2017,
	title = {Unsupervised {Learning} by {Predicting} {Noise}},
	url = {https://arxiv.org/abs/1704.05310},
	urldate = {2017-08-18},
	journal = {arXiv preprint arXiv:1704.05310},
	author = {Bojanowski, Piotr and Joulin, Armand},
	year = {2017},
	file = {Unsupervised Learning by Predicting Noise - bojanowski17a.pdf:/home/user/Zotero/storage/IEDIEUDR/bojanowski17a.pdf:application/pdf}
}

@inproceedings{baram-end--end-2017,
	title = {End-to-{End} {Differentiable} {Adversarial} {Imitation} {Learning}},
	url = {http://proceedings.mlr.press/v70/baram17a.html},
	urldate = {2017-08-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Baram, Nir and Anschel, Oron and Caspi, Itai and Mannor, Shie},
	year = {2017},
	pages = {390--399},
	file = {baram17a.pdf:/home/user/Zotero/storage/PFD23UT3/baram17a.pdf:application/pdf}
}

@book{stoops-parafoveal-2012,
	title = {Parafoveal preview during reading in {Russian}: {Native} speakers and second language learners},
	shorttitle = {Parafoveal preview during reading in {Russian}},
	url = {http://search.proquest.com/openview/ed78004ab7eb3959c56c3219cec40d12/1?pq-origsite=gscholar&cbl=18750&diss=y},
	urldate = {2017-08-18},
	publisher = {University of Illinois at Urbana-Champaign},
	author = {Stoops, Anastasia Anatol},
	year = {2012},
	file = {Anastasia_Stoops.pdf:/home/user/Zotero/storage/28QMVJ84/Anastasia_Stoops.pdf:application/pdf}
}

@article{prichard-evaluating-2016,
	title = {Evaluating {L}2 {Readers}’ {Previewing} {Strategies} {Using} {Eye} {Tracking}},
	volume = {16},
	url = {http://www.readingmatrix.com/files/15-992935s1.pdf},
	number = {2},
	urldate = {2017-08-18},
	journal = {The Reading Matrix: An International Online Journal},
	author = {Prichard, Caleb and Atkins, Andrew},
	year = {2016},
	file = {Microsoft Word - prichard_atkins - 15-992935s1.pdf:/home/user/Zotero/storage/X66GCP3K/15-992935s1.pdf:application/pdf}
}

@inproceedings{kusner-grammar-2017,
	title = {Grammar {Variational} {Autoencoder}},
	url = {http://proceedings.mlr.press/v70/kusner17a.html},
	abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discre...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
	month = jul,
	year = {2017},
	pages = {1945--1954},
	file = {Kusner et al_2017_Grammar Variational Autoencoder.pdf:/home/user/Zotero/storage/BCIU4REF/Kusner et al_2017_Grammar Variational Autoencoder.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VR8GN6NV/kusner17a.html:text/html}
}

@inproceedings{bello-neural-2017,
	title = {Neural {Optimizer} {Search} with {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v70/bello17a.html},
	abstract = {We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string ...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
	month = jul,
	year = {2017},
	pages = {459--468},
	file = {Bello et al_2017_Neural Optimizer Search with Reinforcement Learning.pdf:/home/user/Zotero/storage/PA5DW46Q/Bello et al_2017_Neural Optimizer Search with Reinforcement Learning.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QKIGQKHV/bello17a.html:text/html}
}

@inproceedings{mukkamala-variants-2017,
	title = {Variants of {RMSProp} and {Adagrad} with {Logarithmic} {Regret} {Bounds}},
	url = {http://proceedings.mlr.press/v70/mukkamala17a.html},
	abstract = {Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, orig...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Mukkamala, Mahesh Chandra and Hein, Matthias},
	month = jul,
	year = {2017},
	pages = {2545--2553},
	file = {Mukkamala_Hein_2017_Variants of RMSProp and Adagrad with Logarithmic Regret Bounds.pdf:/home/user/Zotero/storage/42G3MDHZ/Mukkamala_Hein_2017_Variants of RMSProp and Adagrad with Logarithmic Regret Bounds.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZGV8HDGD/mukkamala17a.html:text/html}
}

@inproceedings{hanna-data-efficient-2017,
	title = {Data-{Efficient} {Policy} {Evaluation} {Through} {Behavior} {Policy} {Search}},
	url = {http://proceedings.mlr.press/v70/hanna17a.html},
	abstract = {We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Hanna, Josiah P. and Thomas, Philip S. and Stone, Peter and Niekum, Scott},
	month = jul,
	year = {2017},
	pages = {1394--1403},
	file = {Hanna et al_2017_Data-Efficient Policy Evaluation Through Behavior Policy Search.pdf:/home/user/Zotero/storage/5559XNKT/Hanna et al_2017_Data-Efficient Policy Evaluation Through Behavior Policy Search.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XGKIPEAV/hanna17a.html:text/html}
}

@inproceedings{winner-exact-2017,
	title = {Exact {Inference} for {Integer} {Latent}-{Variable} {Models}},
	url = {http://proceedings.mlr.press/v70/winner17a.html},
	abstract = {Graphical models with latent count variables arise in a number of areas. However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables. Win...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Winner, Kevin and Sujono, Debora and Sheldon, Dan},
	month = jul,
	year = {2017},
	pages = {3761--3770},
	file = {Snapshot:/home/user/Zotero/storage/RKGB4CSM/winner17a.html:text/html;Winner et al_2017_Exact Inference for Integer Latent-Variable Models.pdf:/home/user/Zotero/storage/EREN887X/Winner et al_2017_Exact Inference for Integer Latent-Variable Models.pdf:application/pdf}
}

@inproceedings{ritter-cognitive-2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v70/ritter17a.html},
	abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our u...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jul,
	year = {2017},
	pages = {2940--2949},
	file = {Ritter et al_2017_Cognitive Psychology for Deep Neural Networks.pdf:/home/user/Zotero/storage/VXBM6QKD/Ritter et al_2017_Cognitive Psychology for Deep Neural Networks.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/85TKFAU6/ritter17a.html:text/html}
}

@inproceedings{balog-lost-2017,
	title = {Lost {Relatives} of the {Gumbel} {Trick}},
	url = {http://proceedings.mlr.press/v70/balog17a.html},
	abstract = {The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Balog, Matej and Tripuraneni, Nilesh and Ghahramani, Zoubin and Weller, Adrian},
	month = jul,
	year = {2017},
	pages = {371--379},
	file = {Balog et al_2017_Lost Relatives of the Gumbel Trick.pdf:/home/user/Zotero/storage/6APVIM3T/Balog et al_2017_Lost Relatives of the Gumbel Trick.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/G98B6TT9/balog17a.html:text/html}
}

@inproceedings{cisse-parseval-2017,
	title = {Parseval {Networks}: {Improving} {Robustness} to {Adversarial} {Examples}},
	shorttitle = {Parseval {Networks}},
	url = {http://proceedings.mlr.press/v70/cisse17a.html},
	abstract = {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than \$1\$. Parseval networ...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	month = jul,
	year = {2017},
	pages = {854--863},
	file = {Cisse et al_2017_Parseval Networks.pdf:/home/user/Zotero/storage/R6X7TVQJ/Cisse et al_2017_Parseval Networks.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/CIPATACI/cisse17a.html:text/html}
}

@inproceedings{chou-improving-2017,
	title = {Improving {Stochastic} {Policy} {Gradients} in {Continuous} {Control} with {Deep} {Reinforcement} {Learning} using the {Beta} {Distribution}},
	url = {http://proceedings.mlr.press/v70/chou17a.html},
	abstract = {Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-wor...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Chou, Po-Wei and Maturana, Daniel and Scherer, Sebastian},
	month = jul,
	year = {2017},
	pages = {834--843},
	file = {Chou et al_2017_Improving Stochastic Policy Gradients in Continuous Control with Deep.pdf:/home/user/Zotero/storage/VB888IXT/Chou et al_2017_Improving Stochastic Policy Gradients in Continuous Control with Deep.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/2FGZQ9WZ/chou17a.html:text/html}
}

@inproceedings{grave-efficient-2017,
	title = {Efficient softmax approximation for {GPUs}},
	url = {http://proceedings.mlr.press/v70/grave17a.html},
	abstract = {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency o...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Grave and Joulin, Armand and Cissé, Moustapha and Grangier, David and Jégou, Hervé},
	month = jul,
	year = {2017},
	pages = {1302--1310},
	file = {Grave et al_2017_Efficient softmax approximation for GPUs.pdf:/home/user/Zotero/storage/XVGE5ZKA/Grave et al_2017_Efficient softmax approximation for GPUs.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VUI82D22/grave17a.html:text/html}
}

@inproceedings{wen-latent-2017,
	title = {Latent {Intention} {Dialogue} {Models}},
	url = {http://proceedings.mlr.press/v70/wen17a.html},
	abstract = {Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. The traditional approach...},
	language = {en},
	urldate = {2017-08-18},
	booktitle = {{PMLR}},
	author = {Wen, Tsung-Hsien and Miao, Yishu and Blunsom, Phil and Young, Steve},
	month = jul,
	year = {2017},
	pages = {3732--3741},
	file = {Snapshot:/home/user/Zotero/storage/6HWPSN5J/wen17a.html:text/html;Wen et al_2017_Latent Intention Dialogue Models.pdf:/home/user/Zotero/storage/6C8G34MS/Wen et al_2017_Latent Intention Dialogue Models.pdf:application/pdf}
}

@article{liu-using-2014,
	title = {Using eye tracking to understand the responses of learners to vocabulary learning strategy instruction and use},
	volume = {27},
	issn = {0958-8221},
	url = {http://dx.doi.org/10.1080/09588221.2014.881383},
	doi = {10.1080/09588221.2014.881383},
	abstract = {This study examined the influence of morphological instruction in an eye-tracking English vocabulary recognition task. Sixty-eight freshmen enrolled in an English course and received either traditional or morphological instruction for learning English vocabulary. The experimental part of the study was conducted over two-hour class periods for seven weeks. To investigate the effects of morphological instruction on English vocabulary learning, all participants completed an English vocabulary recognition task. Fixation time and path during recognition were recorded with an eye-tracking device. A comparison between the post-test performances of both groups showed that the experimental group obtained a considerably higher score on the target eye-tracking vocabulary test. The results of the eye-tracking record showed that participants who received morphological instruction had longer fixation times on the vocabulary and morpheme areas compared with the group that received traditional instruction. In addition, the experimental group had dense fixation paths on the morpheme areas of vocabulary. These results indicate that participants who received morphological instruction considered the morphemes as inferring references to read and inferred unknown words with greater success.},
	number = {4},
	urldate = {2017-08-18},
	journal = {Computer Assisted Language Learning},
	author = {Liu, Pei-Lin},
	month = jul,
	year = {2014},
	keywords = {morphology, Eye movement, vocabulary learning strategy},
	pages = {330--343},
	file = {Snapshot:/home/user/Zotero/storage/WMGJHK56/09588221.2014.html:text/html}
}

@article{jackson-using-2012,
	title = {Using eye-tracking to study the on-line processing of case-marking information among intermediate {L}2 learners of {German}},
	volume = {50},
	issn = {0019-042X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593600/},
	abstract = {This study uses eye-tracking to examine the processing of case-marking information in ambiguous subject- and object-first wh-questions in German. The position of the lexical verb was also manipulated via verb tense to investigate whether verb location influences how intermediate L2 learners process L2 sentences. Results show that intermediate L2 German learners were sensitive to case-marking information, exhibiting longer processing times on subject-first than object-first sentences, regardless of verb location. German native speakers exhibited the opposite word order preference, with longer processing times on object-first than subject-first sentences, replicating previous findings. These results are discussed in light of current L2 processing research, highlighting how methodological constraints influence researchers’ abilities to measure the on-line processing of morphosyntactic information among intermediate L2 learners.},
	number = {2},
	urldate = {2017-08-17},
	journal = {IRAL, International review of applied linguistics in language teaching : Revue internationale de linguistique appliquee enseignement des langues. Internationale Zeitschrift fur angewandte Linguistik in der Spracherziehung},
	author = {Jackson, Carrie N. and Dussias, Paola E. and Hristova, Adelina},
	month = may,
	year = {2012},
	pmid = {23493761},
	pmcid = {PMC3593600},
	pages = {101--133},
	file = {Jackson et al_2012_Using eye-tracking to study the on-line processing of case-marking information.pdf:/home/user/Zotero/storage/C32HSMNN/Jackson et al_2012_Using eye-tracking to study the on-line processing of case-marking information.pdf:application/pdf}
}

@article{dolgunsoz-measuring-2015,
	title = {Measuring {Attention} in {Second} {Language} {Reading} {Using} {Eye}-tracking: {The} {Case} of the {Noticing} {Hypothesis}},
	volume = {8},
	shorttitle = {Measuring {Attention} in {Second} {Language} {Reading} {Using} {Eye}-tracking},
	url = {https://bop.unibe.ch/index.php/JEMR/article/view/2413},
	number = {5},
	urldate = {2017-08-17},
	journal = {Journal of Eye Movement Research},
	author = {Dolgunsöz, Emrah},
	year = {2015},
	file = {3609.pdf:/home/user/Zotero/storage/ZXPS3DFE/3609.pdf:application/pdf}
}

@inproceedings{tokui-evaluating-2017,
	title = {Evaluating the {Variance} of {Likelihood}-{Ratio} {Gradient} {Estimators}},
	url = {http://proceedings.mlr.press/v70/tokui17a.html},
	abstract = {The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been pro...},
	language = {en},
	urldate = {2017-08-14},
	booktitle = {{PMLR}},
	author = {Tokui, Seiya and Sato, Issei},
	month = jul,
	year = {2017},
	pages = {3414--3423},
	file = {Snapshot:/home/user/Zotero/storage/HA2EAIX8/tokui17a.html:text/html;Tokui_Sato_2017_Evaluating the Variance of Likelihood-Ratio Gradient Estimators.pdf:/home/user/Zotero/storage/NZNFACK9/Tokui_Sato_2017_Evaluating the Variance of Likelihood-Ratio Gradient Estimators.pdf:application/pdf}
}

@inproceedings{chen-transition-based-2015,
	title = {Transition-based {Dependency} {Parsing} {Using} {Two} {Heterogeneous} {Gated} {Recursive} {Neural} {Networks}.},
	url = {http://www.anthology.aclweb.org/D/D15/D15-1215.pdf},
	urldate = {2017-08-08},
	booktitle = {{EMNLP}},
	author = {Chen, Xinchi and Zhou, Yaqian and Zhu, Chenxi and Qiu, Xipeng and Huang, Xuanjing},
	year = {2015},
	pages = {1879--1889},
	file = {D15-1215.pdf:/home/user/Zotero/storage/TKXJFWSR/D15-1215.pdf:application/pdf}
}

@inproceedings{druck-semi-supervised-2009,
	title = {Semi-supervised learning of dependency parsers using generalized expectation criteria},
	url = {http://dl.acm.org/citation.cfm?id=1687930},
	urldate = {2017-08-08},
	booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 1-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Druck, Gregory and Mann, Gideon and McCallum, Andrew},
	year = {2009},
	pages = {360--368},
	file = {35524.pdf:/home/user/Zotero/storage/KXIXXTVF/35524.pdf:application/pdf}
}

@inproceedings{seginer-fast-2007,
	title = {Fast unsupervised incremental parsing},
	volume = {45},
	url = {http://www.aclweb.org/website/old\_anthology/P/P07/P07-1.pdf#page=422},
	urldate = {2017-08-08},
	booktitle = {Annual {Meeting}-{Association} {For} {Computational} {Linguistics}},
	author = {Seginer, Yoav},
	year = {2007},
	pages = {384},
	file = {P07-1049.pdf:/home/user/Zotero/storage/6VAI2FH8/P07-1049.pdf:application/pdf}
}

@inproceedings{marinho-semi-supervised-2016,
	title = {Semi-{Supervised} {Learning} of {Sequence} {Models} with {Method} of {Moments}.},
	url = {http://www.aclweb.org/website/old\_anthology/D/D16/D16-1028.pdf},
	urldate = {2017-08-08},
	booktitle = {{EMNLP}},
	author = {Marinho, Zita and Martins, André FT and Cohen, Shay B. and Smith, Noah A.},
	year = {2016},
	pages = {287--296},
	file = {emnlp16anchor.pdf:/home/user/Zotero/storage/2P3C472U/emnlp16anchor.pdf:application/pdf}
}

@inproceedings{arora-practical-2013,
	title = {A practical algorithm for topic modeling with provable guarantees},
	url = {http://www.jmlr.org/proceedings/papers/v28/arora13.pdf},
	urldate = {2017-08-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
	year = {2013},
	pages = {280--288},
	file = {A Practical Algorithm for Topic Modeling with Provable Guarantees - AroraEtAl_icml13.pdf:/home/user/Zotero/storage/5HPIAGNR/AroraEtAl_icml13.pdf:application/pdf}
}

@inproceedings{shalev-shwartz-failures-2017,
	title = {Failures of {Gradient}-{Based} {Deep} {Learning}},
	url = {http://proceedings.mlr.press/v70/shalev-shwartz17a.html},
	urldate = {2017-08-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
	year = {2017},
	pages = {3067--3075},
	file = {Failures of Gradient-Based Deep Learning - shalev-shwartz17a.pdf:/home/user/Zotero/storage/4F9NFR9H/shalev-shwartz17a.pdf:application/pdf}
}

@article{mroueh-mcgan:-2017,
	title = {{McGan}: {Mean} and {Covariance} {Feature} {Matching} {GAN}},
	shorttitle = {{McGan}},
	url = {https://arxiv.org/abs/1702.08398},
	urldate = {2017-08-08},
	journal = {arXiv preprint arXiv:1702.08398},
	author = {Mroueh, Youssef and Sercu, Tom and Goel, Vaibhava},
	year = {2017},
	file = {McGan\: Mean and Covariance Feature Matching GAN - mroueh17a.pdf:/home/user/Zotero/storage/WZUN2ZB7/mroueh17a.pdf:application/pdf}
}

@article{achiam-constrained-2017,
	title = {Constrained {Policy} {Optimization}},
	url = {https://arxiv.org/abs/1705.10528},
	urldate = {2017-08-08},
	journal = {arXiv preprint arXiv:1705.10528},
	author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
	year = {2017},
	file = {achiam17a.pdf:/home/user/Zotero/storage/WHB5MDVA/achiam17a.pdf:application/pdf}
}

@inproceedings{zhou-neural-2015,
	title = {A {Neural} {Probabilistic} {Structured}-{Prediction} {Model} for {Transition}-{Based} {Dependency} {Parsing}.},
	url = {http://anthology.aclweb.org/P/P15/P15-1117.pdf},
	urldate = {2017-08-08},
	booktitle = {{ACL} (1)},
	author = {Zhou, Hao and Zhang, Yue and Huang, Shujian and Chen, Jiajun},
	year = {2015},
	pages = {1213--1222},
	file = {P15-1117.pdf:/home/user/Zotero/storage/SW2CQB8R/P15-1117.pdf:application/pdf}
}

@book{cohen-bayesian-2016-3,
	title = {Bayesian analysis in natural language processing},
	isbn = {978-1-62705-873-5},
	abstract = {"Natural language processing (NLP) went through a profound transformation in the mid-1980s when it shifted to make heavy use of corpora and data-driven techniques to analyze language. Since then, the use of statistical techniques in NLP has evolved in several ways. One such example of evolution took place in the late 1990s or early 2000s, when full-fledged Bayesian machinery was introduced to NLP. This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it, especially in the unsupervised setting, where statistical learning is done without target prediction examples. We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area. These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed "in-house" in NLP. We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference, Bayesian estimation, and nonparametric modeling. We also cover fundamental concepts in Bayesian statistics such as prior distributions, conjugacy, and generative modeling. Finally, we cover some of the fundamental modeling techniques in NLP, such as grammar modeling and their use with Bayesian analysis."--Publisher's website.},
	language = {English},
	author = {Cohen, Shay},
	year = {2016},
	note = {OCLC: 953497751},
	file = {s00719ed1v01y201605hlt035.pdf:/home/user/Zotero/storage/BNGCNWG3/s00719ed1v01y201605hlt035.pdf:application/pdf}
}

@article{nelson-neurophysiological-2017,
	title = {Neurophysiological dynamics of phrase-structure building during sentence processing},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1701590114},
	doi = {10.1073/pnas.1701590114},
	language = {en},
	number = {18},
	urldate = {2017-08-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nelson, Matthew J. and El Karoui, Imen and Giber, Kristof and Yang, Xiaofang and Cohen, Laurent and Koopman, Hilda and Cash, Sydney S. and Naccache, Lionel and Hale, John T. and Pallier, Christophe and Dehaene, Stanislas},
	month = may,
	year = {2017},
	pages = {E3669--E3678},
	file = {PNAS-2017-Nelson-E3669-78.pdf:/home/user/Zotero/storage/IH6TKM3B/PNAS-2017-Nelson-E3669-78.pdf:application/pdf}
}

@article{nakagawa-general-2013,
	title = {A general and simple method for obtaining {R}2 from generalized linear mixed-effects models},
	volume = {4},
	issn = {2041-210X},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x/abstract},
	doi = {10.1111/j.2041-210x.2012.00261.x},
	abstract = {*
The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models.



*
The presentation of ‘variance explained’ (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest.



*
One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation).



*
Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems.



*
This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances.},
	language = {en},
	number = {2},
	urldate = {2017-07-31},
	journal = {Methods in Ecology and Evolution},
	author = {Nakagawa, Shinichi and Schielzeth, Holger},
	month = feb,
	year = {2013},
	keywords = {heritability, model fit, coefficient of determination, goodness-of-fit, information criteria, intra-class correlation, linear models, repeatability, variance explained},
	pages = {133--142},
	file = {Full Text PDF:/home/user/Zotero/storage/Q8AT4Z86/Nakagawa and Schielzeth - 2013 - A general and simple method for obtaining R2 from .pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RCXJTT6Q/abstract.html:text/html}
}

@article{nakagawa-general-2013-1,
	title = {A general and simple method for obtaining {R}2 from generalized linear mixed-effects models},
	volume = {4},
	issn = {2041-210X},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x/abstract},
	doi = {10.1111/j.2041-210x.2012.00261.x},
	abstract = {*
The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models.



*
The presentation of ‘variance explained’ (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest.



*
One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation).



*
Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems.



*
This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances.},
	language = {en},
	number = {2},
	urldate = {2017-07-31},
	journal = {Methods in Ecology and Evolution},
	author = {Nakagawa, Shinichi and Schielzeth, Holger},
	month = feb,
	year = {2013},
	keywords = {heritability, model fit, coefficient of determination, goodness-of-fit, information criteria, intra-class correlation, linear models, repeatability, variance explained},
	pages = {133--142},
	file = {Full Text PDF:/home/user/Zotero/storage/RDBKVRV8/Nakagawa and Schielzeth - 2013 - A general and simple method for obtaining R2 from .pdf:application/pdf;Snapshot:/home/user/Zotero/storage/Q9JIP9VN/abstract.html:text/html}
}

@article{chen-thorough-2016,
	title = {A thorough examination of the cnn/daily mail reading comprehension task},
	url = {https://arxiv.org/abs/1606.02858},
	urldate = {2017-07-31},
	journal = {arXiv preprint arXiv:1606.02858},
	author = {Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
	year = {2016},
	file = {A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task - P16-1223:/home/user/Zotero/storage/H83N88F6/P16-1223.pdf:application/pdf}
}

@article{hoeting-bayesian-1999,
	title = {Bayesian model averaging: a tutorial (with comments by {M}. {Clyde}, {David} {Draper} and {E}. {I}. {George}, and a rejoinder by the authors},
	volume = {14},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Bayesian model averaging},
	url = {http://projecteuclid.org/euclid.ss/1009212519},
	doi = {10.1214/ss/1009212519},
	abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA)provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples.In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
	number = {4},
	urldate = {2017-07-31},
	journal = {Statistical Science},
	author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
	month = nov,
	year = {1999},
	mrnumber = {MR1765176},
	keywords = {Learning, Bayesian model averaging, Bayesian graphical models, Markov chain Monte Carlo, model uncertainty},
	pages = {382--417},
	file = {Snapshot:/home/user/Zotero/storage/XV67M7QQ/1009212519.html:text/html}
}

@inproceedings{glorot-understanding-2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	language = {en},
	urldate = {2017-07-31},
	booktitle = {{PMLR}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	pages = {249--256},
	file = {Full Text PDF:/home/user/Zotero/storage/K4ERVRBK/Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GRTSJQ4Q/glorot10a.html:text/html}
}

@article{sorensen-bayesian-2016-1,
	title = {Bayesian linear mixed models using {Stan}: {A} tutorial for psychologists, linguists, and cognitive scientists},
	volume = {12},
	issn = {2292-1354},
	shorttitle = {Bayesian linear mixed models using {Stan}},
	url = {http://www.tqmp.org/RegularArticles/vol12-3/p175},
	doi = {10.20982/tqmp.12.3.p175},
	number = {3},
	urldate = {2017-07-31},
	journal = {The Quantitative Methods for Psychology},
	author = {Sorensen, Tanner and Hohenstein, Sven and Vasishth, Shravan},
	month = oct,
	year = {2016},
	pages = {175--200},
	file = {SorensenHohensteinVasishth2016.pdf:/home/user/Zotero/storage/6DVCE3GX/SorensenHohensteinVasishth2016.pdf:application/pdf}
}

@article{carpenter-stan:-2017,
	title = {Stan: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	issn = {1548-7660},
	shorttitle = {\textit{{Stan}}},
	url = {http://www.jstatsoft.org/v76/i01/},
	doi = {10.18637/jss.v076.i01},
	language = {en},
	number = {1},
	urldate = {2017-07-31},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017}
}

@misc{noauthor-1702.08431.pdf-nodate,
	title = {1702.08431.pdf},
	url = {https://arxiv.org/pdf/1702.08431.pdf},
	urldate = {2017-07-08}
}

@misc{noauthor-1606.03498.pdf-nodate,
	title = {1606.03498.pdf},
	url = {https://arxiv.org/pdf/1606.03498.pdf},
	urldate = {2017-07-08}
}

@misc{noauthor-1701.02386.pdf-nodate,
	title = {1701.02386.pdf},
	url = {https://arxiv.org/pdf/1701.02386.pdf},
	urldate = {2017-07-08}
}

@article{wilson-marginal-2017,
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1705.08292},
	abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
	urldate = {2017-07-08},
	journal = {arXiv:1705.08292 [cs, stat]},
	author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08292},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/P4FTBMCN/1705.html:text/html;Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf:/home/user/Zotero/storage/UH5NHI7K/Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf:application/pdf}
}

@article{botev-practical-2017,
	title = {Practical {Gauss}-{Newton} {Optimisation} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1706.03662},
	abstract = {We present an efficient block-diagonal ap- proximation to the Gauss-Newton matrix for feedforward neural networks. Our result- ing algorithm is competitive against state- of-the-art first order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a labo- rious process, our approach can provide good performance even when used with default set- tings. A side result of our work is that for piecewise linear transfer functions, the net- work objective function can have no differ- entiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
	urldate = {2017-07-08},
	journal = {arXiv:1706.03662 [stat]},
	author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03662},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AV9IJFDK/1706.html:text/html;Botev et al_2017_Practical Gauss-Newton Optimisation for Deep Learning.pdf:/home/user/Zotero/storage/JXT2TT77/Botev et al_2017_Practical Gauss-Newton Optimisation for Deep Learning.pdf:application/pdf}
}

@article{zhang-yellowfin-2017,
	title = {{YellowFin} and the {Art} of {Momentum} {Tuning}},
	url = {http://arxiv.org/abs/1706.03471},
	abstract = {Hyperparameter tuning is one of the big costs of deep learning. State-of-the-art optimizers, such as Adagrad, RMSProp and Adam, make things easier by adaptively tuning an individual learning rate for each variable. This level of fine adaptation is understood to yield a more powerful method. However, our experiments, as well as recent theory by Wilson et al., show that hand-tuned stochastic gradient descent (SGD) achieves better results, at the same rate or faster. The hypothesis put forth is that adaptive methods converge to different minima (Wilson et al.). Here we point out another factor: none of these methods tune their momentum parameter, known to be very important for deep learning applications (Sutskever et al.). Tuning the momentum parameter becomes even more important in asynchronous-parallel systems: recent theory (Mitliagkas et al.) shows that asynchrony introduces momentum-like dynamics, and that tuning down algorithmic momentum is important for efficient parallelization. We revisit the simple momentum SGD algorithm and show that hand-tuning a single learning rate and momentum value makes it competitive with Adam. We then analyze its robustness in learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for both momentum and learning rate in SGD. YellowFin optionally uses a novel momentum-sensing component along with a negative-feedback loop mechanism to compensate for the added dynamics of asynchrony on the fly. We empirically show YellowFin converges in fewer iterations than Adam on large ResNet and LSTM models, with a speedup up to \$2.8\$x in synchronous and \$2.7\$x in asynchronous settings.},
	urldate = {2017-07-08},
	journal = {arXiv:1706.03471 [cs, stat]},
	author = {Zhang, Jian and Mitliagkas, Ioannis and Ré, Christopher},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03471},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/I8W273PJ/1706.html:text/html;Zhang et al_2017_YellowFin and the Art of Momentum Tuning.pdf:/home/user/Zotero/storage/MCK6FAS5/Zhang et al_2017_YellowFin and the Art of Momentum Tuning.pdf:application/pdf}
}

@incollection{grice-logic-1975,
	address = {New York},
	title = {Logic and {Conversation}},
	volume = {3},
	booktitle = {Syntax and {Semantics}},
	author = {Grice, Herbert Paul},
	editor = {Cole, P and Morgan, J},
	year = {1975},
	pages = {41--58}
}

@book{luce-individual-1959,
	address = {New York},
	title = {Individual {Choice} {Behavior}: {A} {Theoretical} {Analysis}},
	author = {Luce, R. Duncan},
	year = {1959}
}

@article{sassoon-typology-2013,
	title = {A typology of multidimensional adjectives},
	volume = {30},
	journal = {Journal of Semantics},
	author = {Sassoon, Galit W.},
	year = {2013},
	pages = {335--380}
}

@article{lassiter-adjectival-2017,
	title = {Adjectival vagueness in a {Bayesian} model of interpretation},
	volume = {194},
	url = {http://web.stanford.edu/~danlass/Lassiter-Goodman-adjectival-vagueness-Synthese.pdf},
	number = {10},
	urldate = {2016-03-24},
	journal = {Synthese},
	author = {Lassiter, Daniel and Goodman, Noah D.},
	year = {2017},
	pages = {3801--3836},
	file = {Lassiter-Goodman-adjectival-vagueness-Synthese.pdf:/home/user/Zotero/storage/QPGMUM6X/Lassiter-Goodman-adjectival-vagueness-Synthese.pdf:application/pdf}
}

@article{sedivy-achieving-1999,
	title = {Achieving incremental semantic interpretation through contextual representation},
	volume = {71},
	issn = {0010-0277},
	abstract = {While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.},
	language = {eng},
	number = {2},
	journal = {Cognition},
	author = {Sedivy, J. C. and Tanenhaus, M. K. and Chambers, C. G. and Carlson, G. N.},
	month = jun,
	year = {1999},
	pmid = {10444906},
	keywords = {Humans, Pattern Recognition, Visual, Eye Movements, Reading, Semantics, Adult, Female, Male, Attention, Color Perception, Discrimination Learning, Psychomotor Performance, Speech Perception},
	pages = {109--147}
}

@inproceedings{redford-good-2016,
	title = {The good, the bad, and the ugly: {Incremental} interpretation of evaluative adjectives},
	booktitle = {{CUNY} {Conference} on {Human} {Sentence} {Processing}},
	author = {Redford, Robert and Chambers, Craig},
	year = {2016}
}

@article{qian-cue-2012,
	title = {Cue {Effectiveness} in {Communicatively} {Efficient} {Discourse} {Production}},
	volume = {36},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2012.01256.x/abstract},
	doi = {10.1111/j.1551-6709.2012.01256.x},
	abstract = {Recent years have seen a surge in accounts motivated by information theory that consider language production to be partially driven by a preference for communicative efficiency. Evidence from discourse production (i.e., production beyond the sentence level) has been argued to suggest that speakers distribute information across discourse so as to hold the conditional per-word entropy associated with each word constant, which would facilitate efficient information transfer (Genzel \& Charniak, 2002). This hypothesis implies that the conditional (contextualized) probabilities of linguistic units affect speakers’ preferences during production. Here, we extend this work in two ways. First, we explore how preceding cues are integrated into contextualized probabilities, a question which so far has received little to no attention. Specifically, we investigate how a cue's maximal informativity about upcoming words (the cue's effectiveness) decays as a function of the cue's recency. Based on properties of linguistic discourses as well as properties of human memory, we analytically derive a model of cue effectiveness decay and evaluate it against cross-linguistic data from 12 languages. Second, we relate the information theoretic accounts of discourse production to well-established mechanistic (activation-based) accounts: We relate contextualized probability distributions over words to their relative activation in a lexical network given preceding discourse.},
	language = {en},
	number = {7},
	urldate = {2018-01-21},
	journal = {Cognitive Science},
	author = {Qian, Ting and Jaeger, T. Florian},
	month = sep,
	year = {2012},
	keywords = {Entropy, Communicative efficiency, Constant entropy rate, Cross-linguistic, Cue effectiveness decay, Cue informativity, Language},
	pages = {1312--1336},
	file = {Full Text PDF:/home/user/Zotero/storage/4XI3H72I/Qian and Jaeger - 2012 - Cue Effectiveness in Communicatively Efficient Dis.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/3SA4WX48/abstract.html:text/html}
}

@article{lin-critical-2017,
	title = {Critical {Behavior} in {Physics} and {Probabilistic} {Formal} {Languages}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/19/7/299},
	doi = {10.3390/e19070299},
	abstract = {We show that the mutual information between two symbols, as a function of the number of symbols between the two, decays exponentially in any probabilistic regular grammar, but can decay like a power law for a context-free grammar. This result about formal languages is closely related to a well-known result in classical statistical mechanics that there are no phase transitions in dimensions fewer than two. It is also related to the emergence of power law correlations in turbulence and cosmological inflation through recursive generative processes. We elucidate these physics connections and comment on potential applications of our results to machine learning tasks like training artificial recurrent neural networks. Along the way, we introduce a useful quantity, which we dub the rational mutual information, and discuss generalizations of our claims involving more complicated Bayesian networks.},
	language = {en},
	number = {7},
	urldate = {2018-01-21},
	journal = {Entropy},
	author = {Lin, Henry W. and Tegmark, Max},
	month = jun,
	year = {2017},
	keywords = {criticality, formal languages, statistical mechanics},
	pages = {299},
	file = {Full Text PDF:/home/user/Zotero/storage/XZG82ZE4/Lin and Tegmark - 2017 - Critical Behavior in Physics and Probabilistic For.pdf:application/pdf}
}

@book{manning-foundations-1999,
	title = {Foundations of {Statistical} {Natural} {Language} {Processing}},
	publisher = {MIT Press},
	author = {Manning, Christopher and Schuetze, Hirich},
	year = {1999}
}

@inproceedings{kennedy-subjective-2016,
	title = {Subjective attitudes and counterstance contingency},
	volume = {26},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Kennedy, Christopher and Willer, Malte},
	year = {2016},
	pages = {913--933},
	file = {sacc.pdf:/home/user/Zotero/storage/T89JTFHS/sacc.pdf:application/pdf}
}

@article{grano-mandarin-2012,
	title = {Mandarin transitive comparatives and the grammar of measurement},
	volume = {21},
	issn = {0925-8558, 1572-8560},
	url = {http://link.springer.com/10.1007/s10831-012-9090-y},
	doi = {10.1007/s10831-012-9090-y},
	language = {en},
	number = {3},
	urldate = {2018-01-21},
	journal = {Journal of East Asian Linguistics},
	author = {Grano, Thomas and Kennedy, Chris},
	month = aug,
	year = {2012},
	pages = {219--266},
	file = {gk-mtc.pdf:/home/user/Zotero/storage/NMX73RE9/gk-mtc.pdf:application/pdf}
}

@article{leffel-university-nodate,
	title = {The {University} of {Chicago}},
	author = {Leffel, Timothy and Xiang, Ming and Kennedy, Chris},
	file = {161.pdf:/home/user/Zotero/storage/AC257Y3B/161.pdf:application/pdf}
}

@inproceedings{aparicio-processing-2016,
	title = {Processing gradable adjectives in context: {A} visual world study},
	volume = {25},
	shorttitle = {Processing gradable adjectives in context},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Aparicio, Helena and Xiang, Ming and Kennedy, Christopher},
	year = {2016},
	pages = {413--432},
	file = {Aparicio_Xiang_Kennedy_Processing_Gradable_adjectives.pdf:/home/user/Zotero/storage/96GYV9SE/Aparicio_Xiang_Kennedy_Processing_Gradable_adjectives.pdf:application/pdf}
}

@article{eslami-attend-2016,
	title = {Attend, {Infer}, {Repeat}: {Fast} {Scene} {Understanding} with {Generative} {Models}},
	shorttitle = {Attend, {Infer}, {Repeat}},
	url = {http://arxiv.org/abs/1603.08575},
	abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
	urldate = {2018-01-21},
	journal = {arXiv:1603.08575 [cs]},
	author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08575},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1603.08575 PDF:/home/user/Zotero/storage/GMBSMTUK/Eslami et al. - 2016 - Attend, Infer, Repeat Fast Scene Understanding wi.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/TKHAWVG4/1603.html:text/html}
}

@inproceedings{larson-events-1998,
	title = {Events and modification in nominals},
	volume = {8},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Larson, Richard K.},
	year = {1998},
	pages = {145--168},
	file = {2543.pdf:/home/user/Zotero/storage/T7HSEPLG/2543.pdf:application/pdf}
}

@article{coco-interaction-2015,
	title = {The interaction of visual and linguistic saliency during syntactic ambiguity resolution},
	volume = {68},
	number = {1},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Coco, Moreno I. and Keller, Frank},
	year = {2015},
	pages = {46--74},
	file = {qjep15.pdf:/home/user/Zotero/storage/UC6GB2LJ/qjep15.pdf:application/pdf}
}

@article{coco-interplay-2014,
	title = {The interplay of bottom-up and top-down mechanisms in visual guidance during object naming},
	volume = {67},
	number = {6},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Coco, Moreno I. and Malcolm, George L. and Keller, Frank},
	year = {2014},
	pages = {1096--1120},
	file = {qjep14.pdf:/home/user/Zotero/storage/G67IV72D/qjep14.pdf:application/pdf}
}

@article{clarke-impact-2013-1,
	title = {The impact of attentional, linguistic, and visual features during object naming},
	volume = {4},
	journal = {Frontiers in psychology},
	author = {Clarke, Alasdair DF and Coco, Moreno I. and Keller, Frank},
	year = {2013},
	file = {frontiers13.pdf:/home/user/Zotero/storage/TFMNSVXZ/frontiers13.pdf:application/pdf}
}

@article{coco-scan-2012,
	title = {Scan patterns predict sentence production in the cross-modal processing of visual scenes},
	volume = {36},
	number = {7},
	journal = {Cognitive Science},
	author = {Coco, Moreno I. and Keller, Frank},
	year = {2012},
	pages = {1204--1223},
	file = {cogsci_journal12.pdf:/home/user/Zotero/storage/NEJ4E99Z/cogsci_journal12.pdf:application/pdf}
}

@inproceedings{rothe-question-2017,
	title = {Question asking as program generation},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rothe, Anselm and Lake, Brenden M. and Gureckis, Todd},
	year = {2017},
	pages = {1046--1055},
	file = {RotheEtAl2017NIPS.pdf:/home/user/Zotero/storage/8VK9VMXK/RotheEtAl2017NIPS.pdf:application/pdf}
}

@inproceedings{rothe-question-2017-1,
	title = {Question asking as program generation},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rothe, Anselm and Lake, Brenden M. and Gureckis, Todd},
	year = {2017},
	pages = {1046--1055},
	file = {RotheEtAl2017NIPS.pdf:/home/user/Zotero/storage/GBRHLEQS/RotheEtAl2017NIPS.pdf:application/pdf}
}

@article{new-category-specific-2007,
	title = {Category-specific attention for animals reflects ancestral priorities, not expertise},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/104/42/16598},
	doi = {10.1073/pnas.0703913104},
	abstract = {National Academy of Sciences},
	language = {en},
	number = {42},
	urldate = {2018-01-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {New, Joshua and Cosmides, Leda and Tooby, John},
	month = oct,
	year = {2007},
	pmid = {17909181},
	pages = {16598--16603},
	file = {Full Text PDF:/home/user/Zotero/storage/6T8K7YBC/New et al. - 2007 - Category-specific attention for animals reflects a.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/HXARAN6R/16598.html:text/html}
}

@article{new-category-specific-2007-1,
	title = {Category-specific attention for animals reflects ancestral priorities, not expertise},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/104/42/16598},
	doi = {10.1073/pnas.0703913104},
	abstract = {Visual attention mechanisms are known to select information to process based on current goals, personal relevance, and lower-level features. Here we present evidence that human visual attention also includes a high-level category-specialized system that monitors animals in an ongoing manner. Exposed to alternations between complex natural scenes and duplicates with a single change (a change-detection paradigm), subjects are substantially faster and more accurate at detecting changes in animals relative to changes in all tested categories of inanimate objects, even vehicles, which they have been trained for years to monitor for sudden life-or-death changes in trajectory. This animate monitoring bias could not be accounted for by differences in lower-level visual characteristics, how interesting the target objects were, experience, or expertise, implicating mechanisms that evolved to direct attention differentially to objects by virtue of their membership in ancestrally important categories, regardless of their current utility.},
	language = {en},
	number = {42},
	urldate = {2018-01-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {New, Joshua and Cosmides, Leda and Tooby, John},
	month = oct,
	year = {2007},
	pmid = {17909181},
	keywords = {visual attention, animacy, category specificity, domain specificity, evolutionary psychology},
	pages = {16598--16603},
	file = {Full Text PDF:/home/user/Zotero/storage/U7236D2H/New et al. - 2007 - Category-specific attention for animals reflects a.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/358UENZ2/16598.html:text/html}
}

@inproceedings{kang-human-2011,
	title = {Human visual attention with context-specific top-down saliency},
	doi = {10.1109/ROBIO.2011.6181594},
	abstract = {Human visual attention is an area of research that has a strong effect on the field of human-robot-interaction. It has various applications for human-care robots and service robots. The ability of human visual attention to acquire knowledge on informative objects through interaction with the environment is an important research issue in the field of human-computer interaction and augmented cognition. Such knowledge can be acquired by automatically detecting visually attentive regions using the fixation extracted from eye-tracking data. Therefore, the optimal fixation must be selected from human eye-tracking data to detect the region of interest. In this paper, eye movement tracking was accurately used to capture human eye movements and to characterize the location and extent of a user's interest as an input mechanism to drive the interaction system. Furthermore, both top-down and bottom-up processes of the human visual system were at work in the process of selective attention. The correlation between human eye movement information and the bottom-up saliency map was calculated and compared with the correlation that was calculated by combining the top-down and bottom-up processes. The experiment results showed that the human visual attention system cannot be constructed with the bottom-up process alone and requires the combination of the top-down and bottom-up processes together.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}},
	author = {Kang, D. and Lee, S. and Lee, Y. B.},
	month = dec,
	year = {2011},
	keywords = {Humans, cognition, augmented cognition, augmented reality, bottom-up process, context-specific top-down saliency, Correlation, Dispersion, eye, eye movement tracking, human eye-tracking data, human visual attention, human-care robots, human-computer interaction, human-robot interaction, human-robot-interaction, Image color analysis, Image edge detection, object detection, service robots, Skin, top-down process, Visualization},
	pages = {2055--2060},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/GMYTGV9I/6181594.html:text/html}
}

@inproceedings{hu-salient-2017,
	title = {Salient locations search based on human visual attention: {An} experimental analysis},
	shorttitle = {Salient locations search based on human visual attention},
	doi = {10.1109/ICNSC.2017.8000167},
	abstract = {To explore whether the conscious options or eye movements of human beings are influenced by bottom-up saliency, both theoretical and experimental analysis are presented in this paper. In the first experiment, the subjects are required to indicate most interesting regions of test pictures in the scene. In the second experiment, the subjects are required to look around test pictures in the scene. Then, the bottom-up saliency values of test pictures are computed by Itti model of that is one well-known visual attention. It is indicated that the interest points and fixation locations are correlated with the bottom-up saliency locations. Thus we propose two methods to study on visual attention that can be used to predict which areas of the images will draw subjects' visual attention: fixation location predictions and interest point predictions.},
	booktitle = {2017 {IEEE} 14th {International} {Conference} on {Networking}, {Sensing} and {Control} ({ICNSC})},
	author = {Hu, Wenting and Yang, Pei and Zhou, Xianzhong and Liu, Zhen and Li, Huaxiong and Zhu, Xianjun},
	month = may,
	year = {2017},
	keywords = {visual attention, Computational modeling, cognition, visual perception, eye, human visual attention, Visualization, Automobiles, bottom-up saliency values, Cats, cognitive processing, Dogs, eye movements, fixation locations, gaze tracking, human computer interaction, human eye movements, interest point predictions, interest points, Itti model, mental activity, Motorcycles, neurophysiology, saliency, salient locations search, Tracking},
	pages = {649--654},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/9RSPKQQR/8000167.html:text/html}
}

@article{kimura-stochastic-2010,
	title = {A stochastic model of human visual attention with a dynamic {Bayesian} network},
	url = {http://arxiv.org/abs/1004.0085},
	abstract = {Recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non-deterministic. People may attend to different locations on the same visual input at the same time. Based on this knowledge, we propose a new stochastic model of visual attention by introducing a dynamic Bayesian network to predict the likelihood of where humans typically focus on a video scene. The proposed model is composed of a dynamic Bayesian network with 4 layers. Our model provides a framework that simulates and combines the visual saliency response and the cognitive state of a person to estimate the most probable attended regions. Sample-based inference with Markov chain Monte-Carlo based particle filter and stream processing with multi-core processors enable us to estimate human visual attention in near real time. Experimental results have demonstrated that our model performs significantly better in predicting human visual attention compared to the previous deterministic models.},
	urldate = {2018-01-12},
	journal = {arXiv:1004.0085 [cs, stat]},
	author = {kimura, Akisato and Pang, Derek and Takeuchi, Tatsuto and Miyazato, Kouji and Yamato, Junji and Kashino, Kunio},
	month = apr,
	year = {2010},
	note = {arXiv: 1004.0085},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, I.2.10, I.5.1, Computer Science - Multimedia, 68U10, I.2.9, I.3.1, I.4.10, I.4.4, I.4.8, I.6.8},
	file = {arXiv\:1004.0085 PDF:/home/user/Zotero/storage/5SE4WWYF/kimura et al. - 2010 - A stochastic model of human visual attention with .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/E6KJ248U/1004.html:text/html}
}

@article{eckstein-rethinking-2013,
	title = {Rethinking human visual attention: spatial cueing effects and optimality of decisions by honeybees, monkeys and humans},
	volume = {85},
	issn = {1878-5646},
	shorttitle = {Rethinking human visual attention},
	doi = {10.1016/j.visres.2012.12.011},
	abstract = {Visual attention is commonly studied by using visuo-spatial cues indicating probable locations of a target and assessing the effect of the validity of the cue on perceptual performance and its neural correlates. Here, we adapt a cueing task to measure spatial cueing effects on the decisions of honeybees and compare their behavior to that of humans and monkeys in a similarly structured two-alternative forced-choice perceptual task. Unlike the typical cueing paradigm in which the stimulus strength remains unchanged within a block of trials, for the monkey and human studies we randomized the contrast of the signal to simulate more real world conditions in which the organism is uncertain about the strength of the signal. A Bayesian ideal observer that weights sensory evidence from cued and uncued locations based on the cue validity to maximize overall performance is used as a benchmark of comparison against the three animals and other suboptimal models: probability matching, ignore the cue, always follow the cue, and an additive bias/single decision threshold model. We find that the cueing effect is pervasive across all three species but is smaller in size than that shown by the Bayesian ideal observer. Humans show a larger cueing effect than monkeys and bees show the smallest effect. The cueing effect and overall performance of the honeybees allows rejection of the models in which the bees are ignoring the cue, following the cue and disregarding stimuli to be discriminated, or adopting a probability matching strategy. Stimulus strength uncertainty also reduces the theoretically predicted variation in cueing effect with stimulus strength of an optimal Bayesian observer and diminishes the size of the cueing effect when stimulus strength is low. A more biologically plausible model that includes an additive bias to the sensory response from the cued location, although not mathematically equivalent to the optimal observer for the case stimulus strength uncertainty, can approximate the benefits of the more computationally complex optimal Bayesian model. We discuss the implications of our findings on the field's common conceptualization of covert visual attention in the cueing task and what aspects, if any, might be unique to humans.},
	language = {eng},
	journal = {Vision Research},
	author = {Eckstein, Miguel P. and Mack, Stephen C. and Liston, Dorion B. and Bogush, Lisa and Menzel, Randolf and Krauzlis, Richard J.},
	month = jun,
	year = {2013},
	pmid = {23298793},
	pmcid = {PMC3968950},
	keywords = {Humans, Attention, Bayes Theorem, Animals, Space Perception, Cues, Bees, Decision Making, Haplorhini, Psychophysics, Signal Detection, Psychological, Visual Perception},
	pages = {5--19}
}

@article{le-callet-visual-2013,
	title = {Visual {Attention} and {Applications} in {Multimedia} {Technologies}},
	volume = {101},
	issn = {0018-9219},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3902206/},
	doi = {10.1109/JPROC.2013.2265801},
	abstract = {Making technological advances in the field of human-machine interactions requires that the capabilities and limitations of the human perceptual system are taken into account. The focus of this report is an important mechanism of perception, visual selective attention, which is becoming more and more important for multimedia applications. We introduce the concept of visual attention and describe its underlying mechanisms. In particular, we introduce the concepts of overt and covert visual attention, and of bottom-up and top-down processing. Challenges related to modeling visual attention and their validation using ad hoc ground truth are also discussed. Examples of the usage of visual attention models in image and video processing are presented. We emphasize multimedia delivery, retargeting and quality assessment of image and video, medical imaging, and the field of stereoscopic 3D images applications.},
	number = {9},
	urldate = {2018-01-12},
	journal = {Proceedings of the IEEE. Institute of Electrical and Electronics Engineers},
	author = {Le Callet, Patrick and Niebur, Ernst},
	month = sep,
	year = {2013},
	pmid = {24489403},
	pmcid = {PMC3902206},
	pages = {2058--2067},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/TR6GG2ZN/Le Callet and Niebur - 2013 - Visual Attention and Applications in Multimedia Te.pdf:application/pdf}
}

@article{kastner-mechanisms-2000,
	title = {Mechanisms of visual attention in the human cortex},
	volume = {23},
	issn = {0147-006X},
	doi = {10.1146/annurev.neuro.23.1.315},
	abstract = {A typical scene contains many different objects that, because of the limited processing capacity of the visual system, compete for neural representation. The competition among multiple objects in visual cortex can be biased by both bottom-up sensory-driven mechanisms and top-down influences, such as selective attention. Functional brain imaging studies reveal that, both in the absence and in the presence of visual stimulation, biasing signals due to selective attention can modulate neural activity in visual cortex in several ways. Although the competition among stimuli for representation is ultimately resolved within visual cortex, the source of top-down biasing signals derives from a network of areas in frontal and parietal cortex.},
	language = {eng},
	journal = {Annual Review of Neuroscience},
	author = {Kastner, S. and Ungerleider, L. G.},
	year = {2000},
	pmid = {10845067},
	keywords = {Humans, Memory, Attention, Animals, Visual Perception, Visual Cortex, Visual Pathways},
	pages = {315--341}
}

@article{mizuhara-degree-2000,
	title = {The degree of human visual attention in the visual search},
	volume = {4},
	issn = {1433-5298, 1614-7456},
	url = {https://link.springer.com/article/10.1007/BF02480857},
	doi = {10.1007/BF02480857},
	abstract = {Human beings can obtain visual information in parallel through the retina, but they cannot pay attention to all the information at the same time. In psychological studies, the human characteristics of visual attention have often been investigated by analyzing the characteristics of the visual search task. Previous studies suggested that the information features of the visual search task are processed in parallel at early stages of processing. However, the authors consider that these features are not processed completely in parallel, and have a reciprocal action to each other. In order to clarify the reciprocal action of the features in a visual search and the continuity of visual attention, the characteristics of reaction times were measured with changing forms of visual stimuli. The experimental results suggested that the reaction time changed when the features of the visual stimuli in the visual search task changed. This means that the features are affected by each other. Furthermore, continuity of reciprocal action is also suggested, and the degree of visual attention is decided by this continuity. The results provided significant basic data to support our proposed mathematical model of visual attention.},
	language = {en},
	number = {2},
	urldate = {2018-01-12},
	journal = {Artificial Life and Robotics},
	author = {Mizuhara, Hiroaki and Wu, Jing-Long and Nishikawa, Yoshikazu},
	month = jun,
	year = {2000},
	pages = {57--61},
	file = {Full Text PDF:/home/user/Zotero/storage/3ME9MYIM/Mizuhara et al. - 2000 - The degree of human visual attention in the visual.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BTHEBJDV/BF02480857.html:text/html}
}

@article{kimura-computational-2013,
	title = {Computational {Models} of {Human} {Visual} {Attention} and {Their} {Implementations}: {A} {Survey}},
	volume = {E96.D},
	issn = {0916-8532, 1745-1361},
	shorttitle = {Computational {Models} of {Human} {Visual} {Attention} and {Their} {Implementations}},
	url = {https://www.jstage.jst.go.jp/article/transinf/E96.D/3/E96.D\_562/\_article},
	doi = {10.1587/transinf.E96.D.562},
	abstract = {Japan's largest platform for academic e-journals: J-STAGE is a full text database for reviewed academic papers published by Japanese societies},
	language = {en},
	number = {3},
	urldate = {2018-01-12},
	journal = {IEICE Transactions on Information and Systems},
	author = {Kimura, Akisato and Yonetani, Ryo and Hirayama, Takatsugu},
	month = mar,
	year = {2013},
	pages = {562--578},
	file = {Full Text PDF:/home/user/Zotero/storage/YDUG29QK/Kimura et al. - 2013 - Computational Models of Human Visual Attention and.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/X6X4SX2Q/_article.html:text/html}
}

@article{kruthiventi-deepfix:-2015-1,
	title = {{DeepFix}: {A} {Fully} {Convolutional} {Neural} {Network} for predicting {Human} {Eye} {Fixations}},
	shorttitle = {{DeepFix}},
	url = {http://arxiv.org/abs/1510.02927},
	abstract = {Understanding and predicting the human visual attentional mechanism is an active area of research in the fields of neuroscience and computer vision. In this work, we propose DeepFix, a first-of-its-kind fully convolutional neural network for accurate saliency prediction. Unlike classical works which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant which prevents them from modeling location dependent patterns (e.g. centre-bias). Our network overcomes this limitation by incorporating a novel Location Biased Convolutional layer. We evaluate our model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that it outperforms other recent approaches by a significant margin.},
	urldate = {2018-01-12},
	journal = {arXiv:1510.02927 [cs]},
	author = {Kruthiventi, Srinivas S. S. and Ayush, Kumar and Babu, R. Venkatesh},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.02927},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1510.02927 PDF:/home/user/Zotero/storage/PRFE2XIB/Kruthiventi et al. - 2015 - DeepFix A Fully Convolutional Neural Network for .pdf:application/pdf}
}

@book{brems-intersubjectivity-2014,
	address = {Amsterdam, NETHERLANDS},
	title = {Intersubjectivity and {Intersubjectification} in {Grammar} and {Discourse}: {Theoretical} and descriptive advances},
	isbn = {978-90-272-6978-2},
	shorttitle = {Intersubjectivity and {Intersubjectification} in {Grammar} and {Discourse}},
	url = {http://ebookcentral.proquest.com/lib/stanford-ebooks/detail.action?docID=1762289},
	abstract = {In this paper we present our views on intersubjectivity and intersubjectification with reference to case studies on adjectives, hedges, tags, honorifics, etc. Building on Diessel’s notion of “joint attention” and Traugott’s approach to intersubjectivity, we propose a distinction between three types of intersubjectivity: attitudinal, responsive, and textual. We evaluate and propose formal recognition criteria to operationalize this essentially semantic typology, such as left versus right periphery and prosodic features. In addition, we address the issue of directionality between subjectification and intersubjectification. Rather than seeing subjectivity as a prerequisite for intersubjectivity, we argue that in our typology intersubjective meanings of constructions may diachronically precede subjective ones.},
	urldate = {2018-01-11},
	publisher = {John Benjamins Publishing Company},
	author = {Brems, Lieselotte and Ghesquière, Lobke},
	year = {2014},
	keywords = {Cognitive grammar., Discourse analysis -- Social aspects., Intersubjectivity., Psycholinguistics., Sociolinguistics.},
	file = {ProQuest Ebook Snapshot:/home/user/Zotero/storage/R2GWCKRL/detail.html:text/html}
}

@article{vehtari-waic-2014,
	title = {{WAIC} and cross-validation in {Stan}},
	volume = {27},
	number = {2015},
	journal = {Submitted. http://www. stat. columbia. edu/∼ gelman/research/unpublished/waic\_ stan. pdf Accessed},
	author = {Vehtari, Aki and Gelman, Andrew},
	year = {2014},
	pages = {5},
	file = {waic_stan.pdf:/home/user/Zotero/storage/NXVLFJ4A/waic_stan.pdf:application/pdf}
}

@article{temperley-minimizing-2018,
	title = {Minimizing {Syntactic} {Dependency} {Lengths}: {Typological}/{Cognitive} {Universal}?},
	volume = {4},
	shorttitle = {Minimizing {Syntactic} {Dependency} {Lengths}},
	journal = {Annu. Rev. Linguist},
	author = {Temperley, David and Gildea, Daniel},
	year = {2018},
	pages = {1--15},
	file = {temperley-gildea-ar18.pdf:/home/user/Zotero/storage/8Q77DLL4/temperley-gildea-ar18.pdf:application/pdf}
}

@article{farkas-division-2017,
	title = {Division of labor in the interpretation of declaratives and interrogatives},
	volume = {34},
	number = {2},
	journal = {Journal of Semantics},
	author = {Farkas, Donka F. and Roelofsen, Floris},
	year = {2017},
	pages = {237--289},
	file = {division-of-labor-final.pdf:/home/user/Zotero/storage/P69PCGVI/division-of-labor-final.pdf:application/pdf}
}

@inproceedings{mishra-learning-2017,
	title = {Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network},
	volume = {1},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Mishra, Abhijit and Dey, Kuntal and Bhattacharyya, Pushpak},
	year = {2017},
	pages = {377--387},
	file = {acl17-cogfeatures.pdf:/home/user/Zotero/storage/7HD5K355/acl17-cogfeatures.pdf:application/pdf}
}

@article{joshi-expect-2017,
	title = {Expect the unexpected: {Harnessing} {Sentence} {Completion} for {Sarcasm} {Detection}},
	shorttitle = {Expect the unexpected},
	journal = {arXiv preprint arXiv:1707.06151},
	author = {Joshi, Aditya and Agrawal, Samarth and Bhattacharyya, Pushpak and Carman, Mark},
	year = {2017},
	file = {pacling17-sarcasm-sentence-completion.pdf:/home/user/Zotero/storage/AZWZZ6XE/pacling17-sarcasm-sentence-completion.pdf:application/pdf}
}

@article{mishra-harnessing-2017,
	title = {Harnessing {Cognitive} {Features} for {Sarcasm} {Detection}},
	journal = {arXiv preprint arXiv:1701.05574},
	author = {Mishra, Abhijit and Kanojia, Diptesh and Nagar, Seema and Dey, Kuntal and Bhattacharyya, Pushpak},
	year = {2017},
	file = {P16-1104.pdf:/home/user/Zotero/storage/CI6XCNBB/P16-1104.pdf:application/pdf}
}

@article{traugott-inter-2010,
	title = {({Inter}) subjectivity and (inter) subjectification: {A} reassessment},
	shorttitle = {({Inter}) subjectivity and (inter) subjectification},
	journal = {Subjectification, intersubjectification and grammaticalization},
	author = {Traugott, Elizabeth Closs},
	year = {2010},
	pages = {29--71},
	file = {TraugottDavidseIntersbfn.pdf:/home/user/Zotero/storage/AJ8ESESN/TraugottDavidseIntersbfn.pdf:application/pdf}
}

@article{prevost-propos-2011,
	title = {A propos from verbal complement to discourse marker: a case of grammaticalization?},
	volume = {49},
	shorttitle = {A propos from verbal complement to discourse marker},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.012/ling.2011.012.xml?format=INT},
	doi = {10.1515/ling.2011.012},
	abstract = {This paper presents an analysis of the evolution of the French preverbal expression à propos (‘by the way’ in Modern French). First I discuss the possibility of analyzing it as a discourse marker. Basing the analysis on Fraser's approach (Journal of Pragmatics 14: 383–395, 1990, Journal of pragmatics 31: 931–952, 1999), I show that à propos falls within the definition of discourse markers, displaying their main characteristics. More specifically it serves to reinforce, or even create, discourse coherence. Secondly I give an account of the historical development of the expression and of the emergence of its pragmatic uses. I argue that it is closely related to the evolution of à ce propos (and to a lesser extent to that of à propos de), and hypothesize that à propos has progressively replaced à ce propos in certain contexts, while also developing in contexts of more abrupt discourse shift. I finally address the issue of the interpretation of à propos as a case of grammaticalization, and show that there are sufficiently convincing arguments to justify its being analyzed as such. I also discuss the relevance of introducing the notion of pragmaticalization, and argue for this being a mere subclass of grammaticalization, though pertaining more specifically to the pragmatic area.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Prevost, Sophie},
	year = {2011},
	pages = {391--413},
	file = {Full Text PDF:/home/user/Zotero/storage/D26JNQSN/Prevost - 2011 - A propos from verbal complement to discourse marke.pdf:application/pdf}
}

@article{simon-vandenbergen-crosslinguistic-2011,
	title = {Crosslinguistic data as evidence in the grammaticalization debate: {The} case of discourse markers},
	volume = {49},
	shorttitle = {Crosslinguistic data as evidence in the grammaticalization debate},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.010/ling.2011.010.xml?format=INT},
	doi = {10.1515/ling.2011.010},
	abstract = {This article examines two case studies of cognate expressions in English and in French, which have developed partly in the same and partly in different directions. One case is the pair actually: actuellement, the other is the set in fact: en fait/de fait/au fait. Monolingual research on their present-day meanings and the study of their translation paradigms bring to light semantic and pragmatic overlap as well as differences between the members of each set. The study also looks at their historical development and compares the stages the expressions have gone through in the two languages concerned. The diachronic data indicate partially parallel paths of development, with salient divergences in some cases.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Simon-Vandenbergen, Anne-Marie and Willems, Dominique},
	year = {2011},
	pages = {333--364}
}

@article{degand-introduction:-2011,
	title = {Introduction: {Grammaticalization} and (inter)subjectification of discourse markers},
	volume = {49},
	shorttitle = {Introduction},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.008/ling.2011.008.xml?format=INT},
	doi = {10.1515/ling.2011.008},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Degand, Liesbeth and Vandenbergen, Anne-Marie Simon},
	year = {2011},
	pages = {287--294},
	file = {Full Text PDF:/home/user/Zotero/storage/IAS6PTYQ/Degand and Vandenbergen - 2011 - Introduction Grammaticalization and (inter)subjec.pdf:application/pdf}
}

@article{evers-vermeul-historical-2011,
	title = {Historical and comparative perspectives on subjectification: {A} corpus-based analysis of {Dutch} and {French} causal connectives},
	volume = {49},
	shorttitle = {Historical and comparative perspectives on subjectification},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.014/ling.2011.014.xml?format=INT},
	doi = {10.1515/ling.2011.014},
	abstract = {In this article, we focus on the diachronic development of causal connectives and investigate whether subjectification occurs. We present the results of ongoing and previous corpus-based analyses of the diachronic development of Dutch want and omdat, and French car and parce que, all four causal connectives roughly meaning ‘because’. In addition, we try to show that “grammaticalization studies can gain from the systematic and principled use of large computerized corpora and the methods which have been developed within corpus linguistics” (Lindquist and Mair, Corpus approaches to grammaticalization in English, John Benjamins, 2004: x). That's why we have combined two historical and two comparative corpus methods to chart the diachronic development of these four causals. Our study reveals that subjectification is not an integral part of the diachronic development of these causals: subjectification does occur in the rise of these connectives, but in the later stages of their development only parce que undergoes subjectification. Our analyses show that the four methods all have their own merits and limitations, but they are most effective when combined.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Evers-Vermeul, Jacqueline and Degand, Liesbeth and Fagard, Benjamin and Mortier, Liesbeth},
	year = {2011},
	pages = {445--478},
	file = {Full Text PDF:/home/user/Zotero/storage/MNWW3UGX/Evers-Vermeul et al. - 2011 - Historical and comparative perspectives on subject.pdf:application/pdf}
}

@article{lewis-discourse-constructional-2011,
	title = {A discourse-constructional approach to the emergence of discourse markers in {English}},
	volume = {49},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.013/ling.2011.013.xml?format=INT},
	doi = {10.1515/ling.2011.013},
	abstract = {This article argues that it is frequency and usage effects, arising in particular contexts and constructions, that lead to the emergence of new semantic and syntactic properties in expressions that become discourse markers (DMs). These changes, it is argued, are comparable to those that take place in the development of other grammaticalizing linguistic items. DMs are taken to be sentence adverbials that express discourse-relational predications. Data from the histories of two English DMs, ‘instead’ and ‘rather’, are examined. These contrastive DMs are often interchangeable in present-day English but have very different origins, ‘instead’ deriving from the phrase ‘in (the) stead of NP’, and ‘rather’ from the comparative VP-adverb rather, meaning ‘sooner’ or ‘faster’. In each of the two cases, there is seen to be increased type frequency (with context expansion) and reanalysis (wider scope). The diachronic evidence supports the idea that the reanalysis resulted from wider interpretation, not from prior change in word order. As DMs, both ‘instead’ and ‘rather’ become associated with high informational salience. Both undergo functional split, leading to new polysemies. Insofar as the newer, DM senses are closer to the grammatical end of the lexical-grammatical cline, the expressions can be said to have grammaticalized. There is no evidence that any qualitatively special changes are involved in the emergence of the DM uses, compared with the developments of the other polysemies of the expressions.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Lewis, Diana M.},
	year = {2011},
	pages = {415--443},
	file = {Full Text PDF:/home/user/Zotero/storage/CA9BAF93/Lewis - 2011 - A discourse-constructional approach to the emergen.pdf:application/pdf}
}

@article{diewald-pragmaticalization-2011,
	title = {Pragmaticalization (defined) as grammaticalization of discourse functions},
	volume = {49},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.011/ling.2011.011.xml?format=INT},
	doi = {10.1515/ling.2011.011},
	abstract = {The article discusses definitions of grammaticalization, pragmaticalization and (inter)subjectification in order to clarify the relations between these terms. While grammaticalization is defined as a functionally motivated, complex type of language change, (inter)subjectification is shown to be a specific type of semantic change. Pragmaticalization, finally, is argued to represent a subclass of grammaticalization, which displays essential core features of grammaticalization processes, but is distinguished from other subtypes of grammaticalization processes by specific characteristic traits (concerning function and domain as well as syntactic integration). This is demonstrated by a survey of the diachronic development of several modal particles in German (among them aber, eben, ruhig). The more general theoretical stance taken here is that the notion of grammar and hence grammaticalization has to be conceived broad enough in order to encompass this type of discourse functions.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Diewald, Gabriele},
	year = {2011},
	pages = {365--390},
	file = {Full Text PDF:/home/user/Zotero/storage/WX43B3GX/Diewald - 2011 - Pragmaticalization (defined) as grammaticalization.pdf:application/pdf}
}

@article{van-i-2011,
	title = {I think and other complement-taking mental predicates: {A} case of and for constructional grammaticalization},
	volume = {49},
	shorttitle = {I think and other complement-taking mental predicates},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.009/ling.2011.009.xml?format=INT},
	doi = {10.1515/ling.2011.009},
	abstract = {This article provides a critical assessment of previous claims that complement-taking mental predicates (CTMPs) like I think, I suppose, etc. are instances of grammaticalization. In so doing, it calls attention to the main problems one encounters when applying commonly agreed-upon grammaticalization criteria to CTMPs. It is demonstrated that the syntactic mobility of CTMPs is crucial to their decategorialization while being at odds with the parameter of positional fixation. In addition, CTMPs' ability to occur both in adverb-like, parenthetical positions, and in verb-like, clause-initial position, suggests that their decategorialization is incomplete. The possibility to reactivate productive verbal properties in expressions that display a high degree of formulaicity is explained in terms of grammatical persistence. Another challenge facing the grammaticalization of CTMPs is the existence of variation in terms of tense, aspect and modality. The aforementioned obstacles are documented by present-day spoken British English corpus data. It is argued that, rather than regarding them as pragmaticalized or lexicalized as has alternatively been suggested, CTMPs should be approached from the usage-based perspective of constructional grammaticalization, which is concerned with the grammaticalization of schematic constructions that are part of a wider taxonomy rather than being isolated sequences.},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Van, Bogaer Julie},
	year = {2011},
	pages = {295--332},
	file = {Full Text PDF:/home/user/Zotero/storage/UNI7WXTF/Van - 2011 - I think and other complement-taking mental predica.pdf:application/pdf}
}

@article{degand-introduction:-2011-1,
	title = {Introduction: {Grammaticalization} and (inter)subjectification of discourse markers},
	volume = {49},
	shorttitle = {Introduction},
	url = {https://www.degruyter.com/view/j/ling.2011.49.issue-2/ling.2011.008/ling.2011.008.xml},
	doi = {10.1515/ling.2011.008},
	number = {2},
	urldate = {2018-01-11},
	journal = {Linguistics},
	author = {Degand, Liesbeth and Vandenbergen, Anne-Marie Simon},
	year = {2011},
	pages = {287--294},
	file = {Full Text PDF:/home/user/Zotero/storage/6NY4DSZA/Degand and Vandenbergen - 2011 - Introduction Grammaticalization and (inter)subjec.pdf:application/pdf}
}

@misc{al-two-nodate,
	title = {Two dimensions of subjective uncertainty: {Clues} from natural language. - {PubMed} - {NCBI}},
	shorttitle = {Two dimensions of subjective uncertainty},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/27442037},
	abstract = {J Exp Psychol Gen. 2016 Oct;145(10):1280-1297. Epub 2016 Jul 21.},
	urldate = {2018-01-10},
	author = {al, et, Ülkümen G.},
	file = {Snapshot:/home/user/Zotero/storage/TRBX6LWY/27442037.html:text/html}
}

@article{ulkumen-two-2016,
	title = {Two dimensions of subjective uncertainty: {Clues} from natural language},
	volume = {145},
	issn = {1939-2222},
	shorttitle = {Two dimensions of subjective uncertainty},
	doi = {10.1037/xge0000202},
	abstract = {We argue that people intuitively distinguish epistemic (knowable) uncertainty from aleatory (random) uncertainty and show that the relative salience of these dimensions is reflected in natural language use. We hypothesize that confidence statements (e.g., “I am fairly confident,” “I am 90\% sure,” “I am reasonably certain”) communicate a subjective assessment of primarily epistemic uncertainty, whereas likelihood statements (e.g., “I believe it is fairly likely,” “I’d say there is a 90\% chance,” “I think there is a high probability”) communicate a subjective assessment of primarily aleatory uncertainty. First, we show that speakers tend to use confidence statements to express epistemic uncertainty and they tend to use likelihood statements to express aleatory uncertainty; we observe this in a 2-year sample of New York Times articles (Study 1), and in participants’ explicit choices of which statements more naturally express different uncertain events (Studies 2A and 2B). Second, we show that when speakers apply confidence versus likelihood statements to the same events, listeners infer different reasoning (Study 3): confidence statements suggest epistemic rationale (singular reasoning, feeling of knowing, internal control), whereas likelihood statements suggest aleatory rationale (distributional reasoning, relative frequency information, external control). Third, we show that confidence versus likelihood statements can differentially prompt epistemic versus aleatory thoughts, respectively, as observed when participants complete sentences that begin with confidence versus likelihood statements (Study 4) and when they quantify these statements based on feeling-of-knowing (epistemic) and frequency (aleatory) information (Study 5).},
	language = {eng},
	number = {10},
	journal = {Journal of Experimental Psychology. General},
	author = {Ülkümen, Gülden and Fox, Craig R. and Malle, Bertram F.},
	month = oct,
	year = {2016},
	pmid = {27442037},
	keywords = {Humans, Adult, Female, Male, Young Adult, Probability, Choice Behavior, Language, Students, Thinking, Uncertainty},
	pages = {1280--1297}
}

@article{vasishth-bayesian-2017,
	title = {Bayesian {Hierarchical} {Finite} {Mixture} {Models} of {Reading} {Times}: {A} {Case} {Study}},
	shorttitle = {Bayesian {Hierarchical} {Finite} {Mixture} {Models} of {Reading} {Times}},
	url = {https://psyarxiv.com/a4hs9/},
	doi = {10.17605/OSF.IO/A4HS9},
	abstract = {We present a case study demonstrating the importance of Bayesian hierarchical mixture models as a modelling tool for evaluating the predictions of competing theories of cognitive processes. As a case study, we revisit two published data sets from psycholinguistics. In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the time taken to complete the dependency (e.g., Gibson 2000). An alternative theory, direct access (McElree, 1993), assumes that retrieval times are a mixture of two distributions (Nicenboim \& Vasishth, 2017): one distribution represents successful retrievals and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. Here, dependency distance has the effect that in long-distance conditions the proportion of reanalyses is higher. We implement both theories as Bayesian hierarchical models and show that the direct-access model fits the Chinese relative clause reading time data better than the dependency-distance account. This work makes several novel contributions. First, we demonstrate how the researcher can reason about the underlying generative process of their data, thereby expressing the underlying cognitive process as a statistical model. Second, we show how models that have been developed in an exploratory manner to represent different underlying generative processes can be compared in terms of their predictive performance, using both K-fold cross validation on existing data, and using completely new data. Finally, we show how the models can be evaluated using simulated data.},
	urldate = {2018-01-08},
	journal = {PsyArXiv},
	author = {Vasishth, Shravan and Nicenboim, Bruno and Chopin, Nicolas and Ryder, Robin},
	month = jul,
	year = {2017}
}

@book{chen-monte-2001,
	title = {Monte {Carlo} {Methods} in {Bayesian} {Computation}},
	isbn = {978-0-387-98935-8},
	abstract = {Sampling from the posterior distribution and computing posterior quanti ties of interest using Markov chain Monte Carlo (MCMC) samples are two major challenges involved in advanced Bayesian computation. This book examines each of these issues in detail and focuses heavily on comput ing various posterior quantities of interest from a given MCMC sample. Several topics are addressed, including techniques for MCMC sampling, Monte Carlo (MC) methods for estimation of posterior summaries, improv ing simulation accuracy, marginal posterior density estimation, estimation of normalizing constants, constrained parameter problems, Highest Poste rior Density (HPD) interval calculations, computation of posterior modes, and posterior computations for proportional hazards models and Dirichlet process models. Also extensive discussion is given for computations in volving model comparisons, including both nested and nonnested models. Marginal likelihood methods, ratios of normalizing constants, Bayes fac tors, the Savage-Dickey density ratio, Stochastic Search Variable Selection (SSVS), Bayesian Model Averaging (BMA), the reverse jump algorithm, and model adequacy using predictive and latent residual approaches are also discussed. The book presents an equal mixture of theory and real applications.},
	language = {en},
	publisher = {Springer New York},
	author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G.},
	month = oct,
	year = {2001},
	note = {Google-Books-ID: R3GeFfshc7wC},
	keywords = {Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Medical / Biostatistics}
}

@article{watanabe-widely-2013,
	title = {A {Widely} {Applicable} {Bayesian} {Information} {Criterion}},
	volume = {14},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.org/papers/v14/watanabe13a.html},
	number = {Mar},
	urldate = {2017-11-25},
	journal = {Journal of Machine Learning Research},
	author = {Watanabe, Sumio},
	year = {2013},
	pages = {867--897},
	file = {Full Text PDF:/home/user/Zotero/storage/LE2JIXNM/Watanabe - 2013 - A Widely Applicable Bayesian Information Criterion.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/IAIW5SZF/watanabe13a.html:text/html}
}

@incollection{seiler-determination:-1978,
	address = {T{\"u}bingen},
	title = {Determination: {A} {Functional} {Dimension} for {Interlanguage} {Comparison}},
	booktitle = {Language {Universals}, {Papers} from the {Conference} held at {Gummersbach}/{Cologne}, {Germany}, {October} 3-8, 1976},
	publisher = {Narr},
	author = {Seiler, Hansjakob},
	editor = {Seiler, Hansjakob},
	year = {1978},
	pages = {301--328}
}

@article{lasersohn-context-2005,
	title = {Context {Dependence}, {Disagreement}, and {Predicates} of {Personal} {Taste}},
	volume = {28},
	issn = {0165-0157, 1573-0549},
	url = {https://link.springer.com/article/10.1007/s10988-005-0596-x},
	doi = {10.1007/s10988-005-0596-x},
	abstract = {This paper argues that truth values of sentences containing predicates of “personal taste” such as fun or tasty must be relativized to individuals. This relativization is of truth value only, and does not involve a relativization of semantic content: If you say roller coasters are fun, and I say they are not, I am negating the same content which you assert, and directly contradicting you. Nonetheless, both our utterances can be true (relative to their separate contexts). A formal semantic theory is presented which gives this result by introducing an individual index, analogous to the world and time indices commonly used, and by treating the pragmatic context as supplying a particular value for this index. The context supplies this value in the derivation of truth values from content, not in the derivation of content from character. Predicates of personal taste therefore display a kind of contextual variation in interpretation which is unlike the familiar variation exhibited by pronouns and other indexicals.},
	language = {en},
	number = {6},
	urldate = {2017-11-01},
	journal = {Linguistics and Philosophy},
	author = {Lasersohn, Peter},
	month = dec,
	year = {2005},
	pages = {643--686},
	file = {Full Text PDF:/home/user/Zotero/storage/WRMW5D7M/Lasersohn - 2005 - Context Dependence, Disagreement, and Predicates o.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/W8GBF6SN/s10988-005-0596-x.html:text/html;Snapshot:/home/user/Zotero/storage/U66IIFRI/s10988-005-0596-x.html:text/html}
}

@article{futrell-large-scale-2015-2,
	title = {Large-scale evidence of dependency length minimization in 37 languages},
	volume = {112},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547262/},
	doi = {10.1073/pnas.1502134112},
	abstract = {We provide the first large-scale, quantitative, cross-linguistic evidence for a universal syntactic property of languages: that dependency lengths are shorter than chance. Our work supports long-standing ideas that speakers prefer word orders with short dependency lengths and that languages do not enforce word orders with long dependency lengths. Dependency length minimization is well motivated because it allows for more efficient parsing and generation of natural language. Over the last 20 y, the hypothesis of a pressure to minimize dependency length has been invoked to explain many of the most striking recurring properties of languages. Our broad-coverage findings support those explanations., Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length—the distance between syntactically related words in a sentence—is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.},
	number = {33},
	urldate = {2017-11-06},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Futrell, Richard and Mahowald, Kyle and Gibson, Edward},
	year = {2015},
	pmid = {26240370},
	pmcid = {PMC4547262},
	pages = {10336--10341},
	file = {Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf:/home/user/Zotero/storage/PDA78C7J/Futrell et al_2015_Large-scale evidence of dependency length minimization in 37 languages.pdf:application/pdf}
}

@article{fassi-fehri-arabic-1999,
	title = {Arabic {Modifying} {Adjectives} and {DP} {Structures}},
	volume = {53},
	issn = {1467-9582},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9582.00042/abstract},
	doi = {10.1111/1467-9582.00042},
	abstract = {Important characteristics of the Arabic adjectival system are investigated, in view of the question of how the DP system is organized. A first series of issues include: (a) how adjective serialization observes quasi-universal hierarchical ordering restrictions (or their mirror image order), (b) how adjectives and other modifiers and determiners alternate in postnominal and prenominal positions, and (c) how distributional classes of adjectives relate to attributive/predicative, or head/modifier uses. A second series concerns (d) inflectional properties of adjectives (including Case, Definiteness, and Number and Gender features), and their Head/Spec dependent status. Such questions are approached through postulating an articulated (fissioned) DP structure. Structural grounds are provided for checking various inflectional features in hierarchically ordered, but autonomous DP domains. AP (or A) movement, as well as N and Possessor raisings, are independently motivated. Cross-linguistic variation follows, depending on whether all, some, or none of these processes are involved, to yield convergent derivations. APs (along with NPs) are treated as DPs, taking into account their inflectional and interpretational behaviours. Definiteness inheritance and Genitive checking are reanalyzed in view of new empirical and theoretical considerations of synthetic possessive structures.},
	language = {en},
	number = {2},
	urldate = {2017-11-15},
	journal = {Studia Linguistica},
	author = {Fassi Fehri, Abdelkader},
	year = {1999},
	pages = {105--154},
	file = {Full Text PDF:/home/user/Zotero/storage/8WZIWP6H/Fehri - 1999 - Arabic Modifying Adjectives and DP Structures.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/552CVQN4/abstract.html:text/html}
}

@book{cinque-syntax-2010,
	title = {The syntax of adjectives: {A} comparative study},
	publisher = {MIT Press},
	author = {Cinque, Guglielmo},
	year = {2010}
}

@book{cinque-adverbs-1999,
	title = {Adverbs and {Functional} {Heads}: {A} {Cross}-{Linguistic} {Perspective}},
	isbn = {978-0-19-535405-8},
	shorttitle = {Adverbs and {Functional} {Heads}},
	abstract = {One of the world's leading syntacticians presents evidence for locating Adverb Phrases in the specifiers of distinct functional projections within a novel and well articulated theory of the clause. In this theory, both adverbs and heads, which encode the functional notions of the clause, are ordered in a rigid sequence. Cinques cutting-edge proposal suggests that the structure of natural language sentences is much richer than previously assumed.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Cinque, Guglielmo},
	month = mar,
	year = {1999},
	note = {Google-Books-ID: sFd79vhd\_n0C},
	keywords = {Language Arts \& Disciplines / Linguistics / General, Language Arts \& Disciplines / Linguistics / Syntax}
}

@article{bates-fitting-2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} lme4},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i01/},
	doi = {10.18637/jss.v067.i01},
	language = {en},
	number = {1},
	urldate = {2017-07-31},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	file = {v67i01.pdf:/home/user/Zotero/storage/E3N9C7KJ/v67i01.pdf:application/pdf}
}

@article{kolbel-faultless-2004,
	title = {Faultless {Disagreement}},
	volume = {104},
	journal = {Proceedings of the Aristotelian Society},
	author = {K{\"o}lbel, Max},
	year = {2004},
	pages = {53--73}
}

@book{kolbel-truth-2002,
	title = {Truth {Without} {Objectivity}},
	isbn = {978-0-415-27245-2},
	abstract = {Truth without Objectivity provides a critique of the mainstream view of 'meaning'. Kölbel examines the standard solutions to the conflict implicit in this view, demonstrating their inadequacy and developing instead his own relativist theory of truth.The mainstream view of meaning assumes that understanding a sentence's meaning implies knowledge of the conditions required for it to be true. This view is challenged by taste judgements, which have meaning, but seem to be neither true nor false.},
	language = {en},
	publisher = {Psychology Press},
	author = {K{\"o}lbel, Max},
	year = {2002},
	keywords = {Philosophy / Criticism, Philosophy / Epistemology}
}

@article{graff-english-2003-1,
	title = {English gigaword corpus},
	journal = {Linguistic Data Consortium},
	author = {Graff, David and Cieri, C},
	year = {2003}
}

@inproceedings{zhu-aligning-2015-1,
	title = {Aligning books and movies: {Towards} story-like visual explanations by watching movies and reading books},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	year = {2015},
	pages = {19--27}
}

@inproceedings{kneser-improved-1995,
	title = {Improved backing-off for m-gram language modeling},
	volume = {1},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 1995. {ICASSP}-95., 1995 {International} {Conference} on},
	publisher = {IEEE},
	author = {Kneser, Reinhard and Ney, Hermann},
	year = {1995},
	pages = {181--184}
}

@article{eckert-three-2012,
	title = {Three waves of variation study: {The} emergence of meaning in the study of sociolinguistic variation},
	volume = {41},
	journal = {Annual review of Anthropology},
	author = {Eckert, Penelope},
	year = {2012},
	pages = {87--100}
}

@book{potts-logic-2005,
	title = {The logic of conventional implicatures},
	number = {7},
	publisher = {Oxford University Press on Demand},
	author = {Potts, Christopher},
	year = {2005}
}

@article{chung-empirical-2014-1,
	title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
	journal = {arXiv preprint arXiv:1412.3555},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	year = {2014}
}

@article{parker-chinese-2011-1,
	title = {Chinese {Gigaword} fifth edition {LDC}2011T13},
	journal = {Linguistic Data Consortium},
	author = {Parker, R and Graff, D and Chen, K and Kong, J and Maeda, K},
	year = {2011}
}

@article{parker-arabic-2011-1,
	title = {Arabic {Gigaword} fifth edition {LDC}2011T11},
	journal = {Philadelphia: Linguistic Data Consortium},
	author = {Parker, Robert and Graff, David and Chen, Ke and Kong, Junbo and Maeda, Kazuaki},
	year = {2011}
}

@inproceedings{godfrey-switchboard:-1992,
	title = {{SWITCHBOARD}: {Telephone} speech corpus for research and development},
	volume = {1},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 1992. {ICASSP}-92., 1992 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Godfrey, John J and Holliman, Edward C and McDaniel, Jane},
	year = {1992},
	pages = {517--520}
}

@book{ziff-semantic-1960,
	address = {Ithaca, NY},
	title = {Semantic {Analysis}},
	publisher = {Cornell University Press},
	author = {Ziff, P},
	year = {1960}
}

@incollection{scott-stacked-2002,
	address = {Oxford},
	title = {Stacked adjectival modification and the structure of nominal phrases},
	booktitle = {The cartography of syntactic structures, {Volume} 1: {Functional} structure in the {DP} and {IP}},
	publisher = {Oxford University Press},
	author = {Scott, G.-J.},
	editor = {Cinque, G},
	year = {2002},
	pages = {91--120}
}

@incollection{mcnally-modification-2016,
	address = {Cambridge},
	title = {Modification},
	booktitle = {Cambridge {Handbook} of {Formal} {Semantics}},
	publisher = {Cambridge University Press},
	author = {McNally, Louise},
	editor = {Aloni, M. and Dekker, P.},
	year = {2016}
}

@inproceedings{hill-beauty-2012,
	title = {Beauty {Before} {Age}? {Applying} {Subjectivity} to {Automatic} {English} {Adjective} {Ordering}},
	booktitle = {Proceedings of the {NAACL} {HLT} 2012 {Student} {Research} {Workshop}},
	author = {Hill, Felix},
	year = {2012},
	pages = {11--16}
}

@article{wulff-multifactorial-2003,
	title = {A multifactorial corpus analysis of adjective order in {English}},
	volume = {8},
	issn = {13846655},
	doi = {10.1075/ijcl.8.2.04wul},
	abstract = {This paper is concerned with the question of which factors govern prenominal adjective order (AO) in English. In particular, the analysis aims to overcome shortfalls of previous analyses by, firstly, adopting a multifactorial approach integrating all variables postulated in the literature, thereby doing justice to the well-established fact that cognitive and psychological processes are multivariate and complex. Secondly, the phenomenon is investigated on the basis of a large corpus, rendering the results obtained more representative and valid of naturally occurring language than those of previous studies. To this end, corpus-linguistic operationalizations of phonological, syntactic, semantic and pragmatic determinants of AO are devised and entered into a Linear Discriminant Analysis, which determines the relative influence of all variables (semantic variables being most important) and yields a classification accuracy of 78\%. Moreover, by means of the operationalizations developed in this analysis, the ordering of yet unanalyzed adjective strings can be predicted with about equal accuracy (73.5\%). \copyright John Benjamins Publishing Company.},
	number = {2},
	journal = {International Journal of Corpus Linguistics},
	author = {Wulff, Stefanie},
	year = {2003},
	keywords = {adjective order, linear discriminant analysis, multifactorial},
	pages = {245--282}
}

@incollection{svenonius-position-2008,
	address = {Oxford},
	title = {The position of adjectives and other phrasal modifiers in the decomposition of {DP}},
	booktitle = {Adjectives and {Adverbs}: {Syntax}, {Semantics}, and {Discourse}},
	publisher = {Oxford University Press},
	author = {Svenonius, Peter},
	editor = {McNally, L and Kennedy, C},
	year = {2008},
	pages = {16--42}
}

@incollection{sproat-cross-linguistic-1991,
	address = {Dordrecht},
	title = {The cross-linguistic distribution of adjective ordering restrictions},
	booktitle = {Interdisciplinary approaches to language: {Essays} in honor of {S}.-{Y}. {Kuroda}},
	publisher = {Kluwer Academic Publishers},
	author = {Sproat, R and Shih, C},
	editor = {Georgopoulos, C and Ishihara, R},
	year = {1991},
	pages = {565--593}
}

@book{dixon-where-1982,
	address = {Berlin},
	title = {Where have all the adjectives gone? {And} other essays in semantics and syntax},
	publisher = {Mouton},
	author = {Dixon, RMW},
	year = {1982}
}

@article{glanzberg-context-2007,
	title = {Context, content, and relativism},
	volume = {136},
	number = {1},
	journal = {Philosophical Studies},
	author = {Glanzberg, Michael},
	year = {2007},
	pages = {1--29}
}

@article{stojanovic-talking-2007,
	title = {Talking about taste: disagreement, implicit arguments, and relative truth},
	volume = {30},
	number = {6},
	journal = {Linguistics and Philosophy},
	author = {Stojanovic, Isidora},
	year = {2007},
	pages = {691--706}
}

@article{sa-ebo-judgment-2009,
	title = {Judgment ascriptions},
	volume = {32},
	number = {4},
	journal = {Linguistics and philosophy},
	author = {S\a ebø, Kjell Johan},
	year = {2009},
	pages = {327--352}
}

@inproceedings{larson-events-1998-1,
	title = {Events and modification in nominals},
	volume = {8},
	booktitle = {Semantics and {Linguistic} {Theory}},
	author = {Larson, Richard K},
	year = {1998},
	pages = {145--168}
}

@phdthesis{stephenson-towards-2007,
	title = {Towards a theory of subjective meaning},
	school = {Massachusetts Institute of Technology},
	author = {Stephenson, Tamina C},
	year = {2007}
}

@incollection{hetzron-relative-1978,
	address = {T{\"u}bingen},
	title = {On the relative order of adjectives},
	booktitle = {Language {Universals}},
	author = {Hetzron, Robert},
	year = {1978},
	pages = {165--184}
}

@article{cinque-cartography-2009,
	title = {The {Cartography} of {Syntactic} {Structures}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199544004.001.0001/oxfordhb-9780199544004-e-003},
	doi = {10.1093/oxfordhb/9780199544004.013.0003},
	abstract = {Syntactic structures are complex objects, whose subtle properties have been highlighted and elucidated by half a century of formal syntactic studies, building on a much older tradition. Their cartography is an attempt to draw maps as precise and detailed as possible of syntactic configurations. Broadly constructed in this way, it is not an approach or a hypothesis, but a research topic asking the question: what are the right structural maps for natural language syntax? This chapter discusses the following: inventory of functional categories, comparative syntax and typology, cartography and minimalism; and hierarchies, syntax, and semantics.},
	urldate = {2017-11-21},
	author = {Cinque, Guglielmo and Rizzi, Luigi},
	month = dec,
	year = {2009},
	file = {Snapshot:/home/user/Zotero/storage/DUQ5JYYM/oxfordhb-9780199544004-e-003.html:text/html}
}

@book{cinque-typological-2014,
	title = {Typological {Studies}: {Word} {Order} and {Relative} {Clauses}},
	isbn = {978-1-317-69123-5},
	shorttitle = {Typological {Studies}},
	abstract = {In this book, Cinque takes a generative perspective on typological questions relating to word order and to the syntax of relative clauses. In particular, Cinque looks at: the position of the Head vis à vis the relative clause in relation to the position of the verb vis à vis his object; a general cross-linguistic analysis of correlatives; the need to distinguish a sentence-grammar, from a discourse-grammar, type of non-restrictives (with languages differing as to whether they possess both, one, the other, or neither); a selective type of extraction from relative clauses; and a tentative sketch of a more ample work in progress on a unified analysis of externally headed, internally headed, and headless relative clauses.},
	language = {en},
	publisher = {Routledge},
	author = {Cinque, Guglielmo},
	month = feb,
	year = {2014},
	note = {Google-Books-ID: eSTKAgAAQBAJ},
	keywords = {Language Arts \& Disciplines / Linguistics / General, Language Arts \& Disciplines / Linguistics / Syntax, Language Arts \& Disciplines / Linguistics / Semantics}
}

@book{macfarlane-assessment-2014,
	title = {Assessment {Sensitivity}: {Relative} {Truth} and its {Applications} - {Oxford} {Scholarship}},
	shorttitle = {Assessment {Sensitivity}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199682751.001.0001/acprof-9780199682751},
	urldate = {2017-11-21},
	publisher = {Oxford University Press},
	author = {MacFarlane, John},
	year = {2014},
	file = {Snapshot:/home/user/Zotero/storage/WJFZKEQY/acprof-9780199682751.html:text/html}
}

@incollection{sproat-cross-linguistic-1991-1,
	address = {Dordrecht},
	title = {The {Cross}-{Linguistic} {Distribution} of {Adjective} {Ordering} {Restrictions}},
	isbn = {978-94-011-3818-5},
	url = {https://doi.org/10.1007/978-94-011-3818-5\_30},
	abstract = {English displays well-known restrictions on the ordering of multiple prenominal adjectival modifiers (see Bloomfield, 1933; Whorf, 1945; Lance, 1968; Vendler, 1968; Quirk et al, 1972 among numerous others). Most descriptions include a hierarchy such as the following: QUALITY {\textgreater} SIZE {\textgreater} SHAPE {\textgreater} COLOR {\textgreater} PROVENANCE.1},
	booktitle = {Interdisciplinary {Approaches} to {Language}: {Essays} in {Honor} of {S}.-{Y}. {Kuroda}},
	publisher = {Springer Netherlands},
	author = {Sproat, Richard and Shih, Chilin},
	editor = {Georgopoulos, Carol and Ishihara, Roberta},
	year = {1991},
	doi = {10.1007/978-94-011-3818-5\_30},
	pages = {565--593}
}

@misc{noauthor-cross-linguistic-nodate-1,
	title = {The {Cross}-{Linguistic} {Distribution} of {Adjective} {Ordering} {Restrictions} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-94-011-3818-5\_30},
	urldate = {2017-11-15},
	file = {The Cross-Linguistic Distribution of Adjective Ordering Restrictions | SpringerLink:/home/user/Zotero/storage/EI56EX3B/978-94-011-3818-5_30.html:text/html}
}

@article{alexiadou-adjective-2001,
	title = {Adjective {Syntax} and {Noun} {Raising}: {Word} {Order} {Asymmetries} in the {DP} as the {Result} of {Adjective} {Distribution}},
	volume = {55},
	issn = {1467-9582},
	shorttitle = {Adjective {Syntax} and {Noun} {Raising}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9582.00080/abstract},
	doi = {10.1111/1467-9582.00080},
	abstract = {The paper investigates the triggers and the motivation for noun raising in combination with the interpretation and placement of adjectives within the DP across languages. I point out that first the nature of noun movement is empirically and conceptually problematic. Second, the post-nominal position of adjectives in languages for which it has been argued to result from noun movement is consistent with an analysis of those that capitalizes on their predicative source. Thus the cross-linguistic asymmetry concerning the relative order of nouns with respect to adjectives has its source in the syntactic configurations available in UG for adjectival modification and is not a result of syntactic head raising within the DP.},
	language = {en},
	number = {3},
	urldate = {2017-11-15},
	journal = {Studia Linguistica},
	author = {Alexiadou, Artemis},
	month = dec,
	year = {2001},
	pages = {217--248},
	file = {Full Text PDF:/home/user/Zotero/storage/3NCNMWB6/Alexiadou - 2001 - Adjective Syntax and Noun Raising Word Order Asym.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9UFEVUFV/abstract.html:text/html}
}

@article{harsha-communication-nodate,
	title = {The {Communication} {Complexity} of {Correlation}},
	abstract = {We examine the communication required for generating random variables remotely. One party Alice will be given a distribution D, and she has to send a message to Bob, who is then required to generate a value with distribution exactly D. Alice and Bob are allowed to share random bits generated without the knowledge of D. There are two settings based on how the distribution D provided to Alice is chosen.},
	language = {en},
	author = {Harsha, Prahladh and Jain, Rahul and McAllester, David and Radhakrishnan, Jaikumar},
	pages = {20},
	file = {Harsha et al. - The Communication Complexity of Correlation ∗.pdf:/home/user/Zotero/storage/5BR7LVGH/Harsha et al. - The Communication Complexity of Correlation ∗.pdf:application/pdf}
}

@article{lohr-generative-2009,
	title = {{ON} {THE} {GENERATIVE} {NATURE} {OF} {PREDICTION}},
	volume = {12},
	issn = {0219-5259, 1793-6802},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0219525909002143},
	doi = {10.1142/S0219525909002143},
	abstract = {Given an observed stochastic process, computational mechanics provides an explicit and eﬃcient method of constructing a minimal hidden Markov model within the class of maximally predictive models. Here, the corresponding so-called ε-machine encodes the mechanisms of prediction. We propose an alternative notion of predictive models in terms of a hidden Markov model capable of generating the underlying stochastic process. A comparison of these two notions of prediction reveals that our approach is less restrictive and thereby allows for predictive models that are more concise than the ε-machine.},
	language = {en},
	number = {02},
	urldate = {2018-06-03},
	journal = {Advances in Complex Systems},
	author = {LöHr, Wolfgang and Ay, Nihat},
	month = apr,
	year = {2009},
	pages = {169--194},
	file = {LöHr and Ay - 2009 - ON THE GENERATIVE NATURE OF PREDICTION.pdf:/home/user/Zotero/storage/B9JRXDJZ/LöHr and Ay - 2009 - ON THE GENERATIVE NATURE OF PREDICTION.pdf:application/pdf}
}

@article{lohr-properties-2009,
	title = {Properties of the {Statistical} {Complexity} {Functional} and {Partially} {Deterministic} {HMMs}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/11/3/385},
	doi = {10.3390/e110300385},
	abstract = {Statistical complexity is a measure of complexity of discrete-time stationary stochastic processes, which has many applications. We investigate its more abstract properties as a non-linear function of the space of processes and show its close relation to the Knight’s prediction process. We prove lower semi-continuity, concavity, and a formula for the ergodic decomposition of statistical complexity. On the way, we show that the discrete version of the prediction process has a continuous Markov transition. We also prove that, given the past output of a partially deterministic hidden Markov model (HMM), the uncertainty of the internal state is constant over time and knowledge of the internal state gives no additional information on the future output. Using this fact, we show that the causal state distribution is the unique stationary representation on prediction space that may have finite entropy.},
	language = {en},
	number = {3},
	urldate = {2018-06-03},
	journal = {Entropy},
	author = {Löhr, Wolfgang},
	month = aug,
	year = {2009},
	keywords = {concavity, ergodic decomposition, lower semi-continuity, partially deterministic hidden Markov models (HMMs), prediction process, statistical complexity},
	pages = {385--401},
	file = {Full Text PDF:/home/user/Zotero/storage/EFM6G3IS/Löhr - 2009 - Properties of the Statistical Complexity Functiona.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/95TECST3/385.html:text/html}
}

@article{lohr-predictive-2012,
	title = {Predictive models and generative complexity},
	volume = {25},
	issn = {1009-6124, 1559-7067},
	url = {https://link.springer.com/article/10.1007/s11424-012-9173-x},
	doi = {10.1007/s11424-012-9173-x},
	abstract = {The causal states of computational mechanics define the minimal sufficient memory for a given discrete stationary stochastic process. Their entropy is an important complexity measure called statistical complexity (or true measure complexity). They induce the ɛ-machine, which is a hidden Markov model (HMM) generating the process. But it is not the minimal one, although generative HMMs also have a natural predictive interpretation. This paper gives a mathematical proof of the idea that the ɛ-machine is the minimal HMM with an additional (partial) determinism condition. Minimal internal state entropy of a generative HMM is in analogy to statistical complexity called generative complexity. This paper also shows that generative complexity depends on the process in a nice way. It is, as a function of the process, lower semi-continuous (w.r.t. weak-* topology), concave, and behaves nice under ergodic decomposition of the process.},
	language = {en},
	number = {1},
	urldate = {2018-06-03},
	journal = {Journal of Systems Science and Complexity},
	author = {Löhr, Wolfgang},
	month = feb,
	year = {2012},
	pages = {30--45},
	file = {Full Text PDF:/home/user/Zotero/storage/STRE737N/Löhr - 2012 - Predictive models and generative complexity.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TWDIHDH5/10.html:text/html}
}

@article{crutchfield-inferring-1989,
	title = {Inferring statistical complexity},
	volume = {63},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.63.105},
	doi = {10.1103/PhysRevLett.63.105},
	abstract = {Statistical mechanics is used to describe the observed information processing complexity of nonlinear dynamical systems. We introduce a measure of complexity distinct from and dual to the information theoretic entropies and dimensions. A technique is presented that directly reconstructs minimal equations of motion from the recursive structure of measurement sequences. Application to the period-doubling cascade demonstrates a form of superuniversality that refers only to the entropy and complexity of a data stream.},
	number = {2},
	urldate = {2018-06-03},
	journal = {Physical Review Letters},
	author = {Crutchfield, James P. and Young, Karl},
	month = jul,
	year = {1989},
	pages = {105--108},
	file = {APS Snapshot:/home/user/Zotero/storage/I29I2IKV/PhysRevLett.63.html:text/html}
}

@article{crutchfield-origins-2017,
	title = {The {Origins} of {Computational} {Mechanics}: {A} {Brief} {Intellectual} {History} and {Several} {Clarifications}},
	shorttitle = {The {Origins} of {Computational} {Mechanics}},
	url = {http://arxiv.org/abs/1710.06832},
	abstract = {The principle goal of computational mechanics is to define pattern and structure so that the organization of complex systems can be detected and quantified. Computational mechanics developed from efforts in the 1970s and early 1980s to identify strange attractors as the mechanism driving weak fluid turbulence via the method of reconstructing attractor geometry from measurement time series and in the mid-1980s to estimate equations of motion directly from complex time series. In providing a mathematical and operational definition of structure it addressed weaknesses of these early approaches to discovering patterns in natural systems. Since then, computational mechanics has led to a range of results from theoretical physics and nonlinear mathematics to diverse applications---from closed-form analysis of Markov and non-Markov stochastic processes that are ergodic or nonergodic and their measures of information and intrinsic computation to complex materials and deterministic chaos and intelligence in Maxwellian demons to quantum compression of classical processes and the evolution of computation and language. This brief review clarifies several misunderstandings and addresses concerns recently raised regarding early works in the field (1980s). We show that misguided evaluations of the contributions of computational mechanics are groundless and stem from a lack of familiarity with its basic goals and from a failure to consider its historical context. For all practical purposes, its modern methods and results largely supersede the early works. This not only renders recent criticism moot and shows the solid ground on which computational mechanics stands but, most importantly, shows the significant progress achieved over three decades and points to the many intriguing and outstanding challenges in understanding the computational nature of complex dynamic systems.},
	urldate = {2018-06-03},
	journal = {arXiv:1710.06832 [cond-mat, physics:nlin]},
	author = {Crutchfield, James P.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06832},
	keywords = {Computer Science - Learning, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Chaotic Dynamics},
	file = {arXiv\:1710.06832 PDF:/home/user/Zotero/storage/JD343ZP4/Crutchfield - 2017 - The Origins of Computational Mechanics A Brief In.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/35YNCY8W/1710.html:text/html}
}

@article{still-optimal-2007,
	title = {Optimal {Causal} {Inference}: {Estimating} {Stored} {Information} and {Approximating} {Causal} {Architecture}},
	shorttitle = {Optimal {Causal} {Inference}},
	url = {http://arxiv.org/abs/0708.1580},
	abstract = {We introduce an approach to inferring the causal architecture of stochastic dynamical systems that extends rate distortion theory to use causal shielding---a natural principle of learning. We study two distinct cases of causal inference: optimal causal filtering and optimal causal estimation. Filtering corresponds to the ideal case in which the probability distribution of measurement sequences is known, giving a principled method to approximate a system's causal structure at a desired level of representation. We show that, in the limit in which a model complexity constraint is relaxed, filtering finds the exact causal architecture of a stochastic dynamical system, known as the causal-state partition. From this, one can estimate the amount of historical information the process stores. More generally, causal filtering finds a graded model-complexity hierarchy of approximations to the causal architecture. Abrupt changes in the hierarchy, as a function of approximation, capture distinct scales of structural organization. For nonideal cases with finite data, we show how the correct number of underlying causal states can be found by optimal causal estimation. A previously derived model complexity control term allows us to correct for the effect of statistical fluctuations in probability estimates and thereby avoid over-fitting.},
	urldate = {2018-06-03},
	journal = {arXiv:0708.1580 [cond-mat, stat]},
	author = {Still, Susanne and Crutchfield, James P. and Ellison, Christopher J.},
	month = aug,
	year = {2007},
	note = {arXiv: 0708.1580},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics},
	file = {arXiv\:0708.1580 PDF:/home/user/Zotero/storage/ZPPMVJUJ/Still et al. - 2007 - Optimal Causal Inference Estimating Stored Inform.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/SVR7NQ6N/0708.html:text/html}
}

@incollection{zhou-non-sufficient-2009,
	address = {Berlin, Heidelberg},
	title = {Non-sufficient {Memories} {That} {Are} {Sufficient} for {Prediction}},
	volume = {4},
	isbn = {978-3-642-02465-8 978-3-642-02466-5},
	url = {http://link.springer.com/10.1007/978-3-642-02466-5\_25},
	abstract = {The causal states of computational mechanics deﬁne the minimal suﬃcient (prescient) memory for a given stationary stochastic process. They induce the ε-machine which is a hidden Markov model (HMM) generating the process. The ε-machine is, however, not the minimal generative HMM and minimal internal state entropy of a generative HMM is a tighter upper bound for excess entropy than provided by statistical complexity. We propose a notion of prediction that does not require suﬃciency. The corresponding models can be substantially smaller than the ε-machine and are closely related to generative HMMs.},
	language = {en},
	urldate = {2018-06-03},
	booktitle = {Complex {Sciences}},
	publisher = {Springer Berlin Heidelberg},
	author = {Löhr, Wolfgang and Ay, Nihat},
	editor = {Zhou, Jie},
	year = {2009},
	doi = {10.1007/978-3-642-02466-5\_25},
	pages = {265--276},
	file = {Löhr and Ay - 2009 - Non-sufficient Memories That Are Sufficient for Pr.pdf:/home/user/Zotero/storage/5VWGHHDR/Löhr and Ay - 2009 - Non-sufficient Memories That Are Sufficient for Pr.pdf:application/pdf}
}

@book{debowski-information-2013,
	address = {Warsaw},
	series = {Information technologies: research and their interdisciplinary applications - monograph series},
	title = {Information theory and statistics},
	isbn = {978-83-63159-06-1 978-83-63159-07-8},
	language = {en},
	number = {1},
	author = {Debowski, \Lukasz},
	year = {2013},
	file = {Debowski - 2013 - Information theory and statistics.pdf:/home/user/Zotero/storage/JS5C9IE6/Debowski - 2013 - Information theory and statistics.pdf:application/pdf}
}

@article{archer-bayesian-nodate,
	title = {Bayesian {Entropy} {Estimation} for {Countable} {Discrete} {Distributions}},
	abstract = {We consider the problem of estimating Shannon’s entropy H from discrete data, in cases where the number of possible symbols is unknown or even countably inﬁnite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably inﬁnite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over H can be computed analytically. We derive formulas for the posterior mean (Bayes’ least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a ﬁxed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over H, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous measures for mixing Pitman-Yor processes to produce an approximately ﬂat prior over H. We show that the resulting “Pitman-Yor Mixture” (PYM) entropy estimator is consistent for a large class of distributions. Finally, we explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.},
	language = {en},
	author = {Archer, Evan},
	pages = {36},
	file = {Archer - Bayesian Entropy Estimation for Countable Discrete.pdf:/home/user/Zotero/storage/EVH7Q947/Archer - Bayesian Entropy Estimation for Countable Discrete.pdf:application/pdf}
}

@article{larson-block-entropy-2011,
	series = {Proceedings of the {International} {Conference} on {Computational} {Science}, {ICCS} 2011},
	title = {Block-{Entropy} {Analysis} of {Climate} {Data}},
	volume = {4},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050911002304},
	doi = {10.1016/j.procs.2011.04.172},
	abstract = {We explore the use of block entropy as a dynamics classifier for meteorological timeseries data. The block entropy estimates define the entropy growth curve H(L) with respect to block length L. For a finitary process, the entropy growth curve tends to an asymptotic linear regime H(L) = E + hμL, with entropy rate hμ and excess entropy E. These quantities apportion the system's information content into ‘memory’ (E) and ‘randomness’ (hμ). We discuss the challenges inherent in analyzing weather data using symbolic techniques, identifying the pitfalls associated with alphabet size, finite sample timeseries length, and stationarity. We apply the block entropy-based techniques in the form of a wet/dry partition to Australian daily precipitation data from the Patched Point Dataset station record collection and version 3 of the Australian Water Availability Project analysis dataset. Preliminary results demonstrate hμ and E are viable climatological classifiers for precipitation, with station records from similar climatic regimes possessing similar values of hμ and E. The resultant clustering reflects expected characteristics of local climatic memory and randomness. The AWAP results show weaker clustering than their PPD counterparts, with different E- and hμ-values reflecting respectively the relative biases and truncation errors in the AWAP analysis system. The entropy rates of convergence analysis rules out finite order Markov processes for orders falling within the range of block sizes considered.},
	urldate = {2018-06-02},
	journal = {Procedia Computer Science},
	author = {Larson, J. Walter and Briggs, Peter R. and Tobis, Michael},
	month = jan,
	year = {2011},
	keywords = {Climate Predictibility, Model-data Evaluation, Symbolic Dynamics, Timeseries Analysis},
	pages = {1592--1601},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/W2SJ5PWT/Larson et al. - 2011 - Block-Entropy Analysis of Climate Data.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/MAT7E7T5/S1877050911002304.html:text/html}
}

@article{feldman-measures-1998,
	title = {Measures of statistical complexity: {Why}?},
	volume = {238},
	issn = {03759601},
	shorttitle = {Measures of statistical complexity},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0375960197008554},
	doi = {10.1016/S0375-9601(97)00855-4},
	language = {en},
	number = {4-5},
	urldate = {2018-06-02},
	journal = {Physics Letters A},
	author = {Feldman, David P and Crutchfield, James P},
	month = feb,
	year = {1998},
	pages = {244--252},
	file = {Feldman and Crutchfield - 1998 - Measures of statistical complexity Why.pdf:/home/user/Zotero/storage/43HMJ422/Feldman and Crutchfield - 1998 - Measures of statistical complexity Why.pdf:application/pdf}
}

@article{feldman-structural-2003,
	title = {Structural information in two-dimensional patterns: {Entropy} convergence and excess entropy},
	volume = {67},
	issn = {1063-651X, 1095-3787},
	shorttitle = {Structural information in two-dimensional patterns},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.67.051104},
	doi = {10.1103/PhysRevE.67.051104},
	language = {en},
	number = {5},
	urldate = {2018-06-02},
	journal = {Physical Review E},
	author = {Feldman, David P. and Crutchfield, James P.},
	month = may,
	year = {2003},
	file = {Feldman and Crutchfield - 2003 - Structural information in two-dimensional patterns.pdf:/home/user/Zotero/storage/2DCHB98U/Feldman and Crutchfield - 2003 - Structural information in two-dimensional patterns.pdf:application/pdf}
}

@misc{noauthor-notitle-nodate-4,
	url = {http://csc.ucdavis.edu/~chaos/chaos/pubs.htm},
	urldate = {2018-06-02},
	file = {:/home/user/Zotero/storage/ZZ8BLZK8/pubs.html:text/html}
}

@article{marzen-statistical-2015,
	title = {Statistical {Signatures} of {Structural} {Organization}: {The} case of long memory in renewal processes},
	shorttitle = {Statistical {Signatures} of {Structural} {Organization}},
	url = {https://arxiv.org/abs/1512.01859},
	doi = {10.1016/j.physleta.2016.02.052},
	language = {en},
	urldate = {2018-06-02},
	author = {Marzen, Sarah E. and Crutchfield, James P.},
	month = dec,
	year = {2015},
	file = {Full Text PDF:/home/user/Zotero/storage/SSYSX7W3/Marzen and Crutchfield - 2015 - Statistical Signatures of Structural Organization.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ULLG9U8P/1512.html:text/html}
}

@article{feldman-synchronizing-2004,
	title = {{SYNCHRONIZING} {TO} {PERIODICITY}: {THE} {TRANSIENT} {INFORMATION} {AND} {SYNCHRONIZATION} {TIME} {OF} {PERIODIC} {SEQUENCES}},
	volume = {07},
	issn = {0219-5259, 1793-6802},
	shorttitle = {{SYNCHRONIZING} {TO} {PERIODICITY}},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0219525904000196},
	doi = {10.1142/S0219525904000196},
	language = {en},
	number = {03n04},
	urldate = {2018-06-02},
	journal = {Advances in Complex Systems},
	author = {Feldman, David P. and Crutchfield, James P.},
	month = sep,
	year = {2004},
	pages = {329--355},
	file = {Feldman and Crutchfield - 2004 - SYNCHRONIZING TO PERIODICITY THE TRANSIENT INFORM.pdf:/home/user/Zotero/storage/L4NEAGX4/Feldman and Crutchfield - 2004 - SYNCHRONIZING TO PERIODICITY THE TRANSIENT INFORM.pdf:application/pdf}
}

@article{ellison-information-2011,
	title = {Information symmetries in irreversible processes},
	volume = {21},
	issn = {1054-1500, 1089-7682},
	url = {http://aip.scitation.org/doi/10.1063/1.3637490},
	doi = {10.1063/1.3637490},
	abstract = {We study dynamical reversibility in stationary stochastic processes from an information theoretic perspective. Extending earlier work on the reversibility of Markov chains, we focus on ﬁnitary processes with arbitrarily long conditional correlations. In particular, we examine stationary processes represented or generated by edge-emitting, ﬁnite-state hidden Markov models. Surprisingly, we ﬁnd pervasive temporal asymmetries in the statistics of such stationary processes with the consequence that the computational resources necessary to generate a process in the forward and reverse temporal directions are generally not the same. In fact, an exhaustive survey indicates that most stationary processes are irreversible. We study the ensuing relations between model topology in diﬀerent representations, the process’s statistical properties, and its reversibility in detail. A process’s temporal asymmetry is eﬃciently captured using two canonical uniﬁlar representations of the generating model, the forward-time and reverse-time -machines. We analyze example irreversible processes whose -machine representations change size under time reversal, including one which has a ﬁnite number of recurrent causal states in one direction, but an inﬁnite number in the opposite. From the forward-time and reverse-time -machines, we are able to construct a symmetrized, but nonuniﬁlar, generator of a process—the bidirectional machine. Using the bidirectional machine, we show how to directly calculate a process’s fundamental information properties, many of which are otherwise only poorly approximated via process samples. The tools we introduce and the insights we oﬀer provide a better understanding of the many facets of reversibility and irreversibility in stochastic processes.},
	language = {en},
	number = {3},
	urldate = {2018-06-02},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Ellison, Christopher J. and Mahoney, John R. and James, Ryan G. and Crutchfield, James P. and Reichardt, Jörg},
	month = sep,
	year = {2011},
	pages = {037107},
	file = {Ellison et al. - 2011 - Information symmetries in irreversible processes.pdf:/home/user/Zotero/storage/CWH9864X/Ellison et al. - 2011 - Information symmetries in irreversible processes.pdf:application/pdf}
}

@article{crutchfield-regularities-2001,
	title = {Regularities {Unseen}, {Randomness} {Observed}: {Levels} of {Entropy} {Convergence}},
	shorttitle = {Regularities {Unseen}, {Randomness} {Observed}},
	url = {http://arxiv.org/abs/cond-mat/0102181},
	abstract = {We study how the Shannon entropy of sequences produced by an information source converges to the source's entropy rate. We synthesize several phenomenological approaches to applying information theoretic measures of randomness and memory to stochastic and deterministic processes by using successive derivatives of the Shannon entropy growth curve. This leads, in turn, to natural measures of apparent memory stored in a source and the amounts of information that must be extracted from observations of a source in order for it to be optimally predicted and for an observer to synchronize to it. One consequence of ignoring these structural properties is that the missed regularities are converted to apparent randomness. We demonstrate that this problem arises particularly for small data sets; e.g., in settings where one has access only to short measurement sequences.},
	language = {en},
	urldate = {2018-06-02},
	journal = {arXiv:cond-mat/0102181},
	author = {Crutchfield, James P. and Feldman, David P.},
	month = feb,
	year = {2001},
	note = {arXiv: cond-mat/0102181},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {Crutchfield and Feldman - 2001 - Regularities Unseen, Randomness Observed Levels o.pdf:/home/user/Zotero/storage/YKT8WA3T/Crutchfield and Feldman - 2001 - Regularities Unseen, Randomness Observed Levels o.pdf:application/pdf}
}

@article{debowski-ergodic-nodate,
	title = {Ergodic decomposition of excess entropy and conditional mutual information∗},
	abstract = {The article discusses excess entropy deﬁned as mutual information between the past and future of a stationary process. The central result is an ergodic decomposition: Excess entropy is the sum of self-information of shift-invariant σ-ﬁeld and the average of excess entropies for the ergodic components of the process. The result is derived using generalized conditional mutual information for ﬁelds of events, developed in the paper anew. Some corollary of the ergodic decomposition is that excess entropy is inﬁnite for the class of processes with uncountably many ergodic components, called here uncountable description processes (UDP’s). UDP’s can be deﬁned without the use of measure theory and the article argues for their potential utility in linguistics. Moreover, it is shown that ﬁnite-order excess entropies (some approximations of excess entropy) are dominated by the expected excess lengths of any universal code. Hence, universal codes may be used for rough estimation of excess entropy. Nevertheless, the excess code lengths diverge to inﬁnity for almost every process with zero excess entropy, which is another corollary of the ergodic decomposition.},
	language = {en},
	author = {Debowski, Łukasz},
	pages = {20},
	file = {Debowski - Ergodic decomposition of excess entropy and condit.pdf:/home/user/Zotero/storage/FXU7GHUN/Debowski - Ergodic decomposition of excess entropy and condit.pdf:application/pdf}
}

@article{travers-infinite-2014,
	title = {Infinite {Excess} {Entropy} {Processes} with {Countable}-{State} {Generators}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/16/3/1396},
	doi = {10.3390/e16031396},
	abstract = {We present two examples of finite-alphabet, infinite excess entropy processes generated by stationary hidden Markov models (HMMs) with countable state sets. The first, simpler example is not ergodic, but the second is. These are the first explicit constructions of processes of this type.},
	language = {en},
	number = {3},
	urldate = {2018-06-02},
	journal = {Entropy},
	author = {Travers, Nicholas F. and Crutchfield, James P.},
	month = mar,
	year = {2014},
	keywords = {mutual information, entropy rate, epsilon-machine, ergodicity, excess entropy, hidden Markov model, stationary stochastic process},
	pages = {1396--1413},
	file = {Full Text PDF:/home/user/Zotero/storage/ATUVH9SN/Travers and Crutchfield - 2014 - Infinite Excess Entropy Processes with Countable-S.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/U95AHAE3/htm.html:text/html}
}

@article{haruna-permutation-2011,
	title = {Permutation {Excess} {Entropy} and {Mutual} {Information} between the {Past} and {Future}},
	url = {http://arxiv.org/abs/1112.2491},
	abstract = {We address the excess entropy, which is a measure of complexity for stationary time series, from the ordinal point of view. We show that the permutation excess entropy is equal to the mutual information between two adjacent semi-infinite blocks in the space of orderings for finite-state stationary ergodic Markov processes. This result may shed a new light on the relationship between complexity and anticipation.},
	urldate = {2018-06-02},
	journal = {arXiv:1112.2491 [physics]},
	author = {Haruna, Taichi and Nakajima, Kohei},
	month = dec,
	year = {2011},
	note = {arXiv: 1112.2491},
	keywords = {Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability, Nonlinear Sciences - Chaotic Dynamics},
	file = {arXiv\:1112.2491 PDF:/home/user/Zotero/storage/PYGFT2LF/Haruna and Nakajima - 2011 - Permutation Excess Entropy and Mutual Information .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/Q3GIT2I5/1112.html:text/html}
}

@article{debowski-hidden-2012,
	title = {On {Hidden} {Markov} {Processes} with {Infinite} {Excess} {Entropy}},
	url = {http://arxiv.org/abs/1211.0834},
	abstract = {We investigate stationary hidden Markov processes for which mutual information between the past and the future is infinite. It is assumed that the number of observable states is finite and the number of hidden states is countably infinite. Under this assumption, we show that the block mutual information of a hidden Markov process is upper bounded by a power law determined by the tail index of the hidden state distribution. Moreover, we exhibit three examples of processes. The first example, considered previously, is nonergodic and the mutual information between the blocks is bounded by the logarithm of the block length. The second example is also nonergodic but the mutual information between the blocks obeys a power law. The third example obeys the power law and is ergodic.},
	urldate = {2018-06-02},
	journal = {arXiv:1211.0834 [cs, math]},
	author = {Debowski, Łukasz},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.0834},
	keywords = {Computer Science - Information Theory, 60J10, 94A17, 37A25},
	file = {arXiv\:1211.0834 PDF:/home/user/Zotero/storage/F9ZT9WH2/Debowski - 2012 - On Hidden Markov Processes with Infinite Excess En.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/W8LDN2X8/1211.html:text/html}
}

@article{ding-entropic-2016,
	title = {An entropic characterization of long memory stationary process},
	url = {http://arxiv.org/abs/1604.05453},
	abstract = {Long memory or long range dependency is an important phenomenon that may arise in the analysis of time series or spatial data. Most of the definitions of long memory of a stationary process \$X=\{X\_1, X\_2,\cdots,\}\$ are based on the second-order properties of the process. The excess entropy of a stationary process is the summation of redundancies which relates to the rate of convergence of the conditional entropy \$H(X\_n{\textbar}X\_{n-1},\cdots, X\_1)\$ to the entropy rate. It is proved that the excess entropy is identical to the mutual information between the past and the future when the entropy \$H(X\_1)\$ is finite. We suggest the definition that a stationary process is long memory if the excess entropy is infinite. Since the definition of excess entropy of a stationary process requires very weak moment condition on the distribution of the process, it can be applied to processes whose distributions without bounded second moment. A significant property of excess entropy is that it is invariant under invertible transformation, which enables us to know the excess entropy of a stationary process from the excess entropy of other process. For stationary Guassian process, the excess entropy characterization of long memory relates to popular characterization well. It is proved that the excess entropy of fractional Gaussian noise is infinite if the Hurst parameter \$H \in (1/2, 1)\$.},
	urldate = {2018-06-02},
	journal = {arXiv:1604.05453 [cs, math, stat]},
	author = {Ding, Yiming and Xiang, Xuyan},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.05453},
	keywords = {Statistics - Methodology, Computer Science - Information Theory, 60G10, 62B10, Mathematics - Probability},
	file = {arXiv\:1604.05453 PDF:/home/user/Zotero/storage/ISJL6KYI/Ding and Xiang - 2016 - An entropic characterization of long memory statio.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/IIESTB8X/1604.html:text/html}
}

@article{sharan-prediction-2016,
	title = {Prediction with a {Short} {Memory}},
	url = {http://arxiv.org/abs/1612.02526},
	abstract = {We consider the problem of predicting the next observation given a sequence of past observations, and consider the extent to which accurate prediction requires complex algorithms that explicitly leverage long-range dependencies. Perhaps surprisingly, our positive results show that for a broad class of sequences, there is an algorithm that predicts well on average, and bases its predictions only on the most recent few observation together with a set of simple summary statistics of the past observations. Speciﬁcally, we show that for any distribution over observations, if the mutual information between past observations and future observations is upper bounded by I, then a simple Markov √model over the most recent I/ observations obtains expected KL error —and hence 1 error —with respect to the optimal predictor that has access to the entire past and knows the data generating distribution. For a Hidden Markov Model with n hidden states, I is bounded by log n, a quantity that does not depend on the mixing time, and we show that the trivial prediction algorithm based on the empirical frequencies of length O(log n/ ) windows of observations achieves this error, provided the length of the sequence is dΩ(log n/ ), where d is the size of the observation alphabet.},
	language = {en},
	urldate = {2018-06-01},
	journal = {arXiv:1612.02526 [cs, stat]},
	author = {Sharan, Vatsal and Kakade, Sham and Liang, Percy and Valiant, Gregory},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.02526},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity},
	file = {Sharan et al. - 2016 - Prediction with a Short Memory.pdf:/home/user/Zotero/storage/BWZ5LHTA/Sharan et al. - 2016 - Prediction with a Short Memory.pdf:application/pdf}
}

@article{paninski-variational-nodate,
	title = {Variational {Minimax} {Estimation} of {Discrete} {Distributions} under {KL} {Loss}},
	abstract = {We develop a family of upper and lower bounds on the worst-case expected KL loss for estimating a discrete distribution on a ﬁnite number m of points, given N i.i.d. samples. Our upper bounds are approximationtheoretic, similar to recent bounds for estimating discrete entropy; the lower bounds are Bayesian, based on averages of the KL loss under Dirichlet distributions. The upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst-case error; the lower bounds are indexed by a one-dimensional parameter and are thus easily maximized. Asymptotic analysis of the bounds demonstrates the uniform KL-consistency of a wide class of estimators as c = N/m → ∞ (no matter how slowly), and shows that no estimator is consistent for c bounded (in contrast to entropy estimation). Moreover, the bounds are asymptotically tight as c → 0 or ∞, and are shown numerically to be tight within a factor of two for all c. Finally, in the sparse-data limit c → 0, we ﬁnd that the Dirichlet-Bayes (add-constant) estimator with parameter scaling like −c log(c) optimizes both the upper and lower bounds, suggesting an optimal choice of the “add-constant” parameter in this regime.},
	language = {en},
	author = {Paninski, Liam},
	pages = {8},
	file = {Paninski - Variational Minimax Estimation of Discrete Distrib.pdf:/home/user/Zotero/storage/F4LXUKGZ/Paninski - Variational Minimax Estimation of Discrete Distrib.pdf:application/pdf}
}

@article{miller-when-2018,
	title = {When {Recurrent} {Models} {Don}'t {Need} {To} {Be} {Recurrent}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Our result applies to a broad range of non-linear recurrent neural networks under a natural stability condition, which we observe is also necessary. Complementing our theoretical ﬁndings, we verify the conclusions of our theory on both real and synthetic tasks. Furthermore, we demonstrate recurrent models satisfying the stability assumption of our theory can have excellent performance on real sequence learning tasks.},
	language = {en},
	urldate = {2018-05-31},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10369},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {Miller and Hardt - 2018 - When Recurrent Models Don't Need To Be Recurrent.pdf:/home/user/Zotero/storage/BWM4S5CJ/Miller and Hardt - 2018 - When Recurrent Models Don't Need To Be Recurrent.pdf:application/pdf}
}

@article{bai-empirical-2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
	language = {en},
	urldate = {2018-05-31},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:/home/user/Zotero/storage/A6PB7P9B/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf}
}

@book{rijkhoff-noun-2004,
	title = {The noun phrase},
	publisher = {Oxford University Press},
	author = {Rijkhoff, Jan},
	year = {2004}
}

@article{snow-creole-nodate,
	title = {Creole {Genesis} and {Universality}: {Case}, {Word} {Order}, and {Agreement}},
	abstract = {The findings and contributions to the field made possible from the data in this thesis are that there are commonalities in the case, word order, and agreement systems of the subject creole languages that qualify as core indispensable features and that these features are generated by universal innate linguistic expectations. These commonalities are: (1) that morphological case inflection is not a core indispensable feature; (2) that SVO word order is a core indispensable feature; and (3) that agreement as a feature, seen only when word order is apparently verb final, occurs only in the signed creole languages and is more accurately interpreted as topicalization incorporated into SVO word order rather than as an independent core feature. Nicaraguan Sign Language presents especially compelling evidence for these conclusions.},
	language = {en},
	journal = {Word Order},
	author = {Snow, Gerald Taylor},
	pages = {83},
	file = {Snow - Creole Genesis and Universality Case, Word Order,.pdf:/home/user/Zotero/storage/8E6PTKHG/Snow - Creole Genesis and Universality Case, Word Order,.pdf:application/pdf}
}

@article{gell-mann-origin-2011,
	title = {The origin and evolution of word order},
	volume = {108},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/108/42/17290},
	doi = {10.1073/pnas.1113716108},
	abstract = {Recent work in comparative linguistics suggests that all, or almost all, attested human languages may derive from a single earlier language. If that is so, then this language—like nearly all extant languages—most likely had a basic ordering of the subject (S), verb (V), and object (O) in a declarative sentence of the type “the man (S) killed (V) the bear (O).” When one compares the distribution of the existing structural types with the putative phylogenetic tree of human languages, four conclusions may be drawn. (i) The word order in the ancestral language was SOV. (ii) Except for cases of diffusion, the direction of syntactic change, when it occurs, has been for the most part SOV {\textgreater} SVO and, beyond that, SVO {\textgreater} VSO/VOS with a subsequent reversion to SVO occurring occasionally. Reversion to SOV occurs only through diffusion. (iii) Diffusion, although important, is not the dominant process in the evolution of word order. (iv) The two extremely rare word orders (OVS and OSV) derive directly from SOV.},
	language = {en},
	number = {42},
	urldate = {2018-05-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gell-Mann, Murray and Ruhlen, Merritt},
	month = oct,
	year = {2011},
	pmid = {21987807},
	pages = {17290--17295},
	file = {Full Text PDF:/home/user/Zotero/storage/NGS229V9/Gell-Mann and Ruhlen - 2011 - The origin and evolution of word order.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/KWBVDMID/17290.html:text/html}
}

@article{degraff-morphology-nodate,
	title = {"{Morphology} and word order in 'creolization' and beyond"},
	language = {en},
	author = {DeGraff, Michel},
	pages = {82},
	file = {DeGraff - Morphology and word order in 'creolization' and b.pdf:/home/user/Zotero/storage/FEHGIUNU/DeGraff - Morphology and word order in 'creolization' and b.pdf:application/pdf}
}

@misc{noauthor-quranic-nodate,
	title = {The {Quranic} {Arabic} {Corpus} - {Word} by {Word} {Grammar}, {Syntax} and {Morphology} of the {Holy} {Quran}},
	url = {http://corpus.quran.com/treebank.jsp},
	urldate = {2018-05-23},
	file = {The Quranic Arabic Corpus - Word by Word Grammar, Syntax and Morphology of the Holy Quran:/home/user/Zotero/storage/PN2LF2M4/treebank.html:text/html}
}

@misc{noauthor-andi-nodate,
	title = {Andi {Wu} - {VP}, {Linguistic} {Research} - {Global} {Bible} {Initiative} {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/in/andi-wu-a8b4062a/de},
	urldate = {2018-05-23},
	file = {Andi Wu - VP, Linguistic Research - Global Bible Initiative | LinkedIn:/home/user/Zotero/storage/EHNS76HY/de.html:text/html}
}

@misc{noauthor-dashboard-nodate,
	title = {Dashboard},
	url = {http://biblicalhumanities.org/dashboard/},
	urldate = {2018-05-23},
	file = {Dashboard:/home/user/Zotero/storage/JJHQKM7C/dashboard.html:text/html}
}

@misc{noauthor-content-nodate,
	title = {Content},
	url = {http://www.globalbibleinitiative.org/content.html},
	abstract = {In addition to developing our tools, we have also developed a trove of linguistic and biblical data as we have worked on our translations.  Most of this data is deeply integrated and interlinked to...},
	language = {en},
	urldate = {2018-05-23},
	journal = {Global Bible Initiative},
	file = {Snapshot:/home/user/Zotero/storage/3IPT7LLW/content.html:text/html}
}

@misc{noauthor-westminster-nodate,
	title = {Westminster {Theological} {Seminary} - {Projects}},
	url = {https://students.wts.edu/resources/alangroves/grovesprojects.html},
	urldate = {2018-05-23},
	file = {Westminster Theological Seminary - Projects:/home/user/Zotero/storage/AIQ58GYW/grovesprojects.html:text/html}
}

@misc{noauthor-janet-nodate,
	title = {Janet {Dyk}: {NTN} with object and complement},
	url = {https://shebanq.ancient-data.org/hebrew/text},
	urldate = {2018-05-23},
	file = {Janet Dyk\: NTN with object and complement:/home/user/Zotero/storage/X7S4BV7S/text.html:text/html}
}

@misc{noauthor-iswoc-treebank:-2017,
	title = {iswoc-treebank: {Official} releases of the {ISWOC} treebank},
	shorttitle = {iswoc-treebank},
	url = {https://github.com/iswoc/iswoc-treebank},
	urldate = {2018-05-23},
	publisher = {The ISWOC Treebank},
	month = nov,
	year = {2017},
	note = {original-date: 2015-08-20T09:04:41Z},
	keywords = {corpus, european-languages, linguistics, old-french, old-portuguese, old-project, old-spanish, treebank}
}

@misc{noauthor-treebank-releases:-2018,
	title = {treebank-releases: {Official} releases of the {TOROT} treebank},
	shorttitle = {treebank-releases},
	url = {https://github.com/torottreebank/treebank-releases},
	urldate = {2018-05-23},
	publisher = {torottreebank},
	month = mar,
	year = {2018},
	note = {original-date: 2016-04-18T10:58:10Z}
}

@misc{noauthor-torot-nodate,
	title = {The {TOROT} {Treebank}},
	url = {http://torottreebank.github.io/},
	urldate = {2018-05-23},
	file = {The TOROT Treebank:/home/user/Zotero/storage/F8BMWA75/torottreebank.github.io.html:text/html}
}

@misc{noauthor-home-nodate,
	title = {Home {Page} - {HittiteCorpus}},
	url = {http://hittitecorpus.ru/},
	urldate = {2018-05-23},
	file = {Home Page - HittiteCorpus:/home/user/Zotero/storage/8UYTSKCL/hittitecorpus.ru.html:text/html}
}

@article{lee-dependency-nodate,
	title = {Dependency {Parsing} using {Prosody} {Markers} from a {Parallel} {Text}},
	abstract = {This paper describes a method to automatically generate dependency trees for ancient Greek sentences by exploiting prosodic annotation in a Hebrew parallel text. The head selection accuracy of the resulting trees, at close to 80\%, is signiﬁcantly higher than what standard statistical parsers might be expected to produce, for a resource-poor language such as ancient Greek. Our evaluation suggests that prosodic markers can be reliable indicators of syntactic structures.},
	language = {en},
	author = {Lee, John},
	pages = {12},
	file = {Lee - Dependency Parsing using Prosody Markers from a Pa.pdf:/home/user/Zotero/storage/27Y7EF9I/Lee - Dependency Parsing using Prosody Markers from a Pa.pdf:application/pdf}
}

@book{noauthor-efficient-nodate,
	title = {An {Efficient} {Approach} to {Ancient} {Chinese} {Treebank} {Construction} {Based} on “{Word} or {POS}” {Match}},
	url = {http://gb.oversea.cnki.net/KCMS/detail/detail.aspx?filename=MESS201704017&dbcode=CJFQ&dbname=CJFDTEMP}
}

@misc{noauthor-langbank-nodate,
	title = {{LangBank} {Project} {Page}},
	url = {http://sfs.uni-tuebingen.de/langbank/resources.html},
	urldate = {2018-05-22},
	file = {LangBank Project Page:/home/user/Zotero/storage/NI457LWJ/resources.html:text/html}
}

@article{weis-annotation-nodate,
	title = {Annotation of an {Early} {New} {High} {German} {Corpus}: {The} {LangBank} {Pipeline}},
	language = {en},
	journal = {Natural Language Processing},
	author = {Weiß, Zarah and Schnelle, Gohar},
	pages = {41},
	file = {Weiß and Schnelle - Annotation of an Early New High German Corpus The.pdf:/home/user/Zotero/storage/AF9PBHJ4/Weiß and Schnelle - Annotation of an Early New High German Corpus The.pdf:application/pdf}
}

@incollection{rocio-automated-2003,
	series = {Text, {Speech} and {Language} {Technology}},
	title = {Automated {Creation} of a {Medieval} {Portuguese} {Partial} {Treebank}},
	isbn = {978-1-4020-1335-5 978-94-010-0201-1},
	url = {https://link.springer.com/chapter/10.1007/978-94-010-0201-1\_12},
	abstract = {The growing trend towards corpus-based linguistics has led researchers to manually annotate large quantities of text. The human effort involved in this task is often enormous, and requires highly specialised linguistically trained manpower. According to our point of view, another approach should be followed, using this highly trained manpower in other activities, more rewarding and creative, in a constructive dialogue among the various kinds of expertise needed for overcoming our ignorance about languages. As an experiment, we used tools and linguistic resources previously built for Contemporary Portuguese for partially automating the process of partial annotation of a Medieval Portuguese corpus. In this paper, we describe the tools used (POS tagger, lexical analyser and partial parser) and demonstrate that the similarities between a language at two different time periods is sufficient for bootstrapping and acquiring lexical knowledge from the partially parsed, automatically annotated corpus.},
	language = {en},
	urldate = {2018-05-22},
	booktitle = {Treebanks},
	publisher = {Springer, Dordrecht},
	author = {Rocio, Vitor and Alves, Mário Amado and Lopes, J. Gabriel and Xavier, Maria Francisca and Vicente, Graça},
	year = {2003},
	doi = {10.1007/978-94-010-0201-1\_12},
	pages = {211--227},
	file = {Snapshot:/home/user/Zotero/storage/6Y2PPUG4/978-94-010-0201-1_12.html:text/html}
}

@article{dukes-dependency-nodate,
	title = {A {Dependency} {Treebank} of the {Quran} using {Traditional} {Arabic} {Grammar}},
	language = {en},
	author = {Dukes, Kais and Buckwalter, Tim},
	pages = {7},
	file = {Dukes and Buckwalter - A Dependency Treebank of the Quran using Tradition.pdf:/home/user/Zotero/storage/2MHF6SHS/Dukes and Buckwalter - A Dependency Treebank of the Quran using Tradition.pdf:application/pdf}
}

@misc{noauthor-cityu-nodate,
	title = {{CityU} {Treebank} of {Classical} {Chinese} {Poems}},
	url = {http://classicalchinese.lt.cityu.edu.hk/},
	urldate = {2018-05-22},
	file = {CityU Treebank of Classical Chinese Poems:/home/user/Zotero/storage/YBMLG4C9/classicalchinese.lt.cityu.edu.hk.html:text/html}
}

@misc{noauthor-ramses-nodate,
	title = {Ramses {Online}},
	url = {http://ramses.ulg.ac.be/},
	urldate = {2018-05-22},
	file = {Ramses Online:/home/user/Zotero/storage/ERAEMHQM/ramses.ulg.ac.be.html:text/html}
}

@article{mambrini-non-projectivity-nodate,
	title = {Non-{Projectivity} in the {Ancient} {Greek} {Dependency} {Treebank}},
	abstract = {In this paper, we provide a quantitative analysis of non-projective constructions attested in the Ancient Greek Dependency Treebank (AGDT). We consider the different types of formal constraints and metrics that have become standardized in the literature on non-projectivity (planarity, wellnestedness, gap-degree, edge-degree). We also discuss some of the linguistic factors that cause non-projective edges in Ancient Greek. Our results conﬁrm the remarkable extension of non-projectivity in the AGDT, both in terms of quantitative incidence of non-projective nodes and for their complexity, which is not paralleled by the corpora of modern languages considered in the literature. At the same time, the usefulness of other constraint (especially well-nestedness) is conﬁrmed by our researches.},
	language = {en},
	author = {Mambrini, Francesco and Passarotti, Marco},
	pages = {10},
	file = {Mambrini and Passarotti - Non-Projectivity in the Ancient Greek Dependency T.pdf:/home/user/Zotero/storage/DXJKRPA9/Mambrini and Passarotti - Non-Projectivity in the Ancient Greek Dependency T.pdf:application/pdf}
}

@article{bohnet-generating-nodate,
	title = {Generating {Non}-{Projective} {Word} {Order} in {Statistical} {Linearization}},
	abstract = {We propose a technique to generate nonprojective word orders in an efﬁcient statistical linearization system. Our approach predicts liftings of edges in an unordered syntactic tree by means of a classiﬁer, and uses a projective algorithm for tree linearization. We obtain statistically signiﬁcant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech.},
	language = {en},
	author = {Bohnet, Bernd and Bjorkelund, Anders and Kuhn, Jonas and Seeker, Wolfgang and Zarriess, Sina},
	pages = {12},
	file = {Bohnet et al. - Generating Non-Projective Word Order in Statistica.pdf:/home/user/Zotero/storage/MY9TGVSP/Bohnet et al. - Generating Non-Projective Word Order in Statistica.pdf:application/pdf}
}

@article{smith-super-convergence:-nodate,
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	abstract = {In this paper, we describe a phenomenon, which we named “super-convergence”, where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simpliﬁcation of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures to replicate this work will be made available upon publication.},
	language = {en},
	author = {Smith, Leslie N and Topin, Nicholay},
	pages = {18},
	file = {Smith and Topin - Super-Convergence Very Fast Training of Neural Ne.pdf:/home/user/Zotero/storage/G5MTB2JN/Smith and Topin - Super-Convergence Very Fast Training of Neural Ne.pdf:application/pdf}
}

@article{wang-galactic-2017,
	title = {The {Galactic} {Dependencies} {Treebanks}: {Getting} {More} {Data} by {Synthesizing} {New} {Languages}},
	shorttitle = {The {Galactic} {Dependencies} {Treebanks}},
	url = {http://arxiv.org/abs/1710.03838},
	abstract = {We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a "nearby" source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.},
	urldate = {2018-05-16},
	journal = {arXiv:1710.03838 [cs]},
	author = {Wang, Dingquan and Eisner, Jason},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03838},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1710.03838 PDF:/home/user/Zotero/storage/MKE4JAHE/Wang and Eisner - 2017 - The Galactic Dependencies Treebanks Getting More .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/XLDMYM5N/1710.html:text/html}
}

@inproceedings{futrell-experiments-2015,
	address = {Lisbon, Portugal},
	title = {Experiments with {Generative} {Models} for {Dependency} {Tree} {Linearization}},
	url = {http://aclweb.org/anthology/D15-1231},
	urldate = {2018-05-16},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Futrell, Richard and Gibson, Edward},
	month = sep,
	year = {2015},
	pages = {1978--1983},
	file = {Full Text PDF:/home/user/Zotero/storage/23MMS6HE/Futrell and Gibson - 2015 - Experiments with Generative Models for Dependency .pdf:application/pdf}
}

@inproceedings{belz-first-2011,
	address = {Nancy, France},
	title = {The {First} {Surface} {Realisation} {Shared} {Task}: {Overview} and {Evaluation} {Results}},
	shorttitle = {The {First} {Surface} {Realisation} {Shared} {Task}},
	url = {http://www.aclweb.org/anthology/W11-2832},
	urldate = {2018-05-16},
	booktitle = {Proceedings of the {Generation} {Challenges} {Session} at the 13th {European} {Workshop} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Belz, Anja and White, Mike and Espinosa, Dominic and Kow, Eric and Hogan, Deirdre and Stent, Amanda},
	month = sep,
	year = {2011},
	pages = {217--226},
	file = {Full Text PDF:/home/user/Zotero/storage/LJ36MKMQ/Belz et al. - 2011 - The First Surface Realisation Shared Task Overvie.pdf:application/pdf}
}

@inproceedings{gildea-optimizing-2007,
	address = {Prague, Czech Republic},
	title = {Optimizing {Grammars} for {Minimum} {Dependency} {Length}},
	url = {http://www.aclweb.org/anthology/P07-1024},
	booktitle = {Proceedings of the 45th {Annual} {Meeting} of the {Association} of {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Gildea, Daniel and Temperley, David},
	month = jun,
	year = {2007},
	pages = {184--191}
}

@article{dillon-structure-sensitivity-2014,
	title = {The structure-sensitivity of memory access: evidence from {Mandarin} {Chinese}},
	volume = {5},
	issn = {1664-1078},
	shorttitle = {The structure-sensitivity of memory access},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.01025/abstract},
	doi = {10.3389/fpsyg.2014.01025},
	language = {en},
	urldate = {2018-05-16},
	journal = {Frontiers in Psychology},
	author = {Dillon, Brian and Chow, Wing-Yee and Wagers, Matthew and Guo, Taomei and Liu, Fengqin and Phillips, Colin},
	month = sep,
	year = {2014},
	file = {Dillon et al. - 2014 - The structure-sensitivity of memory access eviden.pdf:/home/user/Zotero/storage/QBKQZJ2Y/Dillon et al. - 2014 - The structure-sensitivity of memory access eviden.pdf:application/pdf}
}

@article{seifart-nouns-2018,
	title = {Nouns slow down speech across structurally and culturally diverse languages},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2018/05/09/1800708115},
	doi = {10.1073/pnas.1800708115},
	abstract = {By force of nature, every bit of spoken language is produced at a particular speed. However, this speed is not constant—speakers regularly speed up and slow down. Variation in speech rate is influenced by a complex combination of factors, including the frequency and predictability of words, their information status, and their position within an utterance. Here, we use speech rate as an index of word-planning effort and focus on the time window during which speakers prepare the production of words from the two major lexical classes, nouns and verbs. We show that, when naturalistic speech is sampled from languages all over the world, there is a robust cross-linguistic tendency for slower speech before nouns compared with verbs, both in terms of slower articulation and more pauses. We attribute this slowdown effect to the increased amount of planning that nouns require compared with verbs. Unlike verbs, nouns can typically only be used when they represent new or unexpected information; otherwise, they have to be replaced by pronouns or be omitted. These conditions on noun use appear to outweigh potential advantages stemming from differences in internal complexity between nouns and verbs. Our findings suggest that, beneath the staggering diversity of grammatical structures and cultural settings, there are robust universals of language processing that are intimately tied to how speakers manage referential information when they communicate with one another.},
	language = {en},
	urldate = {2018-05-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Seifart, Frank and Strunk, Jan and Danielsen, Swintha and Hartmann, Iren and Pakendorf, Brigitte and Wichmann, Søren and Witzlack-Makarevich, Alena and Jong, Nivja H. de and Bickel, Balthasar},
	month = may,
	year = {2018},
	keywords = {language universals, language processing, nouns, speech rate, word planning},
	pages = {201800708},
	file = {Full Text PDF:/home/user/Zotero/storage/ZPG8HSEH/Seifart et al. - 2018 - Nouns slow down speech across structurally and cul.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/WRR8BQ6D/1800708115.html:text/html}
}

@article{siddharth-learning-2017,
	title = {Learning {Disentangled} {Representations} with {Semi}-{Supervised} {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1706.00400},
	abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
	urldate = {2018-05-12},
	journal = {arXiv:1706.00400 [cs, stat]},
	author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.00400},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1706.00400 PDF:/home/user/Zotero/storage/WPJESVMJ/Siddharth et al. - 2017 - Learning Disentangled Representations with Semi-Su.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/5K44VCGE/1706.html:text/html}
}

@article{reese-wikicorpus:-2010,
	title = {Wikicorpus: {A} word-sense disambiguated multilingual wikipedia corpus},
	author = {Reese, Samuel and Boleda, Gemma and Cuadros, Montse and Rigau, German},
	year = {2010}
}

@inproceedings{khandelwal-sharp-2018,
	title = {Sharp {Nearby}, {Fuzzy} {Far} {Away}: {How} {Neural} {Language} {Models} {Use} {Context}.},
	booktitle = {{ACL}},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	year = {2018}
}

@inproceedings{nivre-universal-2016,
	title = {Universal {Dependencies} v1: {A} {Multilingual} {Treebank} {Collection}.},
	booktitle = {{LREC}},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D and McDonald, Ryan T and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and {others}},
	year = {2016}
}

@article{daniluk-frustratingly-2017,
	title = {Frustratingly {Short} {Attention} {Spans} in {Neural} {Language} {Modeling}},
	url = {http://arxiv.org/abs/1702.04521},
	abstract = {Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.},
	urldate = {2018-05-06},
	journal = {arXiv:1702.04521 [cs]},
	author = {Daniluk, Michał and Rocktäschel, Tim and Welbl, Johannes and Riedel, Sebastian},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04521},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1702.04521 PDF:/home/user/Zotero/storage/ZE8PMNH5/Daniluk et al. - 2017 - Frustratingly Short Attention Spans in Neural Lang.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/QIIW7V6S/1702.html:text/html}
}

@article{yogatama-memory-2018,
	title = {{MEMORY} {ARCHITECTURES} {IN} {RECURRENT} {NEURAL} {NETWORK} {LANGUAGE} {MODELS}},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin \& Mikolov, 2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.},
	language = {en},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	year = {2018},
	pages = {10},
	file = {Yogatama et al. - 2018 - MEMORY ARCHITECTURES IN RECURRENT NEURAL NETWORK L.pdf:/home/user/Zotero/storage/RGMABFH7/Yogatama et al. - 2018 - MEMORY ARCHITECTURES IN RECURRENT NEURAL NETWORK L.pdf:application/pdf}
}

@article{alirezazadeh-forest-nodate,
	title = {Forest {Algebras}, ω-{Algebras} and {A} {Canonical} {Form} for {Certain} {Relatively} {Free} ω-{Algebras}},
	language = {en},
	author = {Alirezazadeh, Saeid and Almeida, Dr Jorge},
	pages = {155},
	file = {Alirezazadeh and Almeida - Forest Algebras, ω-Algebras and A Canonical Form f.pdf:/home/user/Zotero/storage/2QH4W9LK/Alirezazadeh and Almeida - Forest Algebras, ω-Algebras and A Canonical Form f.pdf:application/pdf}
}

@article{debowski-maximal-2016,
	title = {Maximal {Repetition} and {Zero} {Entropy} {Rate}},
	url = {http://arxiv.org/abs/1609.04683},
	abstract = {Maximal repetition of a string is the maximal length of a repeated substring. This paper investigates maximal repetition of strings drawn from stochastic processes. Strengthening previous results, two new bounds for the almost sure growth rate of maximal repetition are identified: an upper bound in terms of conditional R\'enyi entropy of order \$\gamma{\textgreater}1\$ given a sufficiently long past and a lower bound in terms of unconditional Shannon entropy (\$\gamma=1\$). Both the upper and the lower bound can be proved using an inequality for the distribution of recurrence time. We also supply an alternative proof of the lower bound which makes use of an inequality for the expectation of subword complexity. In particular, it is shown that a power-law logarithmic growth of maximal repetition with respect to the string length, recently observed for texts in natural language, may hold only if the conditional R\'enyi entropy rate given a sufficiently long past equals zero. According to this observation, natural language cannot be faithfully modeled by a typical hidden Markov process, which is a class of basic language models used in computational linguistics.},
	urldate = {2018-04-30},
	journal = {arXiv:1609.04683 [cs, math]},
	author = {Debowski, Łukasz},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04683},
	keywords = {Computer Science - Information Theory, 60G10, 94A17},
	file = {arXiv\:1609.04683 PDF:/home/user/Zotero/storage/F54RYC7J/Debowski - 2016 - Maximal Repetition and Zero Entropy Rate.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/X33K27QI/1609.html:text/html}
}

@article{takahira-entropy-2016,
	title = {Entropy {Rate} {Estimates} for {Natural} {Language}—{A} {New} {Extrapolation} of {Compressed} {Large}-{Scale} {Corpora}},
	volume = {18},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/18/10/364},
	doi = {10.3390/e18100364},
	abstract = {One of the fundamental questions about human language is whether its entropy rate is positive. The entropy rate measures the average amount of information communicated per unit time. The question about the entropy of language dates back to experiments by Shannon in 1951, but in 1990 Hilberg raised doubt regarding a correct interpretation of these experiments. This article provides an in-depth empirical analysis, using 20 corpora of up to 7.8 gigabytes across six languages (English, French, Russian, Korean, Chinese, and Japanese), to conclude that the entropy rate is positive. To obtain the estimates for data length tending to inﬁnity, we use an extrapolation function given by an ansatz. Whereas some ansatzes were proposed previously, here we use a new stretched exponential extrapolation function that has a smaller error of ﬁt. Thus, we conclude that the entropy rates of human languages are positive but approximately 20\% smaller than without extrapolation. Although the entropy rate estimates depend on the script kind, the exponent of the ansatz function turns out to be constant across different languages and governs the complexity of natural language in general. In other words, in spite of typological differences, all languages seem equally hard to learn, which partly conﬁrms Hilberg’s hypothesis.},
	language = {en},
	number = {10},
	urldate = {2018-04-30},
	journal = {Entropy},
	author = {Takahira, Ryosuke and Tanaka-Ishii, Kumiko and Debowski, Łukasz},
	month = oct,
	year = {2016},
	pages = {364},
	file = {Takahira et al. - 2016 - Entropy Rate Estimates for Natural Language—A New .pdf:/home/user/Zotero/storage/NVJH5J69/Takahira et al. - 2016 - Entropy Rate Estimates for Natural Language—A New .pdf:application/pdf}
}

@inproceedings{hahn-information-theoretic-2018,
	title = {An information-theoretic explanation of adjective ordering preferences},
	url = {files/cogsci\_2018\_submitted.pdf},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Cognitive} {Science} {Society} ({CogSci})},
	author = {Hahn, Michael and Degen, Judith and Goodman, Noah and Jurafsky, Dan and Futrell, {and} Richard},
	year = {2018}
}

@inproceedings{hahn-wreath-2018,
	title = {Wreath {Products} of {Distributive} {Forest} {Algebras}},
	url = {files/lics-2018-submitted.pdf},
	booktitle = {Logic in {Computer} {Science} ({LICS})},
	author = {Hahn, Michael and Krebs, Andreas and Straubing, Howard},
	year = {2018}
}

@unpublished{roth-forrest-1993,
	type = {Screenplay},
	title = {Forrest {Gump}},
	url = {https://www.weeklyscript.com/Forrest+Gump.html},
	author = {Roth, Eric},
	year = {1993}
}

@article{mcelree-attended-1998,
	title = {Attended and {Non}-{Attended} {States} in {Working} {Memory}: {Accessing} {Categorized} {Structures}},
	volume = {38},
	issn = {0749-596X},
	shorttitle = {Attended and {Non}-{Attended} {States} in {Working} {Memory}},
	url = {http://www.sciencedirect.com/science/article/pii/S0749596X97925456},
	doi = {10.1006/jmla.1997.2545},
	abstract = {Following several recent approaches, working memory is argued to consist of a subset of representations that are within the scope of active processing and a larger set of recently activated, but non-attended representations. A response-signal speed-accuracy tradeoff (SAT) and complementary reaction time variant of a probe recognition task were used to measure the retrieval of categorized material from working memory. In SAT, two distinct retrieval speeds were found: A fast rate for items from the last studied category and a slower rate common to all items from other categories in the list. This difference in retrieval speed was evident in the shapes of the reaction time distributions. The faster retrieval speed for items from the last category is argued to reflect a matching process in which the recognition probe is rapidly compared to the current contents of awareness.},
	number = {2},
	urldate = {2018-04-17},
	journal = {Journal of Memory and Language},
	author = {McElree, Brian},
	month = feb,
	year = {1998},
	pages = {225--252},
	file = {ScienceDirect Full Text PDF:/home/user/Zotero/storage/HH78KPHZ/McElree - 1998 - Attended and Non-Attended States in Working Memory.pdf:application/pdf;ScienceDirect Snapshot:/home/user/Zotero/storage/9XJKNFN7/S0749596X97925456.html:text/html}
}

@article{lewis-computational-2006,
	title = {Computational principles of working memory in sentence comprehension},
	volume = {10},
	issn = {1364-6613},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2239011/},
	doi = {10.1016/j.tics.2006.08.007},
	abstract = {Understanding a sentence requires a working memory of the partial products of comprehension, so that linguistic relations between temporally distal parts of the sentence can be rapidly computed. We describe an emerging theoretical framework for this working memory system that incorporates several independently motivated principles of memory: a sharply limited attentional focus, rapid retrieval of item (but not order) information subject to interference from similar items, and activation decay (forgetting over time). A computational model embodying these principles provides an explanation of the functional capacities and severe limitations of human processing, as well as accounts of reading times. The broad implication is that the detailed nature of crosslinguistic sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension.},
	number = {10},
	urldate = {2018-04-17},
	journal = {Trends in cognitive sciences},
	author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
	month = oct,
	year = {2006},
	pmid = {16949330},
	pmcid = {PMC2239011},
	pages = {447--454},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/S45GFR8Z/Lewis et al. - 2006 - Computational principles of working memory in sent.pdf:application/pdf}
}

@article{mcelree-working-2001,
	title = {Working {Memory} and {Focal} {Attention}},
	volume = {27},
	issn = {0278-7393},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3077110/},
	abstract = {Measures of retrieval speed indicated that only a small subset of representations in working memory falls within the focus of attention. An n-back task, which required tracking an item 1, 2, or 3 back in a sequentially presented list, was used to examine the representation and retrieval of recent events and how control processes can be used to maintain an item in focal attention while concurrently processing new information. A speed–accuracy trade-off procedure was used to derive measures of the availability and the speed with which recent events can be accessed. Results converge with other time course studies in demonstrating that attention can be concurrently allocated only to a small number of memory representations, perhaps just 1 item. Measures of retrieval speed further demonstrate that order information is retrieved by a slow search process when an item is not maintained within focal attention.},
	number = {3},
	urldate = {2018-04-17},
	journal = {Journal of experimental psychology. Learning, memory, and cognition},
	author = {McElree, Brian},
	month = may,
	year = {2001},
	pmid = {11394682},
	pmcid = {PMC3077110},
	pages = {817--835},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/3DD8QV95/McElree - 2001 - Working Memory and Focal Attention.pdf:application/pdf}
}

@article{mcelree-memory-2003,
	title = {Memory structures that subserve sentence comprehension},
	volume = {48},
	issn = {0749596X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X02005156},
	doi = {10.1016/S0749-596X(02)00515-6},
	abstract = {Measures of the speed and accuracy of processing sentences with nonadjacent dependencies derived from the response-signal speed-accuracy tradeoﬀ procedure were used to examine the nature of the memory system that underlies sentence comprehension. Three experiments with diﬀerent sentence structures demonstrated that the accuracy of processing a dependency decreased as more material was interpolated between nonadjacent constituents. However, processing speed was unaﬀected by the amount of interpolated material, indicating that memory representations for previously processed constituents can be accessed directly. These results suggest that a content-addressable memory system mediates sentence comprehension, in which syntactic and semantic information provide direct access to memory representations without the need to search through extraneous representations. Notably, content-addressability appears to underlie the interpretation of sentence structures that also require the recovery of order information, a type of operation that has been shown to necessitate a slow search process in list-learning experiments (McElree, 2001; McElree \& Dosher, 1993).},
	language = {en},
	number = {1},
	urldate = {2018-04-17},
	journal = {Journal of Memory and Language},
	author = {McElree, Brian and Foraker, Stephani and Dyer, Lisbeth},
	month = jan,
	year = {2003},
	pages = {67--91},
	file = {McElree et al. - 2003 - Memory structures that subserve sentence comprehen.pdf:/home/user/Zotero/storage/2QK6PUUX/McElree et al. - 2003 - Memory structures that subserve sentence comprehen.pdf:application/pdf}
}

@article{mcelree-sentence-2000,
	title = {Sentence {Comprehension} {Is} {Mediated} by {Content}-{Addressable} {Memory} {Structures}},
	volume = {29},
	issn = {0090-6905, 1573-6555},
	url = {https://link.springer.com/article/10.1023/A:1005184709695},
	doi = {10.1023/A:1005184709695},
	abstract = {Studies of working memory demonstrate that some forms of information are retrieved by a content-addressable mechanism (McElree \& Dosher, 1989; McElree, 1996, 1998), whereas others require a slower search-based mechanism (McElree \& Dosher, 1993). Measures of the speed and accuracy of processing sentences with filler-gap dependencies demonstrate that the probability of maintaining a representation of a filler item decreases as additional material is processed, but that the speed with which a preserved representation is accessed is unaffected by the amount of interpolated material. These results suggest that basic binding operations in sentence comprehension are mediated by a content-addressable memory system.},
	language = {en},
	number = {2},
	urldate = {2018-04-17},
	journal = {Journal of Psycholinguistic Research},
	author = {McElree, Brian},
	month = mar,
	year = {2000},
	pages = {111--123},
	file = {Full Text PDF:/home/user/Zotero/storage/Y74KNVBV/McElree - 2000 - Sentence Comprehension Is Mediated by Content-Addr.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RG29DDNR/A1005184709695.html:text/html}
}

@incollection{mcelree-accessing-2006,
	title = {Accessing {Recent} {Events}},
	volume = {46},
	isbn = {978-0-12-543346-4},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0079742106460059},
	language = {en},
	urldate = {2018-04-17},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Elsevier},
	author = {McElree, Brian},
	year = {2006},
	doi = {10.1016/S0079-7421(06)46005-9},
	pages = {155--200},
	file = {McElree - 2006 - Accessing Recent Events.pdf:/home/user/Zotero/storage/6U4ESSWA/McElree - 2006 - Accessing Recent Events.pdf:application/pdf}
}


@book{anderson-adaptive-2013,
	title = {The {Adaptive} {Character} of {Thought}},
	isbn = {978-1-134-74781-8},
	abstract = {This important volume examines the phenomena of cognition from an adaptive perspective. Rather than adhering to the typical practice in cognitive psychology of trying to predict behavior from a model of cognitive mechanisms, this book develops a number of models that successfully predict behavior from the structure of the environment to which cognition is adapted. The methodology -- called rational analysis -- involves specifying the information-processing goals of the system, the structure of the environment, and the computational constraints on the system, allowing predictions about behavior to be made by determining what behavior would be optimal under these assumptions. The Adaptive Character of Thought applies this methodology in great detail to four cognitive phenomena: memory, categorization, causal inference, and problem solving.},
	language = {en},
	publisher = {Psychology Press},
	author = {Anderson, John R.},
	month = jan,
	year = {2013},
	note = {Google-Books-ID: T5JBLb1cNUgC},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / General}
}

@article{lewis-predictive-2015,
	title = {A predictive coding framework for rapid neural dynamics during sentence-level language comprehension},
	volume = {68},
	issn = {1973-8102},
	doi = {10.1016/j.cortex.2015.02.014},
	abstract = {There is a growing literature investigating the relationship between oscillatory neural dynamics measured using electroencephalography (EEG) and/or magnetoencephalography (MEG), and sentence-level language comprehension. Recent proposals have suggested a strong link between predictive coding accounts of the hierarchical flow of information in the brain, and oscillatory neural dynamics in the beta and gamma frequency ranges. We propose that findings relating beta and gamma oscillations to sentence-level language comprehension might be unified under such a predictive coding account. Our suggestion is that oscillatory activity in the beta frequency range may reflect both the active maintenance of the current network configuration responsible for representing the sentence-level meaning under construction, and the top-down propagation of predictions to hierarchically lower processing levels based on that representation. In addition, we suggest that oscillatory activity in the low and middle gamma range reflect the matching of top-down predictions with bottom-up linguistic input, while evoked high gamma might reflect the propagation of bottom-up prediction errors to higher levels of the processing hierarchy. We also discuss some of the implications of this predictive coding framework, and we outline ideas for how these might be tested experimentally.},
	language = {eng},
	journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
	author = {Lewis, Ashley G. and Bastiaansen, Marcel},
	month = jul,
	year = {2015},
	pmid = {25840879},
	keywords = {Humans, Electroencephalography, Predictive coding, Nerve Net, Speech Perception, Anticipation, Psychological, Beta, Beta Rhythm, Comprehension, Gamma, Gamma Rhythm, Language comprehension, Neural oscillations},
	pages = {155--168}
}

@misc{dan-parker-michael-shvartsman-and-julie-van-dyke-cue-based-nodate,
	title = {The {Cue}-based {Retrieval} {Theory} of {Sentence} {Comprehension}},
	url = {http://www.wm.edu/as/linguistics/parker/ParkerEtAl2017-Memory.pdf},
	urldate = {2018-04-17},
	author = {Dan Parker, Michael Shvartsman {and} Julie van Dyke},
	file = {ParkerEtAl2017-Memory.pdf:/home/user/Zotero/storage/VXKTQHWE/ParkerEtAl2017-Memory.pdf:application/pdf}
}

@article{vasilev-parafoveal-2017,
	title = {Parafoveal preview effects from word {N} + 1 and word {N} + 2 during reading: {A} critical review and {Bayesian} meta-analysis},
	volume = {24},
	issn = {1531-5320},
	shorttitle = {Parafoveal preview effects from word {N} + 1 and word {N} + 2 during reading},
	doi = {10.3758/s13423-016-1147-x},
	abstract = {The use of gaze-contingent display techniques to study reading has shown that readers attend not only the currently fixated word, but also the word to the right of the current fixation. However, a critical look at the literature shows that a number of questions cannot be readily answered from the available literature reviews on the topic. First, there is no consensus as to whether readers also attend the second word to the right of fixation. Second, it is not clear whether parafoveal processing is more efficient in languages such as Chinese. Third, it is not well understood whether the measured effects are confounded by the properties of the parafoveal mask. In the present study, we addressed these issues by performing a Bayesian meta-analysis of 93 experiments that used the boundary paradigm (Rayner, Cognitive Psychology, 7, 65-81. doi: 10.1016/0010-028590005-5 , 1975). We describe three main findings: (1) The advantage of previewing the second word to the right is modest in size and likely is not centered on zero; (2) Chinese readers do seem to make more efficient use of parafoveal processing, but this is mostly evident in gaze durations; and (3) there are interference effects associated with using different parafoveal masks that roughly increase when the mask is less word-like.},
	language = {eng},
	number = {3},
	journal = {Psychonomic Bulletin \& Review},
	author = {Vasilev, Martin R. and Angele, Bernhard},
	month = jun,
	year = {2017},
	pmid = {27576520},
	keywords = {Humans, Reading, Eye movements, Fixation, Ocular, Bayes Theorem, Language, Visual Perception, Comprehension, Fovea Centralis, Parafoveal processing, Perceptual span, Preview benefit},
	pages = {666--689}
}

@inproceedings{silva-upper-2006,
	title = {Upper {Bound} {Kullback}-{Leibler} {Divergence} for {Hidden} {Markov} {Models} with {Application} as {Discrimination} {Measure} for {Speech} {Recognition}},
	doi = {10.1109/ISIT.2006.261977},
	abstract = {This paper presents a criterion for defining an upper bound Kullback-Leibler divergence (UB-KLD) for Gaussian mixtures models (GMMs). An information theoretic interpretation of this indicator and an algorithm for calculating it based on similarity alignment between mixture components of the models are proposed. This bound is used to characterize an upper bound closed-form expression for the Kullback-Leibler divergence (KLD) for left-to-right transient hidden Markov models (HMMs), where experiments based on real speech data show that this indicator precisely follows the discrimination tendency of the actual KLD},
	booktitle = {2006 {IEEE} {International} {Symposium} on {Information} {Theory}},
	author = {Silva, J. and Narayanan, S.},
	month = jul,
	year = {2006},
	keywords = {speech recognition, Gaussian processes, Automatic speech recognition, Closed-form solution, Context modeling, Electric variables measurement, Gaussian mixtures models, hidden Markov models, Hidden Markov models, Hydrogen, information theoretic interpretation, Probability density function, Speech analysis, Speech recognition, Upper bound, upper bound Kullback-Leibler divergence},
	pages = {2299--2303},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/LZ4IPMBI/4036380.html:text/html;IEEE Xplore Full Text PDF:/home/user/Zotero/storage/6D7Z29X4/Silva and Narayanan - 2006 - Upper Bound Kullback-Leibler Divergence for Hidden.pdf:application/pdf}
}

@article{silva-upper-2008,
	title = {Upper {Bound} {Kullback} \#x2013;{Leibler} {Divergence} for {Transient} {Hidden} {Markov} {Models}},
	volume = {56},
	issn = {1053-587X},
	doi = {10.1109/TSP.2008.924137},
	abstract = {This paper reports an upper bound for the Kullback-Leibler divergence (KLD) for a general family of transient hidden Markov models (HMMs). An upper bound KLD (UBKLD) expression for Gaussian mixtures models (GMMs) is presented which is generalized for the case of HMMs. Moreover, this formulation is extended to the case of HMMs with nonemitting states, where under some general assumptions, the UBKLD is proved to be well defined for a general family of transient models. In particular, the UBKLD has a computationally efficient closed-form for HMMs with left-to-right topology and a final nonemitting state, that we refer to as left-to-right transient HMMs. Finally, the usefulness of the closed-form expression is experimentally evaluated for automatic speech recognition (ASR) applications, where left-to-right transient HMMs are used to model basic acoustic-phonetic units. Results show that the UBKLD is an accurate discrimination indicator for comparing acoustic HMMs used for ASR.},
	number = {9},
	journal = {IEEE Transactions on Signal Processing},
	author = {Silva, J. and Narayanan, S.},
	month = sep,
	year = {2008},
	keywords = {speech recognition, Gaussian processes, Automatic speech recognition, Closed-form solution, Context modeling, Gaussian mixtures models, hidden Markov models, Hidden Markov models, Speech recognition, Upper bound, upper bound Kullback-Leibler divergence, automatic speech recognition applications, basic acoustic-phonetic units, closed-form expression, Convergence, Gaussian mixture models, Helium, hidden Markov processes and hidden Markov models, Kullback–Leibler divergence (KLD), left-to-right topology, Markov chains, Topology, transient analysis, transient hidden Markov models, transient processes, Viterbi algorithm},
	pages = {4176--4188},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/GWMNDLAP/4599176.html:text/html}
}

@inproceedings{vidyasagar-bounds-2007,
	title = {Bounds on the kullback-leibler divergence rate between hidden markov models},
	doi = {10.1109/CDC.2007.4434365},
	abstract = {The Kullback-Leibler (K-L) divergence rate is a natural extension of the familiar K-L divergence between probability vectors, to the situation where one is observing a sequence of dependent samples, such as for example the state sequence of a Markov chain or the output sequence of a hidden Markov model (HMM). In this paper, we study the problem of estimating the K-L divergence rate between two HMMs with a common output space, where the underlying Markov chains can evolve over different state spaces. At present, there is no closed-form formula for the K-L divergence rate for HMMs, though there is a closed-form formula for the K-L divergence rate between Markov chains over a common state space. In this paper, we give an alternate formulation of the K-L divergence rate between two stationary stochastic processes. Using this alternate formulation, we derive an upper bound for the K-L divergence rate between two HMMs. This estimate converges geometrically fast to the correct answer as the length of the observation increases. However, it is not a particularly elegant estimate. It is hoped that future research will lead to tighter estimates if not a closed-form formula for the K-L divergence rate.},
	booktitle = {2007 46th {IEEE} {Conference} on {Decision} and {Control}},
	author = {Vidyasagar, M.},
	month = dec,
	year = {2007},
	keywords = {Information theory, Probability distribution, hidden Markov model, stationary stochastic process, Closed-form solution, hidden Markov models, Hidden Markov models, Upper bound, Computational biology, Kullback-Leibler divergence rate, Markov chain, matrix algebra, output sequence, probability, probability vectors, State estimation, state sequence, state space, state transition matrices, State-space methods, Stochastic processes, USA Councils},
	pages = {6160--6165},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/8U5UTWGR/4434365.html:text/html}
}

@article{collet-loss-2014,
	title = {Loss of memory of hidden {Markov} models and {Lyapunov} exponents},
	volume = {24},
	issn = {1050-5164},
	url = {http://arxiv.org/abs/0908.0077},
	doi = {10.1214/13-AAP929},
	abstract = {In this paper we prove that the asymptotic rate of exponential loss of memory of a finite state hidden Markov model is bounded above by the difference of the first two Lyapunov exponents of a certain product of matrices. We also show that this bound is in fact realized, namely for almost all realizations of the observed process we can find symbols where the asymptotic exponential rate of loss of memory attains the difference of the first two Lyapunov exponents. These results are derived in particular for the observed process and for the filter; that is, for the distribution of the hidden state conditioned on the observed sequence. We also prove similar results in total variation.},
	number = {1},
	urldate = {2018-03-30},
	journal = {The Annals of Applied Probability},
	author = {Collet, Pierre and Leonardi, Florencia},
	month = feb,
	year = {2014},
	note = {arXiv: 0908.0077},
	keywords = {Mathematics - Probability},
	pages = {422--446},
	file = {arXiv\:0908.0077 PDF:/home/user/Zotero/storage/PNCYSGDX/Collet and Leonardi - 2014 - Loss of memory of hidden Markov models and Lyapuno.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6GAIA83L/0908.html:text/html}
}

@article{holliday-entropy-nodate,
	title = {Entropy and {Mutual} {Information} for {Markov} {Channels} with {General} {Inputs}},
	abstract = {We study new formulas based on Lyapunov exponents for entropy, mutual information, and capacity of ﬁnite state discrete time Markov channels. We also develop a method for directly computing mutual information and entropy using continuous state space Markov chains. Our methods allow for arbitrary input processes and channel dynamics, provided both have ﬁnite memory. We show that the entropy rate for a symbol sequence is equal to the primary Lyapunov exponent for a product of random matrices. We then develop a continuous state space Markov chain formulation that allows us to directly compute entropy rates as expectations with respect to the Markov chains’ stationary distributions. We also show that the stationary distributions are continuous functions of the input symbol dynamics. This continuity facilitates optimization of the mutual information and allows the channel capacity to be written in terms of Lyapunov exponents.},
	language = {en},
	author = {Holliday, Tim and Goldsmith, Andrea and Glynn, Peter},
	pages = {10},
	file = {Holliday et al. - Entropy and Mutual Information for Markov Channels.pdf:/home/user/Zotero/storage/MLDARZLC/Holliday et al. - Entropy and Mutual Information for Markov Channels.pdf:application/pdf}
}

@article{khan-history-nodate,
	title = {History ({Forward} {N}-{Gram}) or {Future} ({Backward} {N}-{Gram})? {Which} {Model} to {Consider} for {N}-{Gram} analysis in {Bangla}?},
	abstract = {This paper presents a directional advantage of ngram modeling in terms of backward or forward ngram modeling in Bangla. The most commonly used ngram analysis is predominantly a forward n-gram. However in Bangla it appears that a backward ngram is repeatedly more successful and yields more grammatical results than a forward n-gram. This paper hypothesizes that the rationale behind this success is the syntactic ordering of constituents in Bangla. Bangla is a head-final specifier-initial language as opposed to English, which is head-initial specifier-initial. Hence in Bangla, the head comes after its argument in a phrase. If an n-gram analysis begins with a head and moves backwards it will stretch to its own argument but if you move for-wards then you'll probably grab the argument of an-other head. As probability of occurrence of heads is higher, probability of depending on a head is also higher and hence a backward n-gram will probably have a greater chance of yielding grammatical results. We carried out several experiments to compare different directional results in different applications with an advantage in the backward direction. This will prove a useful linguistic insight in terms of n-gram based analysis depending upon variations of constituent analysis.},
	language = {en},
	author = {Khan, Naira and Habib, Tarek and Alam, Jahangir and Rahman, Rajib and UzZaman, Naushad and Khan, Mumit},
	pages = {5},
	file = {Khan et al. - History (Forward N-Gram) or Future (Backward N-Gra.pdf:/home/user/Zotero/storage/HKBP2EH6/Khan et al. - History (Forward N-Gram) or Future (Backward N-Gra.pdf:application/pdf}
}

@misc{noauthor-[1512.06612]-nodate,
	title = {[1512.06612] {Backward} and {Forward} {Language} {Modeling} for {Constrained} {Sentence} {Generation}},
	url = {https://arxiv.org/abs/1512.06612},
	urldate = {2018-03-27},
	file = {[1512.06612] Backward and Forward Language Modeling for Constrained Sentence Generation:/home/user/Zotero/storage/L45LKGM7/1512.html:text/html}
}

@article{xiong-enhancing-nodate,
	title = {Enhancing {Language} {Models} in {Statistical} {Machine} {Translation} with {Backward} {N}-grams and {Mutual} {Information} {Triggers}},
	abstract = {In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to signiﬁcantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.},
	language = {en},
	author = {Xiong, Deyi and Zhang, Min and Li, Haizhou},
	pages = {10},
	file = {Xiong et al. - Enhancing Language Models in Statistical Machine T.pdf:/home/user/Zotero/storage/V55XEV93/Xiong et al. - Enhancing Language Models in Statistical Machine T.pdf:application/pdf}
}

@article{gibson-linguistic-1998-2,
	title = {Linguistic complexity: locality of syntactic dependencies},
	volume = {68},
	number = {1},
	journal = {Cognition},
	author = {Gibson, Edward},
	year = {1998},
	pages = {1--76}
}

@incollection{lindblom-explaining-1990,
	series = {{NATO} {ASI} {Series}},
	title = {Explaining {Phonetic} {Variation}: {A} {Sketch} of the {H}\&amp;{H} {Theory}},
	isbn = {978-94-010-7414-8 978-94-009-2037-8},
	shorttitle = {Explaining {Phonetic} {Variation}},
	url = {https://link.springer.com/chapter/10.1007/978-94-009-2037-8\_16},
	abstract = {The H\&H theory is developed from evidence showing that speaking and listening are shaped by biologically general processes. Speech production is adaptive. Speakers can, and typically do, tune their performance according to communicative and situational demands, controlling the interplay between production-oriented factors on the one hand, and output-oriented constraints on the other. For the ideal speaker, H\&H claims that such adaptations reflect his tacit awareness of the listener’s access to sources of information independent of the signal and his judgement of the short-term demands for explicit signal information. Hence speakers are expected to vary their output along a continuum of hyper- and hypospeech. The theory suggests that the lack of invariance that speech signals commonly exhibit (Perkell and Klatt 1986) is a direct consequence of this adaptive organization (cf MacNeilage 1970). Accordingly, in the H\&H program the quest for phonetic invariance is replaced by another research task: Explicating the notion of sufficient discriminability and defining the class of speech signals that meet that criterion.},
	language = {en},
	urldate = {2018-01-21},
	booktitle = {Speech {Production} and {Speech} {Modelling}},
	publisher = {Springer, Dordrecht},
	author = {Lindblom, B.},
	year = {1990},
	doi = {10.1007/978-94-009-2037-8\_16},
	pages = {403--439},
	file = {Snapshot:/home/user/Zotero/storage/Z7LN2A8F/978-94-009-2037-8_16.html:text/html}
}

@article{regier-word-2015,
	title = {Word {Meanings} across {Languages} {Support} {Efficient} {Communication}},
	volume = {87},
	url = {https://books.google.com/books?hl=en&lr=&id=wHeDBgAAQBAJ&oi=fnd&pg=PA237&dq=%22Regier,+Charles+Kemp,+and+Paul%22+%22Accounting+for+this+pattern+of+wide+but+constrained+variation+is+a+central+theoretical+challenge%22+%22load,+and+informative,+which+maximizes+communicative+effectiveness.+These+two+constraints%22+&ots=p7FkWb68Mh&sig=tltn2dNHPuigPh2rK8HORaiz9k4},
	urldate = {2017-02-16},
	journal = {The handbook of language emergence},
	author = {Regier, Terry and Kemp, Charles and Kay, Paul},
	year = {2015},
	pages = {237},
	file = {EC-chapter-2014.pdf:/home/user/Zotero/storage/Z7QN6M89/EC-chapter-2014.pdf:application/pdf}
}

@article{bahadur-unbiased-1957,
	title = {On {Unbiased} {Estimates} of {Uniformly} {Minimum} {Variance}},
	volume = {18},
	issn = {0036-4452},
	url = {http://www.jstor.org/stable/25048353},
	number = {3/4},
	urldate = {2017-10-15},
	journal = {Sankhyā: The Indian Journal of Statistics (1933-1960)},
	author = {Bahadur, R. R.},
	year = {1957},
	pages = {211--224},
	file = {JSTOR Full Text PDF:/home/user/Zotero/storage/RPBSKTH8/Bahadur - 1957 - On Unbiased Estimates of Uniformly Minimum Varianc.pdf:application/pdf}
}

@misc{noauthor-fervent-2014,
	title = {A {Fervent} {Defense} of {Frequentist} {Statistics}},
	url = {https://jsteinhardt.wordpress.com/2014/02/10/a-fervent-defense-of-frequentist-statistics/},
	abstract = {[Highlights for the busy: de-bunking standard “Bayes is optimal” arguments; frequentist Solomonoff induction; and a description of the online learning framework.] Short summary. This es…},
	urldate = {2017-10-11},
	journal = {Academically Interesting},
	month = feb,
	year = {2014},
	file = {Snapshot:/home/user/Zotero/storage/257NGV6J/a-fervent-defense-of-frequentist-statistics.html:text/html}
}

@book{brown-index-1986,
	title = {Index},
	isbn = {978-0-940600-10-2},
	url = {https://projecteuclid.org/euclid.lnms/1215466768},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/JMP6MGK2/1215466768.html:text/html}
}

@book{brown-chapter-1986,
	title = {Chapter 6: {The} {Dual} to the {Maximum} {Likelihood} {Estimator}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 6},
	url = {https://projecteuclid.org/euclid.lnms/1215466764},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/BQXMUJ6Q/1215466764.html:text/html}
}

@book{brown-chapter-1986-1,
	title = {Chapter 5: {Maximum} {Likelihood} {Estimation}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 5},
	url = {https://projecteuclid.org/euclid.lnms/1215466763},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/59PP7KHC/1215466763.html:text/html}
}

@book{brown-chapter-1986-2,
	title = {Chapter 2: {Analytic} {Properties}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 2},
	url = {https://projecteuclid.org/euclid.lnms/1215466760},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/EXBCUHE6/1215466760.html:text/html}
}

@book{brown-chapter-1986-3,
	title = {Chapter 1: {Basic} {Properties}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 1},
	url = {https://projecteuclid.org/euclid.lnms/1215466759},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/I3VIJT36/1215466759.html:text/html}
}

@book{brown-references-1986,
	title = {References},
	isbn = {978-0-940600-10-2},
	url = {https://projecteuclid.org/euclid.lnms/1215466767},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/FKSGUU3P/1215466767.html:text/html}
}

@book{brown-appendix-1986,
	title = {Appendix to {Chapter} 4: {Pointwise} {Limits} of {Bayes} {Procedures}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Appendix to {Chapter} 4},
	url = {https://projecteuclid.org/euclid.lnms/1215466766},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/W92C2C2T/1215466766.html:text/html}
}

@book{brown-chapter-1986-4,
	title = {Chapter 7: {Tail} {Probabilities}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 7},
	url = {https://projecteuclid.org/euclid.lnms/1215466765},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/QMZM3W6G/1215466765.html:text/html}
}

@book{brown-chapter-1986-5,
	title = {Chapter 4: {Applications}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 4},
	url = {https://projecteuclid.org/euclid.lnms/1215466762},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/FVCQPA5K/1215466762.html:text/html}
}

@book{brown-chapter-1986-6,
	title = {Chapter 3: {Parametrizations}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Chapter 3},
	url = {https://projecteuclid.org/euclid.lnms/1215466761},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/FQ57IHUA/1215466761.html:text/html}
}

@book{brown-miscellaneous-1986,
	title = {Miscellaneous {Frontmatter}},
	isbn = {978-0-940600-10-2},
	url = {https://projecteuclid.org/euclid.lnms/1215466758},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2017-10-10},
	publisher = {Institute of Mathematical Statistics},
	author = {Brown, Lawrence D.},
	year = {1986},
	file = {Snapshot:/home/user/Zotero/storage/K86T27Z6/1215466758.html:text/html}
}

@book{brown-fundamentals-1986,
	title = {Fundamentals of {Statistical} {Exponential} {Families}: {With} {Applications} in {Statistical} {Decision} {Theory}},
	isbn = {978-0-940600-10-2},
	shorttitle = {Fundamentals of {Statistical} {Exponential} {Families}},
	language = {en},
	publisher = {IMS},
	author = {Brown, Lawrence D.},
	year = {1986},
	note = {Google-Books-ID: hmMLShr43PUC},
	keywords = {Mathematics / General}
}

@misc{noauthor-terra-2017,
	title = {Terra preta},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Terra\_preta&oldid=800778025},
	abstract = {Terra preta (Portuguese pronunciation: [ˈtɛʁɐ ˈpɾetɐ], locally [ˈtɛha ˈpɾeta], literally "black soil" in Portuguese) is a type of very dark, fertile manmade (anthropogenic) soil found in the Amazon Basin. It is also known as "Amazonian dark earth" or "Indian black earth". In Portuguese its full name is terra preta do índio or terra preta de índio ("black soil of the Indian", "Indians' black earth"). Terra mulata ("mulatto earth") is lighter or brownish in color.

Terra preta owes its characteristic black color to its weathered charcoal content, and was made by adding a mixture of charcoal, bone, and manure to the otherwise relatively infertile Amazonian soil. A product of indigenous soil management and slash-and-char agriculture, the charcoal is very stable and remains in the soil for thousands of years, binding and retaining minerals and nutrients.
Terra preta is characterized by the presence of low-temperature charcoal residues in high concentrations; of high quantities of potsherds; of organic matter such as plant residues, animal feces, fish and animal bones and other material; and of nutrients such as nitrogen (N), phosphorus (P), calcium (Ca), zinc (Zn), manganese (Mn). Fertile soils such as terra preta show high levels of microorganic activities and other specific characteristics within particular ecosystems.
Terra preta zones are generally surrounded by terra comum ([ˈtɛhɐ koˈmũ] or [ˈtɛhɐ kuˈmũ]), or "common soil"; these are infertile soils, mainly acrisols, but also ferralsols and arenosols. While deforested arable soils in the Amazon are productive for just a short period of time, and farmers are constantly moving to new areas and clearing more land, the terra preta soil is less prone to nutrient leaching caused by heavy rains and floods because of its high concentration of charcoal, microbial life and organic matter; accumulating nutrients, minerals, and microorganisms.
Terra preta soils are of pre-Columbian nature and were created by humans between 450 BCE and 950 CE. The soil's depth can reach 2 meters (6.6 ft). Thousands of years after its creation, it has been reported to regenerate itself at the rate of 1 centimeter (0.39 in) per year by the local farmers and caboclos in Brazil's Amazonian basin, who seek it for use and for sale as valuable potting soil.},
	language = {en},
	urldate = {2017-10-09},
	journal = {Wikipedia},
	month = sep,
	year = {2017},
	note = {Page Version ID: 800778025},
	file = {Snapshot:/home/user/Zotero/storage/H6TK666F/index.html:text/html}
}

@article{narasimhan-improving-2016-1,
	title = {Improving information extraction by acquiring external evidence with reinforcement learning},
	url = {https://arxiv.org/abs/1603.07954},
	urldate = {2017-10-06},
	journal = {arXiv preprint arXiv:1603.07954},
	author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
	year = {2016},
	file = {rlie16.pdf:/home/user/Zotero/storage/B9PBKHG8/rlie16.pdf:application/pdf}
}

@article{morgan-abstract-2016,
	title = {Abstract knowledge versus direct experience in processing of binomial expressions},
	volume = {157},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027716302335},
	urldate = {2017-10-06},
	journal = {Cognition},
	author = {Morgan, Emily and Levy, Roger},
	year = {2016},
	pages = {384--402},
	file = {MorganLevy_binomials.pdf:/home/user/Zotero/storage/KUH2XRQI/MorganLevy_binomials.pdf:application/pdf}
}

@book{lasersohn-subjectivity-2016,
	title = {Subjectivity and perspective in truth-theoretic semantics},
	volume = {8},
	url = {https://books.google.com/books?hl=en&lr=&id=uHd4DQAAQBAJ&oi=fnd&pg=PP1&dq=%22even+having+truth+conditions,+or+to+try+to+explain+its+meaning+in+terms+of+truth+and%22+%22on+our+views+we+might+also+claim+that+there+are+no+objective+moral+facts,+or+even%22+&ots=AI93VBp4ZN&sig=l-1UQJc5CRBzT08k6izEZwr9cFM},
	urldate = {2017-10-06},
	publisher = {Oxford University Press},
	author = {Lasersohn, Peter},
	year = {2016},
	file = {3070.pdf:/home/user/Zotero/storage/RI8N29X8/3070.pdf:application/pdf}
}

@book{west-coast-conference-on-formal-linguistics-proceedings-2006,
	address = {Somerville, MA},
	title = {Proceedings of the 25th {West} {Coast} {Conference} on {Formal} {Linguistics}},
	url = {http://www.lingref.com/cpp/wccfl/25/index.html},
	abstract = {All proceedings published by Cascadilla Proceedings Project (CPP) are available free on the web.},
	language = {English},
	urldate = {2017-10-06},
	publisher = {Cascadilla Proceedings Project},
	author = {{West Coast Conference on Formal Linguistics} and Baumer, Donald and Montero, David and Scanlon, Michael},
	year = {2006},
	note = {OCLC: 954200793},
	file = {Paper.dvi - paper1473.pdf:/home/user/Zotero/storage/SP5MC97S/paper1473.pdf:application/pdf}
}

@article{he-processing-2016,
	title = {Processing the {Chinese} {Reflexive} “ziji”: {Effects} of {Featural} {Constraints} on {Anaphor} {Resolution}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Processing the {Chinese} {Reflexive} “ziji”},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.00284/full},
	doi = {10.3389/fpsyg.2016.00284},
	abstract = {We present three self-paced reading experiments that investigate the reflexive ziji ‘self’ in Chinese – in particular, we tested whether and how person-feature-based blocking guides comprehenders’ real-time processing and final interpretation of ziji. Prior work claims that in Chinese sentences like “John thought that {I/you/Bill} did not like ZIJI”, (i) the reflexive ziji can refer to the matrix subject John if the intervening subject is also a third person entity (e.g. Bill), but that (ii) an intervening first or second person pronoun blocks reference to the matrix subject, causing ziji to refer to the first or second person pronoun. However, native speakers’ judgments regarding the accessibility of long-distance antecedents are rather unstable, and researchers also disagree on what the exact configurations are that allow blocking. In addition, many open questions persist regarding the real-time processing of reflexives more generally, in particular regarding the accessibility (or lack thereof) of structurally unlicensed antecedents. We conducted three self-paced reading studies where we recorded people’s word-by-word reading times and also asked questions that probed their off-line interpretation of the reflexive ziji. People’s answers to the off-line questions show that blocking is not absolute: Comprehenders do allow significant numbers of non-local choices in both the first and the second person blocking conditions, albeit in small numbers. At the same time, the reading time data, particularly those from Experiments 2 and 3, show that comprehenders use person feature cues to quickly filter out inaccessible long-distance referents. The difference between on-line and off-line patterns points to the possibility that the interpretation of ziji unfolds over time: it seems that initially, during real-time processing, person-feature cues weigh more heavily and constrain what antecedent candidates get considered, but that at some later point, other kinds of information are also integrated and perhaps outweigh the person-feature constraint, resulting in consideration of referents that were initially ‘blocked’ due to the person-feature constraint. In sum, in addition to the structural constraints identified in prior work, person-featural cues also play a key role in regulating the on-line processing of reflexives in Chinese.},
	language = {English},
	urldate = {2017-10-06},
	journal = {Frontiers in Psychology},
	author = {He, Xiao and Kaiser, Elsi},
	year = {2016},
	keywords = {Sentence processing, Self-paced reading, Chinese, Binding Theory, Blocking effects, reflexive pronouns}
}

@book{kotowski-adjectival-2016,
	address = {Berlin, Boston},
	title = {Adjectival {Modification} and {Order} {Restrictions}, {The} {Influence} of {Temporariness} on {Prenominal} {Word} {Order}},
	isbn = {978-3-11-047638-5},
	url = {https://www.degruyter.com/viewbooktoc/product/472464},
	language = {ENGL},
	urldate = {2017-10-06},
	publisher = {De Gruyter Mouton},
	author = {Kotowski, Sven},
	year = {2016},
	doi = {10.1515/9783110478457},
	keywords = {Syntax, word order, Object modification}
}

@article{breban-functional-cognitive-2016,
	title = {A functional-cognitive analysis of the order of adjectival modifiers in the {English} {NP}},
	url = {https://lirias.kuleuven.be/handle/123456789/567069},
	language = {en\_US},
	urldate = {2017-10-06},
	author = {Breban, Tine and Davidse, Kristin},
	month = jan,
	year = {2016},
	file = {Snapshot:/home/user/Zotero/storage/S7BHCVMI/567069.html:text/html}
}

@article{kemmerer-knowledge-2009-1,
	title = {Knowledge of the {Semantic} {Constraints} on {Adjective} {Order} {Can} {Be} {Selectively} {Impaired}},
	volume = {22},
	issn = {0911-6044},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2859704/},
	doi = {10.1016/j.jneuroling.2008.07.001},
	abstract = {When multiple adjectives are used to modify a noun, they tend to be sequenced in the following way according to semantic class: value {\textgreater} size {\textgreater} dimension {\textgreater} various physical properties {\textgreater} color. To investigate the neural substrates of these semantic constraints on adjective order, we administered a battery of three tests to 34 brain-damaged patients and 19 healthy participants. Six patients manifested the following performance profile. First, they failed a test that required them to discriminate between semantically determined correct and incorrect sequences of adjectives—e.g., thick blue towel vs. *blue thick towel. Second, they passed a test that assessed their knowledge of two purely syntactic aspects of adjective order—specifically, that adjectives can precede nouns, and that adjectives can precede other adjectives. Finally, they also passed a test that assessed their knowledge of the categorical (i.e., class-level) features of adjective meanings that interact with the semantic constraints underlying adjective order—e.g., that thick is a dimensional adjective and that blue is a color adjective. Taken together, these behavioral findings suggest that the six patients have selectively impaired knowledge of the abstract principles that determine how different semantic classes of adjectives are typically mapped onto different syntactic positions in NPs. To identify the neuroanatomical lesion patterns that tend to correlate with defective processing of adjective order, we combined lesion data from the six patients just described with lesion data from six other patients who we reported in a previous study as having similar impairments [. Selective impairment of knowledge underlying adjective order: Evidence for the autonomy of grammatical semantics. Journal of Neurolinguistics, 13, 57–82.] We found that the most common areas of damage included the left posterior inferior frontal gyrus and the left inferior parietal lobule. Overall, these results shed new light on the neural substrates of the syntax-semantics interface.},
	number = {1},
	urldate = {2017-10-06},
	journal = {Journal of neurolinguistics},
	author = {Kemmerer, David and Tranel, Daniel and Zdanczyk, Cynthia},
	month = jan,
	year = {2009},
	pmid = {20428488},
	pmcid = {PMC2859704},
	pages = {91--108},
	file = {PubMed Central Full Text PDF:/home/user/Zotero/storage/XEUNCV49/Kemmerer et al. - 2009 - Knowledge of the Semantic Constraints on Adjective.pdf:application/pdf}
}

@misc{noauthor-subjectivity-2016,
	title = {Subjectivity and {Perspective} in {Truth}-{Theoretic} {Semantics} - {Oxford} {Scholarship}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199573677.001.0001/acprof-9780199573677},
	abstract = {This book develops and defends a semantic theory which respects the intuition that there may be no “fact of the matter” which determines truth values for certain kinds of sentences—those dealing with matters of personal taste, and other sentences about which “faultless disagreement” is possible. The analysis nonetheless takes truth and falsity as central to explaining meaning, and uses familiar techniques from modern (formal, truth-conditional, logical) semantic theory. It accounts for non-factual meaning by relativizing the truth values of sentence contents to parameters whose values are not always objectively or factually determinable. Explaining such contents does not require a thoroughgoing reconstruction of the foundations of semantics on non-truth-theoretic grounds; nor does a defense of truth-theoretic semantics require that we deny the existence of sentences whose contents are true or false only subjectively, treating them instead as though they made purely factual claims. The book presents syntactic and semantic rules for a substantial fragment of English, including tense, locatives, infinitival clauses, and other constructions. Special attention is given to the semantics of attitude reports, including reports of de se and other “centered” attitudes. The semantic analysis is paired with a pragmatic theory exploring the nature of assertion and truth assessment if truth is relativized in the way suggested. Finally, the book gives a speculative account of the functional motivation for relativism in truth assignment, by treating truth as idealized reliability.},
	urldate = {2017-10-06},
	month = dec,
	year = {2016},
	file = {Snapshot:/home/user/Zotero/storage/XXRD9NBB/acprof-9780199573677.html:text/html}
}

@article{palmira-semantic-2015,
	title = {The {Semantic} {Significance} of {Faultless} {Disagreement}},
	volume = {96},
	issn = {1468-0114},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/papq.12038/abstract},
	doi = {10.1111/papq.12038},
	abstract = {The article investigates the significance of the so-called phenomenon of apparent faultless disagreement for debates about the semantics of taste discourse. Two kinds of description of the phenomenon are proposed. The first ensures that faultless disagreement raises a distinctive philosophical challenge; yet, it is argued that Contextualist, Realist and Relativist semantic theories do not account for this description. The second, by contrast, makes the phenomenon irrelevant for the problem of what the right semantics of taste discourse should be. Lastly, the following dilemma is assessed: either faultless disagreement provides strong evidence against semantic theories; or its significance should be considerably downplayed.},
	language = {en},
	number = {3},
	urldate = {2017-10-06},
	journal = {Pacific Philosophical Quarterly},
	author = {Palmira, Michele},
	month = sep,
	year = {2015},
	pages = {349--371},
	file = {Full Text PDF:/home/user/Zotero/storage/VJ7S7MP6/Palmira - 2015 - The Semantic Significance of Faultless Disagreemen.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TUJ4JBPF/abstract.html:text/html}
}

@article{li-distributional-2017,
	title = {Distributional {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1706.09549},
	abstract = {We propose a framework for adversarial training that relies on a sample rather than a single sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-sample tests between probability distributions, we propose two such distributional adversaries that operate and predict on samples, and show how they can be easily implemented on top of existing models. Various experimental results show that generators trained with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with pointwise prediction discriminators. The application of our framework to domain adaptation also results in considerable improvement over recent state-of-the-art.},
	urldate = {2017-10-04},
	journal = {arXiv:1706.09549 [cs]},
	author = {Li, Chengtao and Alvarez-Melis, David and Xu, Keyulu and Jegelka, Stefanie and Sra, Suvrit},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09549},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1706.09549 PDF:/home/user/Zotero/storage/ZAQBG4X2/Li et al. - 2017 - Distributional Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/34KPV6XC/1706.html:text/html}
}

@article{patel-exploring-2017,
	title = {Exploring {Cognitive} {Relations} {Between} {Prediction} in {Language} and {Music}},
	volume = {41},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12411/abstract},
	doi = {10.1111/cogs.12411},
	abstract = {The online processing of both music and language involves making predictions about upcoming material, but the relationship between prediction in these two domains is not well understood. Electrophysiological methods for studying individual differences in prediction in language processing have opened the door to new questions. Specifically, we ask whether individuals with musical training predict upcoming linguistic material more strongly and/or more accurately than non-musicians. We propose two reasons why prediction in these two domains might be linked: (a) Musicians may have greater verbal short-term/working memory; (b) music may specifically reward predictions based on hierarchical structure. We provide suggestions as to how to expand upon recent work on individual differences in language processing to test these hypotheses.},
	language = {en},
	urldate = {2017-10-03},
	journal = {Cognitive Science},
	author = {Patel, Aniruddh D. and Morgan, Emily},
	month = mar,
	year = {2017},
	keywords = {language, individual differences, prediction, ERP, music},
	pages = {303--320},
	file = {Full Text PDF:/home/user/Zotero/storage/EKGWIVFP/Patel and Morgan - 2017 - Exploring Cognitive Relations Between Prediction i.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ZQBUPR9X/abstract.html:text/html}
}

@article{narasimhan-deep-2017,
	title = {Deep {Transfer} in {Reinforcement} {Learning} by {Language} {Grounding}},
	url = {http://arxiv.org/abs/1708.00133},
	abstract = {In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized representation to effectively utilize entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments.},
	urldate = {2017-10-03},
	journal = {arXiv:1708.00133 [cs]},
	author = {Narasimhan, Karthik and Barzilay, Regina and Jaakkola, Tommi},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.00133},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1708.00133 PDF:/home/user/Zotero/storage/VPHX3PHJ/Narasimhan et al. - 2017 - Deep Transfer in Reinforcement Learning by Languag.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/S7MKQQD3/1708.html:text/html}
}

@misc{noauthor-timothy-nodate-1,
	title = {Timothy {O}'{Donnell}},
	url = {http://people.linguistics.mcgill.ca/~timothy.odonnell/},
	urldate = {2017-10-03},
	file = {Timothy O'Donnell:/home/user/Zotero/storage/VFGD85E8/~timothy.html:text/html}
}

@article{wang-exploiting-2017,
	title = {Exploiting {Cross}-{Sentence} {Context} for {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1704.04347},
	urldate = {2017-10-02},
	journal = {arXiv preprint arXiv:1704.04347},
	author = {Wang, Longyue and Tu, Zhaopeng and Way, Andy and Liu, Qun},
	year = {2017},
	file = {Exploiting Cross-Sentence Context for Neural Machine Translation - D17-1300.pdf:/home/user/Zotero/storage/G9CMN7BR/D17-1300.pdf:application/pdf}
}

@article{fan-transfer-2017,
	title = {Transfer {Learning} for {Neural} {Semantic} {Parsing}},
	url = {https://arxiv.org/abs/1706.04326},
	urldate = {2017-10-02},
	journal = {arXiv preprint arXiv:1706.04326},
	author = {Fan, Xing and Monti, Emilio and Mathias, Lambert and Dreyer, Markus},
	year = {2017},
	file = {Transfer Learning for Neural Semantic Parsing - W17-2607.pdf:/home/user/Zotero/storage/6QNZPFU9/W17-2607.pdf:application/pdf}
}

@article{tang-rethinking-2017,
	title = {Rethinking {Skip}-thought: {A} {Neighborhood} based {Approach}},
	shorttitle = {Rethinking {Skip}-thought},
	url = {https://arxiv.org/abs/1706.03146},
	urldate = {2017-10-02},
	journal = {arXiv preprint arXiv:1706.03146},
	author = {Tang, Shuai and Jin, Hailin and Fang, Chen and Wang, Zhaowen and de Sa, Virginia R.},
	year = {2017},
	file = {Rethinking Skip-thought\: A Neighborhood based Approach - W17-2625.pdf:/home/user/Zotero/storage/S84U3DEH/W17-2625.pdf:application/pdf}
}

@article{liu-learning-2017,
	title = {Learning {Structured} {Text} {Representations}},
	url = {http://arxiv.org/abs/1705.09207},
	abstract = {In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.},
	urldate = {2017-10-02},
	journal = {arXiv:1705.09207 [cs]},
	author = {Liu, Yang and Lapata, Mirella},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09207},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1705.09207 PDF:/home/user/Zotero/storage/38GZDTTF/Liu and Lapata - 2017 - Learning Structured Text Representations.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/NXVZS275/1705.html:text/html}
}

@book{hawkins-performance-1994,
	title = {A performance theory of order and constituency},
	volume = {73},
	publisher = {Cambridge University Press},
	author = {Hawkins, John A},
	year = {1994}
}

@article{rijkhoff-explaining-1990,
	title = {Explaining word order in the noun phrase},
	volume = {28},
	number = {1},
	journal = {Linguistics},
	author = {Rijkhoff, Jan},
	year = {1990},
	pages = {5--42}
}

@article{abney-memory-1991,
	title = {Memory requirements and local ambiguities of parsing strategies},
	volume = {20},
	number = {3},
	journal = {Journal of Psycholinguistic Research},
	author = {Abney, Steven P and Johnson, Mark},
	year = {1991},
	pages = {233--250}
}

@phdthesis{gibson-computational-1991,
	type = {{PhD} {Thesis}},
	title = {A computational theory of human linguistic processing: {Memory} limitations and processing breakdown},
	school = {Carnegie Mellon University Pittsburgh, PA},
	author = {Gibson, Edward Albert Fletcher},
	year = {1991}
}

@inproceedings{resnik-left-corner-1992,
	title = {Left-corner parsing and psychological plausibility},
	booktitle = {Proceedings of the 14th conference on {Computational} linguistics-{Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Resnik, Philip},
	year = {1992},
	pages = {191--197}
}

@article{grodner-consequences-2005,
	title = {Consequences of the serial nature of linguistic input for sentenial complexity},
	volume = {29},
	number = {2},
	journal = {Cognitive science},
	author = {Grodner, Daniel and Gibson, Edward},
	year = {2005},
	pages = {261--290}
}

@inproceedings{ron-power-nodate,
	title = {The power of amnesia},
	abstract = {We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that gen-erate the process, whereas on large scales, more syntactic and se-mantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long mem-ory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the sta-tistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural En-glish text and applying the learned model to the correction of cor-rupted text. Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar num-ber of states. We also show how the algorithm can be applied to intergenic E. coli DNA base prediction with results comparable to HMM based methods. 1},
	booktitle = {Machine {Learning}},
	author = {Ron, Dana and Singer, Yoram and Tishby, Naftali},
	pages = {25--1996},
	file = {Citeseer - Full Text PDF:/home/user/Zotero/storage/4QR3FFCE/Ron et al. - The power of amnesia.pdf:application/pdf;Citeseer - Snapshot:/home/user/Zotero/storage/EMMVIB4E/summary.html:text/html}
}

@article{apostolico-optimal-nodate,
	title = {Optimal {Amnesic} {Probabilistic} {Automata} or {How} to {Learn} and {Classify} {Proteins} in {Linear} {Time} and {Space}},
	language = {en},
	author = {APOSTOLICO, ALBERTO and BEJERANO, GILL},
	pages = {13},
	file = {APOSTOLICO and BEJERANO - Optimal Amnesic Probabilistic Automata or How to L.pdf:/home/user/Zotero/storage/SLDDQ2SS/APOSTOLICO and BEJERANO - Optimal Amnesic Probabilistic Automata or How to L.pdf:application/pdf}
}

@article{oates-control-2014,
	title = {Control functionals for {Monte} {Carlo} integration},
	url = {http://arxiv.org/abs/1410.2392},
	abstract = {A non-parametric extension of control variates is presented. These leverage gradient information on the sampling density to achieve substantial variance reduction. It is not required that the sampling density be normalised. The novel contribution of this work is based on two important insights; (i) a trade-oﬀ between random sampling and deterministic approximation and (ii) a new gradient-based function space derived from Stein’s identity. Unlike classical control variates, our estimators achieve super-root-n convergence, often requiring orders of magnitude fewer simulations to achieve a ﬁxed level of precision. Theoretical and empirical results are presented, the latter focusing on integration problems arising in hierarchical models and models based on non-linear ordinary diﬀerential equations.},
	language = {en},
	urldate = {2018-07-04},
	journal = {arXiv:1410.2392 [stat]},
	author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.2392},
	keywords = {Statistics - Methodology},
	file = {Oates et al. - 2014 - Control functionals for Monte Carlo integration.pdf:/home/user/Zotero/storage/ZJJQ3SA7/Oates et al. - 2014 - Control functionals for Monte Carlo integration.pdf:application/pdf}
}

@article{mochihashi-infinite-nodate,
	title = {The {Infinite} {Markov} {Model}},
	abstract = {We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data.},
	language = {en},
	author = {Mochihashi, Daichi and Sumita, Eiichiro},
	pages = {8},
	file = {Mochihashi and Sumita - The Infinite Markov Model.pdf:/home/user/Zotero/storage/8IGQGV3Q/Mochihashi and Sumita - The Infinite Markov Model.pdf:application/pdf}
}

@inproceedings{mochihashi-bayesian-2009,
	title = {Bayesian unsupervised word segmentation with nested {Pitman}-{Yor} language modeling},
	volume = {1},
	isbn = {978-1-932432-45-9},
	url = {http://portal.acm.org/citation.cfm?doid=1687878.1687894},
	doi = {10.3115/1687878.1687894},
	abstract = {In this paper, we propose a new Bayesian model for fully unsupervised word segmentation and an efﬁcient blocked Gibbs sampler combined with dynamic programming for inference. Our model is a nested hierarchical Pitman-Yor language model, where Pitman-Yor spelling model is embedded in the word model. We conﬁrmed that it signiﬁcantly outperforms previous reported results in both phonetic transcripts and standard datasets for Chinese and Japanese word segmentation. Our model is also considered as a way to construct an accurate word n-gram language model directly from characters of arbitrary language, without any “word” indications.},
	language = {en},
	urldate = {2018-07-02},
	publisher = {Association for Computational Linguistics},
	author = {Mochihashi, Daichi and Yamada, Takeshi and Ueda, Naonori},
	year = {2009},
	pages = {100},
	file = {Mochihashi et al. - 2009 - Bayesian unsupervised word segmentation with neste.pdf:/home/user/Zotero/storage/JVN3XHGE/Mochihashi et al. - 2009 - Bayesian unsupervised word segmentation with neste.pdf:application/pdf}
}

@article{johnson-improving-nodate,
	title = {Improving nonparameteric {Bayesian} inference: experiments on unsupervised word segmentation with adaptor grammars},
	abstract = {One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for deﬁning a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87\% word token f-score on the standard Brent version of the BernsteinRatner corpus, which is an error reduction of over 35\% over the best previously reported results for this corpus.},
	language = {en},
	author = {Johnson, Mark and Goldwater, Sharon},
	pages = {9},
	file = {Johnson and Goldwater - Improving nonparameteric Bayesian inference exper.pdf:/home/user/Zotero/storage/2F7IKWSI/Johnson and Goldwater - Improving nonparameteric Bayesian inference exper.pdf:application/pdf}
}

@inproceedings{kann-fortification-2018,
	address = {New Orleans, Louisiana},
	title = {Fortification of {Neural} {Morphological} {Segmentation} {Models} for {Polysynthetic} {Minimal}-{Resource} {Languages}},
	url = {http://www.aclweb.org/anthology/N18-1005},
	abstract = {Morphological segmentation for polysynthetic languages is challenging, because a word may consist of many individual morphemes and training data can be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define the state of the art for morphological segmentation in high-resource settings and for (mostly) European languages, we first show that they also obtain competitive performance for Mexican polysynthetic languages in minimal-resource settings. We then propose two novel multi-task training approaches—one with, one without need for external unlabeled resources—, and two corresponding data augmentation methods, improving over the neural baseline for all languages. Finally, we explore cross-lingual transfer as a third way to fortify our neural model and show that we can train one single multi-lingual model for related languages while maintaining comparable or even improved performance, thus reducing the amount of parameters by close to 75\%. We provide our morphological segmentation datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki for future research.},
	urldate = {2018-06-28},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kann, Katharina and Mager Hois, Jesus Manuel and Meza Ruiz, Ivan Vladimir and Schütze, Hinrich},
	month = jun,
	year = {2018},
	pages = {47--57},
	file = {Full Text PDF:/home/user/Zotero/storage/N4XMSKBA/Kann et al. - 2018 - Fortification of Neural Morphological Segmentation.pdf:application/pdf}
}

@article{godard-unsupervised-2018,
	title = {Unsupervised {Word} {Segmentation} from {Speech} with {Attention}},
	url = {http://arxiv.org/abs/1806.06734},
	abstract = {We present a first attempt to perform attentional word segmentation directly from the speech signal, with the final goal to automatically identify lexical units in a low-resource, unwritten language (UL). Our methodology assumes a pairing between recordings in the UL with translations in a well-resourced language. It uses Acoustic Unit Discovery (AUD) to convert speech into a sequence of pseudo-phones that is segmented using neural soft-alignments produced by a neural machine translation model. Evaluation uses an actual Bantu UL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the potential of attentional word segmentation for language documentation.},
	urldate = {2018-06-28},
	journal = {arXiv:1806.06734 [cs]},
	author = {Godard, Pierre and Zanon-Boito, Marcely and Ondel, Lucas and Berard, Alexandre and Yvon, François and Villavicencio, Aline and Besacier, Laurent},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.06734},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1806.06734 PDF:/home/user/Zotero/storage/A998GEZY/Godard et al. - 2018 - Unsupervised Word Segmentation from Speech with At.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/4M63T5D6/1806.html:text/html}
}

@article{chung-audio-2016,
	title = {Audio {Word}2Vec: {Unsupervised} {Learning} of {Audio} {Segment} {Representations} using {Sequence}-to-sequence {Autoencoder}},
	shorttitle = {Audio {Word}2Vec},
	url = {http://arxiv.org/abs/1603.00982},
	abstract = {The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning.},
	urldate = {2018-06-28},
	journal = {arXiv:1603.00982 [cs]},
	author = {Chung, Yu-An and Wu, Chao-Chung and Shen, Chia-Hao and Lee, Hung-Yi and Lee, Lin-Shan},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.00982},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning},
	file = {arXiv\:1603.00982 PDF:/home/user/Zotero/storage/EI3FYYY5/Chung et al. - 2016 - Audio Word2Vec Unsupervised Learning of Audio Seg.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/G9ALSLSN/1603.html:text/html}
}

@article{kamper-unsupervised-2016,
	title = {Unsupervised word segmentation and lexicon discovery using acoustic word embeddings},
	volume = {24},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1603.02845},
	doi = {10.1109/TASLP.2016.2517567},
	abstract = {In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20\% error rate, outperforming a previous HMM-based system by about 10\% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.},
	number = {4},
	urldate = {2018-06-28},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
	month = apr,
	year = {2016},
	note = {arXiv: 1603.02845},
	keywords = {Computer Science - Computation and Language},
	pages = {669--679},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/V75M6523/1603.html:text/html}
}

@article{cao-joint-2016,
	title = {A {Joint} {Model} for {Word} {Embedding} and {Word} {Morphology}},
	url = {http://arxiv.org/abs/1606.02601},
	abstract = {This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.},
	urldate = {2018-06-28},
	journal = {arXiv:1606.02601 [cs]},
	author = {Cao, Kris and Rei, Marek},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.02601},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1606.02601 PDF:/home/user/Zotero/storage/LJBBJPU2/Cao and Rei - 2016 - A Joint Model for Word Embedding and Word Morpholo.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GKPG6DZ3/1606.html:text/html}
}

@article{oates-control-2014-1,
	title = {Control functionals for {Monte} {Carlo} integration},
	url = {http://arxiv.org/abs/1410.2392},
	abstract = {A non-parametric extension of control variates is presented. These leverage gradient information on the sampling density to achieve substantial variance reduction. It is not required that the sampling density be normalised. The novel contribution of this work is based on two important insights; (i) a trade-oﬀ between random sampling and deterministic approximation and (ii) a new gradient-based function space derived from Stein’s identity. Unlike classical control variates, our estimators achieve super-root-n convergence, often requiring orders of magnitude fewer simulations to achieve a ﬁxed level of precision. Theoretical and empirical results are presented, the latter focusing on integration problems arising in hierarchical models and models based on non-linear ordinary diﬀerential equations.},
	language = {en},
	urldate = {2018-07-04},
	journal = {arXiv:1410.2392 [stat]},
	author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.2392},
	keywords = {Statistics - Methodology},
	file = {Oates et al. - 2014 - Control functionals for Monte Carlo integration.pdf:/home/user/Zotero/storage/GYR86XWW/Oates et al. - 2014 - Control functionals for Monte Carlo integration.pdf:application/pdf}
}

@article{ozen-building-2017,
	title = {Building {Morphological} {Chains} for {Agglutinative} {Languages}},
	url = {http://arxiv.org/abs/1705.02314},
	abstract = {In this paper, we build morphological chains for agglutinative languages by using a log-linear model for the morphological segmentation task. The model is based on the unsupervised morphological segmentation system called MorphoChains. We extend MorphoChains log linear model by expanding the candidate space recursively to cover more split points for agglutinative languages such as Turkish, whereas in the original model candidates are generated by considering only binary segmentation of each word. The results show that we improve the state-of-art Turkish scores by 12\% having a F-measure of 72\% and we improve the English scores by 3\% having a F-measure of 74\%. Eventually, the system outperforms both MorphoChains and other well-known unsupervised morphological segmentation systems. The results indicate that candidate generation plays an important role in such an unsupervised log-linear model that is learned using contrastive estimation with negative samples.},
	urldate = {2018-06-28},
	journal = {arXiv:1705.02314 [cs]},
	author = {Ozen, Serkan and Can, Burcu},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02314},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/Z9RPNEM2/1705.html:text/html}
}

@article{kurfali-trie-structured-2017,
	title = {A {Trie}-{Structured} {Bayesian} {Model} for {Unsupervised} {Morphological} {Segmentation}},
	url = {http://arxiv.org/abs/1704.07329},
	abstract = {In this paper, we introduce a trie-structured Bayesian model for unsupervised morphological segmentation. We adopt prior information from different sources in the model. We use neural word embeddings to discover words that are morphologically derived from each other and thereby that are semantically similar. We use letter successor variety counts obtained from tries that are built by neural word embeddings. Our results show that using different information sources such as neural word embeddings and letter successor variety as prior information improves morphological segmentation in a Bayesian model. Our model outperforms other unsupervised morphological segmentation models on Turkish and gives promising results on English and German for scarce resources.},
	urldate = {2018-06-28},
	journal = {arXiv:1704.07329 [cs]},
	author = {Kurfalı, Murathan and Üstün, Ahmet and Can, Burcu},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07329},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/AJ3JNZ5A/1704.html:text/html}
}

@article{ondel-bayesian-2018,
	title = {Bayesian {Models} for {Unit} {Discovery} on a {Very} {Low} {Resource} {Language}},
	url = {http://arxiv.org/abs/1802.06053},
	abstract = {Developing speech technologies for low-resource languages has become a very active research field over the last decade. Among others, Bayesian models have shown some promising results on artificial examples but still lack of in situ experiments. Our work applies state-of-the-art Bayesian models to unsupervised Acoustic Unit Discovery (AUD) in a real low-resource language scenario. We also show that Bayesian models can naturally integrate information from other resourceful languages by means of informative prior leading to more consistent discovered units. Finally, discovered acoustic units are used, either as the 1-best sequence or as a lattice, to perform word segmentation. Word segmentation results show that this Bayesian approach clearly outperforms a Segmental-DTW baseline on the same corpus.},
	urldate = {2018-06-28},
	journal = {arXiv:1802.06053 [cs]},
	author = {Ondel, Lucas and Godard, Pierre and Besacier, Laurent and Larsen, Elin and Hasegawa-Johnson, Mark and Scharenborg, Odette and Dupoux, Emmanuel and Burget, Lukas and Yvon, François and Khudanpur, Sanjeev},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06053},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1802.06053 PDF:/home/user/Zotero/storage/HIKVHH2I/Ondel et al. - 2018 - Bayesian Models for Unit Discovery on a Very Low R.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/NYYL32GJ/1802.html:text/html}
}

@article{ataman-linguistically-2017,
	title = {Linguistically {Motivated} {Vocabulary} {Reduction} for {Neural} {Machine} {Translation} from {Turkish} to {English}},
	url = {http://arxiv.org/abs/1707.09879},
	abstract = {The necessity of using a fixed-size word vocabulary in order to control the model complexity in state-of-the-art neural machine translation (NMT) systems is an important bottleneck on performance, especially for morphologically rich languages. Conventional methods that aim to overcome this problem by using sub-word or character-level representations solely rely on statistics and disregard the linguistic properties of words, which leads to interruptions in the word structure and causes semantic and syntactic losses. In this paper, we propose a new vocabulary reduction method for NMT, which can reduce the vocabulary of a given input corpus at any rate while also considering the morphological properties of the language. Our method is based on unsupervised morphology learning and can be, in principle, used for pre-processing any language pair. We also present an alternative word segmentation method based on supervised morphological analysis, which aids us in measuring the accuracy of our model. We evaluate our method in Turkish-to-English NMT task where the input language is morphologically rich and agglutinative. We analyze different representation methods in terms of translation accuracy as well as the semantic and syntactic properties of the generated output. Our method obtains a significant improvement of 2.3 BLEU points over the conventional vocabulary reduction technique, showing that it can provide better accuracy in open vocabulary translation of morphologically rich languages.},
	urldate = {2018-06-28},
	journal = {arXiv:1707.09879 [cs]},
	author = {Ataman, Duygu and Negri, Matteo and Turchi, Marco and Federico, Marcello},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.09879},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/user/Zotero/storage/VP73T373/1707.html:text/html}
}

@article{gomez-rodriguez-natural-2016,
	title = {Natural language processing and the {Now}-or-{Never} bottleneck},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000795},
	abstract = {Researchers, motivated by the need to improve the efficiency of natural language processing tools to handle web-scale data, have recently arrived at models that remarkably match the expected features of human language processing under the Now-or-Never bottleneck framework. This provides additional support for said framework and highlights the research potential in the interaction between applied computational linguistics and cognitive science.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Gómez-Rodríguez, Carlos},
	month = jan,
	year = {2016},
	pmid = {27561430},
	keywords = {Humans, Linguistics, Models, Theoretical, Natural Language Processing, Language, Cognitive Science},
	pages = {e74}
}

@article{dumitru-gestalt-like-2016,
	title = {Gestalt-like representations hijack {Chunk}-and-{Pass} processing},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000758},
	abstract = {Christiansen \& Chater (C\&C) make two related and somewhat contradictory claims, namely that the ever abstract language representations built during Chunk-and-Pass processing allow for ever greater interference from extra-linguistic information, and that it is nevertheless the language system that re-codes incoming information into abstract representations. I analyse these claims and discuss evidence suggesting that Gestalt-like representations hijack Chunk-and-Pass processing.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Dumitru, Magda L.},
	month = jan,
	year = {2016},
	pmid = {27562687},
	keywords = {Humans, Linguistics, Language},
	pages = {e69}
}

@article{huyck-neural-2016,
	title = {Neural constraints and flexibility in language processing},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000837},
	abstract = {Humans process language with their neurons. Memory in neurons is supported by neural firing and by short- and long-term synaptic weight change; the emergent behaviour of neurons, synchronous firing, and cell assembly dynamics is also a form of memory. As the language signal moves to later stages, it is processed with different mechanisms that are slower but more persistent.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Huyck, Christian R.},
	month = jan,
	year = {2016},
	pmid = {27561969},
	keywords = {Humans, Memory, Models, Neurological, Neurons, Language},
	pages = {e78}
}

@article{kempson-mechanisms-2016,
	title = {Mechanisms for interaction: {Syntax} as procedures for online interactive meaning building},
	volume = {39},
	issn = {1469-1825},
	shorttitle = {Mechanisms for interaction},
	doi = {10.1017/S0140525X15000849},
	abstract = {We argue that to reflect participant interactivity in conversational dialogue, the Christiansen \& Chater (C\&C) perspective needs a formal grammar framework capturing word-by-word incrementality, as in Dynamic Syntax, in which syntax is the incremental building of semantic representations reflecting real-time parsing dynamics. We demonstrate that, with such formulation, syntactic, semantic, and morpho-syntactic dependencies are all analysable as grounded in their potential for interaction.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Kempson, Ruth and Chatzikyriakidis, Stergios and Cann, Ronnie},
	month = jan,
	year = {2016},
	pmid = {27562087},
	keywords = {Humans, Linguistics, Semantics, Communication, Language},
	pages = {e79}
}

@article{levinson-process-2016,
	title = {"{Process} and perish" or multiple buffers with push-down stacks?},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000862},
	abstract = {This commentary raises two issues: (1) Language processing is hastened not only by internal pressures but also externally by turn-taking in language use; (2) the theory requires nested levels of processing, but linguistic levels do not fully nest; further, it would seem to require multiple memory buffers, otherwise there's no obvious treatment for discontinuous structures, or for verbatim recall.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Levinson, Stephen C.},
	month = jan,
	year = {2016},
	pmid = {27562324},
	keywords = {Humans, Linguistics, Memory, Language, Buffers, Mental Recall},
	pages = {e81}
}

@article{lewis-linguistic-2016,
	title = {Linguistic structure emerges through the interaction of memory constraints and communicative pressures},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000874},
	abstract = {If memory constraints were the only limitation on language processing, the best possible language would be one with only one word. But to explain the rich structure of language, we need to posit a second constraint: the pressure to communicate informatively. Many aspects of linguistic structure can be accounted for by appealing to equilibria that result from these two pressures.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Lewis, Molly L. and Frank, Michael C.},
	month = jan,
	year = {2016},
	pmid = {27562423},
	keywords = {Humans, Linguistics, Memory, Communication, Language},
	pages = {e82}
}

@article{lotem-bottleneck-2016,
	title = {The bottleneck may be the solution, not the problem},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000886},
	abstract = {As a highly consequential biological trait, a memory "bottleneck" cannot escape selection pressures. It must therefore co-evolve with other cognitive mechanisms rather than act as an independent constraint. Recent theory and an implemented model of language acquisition suggest that a limit on working memory may evolve to help learning. Furthermore, it need not hamper the use of language for communication.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Lotem, Arnon and Kolodny, Oren and Halpern, Joseph Y. and Onnis, Luca and Edelman, Shimon},
	month = jan,
	year = {2016},
	pmid = {27562516},
	keywords = {Humans, Language Development, Learning, Language, Memory, Short-Term},
	pages = {e83}
}

@article{medeiros-many-2016,
	title = {Many important language universals are not reducible to processing or cognition},
	volume = {39},
	issn = {1469-1825},
	doi = {10.1017/S0140525X15000722},
	abstract = {Christiansen \& Chater (C\&C) ignore the many linguistic universals that cannot be reduced to processing or cognitive constraints, some of which we present. Their claim that grammar is merely acquired language processing skill cannot account for such universals. Their claim that all other universal properties are historically and culturally based is a nonsequitur about language evolution, lacking data.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Medeiros, David P. and Piattelli-Palmarini, Massimo and Bever, Thomas G.},
	month = jan,
	year = {2016},
	pmid = {27562411},
	keywords = {Humans, Linguistics, Cognition, Language, Biological Evolution},
	pages = {e86}
}

@article{chacon-linguistic-2016,
	title = {Linguistic representations and memory architectures: {The} devil is in the details},
	volume = {39},
	issn = {1469-1825},
	shorttitle = {Linguistic representations and memory architectures},
	doi = {10.1017/S0140525X15000746},
	abstract = {Attempts to explain linguistic phenomena as consequences of memory constraints require detailed specification of linguistic representations and memory architectures alike. We discuss examples of supposed locality biases in language comprehension and production, and their link to memory constraints. Findings do not generally favor Christiansen \& Chater's (C\&C's) approach. We discuss connections to debates that stretch back to the nineteenth century.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Chacón, Dustin Alfonso and Momma, Shota and Phillips, Colin},
	month = jan,
	year = {2016},
	pmid = {27562607},
	keywords = {Humans, Linguistics, Memory, Language},
	pages = {e68}
}

@article{christiansen-now-or-never-2016,
	title = {The {Now}-or-{Never} bottleneck: {A} fundamental constraint on language},
	volume = {39},
	issn = {1469-1825},
	shorttitle = {The {Now}-or-{Never} bottleneck},
	doi = {10.1017/S0140525X1500031X},
	abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this "Now-or-Never" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must "eagerly" recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with "Right-First-Time"; once the original input is lost, there is no way for the language system to recover. This is "Chunk-and-Pass" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
	language = {eng},
	journal = {The Behavioral and Brain Sciences},
	author = {Christiansen, Morten H. and Chater, Nick},
	month = jan,
	year = {2016},
	pmid = {25869618},
	keywords = {Humans, psycholinguistics, language acquisition, Time Factors, language evolution, prediction, Cognition, Brain, Language, language processing, chunking, grammaticalization, incremental interpretation, online learning, processing bottleneck},
	pages = {e62}
}

@article{ozen-building-2017-1,
	title = {Building {Morphological} {Chains} for {Agglutinative} {Languages}},
	url = {http://arxiv.org/abs/1705.02314},
	abstract = {In this paper, we build morphological chains for agglutinative languages by using a log-linear model for the morphological segmentation task. The model is based on the unsupervised morphological segmentation system called MorphoChains. We extend MorphoChains log linear model by expanding the candidate space recursively to cover more split points for agglutinative languages such as Turkish, whereas in the original model candidates are generated by considering only binary segmentation of each word. The results show that we improve the state-of-art Turkish scores by 12\% having a F-measure of 72\% and we improve the English scores by 3\% having a F-measure of 74\%. Eventually, the system outperforms both MorphoChains and other well-known unsupervised morphological segmentation systems. The results indicate that candidate generation plays an important role in such an unsupervised log-linear model that is learned using contrastive estimation with negative samples.},
	urldate = {2018-06-28},
	journal = {arXiv:1705.02314 [cs]},
	author = {Ozen, Serkan and Can, Burcu},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02314},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1705.02314 PDF:/home/user/Zotero/storage/HF8I3E5A/Ozen and Can - 2017 - Building Morphological Chains for Agglutinative La.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/4M8GAHZV/1705.html:text/html}
}

@article{kamper-segmental-2017,
	title = {A segmental framework for fully-unsupervised large-vocabulary speech recognition},
	volume = {46},
	issn = {08852308},
	url = {http://arxiv.org/abs/1606.06950},
	doi = {10.1016/j.csl.2017.04.008},
	abstract = {Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units---effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported---in the order of 70--80\% for speaker-dependent and 80--95\% for speaker-independent systems---highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system's discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.},
	urldate = {2018-06-28},
	journal = {Computer Speech \& Language},
	author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
	month = nov,
	year = {2017},
	note = {arXiv: 1606.06950},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {154--174},
	file = {arXiv\:1606.06950 PDF:/home/user/Zotero/storage/Q2E8JDU5/Kamper et al. - 2017 - A segmental framework for fully-unsupervised large.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/LGASVLS3/1606.html:text/html}
}

@article{chen-incremental-2016,
	title = {Incremental {Learning} for {Fully} {Unsupervised} {Word} {Segmentation} {Using} {Penalized} {Likelihood} and {Model} {Selection}},
	url = {http://arxiv.org/abs/1607.05822},
	abstract = {We present a novel incremental learning approach for unsupervised word segmentation that combines features from probabilistic modeling and model selection. This includes super-additive penalties for addressing the cognitive burden imposed by long word formation, and new model selection criteria based on higher-order generative assumptions. Our approach is fully unsupervised; it relies on a small number of parameters that permits flexible modeling and a mechanism that automatically learns parameters from the data. Through experimentation, we show that this intricate design has led to top-tier performance in both phonemic and orthographic word segmentation.},
	urldate = {2018-06-28},
	journal = {arXiv:1607.05822 [cs]},
	author = {Chen, Ruey-Cheng},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.05822},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1607.05822 PDF:/home/user/Zotero/storage/ALPUHFY6/Chen - 2016 - Incremental Learning for Fully Unsupervised Word S.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/WWDGX2I2/1607.html:text/html}
}

@article{rama-fast-2017,
	title = {Fast and unsupervised methods for multilingual cognate clustering},
	url = {http://arxiv.org/abs/1702.04938},
	abstract = {In this paper we explore the use of unsupervised methods for detecting cognates in multilingual word lists. We use online EM to train sound segment similarity weights for computing similarity between two words. We tested our online systems on geographically spread sixteen different language groups of the world and show that the Online PMI system (Pointwise Mutual Information) outperforms a HMM based system and two linguistically motivated systems: LexStat and ALINE. Our results suggest that a PMI system trained in an online fashion can be used by historical linguists for fast and accurate identification of cognates in not so well-studied language families.},
	urldate = {2018-06-28},
	journal = {arXiv:1702.04938 [cs]},
	author = {Rama, Taraka and Wahle, Johannes and Sofroniev, Pavel and Jäger, Gerhard},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04938},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1702.04938 PDF:/home/user/Zotero/storage/PIKY5EUG/Rama et al. - 2017 - Fast and unsupervised methods for multilingual cog.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MFTJSNRV/1702.html:text/html}
}

@article{kamper-unsupervised-2017,
	title = {Unsupervised neural and {Bayesian} models for zero-resource speech processing},
	url = {http://arxiv.org/abs/1701.00851},
	abstract = {In settings where only unlabelled speech data is available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. There are two central problems in zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. In this thesis, we argue that a combination of top-down and bottom-up modelling is advantageous in tackling these two problems. To address the problem of frame-level representation learning, we present the correspondence autoencoder (cAE), a neural network trained with weak top-down supervision from an unsupervised term discovery system. By combining this top-down supervision with unsupervised bottom-up initialization, the cAE yields much more discriminative features than previous approaches. We then present our unsupervised segmental Bayesian model that segments and clusters unlabelled speech into hypothesized words. By imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, our system outperforms several others on multi-speaker conversational English and Xitsonga speech data. Finally, we show that the clusters discovered by the segmental Bayesian model can be made less speaker- and gender-specific by using features from the cAE instead of traditional acoustic features. In summary, the different models and systems presented in this thesis show that both top-down and bottom-up modelling can improve representation learning, segmentation and clustering of unlabelled speech data.},
	urldate = {2018-06-28},
	journal = {arXiv:1701.00851 [cs]},
	author = {Kamper, Herman},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.00851},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1701.00851 PDF:/home/user/Zotero/storage/B82HK378/Kamper - 2017 - Unsupervised neural and Bayesian models for zero-r.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/CKKXQJIN/1701.html:text/html}
}

@article{brent-efficient-1999,
	title = {An {Efficient}, {Probabilistically} {Sound} {Algorithm} for {Segmentation} and {Word} {Discovery}},
	url = {http://arxiv.org/abs/cs/9905007},
	abstract = {This paper presents a model-based, unsupervised algorithm for recovering word boundaries in a natural-language text from which they have been deleted. The algorithm is derived from a probability model of the source that generated the text. The fundamental structure of the model is specified abstractly so that the detailed component models of phonology, word-order, and word frequency can be replaced in a modular fashion. The model yields a language-independent, prior probability distribution on all possible sequences of all possible words over a given alphabet, based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon. The model is unusual in that it treats the generation of a complete corpus, regardless of length, as a single event in the probability space. Accordingly, the algorithm does not estimate a probability distribution on words; instead, it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text. Experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that this algorithm is more effective than other proposed algorithms, at least when utterance boundaries are given and the text includes a substantial number of short utterances. Keywords: Bayesian grammar induction, probability models, minimum description length (MDL), unsupervised learning, cognitive modeling, language acquisition, segmentation},
	urldate = {2018-06-28},
	journal = {arXiv:cs/9905007},
	author = {Brent, Michael R.},
	month = may,
	year = {1999},
	note = {arXiv: cs/9905007},
	keywords = {Computer Science - Computation and Language, I.2.6, I.2.7, Computer Science - Machine Learning, I.2.0},
	file = {arXiv\:cs/9905007 PDF:/home/user/Zotero/storage/DG3RZUPP/Brent - 1999 - An Efficient, Probabilistically Sound Algorithm fo.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/UVP6UQZK/9905007.html:text/html}
}

@article{venkataraman-statistical-1999,
	title = {A statistical model for word discovery in child directed speech},
	url = {http://arxiv.org/abs/cs/9910011},
	abstract = {A statistical model for segmentation and word discovery in child directed speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described and results of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks are also presented.},
	urldate = {2018-06-28},
	journal = {arXiv:cs/9910011},
	author = {Venkataraman, Anand},
	month = oct,
	year = {1999},
	note = {arXiv: cs/9910011},
	keywords = {Computer Science - Computation and Language, I.2.6, I.2.7, Computer Science - Machine Learning},
	file = {arXiv\:cs/9910011 PDF:/home/user/Zotero/storage/3DB4UDQN/Venkataraman - 1999 - A statistical model for word discovery in child di.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9HKA3ME3/9910011.html:text/html}
}

@article{ando-mostly-unsupervised-2003,
	title = {Mostly-{Unsupervised} {Statistical} {Segmentation} of {Japanese} {Kanji} {Sequences}},
	volume = {9},
	issn = {1351-3249, 1469-8110},
	url = {http://arxiv.org/abs/cs/0205009},
	doi = {10.1017/S1351324902002954},
	abstract = {Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and syntactic analysis or on pre-segmented data; but these are labor-intensive, and the lexico-syntactic techniques are vulnerable to the unknown word problem. In contrast, we introduce a novel, more robust statistical method utilizing unsegmented training data. Despite its simplicity, the algorithm yields performance on long kanji sequences comparable to and sometimes surpassing that of state-of-the-art morphological analyzers over a variety of error metrics. The algorithm also outperforms another mostly-unsupervised statistical algorithm previously proposed for Chinese. Additionally, we present a two-level annotation scheme for Japanese to incorporate multiple segmentation granularities, and introduce two novel evaluation metrics, both based on the notion of a compatible bracket, that can account for multiple granularities simultaneously.},
	number = {02},
	urldate = {2018-06-28},
	journal = {Natural Language Engineering},
	author = {Ando, Rie Kubota and Lee, Lillian},
	month = jun,
	year = {2003},
	note = {arXiv: cs/0205009},
	keywords = {Computer Science - Computation and Language, I.2.7},
	pages = {127--149},
	file = {arXiv\:cs/0205009 PDF:/home/user/Zotero/storage/TGCQXE2E/Ando and Lee - 2003 - Mostly-Unsupervised Statistical Segmentation of Ja.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/K3HDV3R7/0205009.html:text/html}
}

@article{creutz-unsupervised-2002,
	title = {Unsupervised {Discovery} of {Morphemes}},
	url = {http://arxiv.org/abs/cs/0205057},
	abstract = {We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.},
	urldate = {2018-06-28},
	journal = {arXiv:cs/0205057},
	author = {Creutz, Mathias and Lagus, Krista},
	month = may,
	year = {2002},
	note = {arXiv: cs/0205057},
	keywords = {Computer Science - Computation and Language, I.2.7},
	file = {arXiv\:cs/0205057 PDF:/home/user/Zotero/storage/ACERALL2/Creutz and Lagus - 2002 - Unsupervised Discovery of Morphemes.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/7Q2XIQXT/0205057.html:text/html}
}

@article{gildea-grammars-2010,
	title = {Do {Grammars} {Minimize} {Dependency} {Length}?},
	volume = {34},
	copyright = {Copyright © 2009 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1551-6709.2009.01073.x},
	doi = {10.1111/j.1551-6709.2009.01073.x},
	abstract = {A well-established principle of language is that there is a preference for closely related words to be close together in the sentence. This can be expressed as a preference for dependency length minimization (DLM). In this study, we explore quantitatively the degree to which natural languages reflect DLM. We extract the dependencies from natural language text and reorder the words in such a way as to minimize dependency length. Comparing the original text with these optimal linearizations (and also with random linearizations) reveals the degree to which natural language minimizes dependency length. Tests on English data show that English shows a strong effect of DLM, with dependency length much closer to optimal than to random; the optimal English grammar also has many specific features in common with English. In German, too, dependency length is significantly less than random, but the effect is much weaker than in English. We conclude by speculating about some possible reasons for this difference between English and German.},
	language = {en},
	number = {2},
	urldate = {2018-08-03},
	journal = {Cognitive Science},
	author = {Gildea, Daniel and Temperley, David},
	month = mar,
	year = {2010},
	keywords = {Syntax, Natural language processing, Word order},
	pages = {286--310},
	file = {Full Text PDF:/home/user/Zotero/storage/ER2N5EHW/Gildea and Temperley - 2010 - Do Grammars Minimize Dependency Length.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/EPMCBCQN/j.1551-6709.2009.01073.html:text/html}
}

@article{rajkumar-investigating-2016,
	title = {Investigating locality effects and surprisal in written {English} syntactic choice phenomena},
	volume = {155},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S001002771630155X},
	doi = {10.1016/j.cognition.2016.06.008},
	abstract = {We investigate the extent to which syntactic choice in written English is influenced by processing considerations as predicted by Gibson’s (2000) Dependency Locality Theory (DLT) and Surprisal Theory (Hale, 2001; Levy, 2008). A long line of previous work attests that languages display a tendency for shorter dependencies, and in a previous corpus study, Temperley (2007) provided evidence that this tendency exerts a strong influence on constituent ordering choices. However, Temperley’s study included no frequency-based controls, and subsequent work on sentence comprehension with broad-coverage eye-tracking corpora found weak or negative effects of DLT-based measures when frequency effects were statistically controlled for (Demberg \& Keller, 2008; van Schijndel, Nguyen, \& Schuler 2013; van Schijndel \& Schuler, 2013), calling into question the actual impact of dependency locality on syntactic choice phenomena. Going beyond Temperley’s work, we show that DLT integration costs are indeed a significant predictor of syntactic choice in written English even in the presence of competing frequency-based and cognitively motivated control factors, including n-gram probability and PCFG surprisal as well as embedding depth (Wu, Bachrach, Cardenas, \& Schuler, 2010; Yngve, 1960). Our study also shows that the predictions of dependency length and surprisal are only moderately correlated, a finding which mirrors Dember \& Keller’s (2008) results for sentence comprehension. Further, we demonstrate that the efficacy of dependency length in predicting the corpus choice increases with increasing head-dependent distances. At the same time, we find that the tendency towards dependency locality is not always observed, and with pre-verbal adjuncts in particular, non-locality cases are found more often than not. In contrast, surprisal is effective in these cases, and the embedding depth measures further increase prediction accuracy. We discuss the implications of our findings for theories of language comprehension and production, and conclude with a discussion of questions our work raises for future research.},
	urldate = {2018-08-03},
	journal = {Cognition},
	author = {Rajkumar, Rajakrishnan and van Schijndel, Marten and White, Michael and Schuler, William},
	month = oct,
	year = {2016},
	keywords = {Surprisal, Language production, Constituent ordering, Dependency locality},
	pages = {204--232},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/8PL8H6N4/S001002771630155X.html:text/html}
}

@article{ros-aiming-2015-1,
	title = {Aiming at shorter dependencies: the role of agreement morphology},
	volume = {30},
	issn = {2327-3798},
	shorttitle = {Aiming at shorter dependencies},
	url = {https://doi.org/10.1080/23273798.2014.994009},
	doi = {10.1080/23273798.2014.994009},
	abstract = {This study examined word order preferences as a function of phrasal length in Basque. Basque is an OV language with flexible sentence word order and rich verb agreement. Contrary to the universal short-before-long preference predicted by availability models, Hawkins has argued that short-before-long orders are preferred in VO languages such as English, whereas long-before-short orders are preferred in OV languages such as Japanese. However, it is unclear how length affects word order preferences when an OV language has rich verb agreement and allows post-verbal arguments. We found a general long-before-short preference, and a tendency to place the verb in a sentence-medial position when one constituent is long. We argue that since agreement morphology signals the thematic role and case of surrounding phrases, it contributes to speeding up sentence processing. We conclude that morphologically rich languages employ both general adjacency mechanisms and language-specific resources to enhance language efficiency.},
	number = {9},
	urldate = {2018-08-03},
	journal = {Language, Cognition and Neuroscience},
	author = {Ros, Idoia and Santesteban, Mikel and Fukumura, Kumiko and Laka, Itziar},
	month = oct,
	year = {2015},
	keywords = {language production, agreement, Basque, OV language, sentence word order},
	pages = {1156--1174},
	file = {Snapshot:/home/user/Zotero/storage/6TP47YYF/23273798.2014.html:text/html}
}

@article{gulordava-multi-lingual-2016,
	title = {Multi-lingual dependency parsing evaluation: a large-scale analysis of word order properties using artificial data},
	volume = {4},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Gulordava, Kristina and Merlo, Paola},
	year = {2016},
	pages = {343--356}
}

@misc{agrawal-arpit;-indian-institute-of-technology-delhi-role-2017,
	title = {Role of expectation and working memory constraints in {Hindi} comprehension: {An} eyetracking corpus analysis},
	shorttitle = {Role of expectation and working memory constraints in {Hindi} comprehension},
	abstract = {We used the Potsdam-Allahabad Hindi eye-tracking corpus to investigate the role of word-level and sentence-level factors during sentence comprehension in Hindi. Extending previous work that used this eye-tracking data, we investigate the role of surprisal and retrieval cost metrics during sentence processing. While controlling for word-level predictors (word complexity, syllable length, unigram and bigram frequencies) as well as sentence-level predictors such as integration and storage costs, we ﬁnd a signiﬁcant effect of surprisal on ﬁrst-pass reading times (higher surprisal value leads to increase in FPRT). Effect of retrieval cost was only found for a higher degree of parser parallelism. Interestingly, while surprisal has a signiﬁcant effect on FPRT, storage cost (another prediction-based metric) does not. A signiﬁcant effect of storage cost shows up only in total ﬁxation time (TFT), thus indicating that these two measures perhaps capture different aspects of prediction. The study replicates previous ﬁndings that both prediction-based and memory-based metrics are required to account for processing patterns during sentence comprehension. The results also show that parser model assumptions are critical in order to draw generalizations about the utility of a metric (e.g. surprisal) across various phenomena in a language.},
	language = {en},
	publisher = {eyemovement.org},
	author = {{Agrawal, Arpit; Indian Institute Of Technology, Delhi} and {Agarwal, Sumeet; Indian Institute Of Technology, Delhi} and {Husain, Samar; Indian Institute Of Technology, Delhi}},
	year = {2017},
	doi = {10.16910/jemr.10.2.4},
	file = {Agrawal, Arpit\; Indian Institute Of Technology, Delhi et al. - 2017 - Role of expectation and working memory constraints.pdf:/home/user/Zotero/storage/JMMUE7CD/Agrawal, Arpit\; Indian Institute Of Technology, Delhi et al. - 2017 - Role of expectation and working memory constraints.pdf:application/pdf}
}

@article{behaghel-zur-1930,
	title = {Zur {Wortstellung} des {Deutschen}},
	volume = {6},
	issn = {0097-8507},
	url = {http://www.jstor.org/stable/521983},
	doi = {10.2307/521983},
	abstract = {Der Aufsatz Zur Wortstellung legt die wichtigsten Gesetze dar, die die deutsche Wortstellung beherrschen: Gesetze geistiger Art: 1) das geistig Zusammengehörige wird zusammengestellt; 2) das weniger Wichtige wird vor das Wichtigere gestellt. Gesetze von physischer Beschaffenheit: 1) längere Glieder werden hinter kürzere gestellt; 2) es wird Wechsel von Hebung und Senkung angestrebt.},
	number = {4},
	urldate = {2018-08-05},
	journal = {Language},
	author = {Behaghel, Otto},
	year = {1930},
	pages = {29--33}
}

@article{hawkins-parsing-1990,
	title = {A {Parsing} {Theory} of {Word} {Order} {Universals}},
	volume = {21},
	issn = {0024-3892},
	url = {http://www.jstor.org/stable/4178670},
	number = {2},
	urldate = {2018-08-05},
	journal = {Linguistic Inquiry},
	author = {Hawkins, John A.},
	year = {1990},
	pages = {223--261}
}

@article{behaghel-beziehungen-2010,
	title = {Beziehungen zwischen {Umfang} und {Reihenfolge} von {Satzgliedern}.},
	volume = {25},
	issn = {1613-0405},
	url = {https://www.degruyter.com/view/j/indo.1909.25.issue-1/9783110242652.110/9783110242652.110.xml},
	doi = {10.1515/9783110242652.110},
	urldate = {2018-08-05},
	journal = {indo},
	author = {Behaghel, Otto},
	year = {2010},
	pages = {110--142}
}

@article{noauthor-parsing-nodate,
	title = {A {Parsing} {Theory} of {Word} {Order} {Universals}},
	language = {en},
	pages = {40},
	file = {A Parsing Theory of Word Order Universals.pdf:/home/user/Zotero/storage/JJ58GNHK/A Parsing Theory of Word Order Universals.pdf:application/pdf}
}

@article{rijkhoff-word-1986,
	title = {Word order universals revisited: {The} {Principle} of {Head} {Proximity}},
	url = {http://home.hum.uva.nl/fdg/working\_papers/WPFG14.pdf},
	urldate = {2018-08-05},
	author = {Rijkhoff, Jan},
	year = {1986},
	file = {WPFG14.pdf:/home/user/Zotero/storage/AMSUC5Y8/WPFG14.pdf:application/pdf}
}

@book{chomsky-lectures-1981,
	title = {Lectures on government and binding: {The} {Pisa} lectures},
	number = {9},
	publisher = {Walter de Gruyter},
	author = {Chomsky, Noam},
	year = {1981}
}

@article{ferrer-i-cancho-placement-2017,
	title = {The {Placement} of the {Head} that {Maximizes} {Predictability}. {An} {Information} {Theoretic} {Approach}},
	abstract = {The minimization of the length of syntactic dependencies is a well-established principle of word order and the basis of a mathematical theory of word order. Here we complete that theory from the perspective of information theory, adding a competing word order principle: the maximization of predictability of a target element. These two principles are in conflict: to maximize the predictability of the head, the head should appear last, which maximizes the costs with respect to dependency length minimization. The implications of such a broad theoretical framework to understand the optimality, diversity and evolution of the six possible orderings of subject, object and verb, are reviewed.},
	language = {en},
	author = {Ferrer-i-Cancho, Ramon},
	year = {2017},
	pages = {34},
	file = {Ferrer-i-Cancho - The Placement of the Head that Maximizes Predictab.pdf:/home/user/Zotero/storage/XGV7NLMI/Ferrer-i-Cancho - The Placement of the Head that Maximizes Predictab.pdf:application/pdf}
}

@article{hawkins-processing-2007-1,
	title = {Processing typology and why psychologists need to know about it},
	volume = {25},
	issn = {0732118X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0732118X07000219},
	doi = {10.1016/j.newideapsych.2007.02.003},
	abstract = {This paper illustrates an interdisciplinary research program based on cross-linguistic comparison that is of relevance for psychologists working on language processing, so-called ‘‘processing typology’’ [Hawkins, J. A. (1994). A performance theory of order and constituency. Cambridge: Cambridge University Press; (2004). Efﬁciency and complexity in grammars. Oxford: Oxford University Press]. Its most original feature is the hypothesis that patterns and preferences found in performance in languages with several structures of a given type (e.g. preferences among alternative word orders) are the same patterns and preferences one ﬁnds across languages in the ﬁxed conventions of grammars that permit less variation (i.e. in ﬁxed word orders). Data supporting this ‘‘performance–grammar correspondence hypothesis’’ are summarized. One of its consequences is that principles of performance can be used to make predictions for patterns of grammatical variation, while preferences in grammars become relevant for the testing of psycholinguistic ideas. Two proposed principles of ordering in performance, in terms of ‘‘end weight’’ and ‘‘memory cost’’, are criticized on the basis of cross-linguistic data. Both predict an asymmetry in ordering, whereby some category A precedes B. But end weight is not a valid cross-linguistic asymmetry, and memory cost cannot explain certain asymmetries for which it has been invoked when different language types are considered. The paper argues for greater mutual awareness between processing theorists and language typologists, for more consideration of non-European grammars and language types in psycholinguistics, and for a greater appeal to processing in the explanation of typological variation. r 2007 Elsevier Ltd. All rights reserved.},
	language = {en},
	number = {2},
	urldate = {2018-08-05},
	journal = {New Ideas in Psychology},
	author = {Hawkins, John A.},
	month = aug,
	year = {2007},
	pages = {87--107},
	file = {Hawkins - 2007 - Processing typology and why psychologists need to .pdf:/home/user/Zotero/storage/K6CM67D6/Hawkins - 2007 - Processing typology and why psychologists need to .pdf:application/pdf}
}

@inproceedings{schouwstra-semantic-2011,
	title = {Semantic structure in improvised communication},
	volume = {33},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Schouwstra, Marieke and Van Leeuwen, Anounschka and Marien, Nicky and Smit, Marianne and De Swart, Henriette},
	year = {2011}
}

@inproceedings{zhang-dependency-2016,
	title = {Dependency parsing as head selection},
	booktitle = {{EACL}},
	author = {Zhang, Xingxing and Cheng, Jianpeng and Lapata, Mirella},
	year = {2016}
}

@article{dozat-deep-2016,
	title = {Deep biaffine attention for neural dependency parsing},
	journal = {arXiv preprint arXiv:1611.01734},
	author = {Dozat, Timothy and Manning, Christopher D},
	year = {2016}
}

@article{dozat-stanfords-2017,
	title = {Stanford's {Graph}-based {Neural} {Dependency} {Parser} at the {CoNLL} 2017 {Shared} {Task}},
	journal = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
	author = {Dozat, Timothy and Qi, Peng and Manning, Christopher D},
	year = {2017},
	pages = {20--30}
}

@book{givon-syntax:-1984,
	title = {Syntax: a functional-typological introduction},
	author = {Givón, Talmy},
	year = {1984}
}

@incollection{dryer-order-2013,
	address = {Leipzig},
	title = {Order of {Object} and {Verb}},
	url = {https://wals.info/chapter/83},
	booktitle = {The {World} {Atlas} of {Language} {Structures} {Online}},
	publisher = {Max Planck Institute for Evolutionary Anthropology},
	author = {Dryer, Matthew S.},
	editor = {Dryer, Matthew S. and Haspelmath, Martin},
	year = {2013}
}

@inproceedings{puduppully-transition-based-2016,
	title = {Transition-based syntactic linearization with lookahead features},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Puduppully, Ratish and Zhang, Yue and Shrivastava, Manish},
	year = {2016},
	pages = {488--493}
}

@article{wasow-post-verbal-2003,
	title = {Post-verbal constituent ordering in {English}},
	journal = {Determinants of grammatical variation in English},
	author = {Wasow, Thomas and Arnold, Jennifer},
	year = {2003},
	pages = {119--54}
}

@book{nivre-universal-2017,
	title = {Universal {Dependencies} 2.1},
	copyright = {Licence Universal Dependencies v2.1},
	author = {Nivre, Joakim and Agic, Zeljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bobicev, Victoria and Börstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Burchardt, Aljoscha and Candito, Marie and Caron, Gauthier and Cebiroğlu Eryiğit, Gülşen and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cinková, Silvie and Çöltekin, Cagri and Connor, Miriam and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Elkahky, Ali and Erjavec, Tomaž and Farkas, Richárd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cláudia and Gajdošová, Katarína and Galbraith, Daniel and Garcia, Marcos and Gärdenfors, Moa and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Gökırmak, Memduh and Goldberg, Yoav and Gómez Guinovart, Xavier and Gonzáles Saavedra, Berta and Grioni, Matias and Gr\= uzītis, Normunds and Guillaume, Bruno and Habash, Nizar and Hajič, Jan and Hajič jr., Jan and Hà My, Linh and Harris, Kim and Haug, Dag and Hladká, Barbora and Hlaváčová, Jaroslava and Hociung, Florinel and Hohle, Petter and Ion, Radu and Irimia, Elena and Jelínek, Tomáš and Johannsen, Anders and Jørgensen, Fredrik and Kaşıkara, Hüner and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerová, Václava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and Lee, John and Le Hong, Phuong and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Ljubešić, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Mărănduc, Cătălina and Mareček, David and Marheinecke, Katrin and Martínez Alonso, Héctor and Martins, André and Mašek, Jan and Matsumoto, Yuji and McDonald, Ryan and Mendonça, Gustavo and Miekka, Niko and Missilä, Anna and Mititelu, Cătălin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shinsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Müürisep, Kaili and Nainwani, Pinkey and Nedoluzhko, Anna and Nešpore-Bērzkalne, Gunta and Nguyen Thi, Luong and Nguyen Thi Minh, Huyen and Nikolaev, Vitaly and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and Östling, Robert and Øvrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Popel, Martin and Pretkalniņa, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and Sagot, Benoît and Saleh, Shadi and Samardžić, Tanja and Sanguinetti, Manuela and Saulīte, Baiba and Schuster, Sebastian and Seddah, Djamé and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simkó, Katalin and Šimková, Mária and Simov, Kiril and Smith, Aaron and Stella, Antonio and Straka, Milan and Strnadová, Jana and Suhr, Alane and Sulubacak, Umut and Szántó, Zsolt and Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Urešová, Zdeňka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Washington, Jonathan North and Wirén, Mats and Wong, Tak-sum and Yu, Zhuoran and Žabokrtský, Zdeněk and Zeldes, Amir and Zeman, Daniel and Zhu, Hanzhi},
	year = {2017}
}

@article{goldwater-bayesian-2009-1,
	title = {A {Bayesian} framework for word segmentation: {Exploring} the effects of context},
	volume = {112},
	issn = {00100277},
	shorttitle = {A {Bayesian} framework for word segmentation},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027709000675},
	doi = {10.1016/j.cognition.2009.03.008},
	abstract = {Since the experiments of Saﬀran et al. (1996a), there has been a great deal of interest in the question of how statistical regularities in the speech stream might be used by infants to begin to identify individual words. In this work, we use computational modeling to explore the eﬀects of diﬀerent assumptions the learner might make regarding the nature of words – in particular, how these assumptions aﬀect the kinds of words that are segmented from a corpus of transcribed child-directed speech. We develop several models within a Bayesian ideal observer framework, and use them to examine the consequences of assuming either that words are independent units, or units that help to predict other units. We show through empirical and theoretical results that the assumption of independence causes the learner to undersegment the corpus, with many two- and three-word sequences (e.g. what’s that, do you, in the house) misidentiﬁed as individual words. In contrast, when the learner assumes that words are predictive, the resulting segmentation is far more accurate. These results indicate that taking context into account is important for a statistical word segmentation strategy to be successful, and raise the possibility that even young infants may be able to exploit more subtle statistical patterns than have usually been considered.},
	language = {en},
	number = {1},
	urldate = {2018-09-10},
	journal = {Cognition},
	author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark},
	month = jul,
	year = {2009},
	pages = {21--54},
	file = {Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:/home/user/Zotero/storage/AYDQ9FF5/Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:application/pdf}
}

@article{harris-distributional-1954,
	title = {Distributional structure},
	volume = {10},
	number = {2-3},
	journal = {Word},
	author = {Harris, Zellig S},
	year = {1954},
	pages = {146--162}
}

@article{saffran-word-1996,
	title = {Word segmentation: {The} role of distributional cues},
	volume = {35},
	number = {4},
	journal = {Journal of memory and language},
	author = {Saffran, Jenny R and Newport, Elissa L and Aslin, Richard N},
	year = {1996},
	pages = {606--621}
}

@inproceedings{sun-chinese-1998,
	title = {Chinese word segmentation without using lexicon and hand-crafted training data},
	booktitle = {Proceedings of the 36th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and 17th {International} {Conference} on {Computational} {Linguistics}-{Volume} 2},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Maosung and Dayang, Shen and Tsou, Benjamin K},
	year = {1998},
	pages = {1265--1271}
}

@article{feng-accessor-2004,
	title = {Accessor variety criteria for {Chinese} word extraction},
	volume = {30},
	number = {1},
	journal = {Computational Linguistics},
	author = {Feng, Haodi and Chen, Kang and Deng, Xiaotie and Zheng, Weimin},
	year = {2004},
	pages = {75--93}
}

@inproceedings{cohen-algorithm-2001,
	title = {An algorithm for segmenting categorical time series into meaningful episodes},
	booktitle = {International symposium on intelligent data analysis},
	publisher = {Springer},
	author = {Cohen, Paul and Adams, Niall},
	year = {2001},
	pages = {198--207}
}

@article{brent-efficient-1999-1,
	title = {An efficient, probabilistically sound algorithm for segmentation and word discovery},
	volume = {34},
	number = {1-3},
	journal = {Machine Learning},
	author = {Brent, Michael R},
	year = {1999},
	pages = {71--105}
}

@article{anonymous-variational-2018,
	title = {Variational {Autoencoders} for {Text} {Modeling} without {Weakening} the {Decoder}},
	url = {https://openreview.net/forum?id=H1eZ6sRcFm},
	abstract = {Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/XDE98AKH/Anonymous - 2018 - Variational Autoencoders for Text Modeling without.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/YJL6ER87/forum.html:text/html}
}

@article{anonymous-variational-2018-1,
	title = {Variational {Sparse} {Coding}},
	url = {https://openreview.net/forum?id=SkeJ6iR9Km},
	abstract = {Variationalauto-encoders(VAEs)offeratractableapproachwhenperformingapproximate inference in otherwise intractable generative models. However, standard VAEs often produce latent codes that are...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/LAEQKIJR/Anonymous - 2018 - Variational Sparse Coding.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/R9W6B8UV/forum.html:text/html}
}

@article{anonymous-non-synergistic-2018,
	title = {Non-{Synergistic} {Variational} {Autoencoders}},
	url = {https://openreview.net/forum?id=Skl3M20qYQ},
	abstract = {Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge. In the following paper we address the...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/SLS3RPAZ/Anonymous - 2018 - Non-Synergistic Variational Autoencoders.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/DIFEUXMI/forum.html:text/html}
}

@article{anonymous-mae:-2018,
	title = {{MAE}: {Mutual} {Posterior}-{Divergence} {Regularization} for {Variational} {AutoEncoders}},
	shorttitle = {{MAE}},
	url = {https://openreview.net/forum?id=Hke4l2AcKQ},
	abstract = {Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations....},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/UX3IEWXI/Anonymous - 2018 - MAE Mutual Posterior-Divergence Regularization fo.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/F22PDIR9/forum.html:text/html}
}

@article{anonymous-towards-2018,
	title = {Towards {Language} {Agnostic} {Universal} {Representations}},
	url = {https://openreview.net/forum?id=r1l9Nj09YQ},
	abstract = {When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/NFGJ7M6J/Anonymous - 2018 - Towards Language Agnostic Universal Representation.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/ET7LZTCZ/forum.html:text/html}
}

@article{anonymous-fixing-2018,
	title = {Fixing {Variational} {Bayes}: {Deterministic} {Variational} {Inference} for {Bayesian} {Neural} {Networks}},
	shorttitle = {Fixing {Variational} {Bayes}},
	url = {https://openreview.net/forum?id=B1l08oAct7},
	abstract = {Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/NGKZYTGH/Anonymous - 2018 - Fixing Variational Bayes Deterministic Variationa.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/H6TAFZCN/forum.html:text/html}
}

@article{anonymous-discrete-2018,
	title = {Discrete flow posteriors for variational inference in discrete dynamical systems},
	url = {https://openreview.net/forum?id=HyxOIoRqFQ},
	abstract = {Each training step for a variational autoencoder (VAE) requires us to sample from the approximate posterior, so we usually choose simple (e.g. factorised) approximate posteriors in which sampling...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/RV2RSLCU/Anonymous - 2018 - Discrete flow posteriors for variational inference.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/VT5EY5QS/forum.html:text/html}
}

@article{anonymous-practical-2018,
	title = {Practical lossless compression with latent variables using bits back coding},
	url = {https://openreview.net/forum?id=ryE98iR5tm},
	abstract = {Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/KFVESF3B/Anonymous - 2018 - Practical lossless compression with latent variabl.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/XMIXME53/forum.html:text/html}
}

@article{anonymous-improving-2018,
	title = {Improving {Gaussian} mixture latent variable model convergence with {Optimal} {Transport}},
	url = {https://openreview.net/forum?id=B1EiIsCctm},
	abstract = {Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/878J7MXY/Anonymous - 2018 - Improving Gaussian mixture latent variable model c.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/C65E7BNS/forum.html:text/html}
}

@article{tolstikhin-wasserstein-2018,
	title = {Wasserstein {Auto}-{Encoders}},
	url = {https://openreview.net/forum?id=HkL7n1-0b},
	abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the...},
	urldate = {2018-09-30},
	author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/DI4GIQLY/Tolstikhin et al. - 2018 - Wasserstein Auto-Encoders.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/GG26C25T/forum.html:text/html}
}

@article{anonymous-improved-2018,
	title = {Improved {Gradient} {Estimators} for {Stochastic} {Discrete} {Variables}},
	url = {https://openreview.net/forum?id=S1lKSjRcY7},
	abstract = {In many applications we seek to optimize an expectation with respect to a distribution over discrete variables. Estimating gradients of such objectives with respect to the distribution parameters...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/2EIXC72E/Anonymous - 2018 - Improved Gradient Estimators for Stochastic Discre.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/TQPFQLXC/forum.html:text/html}
}

@article{anonymous-monge-amp`ere-2018,
	title = {Monge-{Amp}\`ere {Flow} for {Generative} {Modeling}},
	url = {https://openreview.net/forum?id=rkeUrjCcYQ},
	abstract = {We present a deep generative model, named Monge-Amp\`ere flow, which builds on continuous-time gradient flow arising from the Monge-Amp\`ere equation in optimal transport theory. The generative map...},
	urldate = {2018-09-30},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/J2I5IDFC/Anonymous - 2018 - Monge-Amp`ere Flow for Generative Modeling.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/2XR88LDX/forum.html:text/html}
}

@article{anonymous-improving-2018-1,
	title = {Improving {Gaussian} mixture latent variable model convergence with {Optimal} {Transport}},
	url = {https://openreview.net/forum?id=B1EiIsCctm},
	abstract = {Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often...},
	urldate = {2018-10-04},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/7ZFBJEZG/Anonymous - 2018 - Improving Gaussian mixture latent variable model c.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/LI5X7C3Q/forum.html:text/html}
}

@article{anonymous-darts:-2018,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {https://openreview.net/forum?id=S1eYHoC5FX},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement...},
	urldate = {2018-10-04},
	author = {Anonymous},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/IEN6UI39/Anonymous - 2018 - DARTS Differentiable Architecture Search.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/LAM82AHI/forum.html:text/html}
}

@article{papamakarios-masked-2017,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	url = {http://arxiv.org/abs/1705.07057},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2018-10-04},
	journal = {arXiv:1705.07057 [cs, stat]},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07057},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1705.07057 PDF:/home/user/Zotero/storage/9W7WLYBR/Papamakarios et al. - 2017 - Masked Autoregressive Flow for Density Estimation.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MRWW2N6C/1705.html:text/html}
}

@article{colson-overview-2007,
	title = {An overview of bilevel optimization},
	volume = {153},
	issn = {1572-9338},
	url = {https://doi.org/10.1007/s10479-007-0176-2},
	doi = {10.1007/s10479-007-0176-2},
	abstract = {This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present fields of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints).},
	language = {en},
	number = {1},
	urldate = {2018-10-04},
	journal = {Annals of Operations Research},
	author = {Colson, Benoît and Marcotte, Patrice and Savard, Gilles},
	month = sep,
	year = {2007},
	keywords = {Bilevel programming, Mathematical programs with equilibrium constraints, Nonlinear programming, Optimal pricing},
	pages = {235--256},
	file = {Springer Full Text PDF:/home/user/Zotero/storage/2Q5LBCDJ/Colson et al. - 2007 - An overview of bilevel optimization.pdf:application/pdf}
}

@article{strelioff-inferring-2007,
	title = {Inferring {Markov} chains: {Bayesian} estimation, model comparison, entropy rate, and out-of-class modeling},
	volume = {76},
	issn = {1539-3755, 1550-2376},
	shorttitle = {Inferring {Markov} chains},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.76.011106},
	doi = {10.1103/PhysRevE.76.011106},
	language = {en},
	number = {1},
	urldate = {2018-10-05},
	journal = {Physical Review E},
	author = {Strelioff, Christopher C. and Crutchfield, James P. and Hübler, Alfred W.},
	month = jul,
	year = {2007},
	file = {Strelioff et al. - 2007 - Inferring Markov chains Bayesian estimation, mode.pdf:/home/user/Zotero/storage/ZVPWBHAR/Strelioff et al. - 2007 - Inferring Markov chains Bayesian estimation, mode.pdf:application/pdf}
}

@article{marzen-nearly-2017,
	title = {Nearly maximally predictive features and their dimensions},
	volume = {95},
	issn = {2470-0045, 2470-0053},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.95.051301},
	doi = {10.1103/PhysRevE.95.051301},
	language = {en},
	number = {5},
	urldate = {2018-10-05},
	journal = {Physical Review E},
	author = {Marzen, Sarah E. and Crutchfield, James P.},
	month = may,
	year = {2017},
	file = {Marzen and Crutchfield - 2017 - Nearly maximally predictive features and their dim.pdf:/home/user/Zotero/storage/CHP85E6X/Marzen and Crutchfield - 2017 - Nearly maximally predictive features and their dim.pdf:application/pdf}
}

@article{james-trimming-2017,
	title = {Trimming the {Independent} {Fat}: {Sufficient} {Statistics}, {Mutual} {Information}, and {Predictability} from {Effective} {Channel} {States}},
	volume = {95},
	issn = {2470-0045, 2470-0053},
	shorttitle = {Trimming the {Independent} {Fat}},
	url = {http://arxiv.org/abs/1702.01831},
	doi = {10.1103/PhysRevE.95.060102},
	abstract = {One of the most fundamental questions one can ask about a pair of random variables X and Y is the value of their mutual information. Unfortunately, this task is often stymied by the extremely large dimension of the variables. We might hope to replace each variable by a lower-dimensional representation that preserves the relationship with the other variable. The theoretically ideal implementation is the use of minimal sufficient statistics, where it is well-known that either X or Y can be replaced by their minimal sufficient statistic about the other while preserving the mutual information. While intuitively reasonable, it is not obvious or straightforward that both variables can be replaced simultaneously. We demonstrate that this is in fact possible: the information X's minimal sufficient statistic preserves about Y is exactly the information that Y's minimal sufficient statistic preserves about X. As an important corollary, we consider the case where one variable is a stochastic process' past and the other its future and the present is viewed as a memoryful channel. In this case, the mutual information is the channel transmission rate between the channel's effective states. That is, the past-future mutual information (the excess entropy) is the amount of information about the future that can be predicted using the past. Translating our result about minimal sufficient statistics, this is equivalent to the mutual information between the forward- and reverse-time causal states of computational mechanics. We close by discussing multivariate extensions to this use of minimal sufficient statistics.},
	language = {en},
	number = {6},
	urldate = {2018-10-05},
	journal = {Physical Review E},
	author = {James, Ryan G. and Mahoney, John R. and Crutchfield, James P.},
	month = jun,
	year = {2017},
	note = {arXiv: 1702.01831},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Chaotic Dynamics},
	file = {James et al. - 2017 - Trimming the Independent Fat Sufficient Statistic.pdf:/home/user/Zotero/storage/KXBVVHMX/James et al. - 2017 - Trimming the Independent Fat Sufficient Statistic.pdf:application/pdf}
}

@article{marzen-informational-2017,
	title = {Informational and {Causal} {Architecture} of {Continuous}-time {Renewal} {Processes}},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1793-z},
	doi = {10.1007/s10955-017-1793-z},
	language = {en},
	number = {1},
	urldate = {2018-10-05},
	journal = {Journal of Statistical Physics},
	author = {Marzen, Sarah and Crutchfield, James P.},
	month = jul,
	year = {2017},
	pages = {109--127},
	file = {Marzen and Crutchfield - 2017 - Informational and Causal Architecture of Continuou.pdf:/home/user/Zotero/storage/R775B3PJ/Marzen and Crutchfield - 2017 - Informational and Causal Architecture of Continuou.pdf:application/pdf}
}

@article{marzen-predictive-2016,
	title = {Predictive {Rate}-{Distortion} for {Infinite}-{Order} {Markov} {Processes}},
	volume = {163},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-016-1520-1},
	doi = {10.1007/s10955-016-1520-1},
	language = {en},
	number = {6},
	urldate = {2018-10-05},
	journal = {Journal of Statistical Physics},
	author = {Marzen, Sarah E. and Crutchfield, James P.},
	month = jun,
	year = {2016},
	pages = {1312--1338},
	file = {Marzen and Crutchfield - 2016 - Predictive Rate-Distortion for Infinite-Order Mark.pdf:/home/user/Zotero/storage/WZ3H3X44/Marzen and Crutchfield - 2016 - Predictive Rate-Distortion for Infinite-Order Mark.pdf:application/pdf}
}

@article{marzen-informational-2015,
	title = {Informational and {Causal} {Architecture} of {Discrete}-{Time} {Renewal} {Processes}},
	volume = {17},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/17/7/4891},
	doi = {10.3390/e17074891},
	abstract = {Renewal processes are broadly used to model stochastic behavior consisting of isolated events separated by periods of quiescence, whose durations are speciﬁed by a given probability law. Here, we identify the minimal suﬃcient statistic for their prediction (the set of causal states), calculate the historical memory capacity required to store those states (statistical complexity), delineate what information is predictable (excess entropy), and decompose the entropy of a single measurement into that shared with the past, future, or both. The causal state equivalence relation deﬁnes a new subclass of renewal processes with a ﬁnite number of causal states despite having an unbounded interevent count distribution. We use these formulae to analyze the output of the parametrized Simple Nonuniﬁlar Source, generated by a simple two-state hidden Markov model, but with an inﬁnite-state -machine presentation. All in all, the results lay the groundwork for analyzing processes with inﬁnite statistical complexity and inﬁnite excess entropy.},
	language = {en},
	number = {12},
	urldate = {2018-10-05},
	journal = {Entropy},
	author = {Marzen, Sarah and Crutchfield, James},
	month = jul,
	year = {2015},
	pages = {4891--4917},
	file = {Marzen and Crutchfield - 2015 - Informational and Causal Architecture of Discrete-.pdf:/home/user/Zotero/storage/3GCXPZTP/Marzen and Crutchfield - 2015 - Informational and Causal Architecture of Discrete-.pdf:application/pdf}
}

@article{still-optimal-2010,
	title = {Optimal causal inference: {Estimating} stored information and approximating causal architecture},
	volume = {20},
	issn = {1054-1500, 1089-7682},
	shorttitle = {Optimal causal inference},
	url = {http://aip.scitation.org/doi/10.1063/1.3489885},
	doi = {10.1063/1.3489885},
	language = {en},
	number = {3},
	urldate = {2018-10-05},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Still, Susanne and Crutchfield, James P. and Ellison, Christopher J.},
	month = sep,
	year = {2010},
	pages = {037111},
	file = {Still et al. - 2010 - Optimal causal inference Estimating stored inform.pdf:/home/user/Zotero/storage/WSAGVRYY/Still et al. - 2010 - Optimal causal inference Estimating stored inform.pdf:application/pdf}
}

@article{ruebeck-prediction-2018,
	title = {Prediction and {Generation} of {Binary} {Markov} {Processes}: {Can} a {Finite}-{State} {Fox} {Catch} a {Markov} {Mouse}?},
	volume = {28},
	issn = {1054-1500, 1089-7682},
	shorttitle = {Prediction and {Generation} of {Binary} {Markov} {Processes}},
	url = {http://arxiv.org/abs/1708.00113},
	doi = {10.1063/1.5003041},
	abstract = {Understanding the generative mechanism of a natural system is a vital component of the scientific method. Here, we investigate one of the fundamental steps toward this goal by presenting the minimal generator of an arbitrary binary Markov process. This is a class of processes whose predictive model is well known. Surprisingly, the generative model requires three distinct topologies for different regions of parameter space. We show that a previously proposed generator for a particular set of binary Markov processes is, in fact, not minimal. Our results shed the first quantitative light on the relative (minimal) costs of prediction and generation. We find, for instance, that the difference between prediction and generation is maximized when the process is approximately independently, identically distributed.},
	language = {en},
	number = {1},
	urldate = {2018-10-05},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Ruebeck, J. and James, R. G. and Mahoney, J. R. and Crutchfield, J. P.},
	month = jan,
	year = {2018},
	note = {arXiv: 1708.00113},
	keywords = {Computer Science - Computational Complexity, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Chaotic Dynamics},
	pages = {013109},
	file = {Ruebeck et al. - 2018 - Prediction and Generation of Binary Markov Process.pdf:/home/user/Zotero/storage/NENECNGW/Ruebeck et al. - 2018 - Prediction and Generation of Binary Markov Process.pdf:application/pdf}
}

@article{huang-learnable-2017,
	title = {Learnable {Explicit} {Density} for {Continuous} {Latent} {Space} and {Variational} {Inference}},
	url = {http://arxiv.org/abs/1710.02248},
	abstract = {In this paper, we study two aspects of the variational autoencoder (VAE): the prior distribution over the latent variables and its corresponding posterior. First, we decompose the learning of VAEs into layerwise density estimation, and argue that having a ﬂexible prior is beneﬁcial to both sample generation and inference. Second, we analyze the family of inverse autoregressive ﬂows (inverse AF) and show that with further improvement, inverse AF could be used as universal approximation to any complicated posterior. Our analysis results in a uniﬁed approach to parameterizing a VAE, without the need to restrict ourselves to use factorial Gaussians in the latent real space.},
	language = {en},
	urldate = {2018-10-05},
	journal = {arXiv:1710.02248 [cs, stat]},
	author = {Huang, Chin-Wei and Touati, Ahmed and Dinh, Laurent and Drozdzal, Michal and Havaei, Mohammad and Charlin, Laurent and Courville, Aaron},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02248},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Huang et al. - 2017 - Learnable Explicit Density for Continuous Latent S.pdf:/home/user/Zotero/storage/24RI2LUL/Huang et al. - 2017 - Learnable Explicit Density for Continuous Latent S.pdf:application/pdf}
}

@article{oliva-transformation-2018,
	title = {Transformation {Autoregressive} {Networks}},
	url = {http://arxiv.org/abs/1801.09819},
	abstract = {The fundamental task of general density estimation p(x) has been of keen interest to machine learning. In this work, we attempt to systematically characterize methods for density estimation. Broadly speaking, most of the existing methods can be categorized into either using: a) autoregressive models to estimate the conditional factors of the chain rule, p(xi xi−1, . . .); or b) non-linear transformations of variables of a simple base distribution. Based on the study of the characteristics of these categories, we propose multiple novel methods for each category. For example we propose RNN based transformations to model non-Markovian dependencies. Further, through a comprehensive study over both real world and synthetic data, we show that jointly leveraging transformations of variables and autoregressive conditional models, results in a considerable improvement in performance. We illustrate the use of our models in outlier detection and image modeling. Finally we introduce a novel data driven framework for learning a family of distributions.},
	language = {en},
	urldate = {2018-10-05},
	journal = {arXiv:1801.09819 [stat]},
	author = {Oliva, Junier B. and Dubey, Avinava and Zaheer, Manzil and Póczos, Barnabás and Salakhutdinov, Ruslan and Xing, Eric P. and Schneider, Jeff},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.09819},
	keywords = {Statistics - Machine Learning},
	file = {Oliva et al. - 2018 - Transformation Autoregressive Networks.pdf:/home/user/Zotero/storage/SQWETVDW/Oliva et al. - 2018 - Transformation Autoregressive Networks.pdf:application/pdf}
}

@article{louizos-multiplicative-2017,
	title = {Multiplicative {Normalizing} {Flows} for {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.01961},
	abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efﬁcient and straightforward to improve the approximation by employing normalizing ﬂows (Rezende \& Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maaløe et al., 2016). In experiments we show that with this new approximation we can signiﬁcantly improve upon classical mean ﬁeld for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
	language = {en},
	urldate = {2018-10-06},
	journal = {arXiv:1703.01961 [cs, stat]},
	author = {Louizos, Christos and Welling, Max},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01961},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Louizos and Welling - 2017 - Multiplicative Normalizing Flows for Variational B.pdf:/home/user/Zotero/storage/J2E9JZVC/Louizos and Welling - 2017 - Multiplicative Normalizing Flows for Variational B.pdf:application/pdf}
}

@article{still-information-2014,
	title = {Information {Bottleneck} {Approach} to {Predictive} {Inference}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/16/2/968},
	doi = {10.3390/e16020968},
	abstract = {This paper synthesizes a recent line of work on automated predictive model making inspired by Rate-Distortion theory, in particular by the Information Bottleneck method. Predictive inference is interpreted as a strategy for efficient communication. The relationship to thermodynamic efficiency is discussed. The overall aim of this paper is to explain how this information theoretic approach provides an intuitive, overarching framework for predictive inference.},
	language = {en},
	number = {2},
	urldate = {2018-10-07},
	journal = {Entropy},
	author = {Still, Susanne},
	month = feb,
	year = {2014},
	keywords = {computing engines, dynamical systems, far-from-equilibrium thermodynamics, information bottleneck method, predictive inference, thermodynamic efficiency},
	pages = {968--989},
	file = {Full Text PDF:/home/user/Zotero/storage/LZQEKWDK/Still and Still - 2014 - Information Bottleneck Approach to Predictive Infe.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RN22XHXZ/968.html:text/html}
}

@article{creutzig-past-future-2009,
	title = {Past-future information bottleneck in dynamical systems},
	volume = {79},
	issn = {1539-3755, 1550-2376},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.79.041925},
	doi = {10.1103/PhysRevE.79.041925},
	language = {en},
	number = {4},
	urldate = {2018-10-07},
	journal = {Physical Review E},
	author = {Creutzig, Felix and Globerson, Amir and Tishby, Naftali},
	month = apr,
	year = {2009},
	file = {Creutzig et al. - 2009 - Past-future information bottleneck in dynamical sy.pdf:/home/user/Zotero/storage/E5PGNLBC/Creutzig et al. - 2009 - Past-future information bottleneck in dynamical sy.pdf:application/pdf}
}

@article{alemi-deep-2016,
	title = {Deep {Variational} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/1612.00410},
	abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
	urldate = {2018-10-07},
	journal = {arXiv:1612.00410 [cs, math]},
	author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.00410},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning},
	file = {arXiv\:1612.00410 PDF:/home/user/Zotero/storage/ZVFUKLAN/Alemi et al. - 2016 - Deep Variational Information Bottleneck.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/FET76F4D/1612.html:text/html}
}

@article{parker-symmetry-2010,
	title = {Symmetry {Breaking} in {Soft} {Clustering} {Decoding} of {Neural} {Codes}},
	volume = {56},
	issn = {0018-9448},
	doi = {10.1109/TIT.2009.2037045},
	abstract = {Information-based distortion methods have been used successfully in the analysis of neural coding problems. These approaches allow the discovery of neural symbols and the corresponding stimulus space of a neuron or neural ensemble quantitatively, while making few assumptions about the nature of either the code or of relevant stimulus features. The neural codebook is derived by quantizing sensory stimuli and neural responses into a small set of clusters, and optimizing the quantization to minimize an information distortion function. The method of annealing has been used to solve the corresponding high-dimensional nonlinear optimization problem. The annealing solutions undergo a series of bifurcations, which we study using bifurcation theory in the presence of symmetries. In this contribution we describe these symmetry breaking bifurcations in detail, and indicate some of the consequences of the form of the bifurcations. In particular, we show that the annealing solutions break symmetry at pitchfork bifurcations, and that subcritical branches can exist. Thus, at a subcritical bifurcation, there are local information distortion solutions which are not found by the method of annealing. Since the annealing procedure is guaranteed to converge to a local solution eventually, the subcritical branch must turn and become optimal at some later saddle-node bifurcation, which we have shown occur generically for this class of problems. This implies that the rate distortion curve, while convex for noninformation-based distortion measures, is not convex for information-based distortion methods.},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Parker, A. E. and Dimitrov, A. G. and Gedeon, T.},
	month = feb,
	year = {2010},
	keywords = {neural coding, clustering, Neurons, neurophysiology, annealing, Annealing, bifurcation, Bifurcation, bifurcations, decoding, Decoding, Distortion measurement, encoding, Information analysis, information distortion, information distortion function, neural codes, Nonlinear distortion, nonlinear optimization, Optimization methods, quantization, Quantization, rate distortion curve, Rate-distortion, sensory stimuli, soft clustering decoding, symmetry breaking},
	pages = {901--927},
	file = {IEEE Xplore Abstract Record:/home/user/Zotero/storage/VQQIQXUP/5420292.html:text/html;IEEE Xplore Full Text PDF:/home/user/Zotero/storage/X6DMT8GY/Parker et al. - 2010 - Symmetry Breaking in Soft Clustering Decoding of N.pdf:application/pdf}
}

@article{rainforth-tighter-2018,
	title = {Tighter {Variational} {Bounds} are {Not} {Necessarily} {Better}},
	url = {http://arxiv.org/abs/1802.04537},
	abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
	language = {en},
	urldate = {2018-10-07},
	journal = {arXiv:1802.04537 [cs, stat]},
	author = {Rainforth, Tom and Kosiorek, Adam R. and Le, Tuan Anh and Maddison, Chris J. and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04537},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Rainforth et al. - 2018 - Tighter Variational Bounds are Not Necessarily Bet.pdf:/home/user/Zotero/storage/INWZVJ9C/Rainforth et al. - 2018 - Tighter Variational Bounds are Not Necessarily Bet.pdf:application/pdf}
}

@article{burda-importance-2015-4,
	title = {Importance {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	abstract = {The variational autoencoder (VAE; Kingma \& Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simpliﬁed representations which fail to use the network’s entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased ﬂexibility to model complex posteriors which do not ﬁt the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
	language = {en},
	urldate = {2018-10-07},
	journal = {arXiv:1509.00519 [cs, stat]},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.00519},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Burda et al. - 2015 - Importance Weighted Autoencoders.pdf:/home/user/Zotero/storage/AH9HU33Y/Burda et al. - 2015 - Importance Weighted Autoencoders.pdf:application/pdf}
}

@article{saxe-information-2018,
	title = {On the {Information} {Bottleneck} {Theory} of {Deep} {Learning}},
	url = {https://openreview.net/forum?id=ry\_WPG-A-},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB)...},
	urldate = {2018-10-07},
	author = {Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/DIM8U68U/Saxe et al. - 2018 - On the Information Bottleneck Theory of Deep Learn.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/BFYHPTRR/forum.html:text/html}
}

@inproceedings{tishby-information-1999,
	title = {The {Information} {Bottleneck} {Method}},
	abstract = {We deﬁne the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of ﬁnding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a ‘bottleneck’ formed by a limited set of codewords X˜ . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x˜) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X → X˜ and X˜ → Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	language = {en},
	booktitle = {Allerton {Conference} on {Communication}, {Control}, and {Computing}},
	author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
	year = {1999},
	pages = {368--377},
	file = {Tishby et al. - The Information Bottleneck Method.pdf:/home/user/Zotero/storage/QPNN8G6X/Tishby et al. - The Information Bottleneck Method.pdf:application/pdf}
}

@article{strouse-deterministic-2017,
	title = {The deterministic information bottleneck},
	volume = {29},
	number = {6},
	journal = {Neural computation},
	author = {Strouse, DJ and Schwab, David J},
	year = {2017},
	pages = {1611--1630}
}

@article{kingma-improving-2016,
	title = {Improving {Variational} {Inference} with {Inverse} {Autoregressive} {Flow}},
	url = {http://arxiv.org/abs/1606.04934},
	abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
	urldate = {2018-10-08},
	journal = {arXiv:1606.04934 [cs, stat]},
	author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04934},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1606.04934 PDF:/home/user/Zotero/storage/C5T7JLFG/Kingma et al. - 2016 - Improving Variational Inference with Inverse Autor.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/J3XAF55K/1606.html:text/html}
}

@inproceedings{germain-made:-2015-1,
	title = {Made: {Masked} autoencoder for distribution estimation},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	year = {2015},
	pages = {881--889}
}

@article{huang-neural-2018,
	title = {Neural {Autoregressive} {Flows}},
	journal = {arXiv preprint arXiv:1804.00779},
	author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
	year = {2018}
}

@article{chen-stability-nodate,
	title = {Stability and {Convergence} {Trade}-oﬀ of {Iterative} {Optimization} {Algorithms}},
	abstract = {The overall performance or expected excess risk of an iterative machine learning algorithm can be decomposed into training error and generalization error. While the former is controlled by its convergence analysis, the latter can be tightly handled by algorithmic stability (Bousquet and Elisseeﬀ, 2002). The machine learning community has a rich history investigating convergence and stability separately. However, the question about the trade-oﬀ between these two quantities remains open.},
	language = {en},
	author = {Chen, Yuansi and Jin, Chi and Yu, Bin},
	pages = {45},
	file = {Chen et al. - Stability and Convergence Trade-oﬀ of Iterative Op.pdf:/home/user/Zotero/storage/D3KX3QAI/Chen et al. - Stability and Convergence Trade-oﬀ of Iterative Op.pdf:application/pdf}
}

@misc{grover-cs-nodate,
	title = {{CS} 236: {Deep} {Generative} {Models}},
	shorttitle = {{CS} 236},
	url = {https://deepgenerativemodels.github.io/},
	abstract = {Fall 2018-2019},
	language = {en-US},
	urldate = {2018-10-09},
	journal = {CS 236: Deep Generative Models},
	author = {Grover, Aditya},
	file = {Snapshot:/home/user/Zotero/storage/P7P9QFCC/deepgenerativemodels.github.io.html:text/html}
}

@misc{grover-ijcai-ecai-nodate,
	title = {{IJCAI}-{ECAI} 2018 {Tutorial} on {Deep} {Generative} {Models}},
	url = {https://ermongroup.github.io/generative-models/},
	abstract = {Aditya Grover and Stefano Ermon},
	language = {en-US},
	urldate = {2018-10-09},
	journal = {Deep Generative Models},
	author = {Grover, Aditya},
	file = {Snapshot:/home/user/Zotero/storage/SKSXY8U3/generative-models.html:text/html}
}

@misc{noauthor-cvpr-nodate,
	title = {{CVPR} 2018 {Tutorial} on {GANs}},
	url = {https://sites.google.com/view/cvpr2018tutorialongans/},
	language = {en-GB},
	urldate = {2018-10-09},
	file = {Snapshot:/home/user/Zotero/storage/4PTKB9BA/cvpr2018tutorialongans.html:text/html}
}

@article{ebeling-entropy-1994,
	title = {Entropy and {Long}-{Range} {Correlations} in {Literary} {English}},
	volume = {26},
	issn = {0295-5075, 1286-4854},
	url = {http://stacks.iop.org/0295-5075/26/i=4/a=001?key=crossref.4a8da3b3f5e80b828e6995b6bfc3e5be},
	doi = {10.1209/0295-5075/26/4/001},
	abstract = {We investigated long range correlations in two literary texts, Moby Dick by H. Melville and Grimm’s tales. The analysis is based on the calculation of entropy like quantities as the mutual information for pairs of letters and the entropy, the mean uncertainty, per letter. We further estimate the number of diﬀerent subwords of a given length n. Filtering out the contributions due to the eﬀects of the ﬁnite length of the texts, we ﬁnd correlations ranging to a few hundred letters. Scaling laws for the mutual information (decay with a power law), for the entropy per letter (decay with the inverse square root of n) and for the word numbers (stretched exponential growth with n and with a power law of the text length) were found.},
	language = {en},
	number = {4},
	urldate = {2018-10-09},
	journal = {Europhysics Letters (EPL)},
	author = {Ebeling, W and Pöschel, T},
	month = may,
	year = {1994},
	pages = {241--246},
	file = {Ebeling and Pöschel - 1994 - Entropy and Long-Range Correlations in Literary En.pdf:/home/user/Zotero/storage/C7G2A6PS/Ebeling and Pöschel - 1994 - Entropy and Long-Range Correlations in Literary En.pdf:application/pdf}
}

@article{kolchinsky-nonlinear-2017,
	title = {Nonlinear {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/1705.02436},
	abstract = {Information bottleneck [IB] is a technique for extracting information in some ‘input’ random variable that is relevant for predicting some different ‘output’ random variable. IB works by encoding the input in a compressed ‘bottleneck variable’ from which the output can then be accurately decoded. IB can be difﬁcult to compute in practice, and has been mainly developed for two limited cases: (1) discrete random variables with small state spaces, and (2) continuous random variables that are jointly Gaussian distributed (in which case the encoding and decoding maps are linear). We propose a method to perform IB in more general domains. Our approach can be applied to discrete or continuous inputs and outputs, and allows for nonlinear encoding and decoding maps. The method uses a novel upper bound on the IB objective, derived using a non-parametric estimator of mutual information and a variational approximation. We show how to implement the method using neural networks and gradient-based optimization, and demonstrate its performance on the MNIST dataset.},
	language = {en},
	urldate = {2018-10-16},
	journal = {arXiv:1705.02436 [cs, math, stat]},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Wolpert, David H.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02436},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Machine Learning},
	file = {Kolchinsky et al. - 2017 - Nonlinear Information Bottleneck.pdf:/home/user/Zotero/storage/5M6SSJC4/Kolchinsky et al. - 2017 - Nonlinear Information Bottleneck.pdf:application/pdf}
}

@article{chalk-relevant-2016,
	title = {Relevant sparse codes with variational information bottleneck},
	url = {http://arxiv.org/abs/1605.07332},
	abstract = {In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y.},
	urldate = {2018-10-16},
	journal = {arXiv:1605.07332 [stat]},
	author = {Chalk, Matthew and Marre, Olivier and Tkacik, Gasper},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07332},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1605.07332 PDF:/home/user/Zotero/storage/IBHK8FXT/Chalk et al. - 2016 - Relevant sparse codes with variational information.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/TTTWHUER/1605.html:text/html}
}

@inproceedings{singh-learning-2003,
	title = {Learning predictive state representations},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Machine} {Learning} ({ICML}-03)},
	author = {Singh, Satinder P and Littman, Michael L and Jong, Nicholas K and Pardoe, David and Stone, Peter},
	year = {2003},
	pages = {712--719}
}

@inproceedings{singh-predictive-2004,
	title = {Predictive state representations: {A} new theory for modeling dynamical systems},
	booktitle = {Proceedings of the 20th conference on {Uncertainty} in artificial intelligence},
	publisher = {AUAI Press},
	author = {Singh, Satinder and James, Michael R and Rudary, Matthew R},
	year = {2004},
	pages = {512--519}
}

@article{creutzig-predictive-2008,
	title = {Predictive coding and the slowness principle: {An} information-theoretic approach},
	volume = {20},
	number = {4},
	journal = {Neural Computation},
	author = {Creutzig, Felix and Sprekeler, Henning},
	year = {2008},
	pages = {1026--1041}
}

@inproceedings{goodkind-predictive-2018,
	title = {Predictive power of word surprisal for reading times is a linear function of language model quality},
	booktitle = {Proceedings of the 8th {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics} ({CMCL} 2018)},
	author = {Goodkind, Adam and Bicknell, Klinton},
	year = {2018},
	pages = {10--18}
}

@inproceedings{van-schijndel-modeling-2018,
	title = {Modeling garden path effects without explicit hierarchical syntax},
	booktitle = {Proceedings of the 40th {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	author = {van Schijndel, Marten and Linzen, Tal},
	year = {2018},
	pages = {2600--2605}
}

@book{zipf-human-1949,
	address = {Cambridge, Mass.},
	title = {Human behavior and the principle of least effort: {An} introduction to human ecology},
	publisher = {Addison-Wesley Press},
	author = {Zipf, George Kingsley},
	year = {1949}
}

@inproceedings{white-minimal-2012,
	title = {Minimal dependency length in realization ranking},
	booktitle = {Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning},
	publisher = {Association for Computational Linguistics},
	author = {White, Michael and Rajkumar, Rajakrishnan},
	year = {2012},
	pages = {244--255}
}

@article{bartek-search-2011,
	title = {In search of on-line locality effects in sentence comprehension.},
	volume = {37},
	number = {5},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Bartek, Brian and Lewis, Richard L and Vasishth, Shravan and Smith, Mason R},
	year = {2011},
	pages = {1178}
}

@article{chi-statistical-1999,
	title = {Statistical properties of probabilistic context-free grammars},
	volume = {25},
	number = {1},
	journal = {Computational Linguistics},
	author = {Chi, Zhiyi},
	year = {1999},
	pages = {131--160}
}

@article{earley-efficient-1970,
	title = {An efficient context-free parsing algorithm},
	volume = {13},
	number = {2},
	journal = {Communications of the ACM},
	author = {Earley, Jay},
	year = {1970},
	pages = {94--102}
}

@article{nguyen-bktreebank:-2017,
	title = {{BKTreebank}: {Building} a {Vietnamese} {Dependency} {Treebank}},
	abstract = {Dependency treebank is an important resource in any language. In this paper, we present our work on building BKTreebank, a dependency treebank for Vietnamese. Important points on designing POS tagset, dependency relations, and annotation guidelines are discussed. We describe experiments on POS tagging and dependency parsing on the treebank. Experimental results show that the treebank is a useful resource for Vietnamese language processing.},
	language = {en},
	author = {Nguyen, Kiem-Hieu},
	year = {2017},
	pages = {5},
	file = {Nguyen - BKTreebank Building a Vietnamese Dependency Treeb.pdf:/home/user/Zotero/storage/Z4RRJ2SC/Nguyen - BKTreebank Building a Vietnamese Dependency Treeb.pdf:application/pdf}
}

@book{xue-chinese-2013,
	address = {Philadelphia},
	title = {Chinese  {Treebank} 8.0 {LDC}2013T2},
	author = {Xue, Nianwen},
	year = {2013}
}

@article{marcus-building-1993,
	title = {Building a large annotated corpus of {English}: {The} {Penn} {Treebank}},
	volume = {19},
	number = {2},
	journal = {Computational linguistics},
	author = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
	year = {1993},
	pages = {313--330}
}

@incollection{miller-finitary-1963,
	title = {Finitary models of language users},
	booktitle = {Handbook of {Mathematical} {Psychology}},
	author = {Miller, George A and Chomsky, Noam},
	year = {1963}
}

@book{berwick-grammatical-1986,
	title = {The grammatical basis of linguistic performance: {Language} use and acquisition},
	publisher = {MIT press},
	author = {Berwick, Robert C and Weinberg, Amy S},
	year = {1986}
}

@article{crutchfield-inferring-1989-1,
	title = {Inferring statistical complexity},
	volume = {63},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.63.105},
	doi = {10.1103/PhysRevLett.63.105},
	number = {2},
	journal = {Phys. Rev. Lett.},
	author = {Crutchfield, James P. and Young, Karl},
	month = jul,
	year = {1989},
	pages = {105--108}
}

@article{debowski-excess-2011,
	title = {Excess entropy in natural language: {Present} state and perspectives},
	volume = {21},
	number = {3},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Debowski, \Lukasz},
	year = {2011},
	pages = {037105}
}

@book{von-der-gabelentz-sprachwissenschaft.-1891,
	address = {Leipzig},
	title = {Die {Sprachwissenschaft}. {Ihre} {Aufgaben}, {Methoden} und bisherigen {Ergebnisse}},
	author = {von der Gabelentz, Georg},
	year = {1891}
}

@article{horn-toward-1984,
	title = {Toward a new taxonomy for pragmatic inference: {Q}-based and {R}-based implicature},
	volume = {11},
	journal = {Meaning, form, and use in context: Linguistic applications},
	author = {Horn, Laurence},
	year = {1984},
	pages = {42}
}

@article{xie2017data,
  title={Data noising as smoothing in neural network language models},
  author={Xie, Ziang and Wang, Sida I and Li, Jiwei and L{\'e}vy, Daniel and Nie, Aiming and Jurafsky, Dan and Ng, Andrew Y},
  journal={arXiv preprint arXiv:1703.02573},
  year={2017}
}


@incollection{greenberg1963universals,
  address =       {Cambridge, MA},
  author =        {Greenberg, Joseph H.},
  booktitle =     {Universals of Language},
  editor =        {Joseph H. Greenberg},
  pages =         {73--113},
  publisher =     {MIT Press},
  title =         {Some universals of grammar with particular reference
                   to the order of meaningful elements},
  year =          {1963},
}

@book{chomsky1993lectures,
  author =        {Chomsky, Noam},
  number =        {9},
  publisher =     {Walter de Gruyter},
  title =         {Lectures on government and binding: The Pisa
                   lectures},
  year =          {1993},
}

@article{hawkins2007processing,
  author =        {Hawkins, John A},
  journal =       {New Ideas in Psychology},
  number =        {2},
  pages =         {87--107},
  publisher =     {Elsevier},
  title =         {Processing typology and why psychologists need to
                   know about it},
  volume =        {25},
  year =          {2007},
}

@inproceedings{bender2009linguistically,
  author =        {Bender, Emily M.},
  booktitle =     {Proceedings of the EACL 2009 Workshop on the
                   Interaction between Linguistics and Computational
                   Linguistics: Virtuous, Vicious or Vacuous?},
  organization =  {Association for Computational Linguistics},
  pages =         {26--32},
  title =         {Linguistically na{\"\i}ve!= language independent: Why
                   NLP needs linguistic typology},
  year =          {2009},
}

@book{bender2013linguistic,
  author =        {Bender, Emily M.},
  publisher =     {Morgan \& Claypool Publishers},
  series =        {Synthesis Lectures on Human Language Technologies},
  title =         {Linguistic fundamentals for natural language
                   processing: 100 essentials from morphology and
                   syntax},
  volume =        {6},
  year =          {2013},
}

@article{chomsky2005three,
  author =        {Noam Chomsky},
  journal =       {Linguistic Inquiry},
  number =        {1},
  pages =         {1--61},
  title =         {Three factors in language design},
  volume =        {36},
  year =          {2005},
}

@article{hauser2002faculty,
  author =        {Hauser, M.D. and Chomsky, N. and Fitch, W.},
  journal =       {Science},
  number =        {5598},
  pages =         {1569},
  publisher =     {American Association for the Advancement of Science},
  title =         {The faculty of language: What is it, who has it, and
                   how did it evolve?},
  volume =        {298},
  year =          {2002},
}

@book{berwick1984grammatical,
  author =        {Berwick, Robert C and Weinberg, Amy},
  publisher =     {Cambridge, MA: MIT Press},
  title =         {The grammatical basis of linguistic performance},
  year =          {1984},
}

@book{hawkins1994performance,
  address =       {Cambridge},
  author =        {Hawkins, John A.},
  publisher =     {Cambridge University Press},
  title =         {A performance theory of order and constituency},
  year =          {1994},
}

@book{zipf1949human,
  address =       {Oxford, UK},
  author =        {Zipf, George Kingsley},
  publisher =     {{A}ddison-{W}esley Press},
  title =         {Human behavior and the principle of least effort},
  year =          {1949},
}

@book{Croft:Cruse:2004,
  address =       {Cambridge, UK},
  author =        {William Croft and Alan Cruse},
  publisher =     {Cambridge University Press},
  title =         {Cognitive Linguistics},
  year =          {2004},
}

@book{Goldberg:2005,
  address =       {Oxford, UK},
  author =        {Adele Goldberg},
  publisher =     {Oxford University Press},
  title =         {Constructions at work: The nature of generalization
                   in language},
  year =          {2005},
}

@article{pinker1990natural,
  author =        {Steven Pinker and Paul Bloom},
  journal =       {{Behavioral and brain sciences}},
  pages =         {707--784},
  title =         {{Natural language and natural selection}},
  volume =        {13},
  year =          {1990},
}

@book{wals,
  address =       {Leipzig},
  author =        {Matthew S. Dryer and Martin Haspelmath},
  publisher =     {Max Planck Institute for Evolutionary Anthropology},
  title =         {WALS Online},
  year =          {2013},
  url =           {https://wals.info/},
}

@article{dryer1992greenbergian,
  author =        {Dryer, Matthew S},
  journal =       {Language},
  number =        {1},
  pages =         {81--138},
  publisher =     {Linguistic Society of America},
  title =         {The {Greenbergian} word order correlations},
  volume =        {68},
  year =          {1992},
}

@article{lehmann1973structural,
  author =        {W. P. Lehmann},
  journal =       {Language},
  pages =         {47-66},
  title =         {A structural principle of language and its
                   implications},
  volume =        {49},
  year =          {1973},
}

@article{vennemann1974theoretical,
  author =        {Vennemann, Theo},
  journal =       {Papiere zur Linguistik},
  pages =         {5--25},
  title =         {Theoretical word order studies: {R}esults and
                   problems},
  volume =        {7},
  year =          {1974},
}

@book{jackendoff1977x,
  author =        {Jackendoff, Ray},
  publisher =     {MIT Press},
  title =         {X-bar syntax: A study in phrase structure},
  year =          {1977},
}

@article{frazier1985syntactic,
  author =        {Frazier, Lyn},
  journal =       {Natural language parsing: {P}sychological,
                   computational, and theoretical perspectives},
  pages =         {129--189},
  publisher =     {Cambridge University Press New York},
  title =         {Syntactic complexity},
  year =          {1985},
}

@book{chomsky1988language,
  address =       {Cambridge, MA},
  author =        {Noam Chomsky},
  publisher =     {MIT Press},
  title =         {Language and Problems of Knowledge: The Managua
                   Lectures},
  year =          {1988},
}

@book{gabelentz1901sprachwissenschaft,
  address =       {Leipzig},
  author =        {Gabelentz, Georg von der},
  publisher =     {Weigel},
  title =         {Die Sprachwissenschaft, ihre Aufgaben, Methoden, und
                   bisherigen Ergebnisse},
  year =          {1901},
}

@article{hockett1960origin,
  author =        {Hockett, Charles F.},
  journal =       {Scientific American},
  number =        {3},
  pages =         {88--96},
  title =         {The origin of language},
  volume =        {203},
  year =          {1960},
}

@article{givon1991markedness,
  author =        {Talmy Giv\'on},
  journal =       {Stud Lang},
  pages =         {335–370},
  title =         {Markedness in grammar: distributional, communicative
                   and cognitive correlates of syntactic structure},
  volume =        {15},
  year =          {1991},
}

@book{hawkins2004efficiency,
  address =       {Oxford},
  author =        {Hawkins, John A.},
  publisher =     {Oxford University Press},
  title =         {Efficiency and complexity in grammars},
  year =          {2004},
}

@book{hawkins2014crosslinguistic,
  address =       {Oxford},
  author =        {Hawkins, John A.},
  publisher =     {Oxford University Press},
  title =         {Cross-linguistic variation and efficiency},
  year =          {2014},
}

@incollection{croft2001functional,
  address =       {Oxford},
  author =        {William A. Croft},
  booktitle =     {International Encyclopedia of the Social and
                   Behavioral Sciences},
  editor =        {Neil J. Smelser and Paul B. Baltes},
  pages =         {6323--6330},
  publisher =     {Elsevier Sciences},
  title =         {Functional approaches to grammar},
  year =          {2001},
}

@incollection{haspelmath2008parametric,
  address =       {Amsterdam},
  author =        {Martin Haspelmath},
  booktitle =     {The Limits of Syntactic Variation},
  editor =        {T. Biberauer},
  pages =         {75--107},
  publisher =     {John Benjamins},
  title =         {Parametric versus functional explanations of
                   syntactic universals},
  year =          {2008},
}

@article{jaeger2011language,
  author =        {T. Florian Jaeger and Harry J. Tily},
  journal =       {Wiley Interdisciplinary Reviews: Cognitive Science},
  number =        {3},
  pages =         {323--335},
  publisher =     {Wiley Online Library},
  title =         {On language `utility': {P}rocessing complexity and
                   communicative efficiency},
  volume =        {2},
  year =          {2011},
}

@article{horn1984toward,
  author =        {Horn, Laurence},
  journal =       {Meaning, form, and use in context: Linguistic
                   applications},
  pages =         {42},
  title =         {Toward a new taxonomy for pragmatic inference:
                   Q-based and R-based implicature},
  volume =        {11},
  year =          {1984},
}

@article{schwartz1997dispersion,
  author =        {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and
                   Vall{\'e}e, Nathalie and Abry, Christian},
  journal =       {Journal of phonetics},
  number =        {3},
  pages =         {255--286},
  publisher =     {Elsevier},
  title =         {The dispersion-focalization theory of vowel systems},
  volume =        {25},
  year =          {1997},
}

@article{ferreri2003least,
  author =        {Ferrer i Cancho, Ramon and Sol{\'e}, Ricard V},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {3},
  pages =         {788--791},
  publisher =     {National Acad Sciences},
  title =         {Least effort and the origins of scaling in human
                   language},
  volume =        {100},
  year =          {2003},
}

@article{frank2012predicting,
  author =        {Frank, Michael C and Goodman, Noah D},
  journal =       {Science},
  number =        {6084},
  pages =         {998--998},
  publisher =     {American Association for the Advancement of Science},
  title =         {Predicting pragmatic reasoning in language games},
  volume =        {336},
  year =          {2012},
}

@article{zaslavsky2018efficient,
  author =        {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and
                   Tishby, Naftali},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {31},
  pages =         {7937--7942},
  publisher =     {National Acad Sciences},
  title =         {Efficient compression in color naming and its
                   evolution},
  volume =        {115},
  year =          {2018},
}

@article{kemp2012kinship,
  author =        {Kemp, Charles and Regier, Terry},
  journal =       {Science},
  number =        {6084},
  pages =         {1049--1054},
  publisher =     {American Association for the Advancement of Science},
  title =         {Kinship categories across languages reflect general
                   communicative principles},
  volume =        {336},
  year =          {2012},
}

@book{fodor1975language,
  address =       {Cambridge, MA},
  author =        {Jerry A. Fodor},
  publisher =     {Harvard University Press},
  title =         {{The language of thought}},
  year =          {1975},
  isbn =          {0674510305},
}

@book{jackendoff1983semantics,
  author =        {Jackendoff, Ray},
  publisher =     {MIT press},
  title =         {Semantics and cognition},
  volume =        {8},
  year =          {1983},
}

@article{hays1964dependency,
  author =        {Hays, David G.},
  journal =       {Language},
  pages =         {511--525},
  publisher =     {Linguistic Society of America},
  title =         {Dependency theory: A formalism and some observations},
  volume =        {40},
  year =          {1964},
}

@book{melcuk1988dependency,
  author =        {Mel{\v{c}}uk, Igor Aleksandrovi{\v{c}}},
  publisher =     {SUNY Press},
  title =         {Dependency syntax: {T}heory and practice},
  year =          {1988},
}

@book{corbett1993heads,
  address =       {Cambridge},
  author =        {Greville G. Corbett and N. M. Fraser and
                   S. McGlashan},
  publisher =     {Cambridge University Press},
  title =         {Heads in Grammatical Theory},
  year =          {1993},
}

@book{tesniere2015elements,
  author =        {Tesni{\`e}re, Lucien and Kahane, Sylvain},
  publisher =     {John Benjamins Publishing Company New York},
  title =         {Elements of structural syntax},
  year =          {2015},
}

@misc{nivre2017universal,
  author =        {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and
                   Ahrenberg, Lars and Antonsen, Lene and
                   Aranzabe, Maria Jesus and Asahara, Masayuki and
                   Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and
                   Badmaeva, Elena and Ballesteros, Miguel and
                   Banerjee, Esha and Bank, Sebastian and Bauer, John and
                   Bengoetxea, Kepa and Bhat, Riyaz Ahmad and
                   Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and
                   Bowman, Sam and Burchardt, Aljoscha and
                   Candito, Marie and Caron, Gauthier and
                   Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and
                   Celano, Giuseppe G. A. and Cetin, Savas and
                   Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and
                   Cinkov{\'a}, Silvie and
                   {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and
                   Connor, Miriam and de Marneffe, Marie-Catherine and
                   de Paiva, Valeria and Diaz de Ilarraza, Arantza and
                   Dobrovoljc, Kaja and Dozat, Timothy and
                   Droganova, Kira and Eli, Marhaba and Elkahky, Ali and
                   Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and
                   Fernandez Alcalde, Hector and Foster, Jennifer and
                   Freitas, Cl{\'a}udia and
                   Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and
                   Galbraith, Daniel and Garcia, Marcos and
                   Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and
                   G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and
                   G{\'o}mez Guinovart, Xavier and
                   Gonz{\'a}les Saavedra, Berta and Grioni, Matias and
                   Gr{\= u}z{\={\i}}tis, Normunds and Guillaume, Bruno and
                   Habash, Nizar and Haji{\v c}, Jan and
                   Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and
                   Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and
                   Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hohle, Petter and
                   Ion, Radu and Irimia, Elena and Johannsen, Anders and
                   J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and
                   Kanayama, Hiroshi and Kanerva, Jenna and
                   Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and
                   Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and
                   Kwak, Sookyoung and Laippala, Veronika and
                   Lambertino, Lorenzo and Lando, Tatiana and
                   L{\^e} H{\`{\^o}}ng, Phương and Lenci, Alessandro and
                   Lertpradit, Saran and Leung, Herman and
                   Li, Cheuk Ying and Li, Josie and
                   Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and
                   Lyashevskaya, Olga and Lynn, Teresa and
                   Macketanz, Vivien and Makazhanov, Aibek and
                   Mandl, Michael and Manning, Christopher and
                   Manurung, Ruli and
                   M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and
                   Mare{\v c}ek, David and Marheinecke, Katrin and
                   Mart{\'{\i}}nez Alonso, H{\'e}ctor and
                   Martins, Andr{\'e} and Ma{\v s}ek, Jan and
                   Matsumoto, Yuji and {McDonald}, Ryan and
                   Mendon{\c c}a, Gustavo and Missil{\"a}, Anna and
                   Mititelu, Verginica and Miyao, Yusuke and
                   Montemagni, Simonetta and More, Amir and
                   Moreno Romero, Laura and Mori, Shunsuke and
                   Moskalevskyi, Bohdan and Muischnek, Kadri and
                   Mustafina, Nina and M{\"u}{\"u}risep, Kaili and
                   Nainwani, Pinkey and Nedoluzhko, Anna and
                   Nguy{\~{\^e}}n Th{\d i}, Lương and
                   Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and
                   Nikolaev, Vitaly and Nitisaroj, Rattima and
                   Nurmi, Hanna and Ojala, Stina and Osenova, Petya and
                   {\O}vrelid, Lilja and Pascual, Elena and
                   Passarotti, Marco and Perez, Cenel-Augusto and
                   Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and
                   Pitler, Emily and Plank, Barbara and Popel, Martin and
                   Pretkalni{\c n}a, Lauma and Prokopidis, Prokopis and
                   Puolakainen, Tiina and Pyysalo, Sampo and
                   Rademaker, Alexandre and Real, Livy and Reddy, Siva and
                   Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and
                   Rosa, Rudolf and Rovati, Davide and Saleh, Shadi and
                   Sanguinetti, Manuela and Saul{\={\i}}te, Baiba and
                   Sawanakunanon, Yanin and Schuster, Sebastian and
                   Seddah, Djam{\'e} and Seeker, Wolfgang and
                   Seraji, Mojgan and Shakurova, Lena and Shen, Mo and
                   Shimada, Atsuko and Shohibussirri, Muh and
                   Silveira, Natalia and Simi, Maria and
                   Simionescu, Radu and Simk{\'o}, Katalin and
                   {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and
                   Smith, Aaron and Stella, Antonio and
                   Strnadov{\'a}, Jana and Suhr, Alane and
                   Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and
                   Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and
                   Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and
                   Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and
                   Uria, Larraitz and Uszkoreit, Hans and
                   van Noord, Gertjan and Varga, Viktor and
                   Vincze, Veronika and Washington, Jonathan North and
                   Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and
                   Zeman, Daniel and Zhu, Hanzhi},
  note =          {{LINDAT}/{CLARIN} digital library at the Institute of
                   Formal and Applied Linguistics ({{\'U}FAL}), Faculty
                   of Mathematics and Physics, Charles University},
  title =         {Universal Dependencies 2.0 – {CoNLL} 2017 Shared
                   Task Development and Test Data},
  year =          {2017},
  url =           {http://hdl.handle.net/11234/1-2184},
}

@article{shannon1948mathematical,
  author =        {Claude E. Shannon},
  journal =       {{Bell System Technical Journal}},
  pages =         {623--656},
  title =         {{A Mathematical Theory of Communication}},
  volume =        {27},
  year =          {1948},
}

@article{ferrericancho2002zipf,
  author =        {{Ferrer i Cancho}, Ramon and Sol{\'e}, R.V.},
  journal =       {Advances in Complex Systems},
  number =        {1},
  pages =         {1--6},
  publisher =     {World Scientific Publishing Co.},
  title =         {Zipf's law and random texts},
  volume =        {5},
  year =          {2002},
}

@article{ferrericancho2007global,
  author =        {Ramon {Ferrer i Cancho} and Albert D{\'\i}az-Guilera},
  journal =       {Journal of Statistical Mechanics: Theory and
                   Experiment},
  number =        {06},
  pages =         {P06009},
  publisher =     {IOP Publishing},
  title =         {The global minima of the communicative energy of
                   natural communication systems},
  volume =        {2007},
  year =          {2007},
}

@phdthesis{futrell2017memory,
  address =       {Cambridge, MA},
  author =        {Richard Futrell},
  school =        {Massachusetts Institute of Technology},
  title =         {Memory and locality in natural language},
  year =          {2017},
}

@inproceedings{tishby1999information,
  author =        {Tishby, Naftali and Pereira, Fernando and
                   Bialek, William},
  booktitle =     {Invited paper to The 37th annual Allerton Conference
                   on Communication, Control, and Computing},
  title =         {The information bottleneck method},
  year =          {1999},
}

@incollection{regier2015word,
  address =       {Hoboken, NJ},
  author =        {Terry Regier and Charles Kemp and Paul Kay},
  booktitle =     {The Handbook of Language Emergence},
  pages =         {237--263},
  publisher =     {Wiley-Blackwell},
  title =         {Word meanings across languages support efficient
                   communication},
  year =          {2015},
}

@inproceedings{hale2001probabilistic,
  author =        {Hale, John T.},
  booktitle =     {Proceedings of the Second Meeting of the North
                   American Chapter of the Association for Computational
                   Linguistics and Language Technologies},
  pages =         {1--8},
  title =         {A probabilistic {Earley} parser as a psycholinguistic
                   model},
  year =          {2001},
}

@article{levy2008expectation,
  author =        {Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {1126--1177},
  publisher =     {Elsevier},
  title =         {Expectation-based syntactic comprehension},
  volume =        {106},
  year =          {2008},
}

@article{smith2013effect,
  author =        {Smith, Nathaniel J. and Levy, Roger},
  journal =       {Cognition},
  number =        {3},
  pages =         {302--319},
  publisher =     {Elsevier},
  title =         {The effect of word predictability on reading time is
                   logarithmic},
  volume =        {128},
  year =          {2013},
}

@article{friston2009predictive,
  author =        {Karl Friston and S. Kiebel},
  journal =       {Philosophical Transactions of the Royal Society B:
                   Biological Sciences},
  pages =         {1211--1221},
  title =         {Predictive coding under the free-energy principle},
  volume =        {364},
  year =          {2009},
}

@book{li2008introduction,
  address =       {New York},
  author =        {M. Li and P.M.B. Vit{\'a}nyi},
  publisher =     {Springer-Verlag},
  title =         {{An introduction to Kolmogorov complexity and its
                   applications}},
  year =          {2008},
  isbn =          {0387339981},
}

@article{frank2011insensitivity,
  author =        {Frank, Stefan L. and Bod, Rens},
  journal =       {Psychological Science},
  number =        {6},
  pages =         {829--834},
  publisher =     {Sage Publications},
  title =         {Insensitivity of the human sentence-processing system
                   to hierarchical structure},
  volume =        {22},
  year =          {2011},
}

@inproceedings{goodkind2018predictive,
  address =       {Salt Lake City, UT},
  author =        {Adam Goodkind and Klinton Bicknell},
  booktitle =     {Proceedings of the 8th Workshop on Cognitive Modeling
                   and Computational Linguistics (CMCL 2018)},
  pages =         {10--18},
  publisher =     {Association for Computational Linguistics},
  title =         {Predictive power of word surprisal for reading times
                   is a linear function of language model quality},
  year =          {2018},
}

@article{dozat2017stanford,
  author =        {Dozat, Timothy and Qi, Peng and
                   Manning, Christopher D},
  journal =       {Proceedings of the CoNLL 2017 Shared Task:
                   Multilingual Parsing from Raw Text to Universal
                   Dependencies},
  pages =         {20--30},
  title =         {Stanford's Graph-based Neural Dependency Parser at
                   the CoNLL 2017 Shared Task},
  year =          {2017},
}

@inproceedings{zhang2017dependency,
  address =       {Valencia, Spain},
  author =        {Xingxing Zhang and Jianpeng Cheng and Mirella Lapata},
  booktitle =     {Proceedings of the 15th Conference of the European
                   Chapter of the Association for Computational
                   Linguistics: Volume 1, Long Papers},
  pages =         {665--676},
  title =         {Dependency parsing as head selection},
  year =          {2017},
}

@article{futrell2015largescale,
  author =        {Richard Futrell and Kyle Mahowald and Edward Gibson},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {33},
  pages =         {10336--10341},
  title =         {Large-scale evidence of dependency length
                   minimization in 37 languages},
  volume =        {112},
  year =          {2015},
  doi =           {10.1073/pnas.1502134112},
  url =           {http://www.pnas.org/content/early/2015/07/28/
                  1502134112.abstract},
}

@article{liu2017dependency,
  author =        {Haitao Liu and Chunshan Xu and Junying Liang},
  journal =       {Physics of Life Reviews},
  title =         {Dependency distance: {A} new perspective on syntactic
                   patterns in natural languages},
  year =          {2017},
}

@article{temperley2018minimizing,
  author =        {David Temperley and Dan Gildea},
  journal =       {Annual Review of Linguistics},
  pages =         {1--15},
  title =         {Minimizing Syntactic Dependency Lengths:
                   {T}ypological/Cognitive Universal?},
  volume =        {4},
  year =          {2018},
}

@article{gibson1998linguistic,
  author =        {Gibson, Edward},
  journal =       {Cognition},
  number =        {1},
  pages =         {1--76},
  title =         {Linguistic complexity: Locality of syntactic
                   dependencies},
  volume =        {68},
  year =          {1998},
}

@inproceedings{gibson2000dependency,
  author =        {Gibson, E.},
  booktitle =     {Image, Language, Brain: Papers from the First Mind
                   Articulation Project Symposium},
  chapter =       {5},
  editor =        {Alec Marantz and Yasushi Miyashita and Wayne O'Neil},
  pages =         {95--126},
  title =         {The dependency locality theory: A distance-based
                   theory of linguistic complexity},
  year =          {2000},
}

@article{fedzechkina2012language,
  author =        {Fedzechkina, Maryia and Jaeger, T Florian and
                   Newport, Elissa L},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {44},
  pages =         {17897--17902},
  publisher =     {National Academy of Sciences},
  title =         {Language learners restructure their input to
                   facilitate efficient communication},
  volume =        {109},
  year =          {2012},
}

@article{culbertson2012learning,
  author =        {Culbertson, J. and Smolensky, Paul and Legendre, G.},
  journal =       {Cognition},
  pages =         {306--329},
  publisher =     {Elsevier},
  title =         {Learning biases predict a word order universal},
  volume =        {122},
  year =          {2012},
}

@incollection{chomsky2010some,
  address =       {Cambridge},
  author =        {Noam Chomsky},
  booktitle =     {The Evolution of Human Language},
  pages =         {45--62},
  publisher =     {Cambridge University Press},
  title =         {Some simple evo devo theses: How true might they be
                   for language?},
  year =          {2010},
}

@book{jespersen1922,
  address =       {New York},
  author =        {Otto. Jespersen},
  publisher =     {Henry {H}olt and {C}o.},
  title =         {{Language: Its nature, development, and origin.}},
  year =          {1922},
}

@inproceedings{mcfadden2003morphological,
  author =        {Thomas McFadden},
  booktitle =     {Proceedings of the Berkeley Linguistics Society},
  title =         {On morphological case and word-order freedom},
  year =          {2003},
}

@inproceedings{futrell2015quantifying,
  address =       {Uppsala, Sweden},
  author =        {Richard Futrell and Kyle Mahowald and Edward Gibson},
  booktitle =     {Proceedings of the Third International Conference on
                   Dependency Linguistics (Depling 2015)},
  pages =         {91--100},
  title =         {Quantifying Word Order Freedom in Dependency Corpora},
  year =          {2015},
}


@article{PhysRevLett.115.098701,
  title = {Space-Bounded Church-Turing Thesis and Computational Tractability of Closed Systems},
  author = {Braverman, Mark and Schneider, Jonathan and Rojas, Crist\'obal},
  journal = {Phys. Rev. Lett.},
  volume = {115},
  issue = {9},
  pages = {098701},
  numpages = {5},
  year = {2015},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.115.098701},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.115.098701}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{daniluk2017frustratingly,
  author    = {Michal Daniluk and
               Tim Rockt{\"{a}}schel and
               Johannes Welbl and
               Sebastian Riedel},
  title     = {Frustratingly Short Attention Spans in Neural Language Modeling},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=ByIAPUcee},
  timestamp = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/iclr/DanilukRW017},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

