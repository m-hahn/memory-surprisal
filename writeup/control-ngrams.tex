
%bounds.append(["alpha", float, 0.95, 1.0]) # + [x/20.0 for x in range(15, 21)])
%bounds.append(["gamma", int, 1, 2, 3, 4, 5, 8, 10, 15, 20, 25, 30]) # , 200, 300
%bounds.append(["delta", int, 0.1, 0.2, 0.5, 1.0 , 2.0, 3.0, 4.0, 5.0, 8.0, 10.0]) #, 1024]) #, 1024]) # 64, 128,
%bounds.append(["cutoff", int, 2,3,4,5,6,7,8,9,10]) #,7,8,9,10]) #, 1024]) #, 1024]) # 64, 128,
%

We use Interpolated Kneser-Ney.
Hyperparameters are tuned with the same strategy as for the neural network models.



\begin{table}
\begin{tabular}{ccccccccccccccclll}
\input{tables/medians_ngrams_0.tex}
\end{tabular}
	\caption{Medians (estimated using n-gram models): For each memory budget, we provide the median surprisal for real and random languages. Solid lines indicate sample medians for ngrams, dashed lines indicate 95 \% confidence intervals for the population median. Green: Random baselines; blue: real language; red: maximum-likelihood grammars fit to real orderings.}\label{tab:medians_ngrams}
\end{table}

\begin{table}
\begin{tabular}{ccccccccccccccclll}
\input{tables/medians_ngrams_1.tex}
\end{tabular}
	\caption{Medians (cont.)}
\end{table}

\begin{table}
\begin{tabular}{ccccccccccccccclll}
\input{tables/medians_ngrams_2.tex}
\end{tabular}
	\caption{Medians (cont.)}
\end{table}

\begin{table}
\begin{tabular}{ccccccccccccccclll}
\input{tables/medians_ngrams_3.tex}
\end{tabular}
	\caption{Medians (cont.)}
\end{table}


